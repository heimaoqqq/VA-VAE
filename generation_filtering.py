"""
ç”Ÿæˆå¹¶ç­›é€‰é«˜è´¨é‡æ ·æœ¬è„šæœ¬
åŸºäº generate_and_filter_samples.py æ”¹è¿›ï¼Œæ”¯æŒå¤šæŒ‡æ ‡ç­›é€‰
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist

# æŠ‘åˆ¶torch._dynamoé”™è¯¯ï¼Œå›é€€åˆ°eageræ¨¡å¼
try:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True
except ImportError:
    pass
from torch.nn.parallel import DistributedDataParallel as DDP
import sys
import os
import yaml
import numpy as np
from PIL import Image
from tqdm import tqdm
import argparse
from pathlib import Path
import torchvision.transforms as transforms
from scipy.spatial.distance import mahalanobis
from sklearn.metrics.pairwise import cosine_similarity
import json
from collections import defaultdict

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from models.lightningdit import LightningDiT_models
from transport import create_transport, Sampler
from simplified_vavae import SimplifiedVAVAE


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if 'RANK' in os.environ:
        rank = int(os.environ['RANK'])
        local_rank = int(os.environ['LOCAL_RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend='nccl')
        
        return rank, local_rank, world_size
    else:
        return 0, 0, 1


def load_weights_with_shape_check(model, checkpoint, rank=0):
    """ä½¿ç”¨å½¢çŠ¶æ£€æŸ¥åŠ è½½æƒé‡ï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹å®ç°ï¼‰"""
    model_state_dict = model.state_dict()
    # check shape and load weights
    for name, param in checkpoint['model'].items():
        if name in model_state_dict:
            if param.shape == model_state_dict[name].shape:
                model_state_dict[name].copy_(param)
            elif name == 'x_embedder.proj.weight':
                # special case for x_embedder.proj.weight
                weight = torch.zeros_like(model_state_dict[name])
                weight[:, :16] = param[:, :16]
                model_state_dict[name] = weight
            else:
                if rank == 0:
                    print(f"Skipping loading parameter '{name}' due to shape mismatch: "
                        f"checkpoint shape {param.shape}, model shape {model_state_dict[name].shape}")
        else:
            if rank == 0:
                print(f"Parameter '{name}' not found in model, skipping.")
    # load state dict
    model.load_state_dict(model_state_dict, strict=False)
    
    return model


def load_model_and_config(checkpoint_path, config_path, local_rank):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®ï¼ˆæŒ‰ç…§å®˜æ–¹æ–¹å¼ï¼‰"""
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device(f'cuda:{local_rank}')
    
    # åˆ›å»ºDiTæ¨¡å‹
    latent_size = config['data']['image_size'] // config['vae']['downsample_ratio']
    model = LightningDiT_models[config['model']['model_type']](
        input_size=latent_size,
        num_classes=config['data']['num_classes'],
        class_dropout_prob=config['model'].get('class_dropout_prob', 0.1),
        use_qknorm=config['model']['use_qknorm'],
        use_swiglu=config['model'].get('use_swiglu', False),
        use_rope=config['model'].get('use_rope', False),
        use_rmsnorm=config['model'].get('use_rmsnorm', False),
        wo_shift=config['model'].get('wo_shift', False),
        in_channels=config['model'].get('in_chans', 4),
        use_checkpoint=config['model'].get('use_checkpoint', False),
    ).to(device)
    
    # æŒ‰ç…§å®˜æ–¹æ–¹å¼åŠ è½½æƒé‡
    if os.path.exists(checkpoint_path):
        if local_rank == 0:
            print(f"ğŸ“¦ ä»checkpointåŠ è½½æƒé‡: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)
        
        # å¤„ç†æƒé‡é”®åï¼ˆæŒ‰ç…§å®˜æ–¹æ–¹å¼ï¼‰
        if 'ema' in checkpoint:
            checkpoint_weights = {'model': checkpoint['ema']}
            if local_rank == 0:
                print("ğŸ“¦ ä½¿ç”¨EMAæƒé‡è¿›è¡Œæ¨ç†")
        elif 'model' in checkpoint:
            checkpoint_weights = checkpoint
            if local_rank == 0:
                print("ğŸ“¦ ä½¿ç”¨æ¨¡å‹æƒé‡è¿›è¡Œæ¨ç†")
        else:
            checkpoint_weights = {'model': checkpoint}
            if local_rank == 0:
                print("ğŸ“¦ ä½¿ç”¨ç›´æ¥æƒé‡è¿›è¡Œæ¨ç†")
        
        # æ¸…ç†é”®å
        checkpoint_weights['model'] = {k.replace('module.', ''): v for k, v in checkpoint_weights['model'].items()}
        
        # ä½¿ç”¨å®˜æ–¹æƒé‡åŠ è½½å‡½æ•°
        model = load_weights_with_shape_check(model, checkpoint_weights, rank=local_rank)
        
        if local_rank == 0:
            print("âœ… æƒé‡åŠ è½½å®Œæˆ")
    else:
        if local_rank == 0:
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°checkpointæ–‡ä»¶ {checkpoint_path}")
            print("âš ï¸ ä½¿ç”¨æœªè®­ç»ƒçš„éšæœºæƒé‡ï¼Œç”Ÿæˆç»“æœå°†æ˜¯å™ªå£°ï¼")
    
    model.eval()
    
    # åˆ›å»ºVAEï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹train_dit_s_official.pyæ–¹å¼ï¼‰
    vae = None
    try:
        # æ·»åŠ LightningDiTè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
        import sys
        lightningdit_path = os.path.join(os.getcwd(), 'LightningDiT')
        if lightningdit_path not in sys.path:
            sys.path.insert(0, lightningdit_path)
        
        from tokenizer.vavae import VA_VAE
        import tempfile
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„VAEæ¨¡å‹è·¯å¾„
        custom_vae_checkpoint = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
        
        # åˆ›å»ºä¸train_dit_s_official.pyå®Œå…¨ä¸€è‡´çš„é…ç½®
        vae_config = {
            'ckpt_path': custom_vae_checkpoint,
            'model': {
                'base_learning_rate': 2.0e-05,
                'target': 'ldm.models.autoencoder.AutoencoderKL',
                'params': {
                    'monitor': 'val/rec_loss',
                    'embed_dim': 32,
                    'use_vf': 'dinov2',
                    'reverse_proj': True,
                    'ddconfig': {
                        'double_z': True, 'z_channels': 32, 'resolution': 256,
                        'in_channels': 3, 'out_ch': 3, 'ch': 128,
                        'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2,
                        'attn_resolutions': [16], 'dropout': 0.0
                    },
                    'lossconfig': {
                        'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                        'params': {
                            'disc_start': 1, 'disc_num_layers': 3, 'disc_weight': 0.5,
                            'disc_factor': 1.0, 'disc_in_channels': 3, 'disc_conditional': False,
                            'disc_loss': 'hinge', 'pixelloss_weight': 1.0, 'perceptual_weight': 1.0,
                            'kl_weight': 1e-6, 'logvar_init': 0.0, 'use_actnorm': False,
                            'pp_style': False, 'vf_weight': 0.1, 'adaptive_vf': False,
                            'distmat_weight': 1.0, 'cos_weight': 1.0,
                            'distmat_margin': 0.25, 'cos_margin': 0.5
                        }
                    }
                }
            }
        }
        
        # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
        temp_config_fd, temp_config_path = tempfile.mkstemp(suffix='.yaml')
        with open(temp_config_path, 'w') as f:
            yaml.dump(vae_config, f, default_flow_style=False)
        os.close(temp_config_fd)
        
        try:
            # ä½¿ç”¨å®˜æ–¹VA_VAEç±»åŠ è½½
            vae = VA_VAE(temp_config_path)
            # æ£€æŸ¥æ˜¯å¦æœ‰.to()æ–¹æ³•
            if hasattr(vae, 'to'):
                vae = vae.to(device)
            if hasattr(vae, 'eval'):
                vae.eval()
            if local_rank == 0:
                print(f"âœ… VAEåŠ è½½å®Œæˆ: {custom_vae_checkpoint}")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_config_path)
            
    except Exception as e:
        if local_rank == 0:
            print(f"âš ï¸ VAEåŠ è½½å¤±è´¥: {e}")
            print("âš ï¸ å°è¯•ä½¿ç”¨ç®€åŒ–VAEä½œä¸ºå¤‡ç”¨")
        # å¤‡ç”¨æ–¹æ¡ˆ
        try:
            vae = SimplifiedVAVAE(config['vae']['model_name']).to(device)
            vae.eval()
            if local_rank == 0:
                print(f"âœ… å¤‡ç”¨VAEåŠ è½½å®Œæˆ: {config['vae']['model_name']}")
        except Exception as e2:
            if local_rank == 0:
                print(f"âš ï¸ å¤‡ç”¨VAEä¹ŸåŠ è½½å¤±è´¥: {e2}")
            vae = None
    
    # åˆ›å»ºtransport
    transport = create_transport(
        config['transport']['path_type'],
        config['transport']['prediction'],
        config['transport']['loss_weight'],
        config['transport']['train_eps'],
        config['transport']['sample_eps'],
        use_cosine_loss=config['transport'].get('use_cosine_loss', False),
        use_lognorm=config['transport'].get('use_lognorm', False),
        partitial_train=config['transport'].get('partitial_train', None),
        partial_ratio=config['transport'].get('partial_ratio', 1.0),
        shift_lg=config['transport'].get('shift_lg', False),
    )
    
    return model, vae, transport, config, device


def load_classifier(checkpoint_path, device):
    """åŠ è½½é¢„è®­ç»ƒçš„åˆ†ç±»å™¨"""
    import torchvision.models as models
    
    # åˆ›å»ºä¸train_calibrated_classifier.pyå®Œå…¨ä¸€è‡´çš„DomainAdaptiveClassifierç»“æ„
    class DomainAdaptiveClassifier(nn.Module):
        def __init__(self, num_classes=31, dropout_rate=0.3, feature_dim=512):
            super().__init__()
            
            # ä½¿ç”¨ResNet18ä½œä¸ºbackbone
            self.backbone = models.resnet18(pretrained=False)
            backbone_dim = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
            
            # ç‰¹å¾æŠ•å½±å±‚ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            self.feature_projector = nn.Sequential(
                nn.Linear(backbone_dim, feature_dim),
                nn.BatchNorm1d(feature_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate)
            )
            
            # åˆ†ç±»å¤´ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            self.classifier = nn.Sequential(
                nn.Linear(feature_dim, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(256, num_classes)
            )
            
            # èº«ä»½ç‰¹å¾è®°å¿†åº“
            self.register_buffer('feature_bank', torch.zeros(num_classes, feature_dim))
            self.register_buffer('feature_count', torch.zeros(num_classes))
        
        def forward(self, x):
            backbone_features = self.backbone(x)
            features = self.feature_projector(backbone_features)
            logits = self.classifier(features)
            return logits
    
    # åˆ›å»ºæ¨¡å‹
    model = DomainAdaptiveClassifier(num_classes=31)
    
    # åŠ è½½æƒé‡ - ç°åœ¨ç»“æ„å®Œå…¨åŒ¹é…
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    model = model.to(device)
    model.eval()
    
    print(f"âœ… åˆ†ç±»å™¨åŠ è½½å®Œæˆ: {checkpoint_path}")
    if 'epoch' in checkpoint:
        val_acc = checkpoint.get('val_acc', 0) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        print(f"ğŸ“Š Epoch: {checkpoint['epoch']}, Val Acc: {val_acc:.2f}%, ECE: {checkpoint.get('val_ece', 0):.4f}")
    
    return model


def extract_features(images, classifier, device):
    """æå–å›¾åƒç‰¹å¾ç”¨äºå¤šæ ·æ€§è¯„ä¼°"""
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    features_list = []
    
    with torch.no_grad():
        for img in images:
            img_tensor = transform(img).unsqueeze(0).to(device)
            # æå–backboneç‰¹å¾ï¼ˆåœ¨åˆ†ç±»å¤´ä¹‹å‰ï¼‰
            features = classifier.backbone(img_tensor)
            features_list.append(features.cpu().numpy().flatten())
    
    return np.array(features_list)


def compute_diversity_metrics(features):
    """è®¡ç®—ç‰¹å¾å¤šæ ·æ€§æŒ‡æ ‡"""
    if len(features) < 2:
        return {'diversity_score': 0.0, 'avg_pairwise_dist': 0.0}
    
    cosine_sim_matrix = cosine_similarity(features)
    upper_triangle = np.triu(cosine_sim_matrix, k=1)
    avg_similarity = np.sum(upper_triangle) / (len(features) * (len(features) - 1) / 2)
    diversity_score = 1.0 - avg_similarity
    
    from scipy.spatial.distance import pdist
    pairwise_distances = pdist(features, metric='euclidean')
    avg_pairwise_dist = np.mean(pairwise_distances)
    
    return {
        'diversity_score': diversity_score,
        'avg_pairwise_dist': avg_pairwise_dist,
        'avg_similarity': avg_similarity
    }


def simple_quality_check(images):
    """ç®€åŒ–çš„å›¾åƒè´¨é‡æ£€æŸ¥ï¼ˆä»…æ£€æµ‹åŸºæœ¬å¼‚å¸¸ï¼‰"""
    quality_scores = []
    
    for img in images:
        img_array = np.array(img)
        
        # åªæ£€æµ‹åŸºæœ¬åƒç´ å€¼å¼‚å¸¸
        pixel_mean = np.mean(img_array)
        pixel_std = np.std(img_array)
        
        # ç®€å•çš„è´¨é‡åˆ†æ•°ï¼šåªæ£€æŸ¥æ˜¯å¦å…¨é»‘/å…¨ç™½æˆ–æ— å˜åŒ–
        is_valid = (
            10 < pixel_mean < 245 and  # ä¸æ˜¯å…¨é»‘æˆ–å…¨ç™½
            pixel_std > 5              # æœ‰ä¸€å®šå˜åŒ–
        )
        
        quality_score = {
            'pixel_mean': pixel_mean,
            'pixel_std': pixel_std,
            'is_valid': is_valid,
            'overall': 1.0 if is_valid else 0.0
        }
        
        quality_scores.append(quality_score)
    
    return quality_scores


def compute_user_specific_metrics(images, classifier, user_id, device, user_prototypes=None):
    """è®¡ç®—ç”¨æˆ·ç‰¹å®šæŒ‡æ ‡"""
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    user_metrics_list = []
    
    for img in images:
        img_tensor = transform(img).unsqueeze(0).to(device)
        
        with torch.no_grad():
            # è·å–åˆ†ç±»å™¨è¾“å‡ºå’Œç‰¹å¾
            outputs = classifier(img_tensor)
            
            # å¤„ç†åˆ†ç±»å™¨è¾“å‡ºæ ¼å¼ï¼ˆå¯èƒ½æ˜¯tupleï¼‰
            if isinstance(outputs, tuple):
                logits = outputs[0]  # é€šå¸¸ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯logits
            else:
                logits = outputs
            
            probs = F.softmax(logits, dim=1)
            features = classifier.backbone(img_tensor)
            
            # 1. åŸºæœ¬æŒ‡æ ‡
            confidence, pred = torch.max(probs, dim=1)
            sorted_probs, _ = torch.sort(probs, dim=1, descending=True)
            margin = (sorted_probs[0, 0] - sorted_probs[0, 1]).item()
            
            # 2. ç”¨æˆ·ç‰¹å¼‚æ€§åˆ†æ•°ï¼ˆä¸å…¶ä»–ç”¨æˆ·çš„åŒºåˆ†åº¦ï¼‰
            user_prob = probs[0, user_id].item()
            other_probs = torch.cat([probs[0, :user_id], probs[0, user_id+1:]])
            max_other_prob = torch.max(other_probs).item()
            # ç»Ÿä¸€ä¸ºå·®å€¼æ³•ï¼ˆä¸analyze_filtering_metrics.pyä¸€è‡´ï¼‰
            user_specificity = user_prob - max_other_prob  # èŒƒå›´[-1, 1]ï¼Œæ­£å€¼è¡¨ç¤ºç›®æ ‡ç”¨æˆ·æ¦‚ç‡æ›´é«˜
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒæ˜¯å¦åˆç†
            if user_specificity > 0.9:
                top3_probs = torch.topk(probs[0], k=3)
                print(f"[DEBUG] å¼‚å¸¸é«˜ç‰¹å¼‚æ€§ {user_specificity:.3f}: ç›®æ ‡ç”¨æˆ·{user_id}={user_prob:.4f}, æœ€é«˜å…¶ä»–={max_other_prob:.4f}")
                print(f"[DEBUG] Top3æ¦‚ç‡: {top3_probs.values.tolist()}, ç´¢å¼•: {top3_probs.indices.tolist()}")
            
            # 3. é¢„æµ‹ç¨³å®šæ€§ï¼ˆé’ˆå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ä¼˜åŒ–ï¼‰
            # ç”±äºå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾å¯¹å™ªå£°æå…¶æ•æ„Ÿï¼Œç§»é™¤å™ªå£°æ‰°åŠ¨æµ‹è¯•
            # ç¨³å®šæ€§ä¸ç½®ä¿¡åº¦ç›¸åŒï¼Œå› æ­¤ç§»é™¤é‡å¤è®¡ç®—
            # stability = confidence.item()  # å·²ä¸confidenceé‡å¤ï¼Œç§»é™¤
            
            # 4. ä¸ç”¨æˆ·åŸå‹çš„ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæä¾›ï¼‰
            prototype_similarity = 0.0
            if user_prototypes is not None and user_id in user_prototypes:
                prototype_features = user_prototypes[user_id]
                current_features = features.cpu().numpy().flatten()
                prototype_similarity = cosine_similarity(
                    [current_features], [prototype_features]
                )[0, 0]
            
            metrics = {
                'predicted': pred.item(),
                'confidence': confidence.item(),
                'margin': margin,
                'user_specificity': user_specificity,
                'prototype_similarity': prototype_similarity,
                'correct': pred.item() == user_id,
                'features': features.cpu().numpy().flatten()
            }
            
            user_metrics_list.append(metrics)
    
    return user_metrics_list


def generate_and_filter_advanced(model, vae, transport, classifier, user_id, 
                                 target_samples=800, batch_size=100, 
                                 confidence_threshold=0.9, margin_threshold=0.8,
                                 diversity_threshold=0.1,
                                 user_specificity_threshold=0.7, 
                                 conservative_mode=False, cfg_scale=12.0, 
                                 domain_coverage=True,
                                 output_dir='./filtered_samples', device=None, rank=0,
                                 user_prototypes=None):
    """ä¸ºå•ä¸ªç”¨æˆ·ç”Ÿæˆå¹¶ä½¿ç”¨å¤šæŒ‡æ ‡ç­›é€‰æ ·æœ¬"""
    
    # ç”Ÿæˆæ¡ä»¶è®¾ç½®ï¼ˆdomain_coverageå‚æ•°å·²åºŸå¼ƒï¼Œç»Ÿä¸€ä½¿ç”¨å•ä¸€æœ€ä¼˜æ¡ä»¶ï¼‰
    # é‡‡ç”¨CFG10çš„è´¨é‡-å¤šæ ·æ€§å¹³è¡¡ç‚¹ + 300æ­¥é«˜è´¨é‡é‡‡æ ·
    domain_conditions = [{"cfg": cfg_scale, "steps": 300, "name": "optimized"}]
    samples_per_condition = target_samples
    
    # åˆ›å»ºé‡‡æ ·å™¨ï¼ˆå°†åœ¨å¾ªç¯ä¸­åŠ¨æ€è°ƒæ•´å‚æ•°ï¼‰
    sampler = Sampler(transport)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    user_dir = Path(output_dir) / f"User_{user_id:02d}"
    user_dir.mkdir(parents=True, exist_ok=True)
    
    collected_samples = []
    total_generated = 0
    
    # ç®€åŒ–ç»Ÿè®¡ä¿¡æ¯ï¼ˆåªç»Ÿè®¡æˆåŠŸçš„ï¼‰
    stats = {
        'total_generated': 0,
        'total_accepted': 0,
        'collected_diversities': [],  # å­˜å‚¨æ¥å—æ ·æœ¬çš„å¤šæ ·æ€§åˆ†æ•°
        'collected_metrics': {  # å­˜å‚¨æ¥å—æ ·æœ¬çš„è¯¦ç»†æŒ‡æ ‡
            'confidences': [],
            'user_specificities': [],
            'margins': [],
            'diversities': []
        },
        'domain_stats': {cond["name"]: {'generated': 0, 'accepted': 0} for cond in domain_conditions}
    }
    
    # ç§»é™¤printè¾“å‡ºï¼Œé¿å…è¿›åº¦æ¡æ»šåŠ¨
    pass
    
    # å­˜å‚¨å·²æ”¶é›†çš„ç‰¹å¾ç”¨äºå¤šæ ·æ€§è¯„ä¼°
    collected_features = []
    condition_stats = {cond["name"]: 0 for cond in domain_conditions}
    
    # ä½¿ç”¨ç®€å•çš„çŠ¶æ€è¾“å‡ºè€Œä¸æ˜¯tqdmè¿›åº¦æ¡
    import time
    last_update = time.time()
    
    def update_progress(current, total, stats):
        nonlocal last_update
        now = time.time()
        if now - last_update > 2.0:  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡ï¼Œä½†ä¸åœ¨è¿™é‡Œæ˜¾ç¤º100/100
            success_rate = len(collected_samples) / stats['total_generated'] * 100 if stats['total_generated'] > 0 else 0
            print(f"\r[GPU{rank}]User_{user_id:02d}: {current}/{total} | ç”Ÿæˆ:{stats['total_generated']} | é€šè¿‡:{success_rate:.1f}%", end='', flush=True)
            last_update = now
    
    with torch.no_grad():
        # æŒ‰åŸŸæ¡ä»¶å¾ªç¯ç”Ÿæˆ
        for condition in domain_conditions:
            condition_target = samples_per_condition
            condition_collected = 0
            
            # ä¸ºå½“å‰æ¡ä»¶åˆ›å»ºç‰¹å®šçš„é‡‡æ ·å‡½æ•°
            current_sample_fn = sampler.sample_ode(
                sampling_method="dopri5",
                num_steps=condition["steps"],
                atol=1e-6,
                rtol=1e-3,
                reverse=False
            )
            
            # åªè¾“å‡ºä¸€æ¬¡åŸŸåˆ‡æ¢ä¿¡æ¯ï¼Œé¿å…æ»šåŠ¨
            pass
            
            while condition_collected < condition_target and len(collected_samples) < target_samples:
                # ç”Ÿæˆä¸€æ‰¹æ ·æœ¬
                remaining_total = target_samples - len(collected_samples)
                remaining_condition = condition_target - condition_collected
                current_batch_size = min(batch_size, remaining_condition, remaining_total)
                
                # å‡†å¤‡æ¡ä»¶
                y = torch.tensor([user_id] * current_batch_size, device=device)
                
                # åˆ›å»ºéšæœºå™ªå£°
                z = torch.randn(current_batch_size, 32, 16, 16, device=device)
                
                # ä½¿ç”¨å½“å‰åŸŸæ¡ä»¶ç”Ÿæˆæ ·æœ¬
                current_cfg = condition["cfg"]
                if current_cfg > 1.0:
                    # CFGé‡‡æ ·ï¼ˆæŒ‰ç…§generate_and_filter_samples.pyçš„å®ç°ï¼‰
                    z_cfg = torch.cat([z, z], 0)  # å¤åˆ¶å™ªå£°tensor
                    y_null = torch.tensor([31] * current_batch_size, device=device)
                    y_cfg = torch.cat([y, y_null], 0)
                    
                    cfg_interval_start = 0.11
                    model_kwargs = dict(y=y_cfg, cfg_scale=current_cfg, cfg_interval=True, cfg_interval_start=cfg_interval_start)
                    
                    if hasattr(model, 'forward_with_cfg'):
                        samples = current_sample_fn(z_cfg, model.forward_with_cfg, **model_kwargs)
                    else:
                        def model_fn_cfg(x, t, **kwargs):
                            pred = model(x, t, **kwargs)
                            pred_cond, pred_uncond = pred.chunk(2, dim=0)
                            return pred_uncond + current_cfg * (pred_cond - pred_uncond)
                        samples = current_sample_fn(z_cfg, model_fn_cfg, **model_kwargs)
                    
                    samples = samples[-1]
                    samples, _ = samples.chunk(2, dim=0)  # åªä¿ç•™conditionalæ ·æœ¬
                else:
                    # ä¸ä½¿ç”¨CFG
                    samples = current_sample_fn(z, model, **dict(y=y))
                    samples = samples[-1]
                
                # åå½’ä¸€åŒ–ï¼ˆæŒ‰ç…§generate_and_filter_samples.pyçš„å®ç°ï¼‰
                latent_stats_path = '/kaggle/working/VA-VAE/latents_safetensors/train/latent_stats.pt'
                if os.path.exists(latent_stats_path):
                    latent_stats = torch.load(latent_stats_path, map_location=device)
                    mean = latent_stats['mean'].to(device)
                    std = latent_stats['std'].to(device)
                    latent_multiplier = 1.0
                    samples_denorm = (samples * std) / latent_multiplier + mean
                else:
                    # å¤‡ç”¨ï¼šæ— åå½’ä¸€åŒ–
                    print("âš ï¸ æ— æ³•åŠ è½½latentç»Ÿè®¡ï¼Œä½¿ç”¨åŸå§‹æ ·æœ¬")
                    samples_denorm = samples
                
                # VAEè§£ç 
                if vae is not None:
                    try:
                        decoded_images = vae.decode_to_images(samples_denorm)
                        images_pil = [Image.fromarray(img) for img in decoded_images]
                        
                        # è®¡ç®—ç”¨æˆ·ç‰¹å®šæŒ‡æ ‡
                        metrics_list = compute_user_specific_metrics(
                            images_pil, classifier, user_id, device, user_prototypes
                        )
                        
                        # ç®€åŒ–çš„è´¨é‡æ£€æŸ¥
                        visual_quality_scores = simple_quality_check(images_pil)
                        
                        # æå–å½“å‰æ‰¹æ¬¡çš„ç‰¹å¾
                        current_features = [m['features'] for m in metrics_list]
                        
                        # ç­›é€‰é«˜è´¨é‡æ ·æœ¬
                        batch_accepted = 0
                        batch_candidates = []  # å€™é€‰æ ·æœ¬
                        
                        # ç¬¬ä¸€æ­¥ï¼šåŸºæœ¬è´¨é‡ç­›é€‰
                        stats['total_generated'] += current_batch_size
                        stats['domain_stats'][condition["name"]]['generated'] += current_batch_size
                        
                        for i, metrics in enumerate(metrics_list):
                            # åº”ç”¨ä¿å®ˆæ¨¡å¼è°ƒæ•´é˜ˆå€¼
                            actual_conf_thresh = confidence_threshold * (1.05 if conservative_mode else 1.0)
                            actual_margin_thresh = margin_threshold * (1.1 if conservative_mode else 1.0)
                            
                            # å®ç¼ºæ¯‹æ»¥ç­–ç•¥ï¼šä¸¥æ ¼ç­›é€‰é˜²æ­¢æ•°æ®æ±¡æŸ“
                            # æ–‡çŒ®æ”¯æŒï¼šé«˜ç½®ä¿¡åº¦(0.9+)æ˜¯é˜²æ­¢å™ªå£°æ ‡ç­¾çš„å…³é”®
                            if (metrics['confidence'] >= actual_conf_thresh and      # 1. ç½®ä¿¡åº¦ï¼ˆé˜²æ±¡æŸ“å…³é”®ï¼‰
                                metrics['user_specificity'] >= user_specificity_threshold and  # 2. ç”¨æˆ·ç‰¹å¼‚æ€§
                                metrics['margin'] >= actual_margin_thresh and        # 3. å†³ç­–è¾¹ç•Œ
                                visual_quality_scores[i]['is_valid'] and            # 4. åŸºæœ¬æœ‰æ•ˆæ€§
                                (metrics['predicted'] == user_id or                 # 5. ä¸¥æ ¼è¦æ±‚ï¼šå¿…é¡»é¢„æµ‹ä¸ºç›®æ ‡ç”¨æˆ·
                                 (metrics['user_specificity'] >= 0.8 and metrics['confidence'] >= 0.92))):  # æˆ–æé«˜è´¨é‡ï¼ˆå·®å€¼æ³•0.8æ¥è¿‘50%åˆ†ä½ï¼‰
                                
                                # ç®€åŒ–çš„ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ï¼ˆä»…åœ¨ä¿å®ˆæ¨¡å¼ä¸‹ï¼‰
                                if conservative_mode and len(collected_features) > 15:
                                    try:
                                        # ä½¿ç”¨ç®€å•çš„Z-scoreæ£€æµ‹
                                        feature_array = np.array(collected_features)
                                        current_feature = metrics['features']
                                        
                                        # è®¡ç®—ä¸ç°æœ‰æ ·æœ¬çš„æ¬§æ°è·ç¦»
                                        distances = [np.linalg.norm(current_feature - f) for f in feature_array]
                                        mean_dist = np.mean(distances)
                                        std_dist = np.std(distances)
                                        
                                        # Z-score > 2.5 è¢«è®¤ä¸ºå¼‚å¸¸
                                        if std_dist > 0 and (distances[-1] - mean_dist) / std_dist > 2.5:
                                            continue
                                    except:
                                        pass
                                
                                batch_candidates.append({
                                    'image': images_pil[i],
                                    'features': metrics['features'],
                                    'metrics': metrics,
                                    'index': i
                                })
                        
                        # ç¬¬äºŒæ­¥ï¼šå¤šæ ·æ€§ç­›é€‰
                        for candidate in batch_candidates:
                            # æ£€æŸ¥ä¸å·²æ”¶é›†æ ·æœ¬çš„å¤šæ ·æ€§
                            diversity_score = 1.0  # é»˜è®¤å¤šæ ·æ€§åˆ†æ•°ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
                            if len(collected_features) > 0:
                                candidate_features = candidate['features'].reshape(1, -1)
                                collected_array = np.array(collected_features)
                                
                                # è®¡ç®—ä¸ç°æœ‰æ ·æœ¬çš„æœ€å¤§ç›¸ä¼¼åº¦
                                similarities = cosine_similarity(candidate_features, collected_array)[0]
                                max_similarity = np.max(similarities)
                                diversity_score = 1.0 - max_similarity
                            
                            # åº”ç”¨å¤šæ ·æ€§é˜ˆå€¼
                            if diversity_score >= diversity_threshold:
                                collected_samples.append(candidate['image'])
                                collected_features.append(candidate['features'])
                                batch_accepted += 1
                                
                                # ä¿å­˜å›¾åƒåˆ°ç£ç›˜
                                img_filename = f"sample_{len(collected_samples):04d}_conf{candidate['metrics']['confidence']:.3f}_spec{candidate['metrics']['user_specificity']:.3f}.png"
                                img_path = user_dir / img_filename
                                candidate['image'].save(img_path)
                                
                                # è®°å½•è¯¥æ ·æœ¬çš„æŒ‡æ ‡ç”¨äºåç»­ç»Ÿè®¡
                                stats['collected_metrics']['confidences'].append(candidate['metrics']['confidence'])
                                stats['collected_metrics']['user_specificities'].append(candidate['metrics']['user_specificity'])
                                stats['collected_metrics']['margins'].append(candidate['metrics']['margin'])
                                stats['collected_metrics']['diversities'].append(diversity_score)
                                
                                if len(collected_samples) >= target_samples:
                                    break
                        
                        # æ›´æ–°æ¡ä»¶ç»Ÿè®¡
                        condition_collected += batch_accepted
                        condition_stats[condition["name"]] += batch_accepted
                        
                        if len(collected_samples) >= target_samples:
                            break
                        
                        # ä½¿ç”¨ç®€å•çš„çŠ¶æ€æ›´æ–°
                        update_progress(len(collected_samples), target_samples, stats)
                        
                        # æ£€æŸ¥æ˜¯å¦å®Œæˆç›®æ ‡
                        if len(collected_samples) >= target_samples:
                            # æœ€ç»ˆæ›´æ–°æ˜¾ç¤º100/100
                            success_rate = len(collected_samples) / stats['total_generated'] * 100 if stats['total_generated'] > 0 else 0
                            print(f"\r[GPU{rank}]User_{user_id:02d}: {len(collected_samples)}/{target_samples} | ç”Ÿæˆ:{stats['total_generated']} | é€šè¿‡:{success_rate:.1f}%", flush=True)
                    
                    except Exception as e:
                        # é™é»˜å¤„ç†é”™è¯¯ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡
                        pass
                else:
                    # é™é»˜å¤„ç†VAEé”™è¯¯
                    pass
                    break
    
    # å®Œæˆæ—¶è¾“å‡ºæœ€ç»ˆçŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
    print()  # æ¢è¡Œ
    
    # è¾“å‡ºç”¨æˆ·å®Œæˆç»Ÿè®¡
    if len(collected_samples) > 0:
        metrics = stats['collected_metrics']
        avg_confidence = sum(metrics['confidences']) / len(metrics['confidences']) if metrics['confidences'] else 0
        avg_user_spec = sum(metrics['user_specificities']) / len(metrics['user_specificities']) if metrics['user_specificities'] else 0
        avg_margin = sum(metrics['margins']) / len(metrics['margins']) if metrics['margins'] else 0
        avg_diversity = sum(metrics['diversities']) / len(metrics['diversities']) if metrics['diversities'] else 0
        
        final_success_rate = len(collected_samples) / stats['total_generated'] * 100 if stats['total_generated'] > 0 else 0
        
        print(f"[GPU{rank}] âœ… User_{user_id:02d} å®Œæˆç»Ÿè®¡:")
        print(f"  ğŸ“Š æ”¶é›†æ ·æœ¬: {len(collected_samples)}/{target_samples} | æ€»ç”Ÿæˆ: {stats['total_generated']} | é€šè¿‡ç‡: {final_success_rate:.1f}%")
        print(f"  ğŸ“ˆ å¹³å‡æŒ‡æ ‡: ç½®ä¿¡åº¦={avg_confidence:.3f} | ç”¨æˆ·ç‰¹å¼‚æ€§={avg_user_spec:.3f} | å†³ç­–è¾¹ç•Œ={avg_margin:.3f} | å¤šæ ·æ€§={avg_diversity:.3f}")
        print()
    
    return len(collected_samples)


def main():
    parser = argparse.ArgumentParser(description='Generate and filter high-quality samples with multi-metrics')
    parser.add_argument('--dit_checkpoint', type=str, 
                       default='/kaggle/input/50000-pt/0050000.pt', 
                       help='DiT model checkpoint path')
    parser.add_argument('--classifier_checkpoint', type=str,
                       default='./calibrated_classifier/best_calibrated_model.pth',
                       help='Classifier checkpoint path')
    parser.add_argument('--config', type=str, 
                       default='configs/dit_s_microdoppler.yaml', 
                       help='Config file path')
    parser.add_argument('--output_dir', type=str, 
                       default='./filtered_samples_multi', 
                       help='Output directory')
    parser.add_argument('--target_samples', type=int, default=800, 
                       help='Target number of samples per user')
    parser.add_argument('--batch_size', type=int, default=100, 
                       help='Batch size for generation')
    # é˜²æ•°æ®æ±¡æŸ“çº§ä¸¥æ ¼ç­›é€‰é˜ˆå€¼ï¼ˆåŸºäº75%åˆ†ä½æ•°ç­–ç•¥ï¼‰
    parser.add_argument('--confidence_threshold', type=float, default=0.92,
                       help='ç½®ä¿¡åº¦è¦æ±‚ï¼ˆå®ç¼ºæ¯‹æ»¥ï¼š0.92å¯¹åº”~60%é€šè¿‡ç‡ï¼Œæ–‡çŒ®å»ºè®®0.9+é˜²æ­¢å™ªå£°ï¼‰')
    parser.add_argument('--margin_threshold', type=float, default=0.85,
                       help='å†³ç­–è¾¹ç•Œè¦æ±‚ï¼ˆå®ç¼ºæ¯‹æ»¥ï¼š0.85å¯¹åº”~70%é€šè¿‡ç‡ï¼Œç¡®ä¿å†³ç­–æ¸…æ™°ï¼‰')
    # stability_threshold å·²ç§»é™¤ï¼Œå› ä¸ºä¸confidenceé‡å¤
    parser.add_argument('--diversity_threshold', type=float, default=0.035,
                       help='ç‰¹å¾å¤šæ ·æ€§é˜ˆå€¼ï¼ˆ0.035å¯¹åº”~40%é€šè¿‡ç‡ï¼Œå¹³è¡¡å¤šæ ·æ€§ä¸è´¨é‡ï¼‰')
    parser.add_argument('--user_specificity_threshold', type=float, default=0.8,
                       help='ç”¨æˆ·ç‰¹å¼‚æ€§è¦æ±‚ï¼ˆé˜²æ±¡æŸ“ï¼š0.7å¯¹åº”~55%é€šè¿‡ç‡ï¼Œä»‹äº25%-50%åˆ†ä½æ•°ï¼‰')
    # ç§»é™¤visual_quality_thresholdå‚æ•°
    parser.add_argument('--conservative_mode', action='store_true',
                       help='Enable conservative filtering (stricter thresholds)')
    # ç§»é™¤max_outlier_ratioå‚æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ä¸éœ€è¦ï¼‰
    parser.add_argument('--cfg_scale', type=float, default=10.0, 
                       help='CFG scaleï¼ˆ10.0=èº«ä»½å‡†ç¡®æ€§ä¸å¤šæ ·æ€§å¹³è¡¡ï¼Œé˜²æ­¢æ•°æ®æ±¡æŸ“ï¼‰')
    # domain_coverageå‚æ•°å·²ç§»é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨å•ä¸€æœ€ä¼˜æ¡ä»¶
    parser.add_argument('--start_user', type=int, default=0,
                       help='Starting user ID')
    parser.add_argument('--end_user', type=int, default=30,
                       help='Ending user ID (inclusive)')
    
    args = parser.parse_args()
    
    # è®¾ç½®åˆ†å¸ƒå¼
    rank, local_rank, world_size = setup_distributed()
    
    if rank == 0:
        print(f"ğŸš€ ç”Ÿæˆå¹¶ç­›é€‰é«˜è´¨é‡æ ·æœ¬ï¼ˆå¤šæŒ‡æ ‡ç‰ˆæœ¬ï¼‰")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"ğŸ¯ ç›®æ ‡: æ¯ç”¨æˆ· {args.target_samples} å¼ æ ·æœ¬")
        print(f"ğŸ“Š ç­›é€‰é˜ˆå€¼:")
        print(f"   - ç½®ä¿¡åº¦: {args.confidence_threshold}")
        print(f"   - å†³ç­–è¾¹ç•Œ: {args.margin_threshold}")
        print(f"   - å¤šæ ·æ€§: {args.diversity_threshold}")
        print(f"   - ç”¨æˆ·ç‰¹å¼‚æ€§: {args.user_specificity_threshold}")
        print(f"   - ç®€åŒ–è´¨é‡æ£€æŸ¥: å¼€å¯")
        if args.conservative_mode:
            print(f"   - ä¿å®ˆæ¨¡å¼: å¼€å¯ï¼ˆæ›´ä¸¥æ ¼çš„ç»Ÿè®¡æ£€æµ‹ï¼‰")
        print(f"âš™ï¸ ç”Ÿæˆç­–ç•¥: å•ä¸€æœ€ä¼˜æ¡ä»¶ (CFG{args.cfg_scale} + 300æ­¥)")
        print(f"âš™ï¸ åŸºç¡€CFG: {args.cfg_scale}")
    
    # åŠ è½½DiTæ¨¡å‹
    model, vae, transport, config, device = load_model_and_config(
        args.dit_checkpoint, args.config, local_rank
    )
    
    # åŠ è½½åˆ†ç±»å™¨
    classifier = load_classifier(args.classifier_checkpoint, device)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if rank == 0:
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    if world_size > 1:
        dist.barrier()
    
    # åˆ†å¸ƒå¼å¤„ç†ï¼šæ¯ä¸ªGPUå¤„ç†ä¸åŒçš„ç”¨æˆ·
    total_collected = 0
    user_list = list(range(args.start_user, args.end_user + 1))
    
    # å°†ç”¨æˆ·åˆ†é…ç»™ä¸åŒçš„GPU
    users_per_gpu = len(user_list) // world_size
    extra_users = len(user_list) % world_size
    
    # è®¡ç®—å½“å‰GPUè´Ÿè´£çš„ç”¨æˆ·èŒƒå›´
    start_idx = rank * users_per_gpu + min(rank, extra_users)
    end_idx = start_idx + users_per_gpu + (1 if rank < extra_users else 0)
    
    my_users = user_list[start_idx:end_idx]
    
    # ç§»é™¤åˆ†å¸ƒå¼ä¿¡æ¯è¾“å‡º
    pass
    
    # ç§»é™¤ç”¨æˆ·åˆ†é…ä¿¡æ¯è¾“å‡º
    pass
    
    # æ¯ä¸ªGPUå¤„ç†è‡ªå·±åˆ†é…çš„ç”¨æˆ·
    for user_id in my_users:
        collected = generate_and_filter_advanced(
            model, vae, transport, classifier, user_id,
            target_samples=args.target_samples,
            batch_size=args.batch_size,
            confidence_threshold=args.confidence_threshold,
            margin_threshold=args.margin_threshold,
            diversity_threshold=args.diversity_threshold,
            user_specificity_threshold=args.user_specificity_threshold,
            # visual_quality_threshold å‚æ•°å·²ç§»é™¤
            conservative_mode=args.conservative_mode,
            cfg_scale=args.cfg_scale,
            domain_coverage=True,  # å‚æ•°ä¿æŒå…¼å®¹æ€§ï¼Œä½†å†…éƒ¨é€»è¾‘å·²ç®€åŒ–
            output_dir=args.output_dir,
            device=device,
            rank=rank
        )
        total_collected += collected
    
    # æ‰“å°å½“å‰GPUå®Œæˆæƒ…å†µ
    print(f"GPU {rank} å®Œæˆ: æ”¶é›†äº† {total_collected} ä¸ªæ ·æœ¬ (å¤„ç†äº† {len(my_users)} ä¸ªç”¨æˆ·)")
    
    # åŒæ­¥æ‰€æœ‰GPUçš„ç»“æœ
    if world_size > 1:
        try:
            dist.barrier()
            total_tensor = torch.tensor([total_collected], device=device)
            dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)
            total_collected = total_tensor.item()
        except Exception as e:
            print(f"âš ï¸ GPU {rank} åŒæ­¥å¤±è´¥: {e}")
    
    if rank == 0:
        print(f"ğŸ¯ ç”Ÿæˆå®Œæˆï¼")
        print(f"âœ… æ€»å…±æ”¶é›†äº† {total_collected} ä¸ªé«˜è´¨é‡æ ·æœ¬")
        print(f"ğŸ“ æ ·æœ¬ä¿å­˜åœ¨: {args.output_dir}")
        if world_size > 1:
            expected_total = len(user_list) * args.target_samples
            print(f"ğŸ“Š é¢„æœŸæ€»æ•°: {expected_total} (31ç”¨æˆ· Ã— {args.target_samples}æ ·æœ¬/ç”¨æˆ·)")
    
    # æ¸…ç†åˆ†å¸ƒå¼
    if world_size > 1:
        dist.destroy_process_group()


if __name__ == "__main__":
    main()

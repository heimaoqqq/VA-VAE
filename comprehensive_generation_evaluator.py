#!/usr/bin/env python3
"""
å¾®å¤šæ™®å‹’ç”Ÿæˆæ ·æœ¬çš„ç»¼åˆè¯„ä¼°æ¡†æž¶
èžåˆèº«ä»½ä¿æŒåº¦ã€ç±»å†…å¤šæ ·æ€§ã€ç‰¹å¾è¦†ç›–åº¦å’Œé¢‘è°±ä¸€è‡´æ€§
"""

import torch
import torch.nn.functional as F
import numpy as np
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_similarity
import lpips
from pathlib import Path
from PIL import Image
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import argparse


class ComprehensiveGenerationEvaluator:
    """
    ç»¼åˆç”Ÿæˆè´¨é‡è¯„ä¼°å™¨
    è¯„ä¼°ä¸‰ä¸ªç»´åº¦ï¼šèº«ä»½ä¿æŒåº¦ã€ç±»å†…å¤šæ ·æ€§ã€ç‰¹å¾è¦†ç›–åº¦
    """
    
    def __init__(self, classifier_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½åˆ†ç±»å™¨ç”¨äºŽèº«ä»½è¯„ä¼°
        self.classifier = self.load_classifier(classifier_path)
        
        # åˆå§‹åŒ–LPIPSç”¨äºŽæ„ŸçŸ¥è·ç¦»
        self.lpips_fn = lpips.LPIPS(net='alex').to(self.device)
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def load_classifier(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„åˆ†ç±»å™¨"""
        from improved_classifier_training import ImprovedClassifier
        
        model = ImprovedClassifier(num_classes=31)
        checkpoint = torch.load(model_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        return model
    
    def load_images(self, image_paths):
        """æ‰¹é‡åŠ è½½å›¾åƒ"""
        images = []
        valid_paths = []
        
        if not image_paths:
            print("Warning: No image paths provided")
            return torch.empty(0, 3, 224, 224).to(self.device), []
        
        for img_path in tqdm(image_paths, desc="Loading images"):
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = self.transform(img)
                images.append(img_tensor)
                valid_paths.append(img_path)
            except Exception as e:
                print(f"Error loading {img_path}: {e}")
        
        if not images:
            print("Warning: No images successfully loaded")
            return torch.empty(0, 3, 224, 224).to(self.device), []
        
        return torch.stack(images).to(self.device), valid_paths
    
    def compute_identity_preservation(self, generated_samples, target_user_id):
        """
        1. èº«ä»½ä¿æŒåº¦è¯„ä¼°
        è¡¡é‡ç”Ÿæˆæ ·æœ¬æ˜¯å¦ä¿æŒç›®æ ‡ç”¨æˆ·çš„èº«ä»½ç‰¹å¾
        """
        with torch.no_grad():
            logits = self.classifier(generated_samples)
            predictions = torch.softmax(logits, dim=1)
            
            # ç›®æ ‡ç”¨æˆ·çš„é¢„æµ‹æ¦‚çŽ‡
            target_confidence = predictions[:, target_user_id]
            
            # Top-1å‡†ç¡®çŽ‡
            top1_accuracy = (torch.argmax(predictions, dim=1) == target_user_id).float().mean()
            
            # å¹³å‡ç½®ä¿¡åº¦
            avg_confidence = target_confidence.mean()
            
            # ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡
            confidence_std = target_confidence.std()
            
        return {
            'identity_accuracy': top1_accuracy.item(),
            'identity_confidence': avg_confidence.item(),
            'confidence_std': confidence_std.item(),
            'identity_score': (top1_accuracy * avg_confidence).item()
        }
    
    def compute_intra_class_diversity(self, generated_samples, sample_size=50):
        """
        2. ç±»å†…å¤šæ ·æ€§è¯„ä¼°
        åŸºäºŽLPIPSçš„æ„ŸçŸ¥å¤šæ ·æ€§ + ç‰¹å¾ç©ºé—´å¤šæ ·æ€§
        """
        # ä¸ºäº†è®¡ç®—æ•ˆçŽ‡ï¼Œéšæœºé‡‡æ ·éƒ¨åˆ†æ ·æœ¬
        if len(generated_samples) > sample_size:
            indices = torch.randperm(len(generated_samples))[:sample_size]
            samples = generated_samples[indices]
        else:
            samples = generated_samples
        
        n_samples = len(samples)
        
        # LPIPSæ„ŸçŸ¥è·ç¦»
        lpips_distances = []
        with torch.no_grad():
            for i in range(min(n_samples, 20)):  # é™åˆ¶è®¡ç®—é‡
                for j in range(i+1, min(n_samples, 20)):
                    dist = self.lpips_fn(samples[i:i+1], samples[j:j+1])
                    lpips_distances.append(dist.item())
        
        # ç‰¹å¾ç©ºé—´å¤šæ ·æ€§
        with torch.no_grad():
            features, _ = self.classifier(samples, return_features=True)
            features_np = features.cpu().numpy()
            
            # è®¡ç®—ç‰¹å¾é—´çš„ä½™å¼¦è·ç¦»
            cosine_sim_matrix = cosine_similarity(features_np)
            # æå–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆæŽ’é™¤å¯¹è§’çº¿ï¼‰
            upper_tri_indices = np.triu_indices_from(cosine_sim_matrix, k=1)
            cosine_distances = 1 - cosine_sim_matrix[upper_tri_indices]
        
        return {
            'mean_lpips_distance': np.mean(lpips_distances) if lpips_distances else 0,
            'std_lpips_distance': np.std(lpips_distances) if lpips_distances else 0,
            'mean_feature_distance': np.mean(cosine_distances),
            'std_feature_distance': np.std(cosine_distances),
            'diversity_score': np.mean(lpips_distances) if lpips_distances else 0
        }
    
    def compute_feature_coverage(self, generated_samples, real_user_samples, k=5, threshold=0.5):
        """
        3. ç‰¹å¾ç©ºé—´è¦†ç›–åº¦
        åŸºäºŽæ”¹è¿›çš„Precision & Recallæ€æƒ³
        """
        with torch.no_grad():
            # æå–ç‰¹å¾
            gen_features, _ = self.classifier(generated_samples, return_features=True)
            real_features, _ = self.classifier(real_user_samples, return_features=True)
            
            gen_features_np = gen_features.cpu().numpy()
            real_features_np = real_features.cpu().numpy()
        
        # æž„å»ºk-NN
        nbrs_real = NearestNeighbors(n_neighbors=min(k, len(real_features_np))).fit(real_features_np)
        nbrs_gen = NearestNeighbors(n_neighbors=min(k, len(gen_features_np))).fit(gen_features_np)
        
        # Precision: ç”Ÿæˆæ ·æœ¬æœ‰å¤šå°‘åœ¨çœŸå®žæ•°æ®æµå½¢é™„è¿‘
        if len(gen_features_np) > 0:
            distances_gen_to_real, _ = nbrs_real.kneighbors(gen_features_np)
            precision = (distances_gen_to_real[:, -1] < threshold).mean()
        else:
            precision = 0
        
        # Recall: çœŸå®žæ ·æœ¬æœ‰å¤šå°‘è¢«ç”Ÿæˆæ ·æœ¬è¦†ç›–
        if len(real_features_np) > 0 and len(gen_features_np) > 0:
            distances_real_to_gen, _ = nbrs_gen.kneighbors(real_features_np)
            recall = (distances_real_to_gen[:, -1] < threshold).mean()
        else:
            recall = 0
        
        # F1 Score
        f1_score = 2 * precision * recall / (precision + recall + 1e-8) if (precision + recall) > 0 else 0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'coverage_score': f1_score
        }
    
    
    def comprehensive_evaluate(self, generated_dir, real_user_dir, target_user_id, 
                              max_samples=200):
        """
        ç»¼åˆè¯„ä¼°ä¸»å‡½æ•°
        """
        print(f"Starting comprehensive evaluation for User {target_user_id}...")
        
        # åŠ è½½ç”Ÿæˆæ ·æœ¬
        gen_paths = list(Path(generated_dir).glob("*.jpg")) + list(Path(generated_dir).glob("*.png"))
        if len(gen_paths) > max_samples:
            gen_paths = gen_paths[:max_samples]
        
        generated_samples, _ = self.load_images(gen_paths)
        
        # åŠ è½½çœŸå®žç”¨æˆ·æ ·æœ¬
        real_paths = list(Path(real_user_dir).glob("*.jpg")) + list(Path(real_user_dir).glob("*.png"))
        if len(real_paths) > max_samples:
            real_paths = real_paths[:max_samples]
        
        real_user_samples, _ = self.load_images(real_paths)
        
        print(f"Loaded {len(generated_samples)} generated samples and {len(real_user_samples)} real samples")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œè¯„ä¼°
        if len(generated_samples) == 0:
            print("âŒ No generated samples found or loaded successfully")
            print("Please check the generated_dir path and image files")
            return None
        
        if len(real_user_samples) == 0:
            print("âŒ No real user samples found or loaded successfully")
            print("Please check the real_user_dir path and image files")
            return None
        
        # ä¸‰ä¸ªç»´åº¦çš„è¯„ä¼°
        results = {}
        
        # 1. èº«ä»½ä¿æŒåº¦
        print("Evaluating identity preservation...")
        results['identity'] = self.compute_identity_preservation(generated_samples, target_user_id)
        
        # 2. ç±»å†…å¤šæ ·æ€§  
        print("Evaluating intra-class diversity...")
        results['diversity'] = self.compute_intra_class_diversity(generated_samples)
        
        # 3. ç‰¹å¾è¦†ç›–åº¦
        print("Evaluating feature coverage...")
        results['coverage'] = self.compute_feature_coverage(generated_samples, real_user_samples)
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        results['overall'] = self.compute_overall_score(results)
        
        return results
    
    def compute_overall_score(self, results):
        """
        è®¡ç®—ç»¼åˆå¾—åˆ†
        æƒè¡¡ä¸‰ä¸ªç»´åº¦ï¼šèº«ä»½ä¿æŒã€å¤šæ ·æ€§ã€è¦†ç›–åº¦
        """
        # æå–å„ç»´åº¦å¾—åˆ†
        identity_score = results['identity']['identity_score']
        diversity_score = min(1.0, results['diversity']['diversity_score'] * 10)  # LPIPSé€šå¸¸å¾ˆå°
        coverage_score = results['coverage']['coverage_score'] 
        
        # æƒé‡è®¾è®¡ï¼šèº«ä»½ä¿æŒæœ€é‡è¦ï¼Œå…¶æ¬¡æ˜¯å¤šæ ·æ€§
        weights = {
            'identity': 0.5,    # 50% - å¿…é¡»ä¿æŒç”¨æˆ·èº«ä»½
            'diversity': 0.3,   # 30% - æ ·æœ¬é—´å¤šæ ·æ€§
            'coverage': 0.2,    # 20% - ç‰¹å¾ç©ºé—´è¦†ç›–
        }
        
        overall_score = (
            weights['identity'] * identity_score +
            weights['diversity'] * diversity_score + 
            weights['coverage'] * coverage_score
        )
        
        return {
            'overall_score': overall_score,
            'identity_component': weights['identity'] * identity_score,
            'diversity_component': weights['diversity'] * diversity_score,
            'coverage_component': weights['coverage'] * coverage_score,
            'weights': weights
        }
    
    def print_report(self, results, user_id):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "="*60)
        print(f"COMPREHENSIVE GENERATION EVALUATION - USER {user_id}")
        print("="*60)
        
        # èº«ä»½ä¿æŒåº¦
        identity = results['identity']
        print(f"\nðŸ” IDENTITY PRESERVATION:")
        print(f"   â€¢ Accuracy: {identity['identity_accuracy']:.1%}")
        print(f"   â€¢ Confidence: {identity['identity_confidence']:.3f}")
        print(f"   â€¢ Identity Score: {identity['identity_score']:.3f}")
        
        # å¤šæ ·æ€§
        diversity = results['diversity'] 
        print(f"\nðŸŽ¨ INTRA-CLASS DIVERSITY:")
        print(f"   â€¢ LPIPS Distance: {diversity['mean_lpips_distance']:.3f} Â± {diversity['std_lpips_distance']:.3f}")
        print(f"   â€¢ Feature Distance: {diversity['mean_feature_distance']:.3f} Â± {diversity['std_feature_distance']:.3f}")
        print(f"   â€¢ Diversity Score: {diversity['diversity_score']:.3f}")
        
        # è¦†ç›–åº¦
        coverage = results['coverage']
        print(f"\nðŸ“Š FEATURE COVERAGE:")
        print(f"   â€¢ Precision: {coverage['precision']:.3f}")
        print(f"   â€¢ Recall: {coverage['recall']:.3f}")
        print(f"   â€¢ F1-Score: {coverage['f1_score']:.3f}")
        
        # ç»¼åˆå¾—åˆ†
        overall = results['overall']
        print(f"\nðŸ† OVERALL ASSESSMENT:")
        print(f"   â€¢ Overall Score: {overall['overall_score']:.3f}")
        print(f"   â€¢ Identity Component: {overall['identity_component']:.3f} (50%)")
        print(f"   â€¢ Diversity Component: {overall['diversity_component']:.3f} (30%)")
        print(f"   â€¢ Coverage Component: {overall['coverage_component']:.3f} (20%)")
        
        # è´¨é‡è¯„ä¼°
        score = overall['overall_score']
        if score >= 0.7:
            grade = "ðŸŸ¢ EXCELLENT"
            recommendation = "High-quality generation with good identity-diversity balance"
        elif score >= 0.5:
            grade = "ðŸŸ¡ GOOD" 
            recommendation = "Acceptable quality, consider improving diversity or coverage"
        elif score >= 0.3:
            grade = "ðŸŸ  MODERATE"
            recommendation = "Needs improvement in multiple aspects"
        else:
            grade = "ðŸ”´ POOR"
            recommendation = "Significant issues with generation quality"
        
        print(f"\nðŸ“‹ QUALITY GRADE: {grade}")
        print(f"ðŸ’¡ RECOMMENDATION: {recommendation}")
        print("="*60)


def main():
    parser = argparse.ArgumentParser(description='Comprehensive generation evaluation')
    parser.add_argument('--classifier_path', required=True, help='Path to trained classifier')
    parser.add_argument('--generated_dir', required=True, help='Generated samples directory')
    parser.add_argument('--real_user_dir', required=True, help='Real user samples directory')  
    parser.add_argument('--user_id', type=int, required=True, help='Target user ID (0-30)')
    parser.add_argument('--max_samples', type=int, default=200, help='Maximum samples to evaluate')
    
    args = parser.parse_args()
    
    evaluator = ComprehensiveGenerationEvaluator(args.classifier_path)
    results = evaluator.comprehensive_evaluate(
        args.generated_dir, 
        args.real_user_dir,
        args.user_id,
        args.max_samples
    )
    
    evaluator.print_report(results, args.user_id)


if __name__ == "__main__":
    main()

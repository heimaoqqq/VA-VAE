"""
åŸºäºå®˜æ–¹extract_features.pyï¼Œé€‚é…å¾®å¤šæ™®å‹’æ•°æ®é›†
å®Œå…¨æŒ‰ç…§å®˜æ–¹æ ¼å¼ç”Ÿæˆsafetensorsæ–‡ä»¶
"""

import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
import argparse
import os
import json
import numpy as np
from pathlib import Path
from safetensors.torch import save_file
from datetime import datetime
from PIL import Image
import sys

# æ·»åŠ LightningDiTè·¯å¾„
lightningdit_path = '/kaggle/working/VA-VAE/LightningDiT'
if not os.path.exists(lightningdit_path):
    lightningdit_path = './LightningDiT'  # å¤‡ç”¨è·¯å¾„
sys.path.append(lightningdit_path)

class MicrodopplerDataset(torch.utils.data.Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›†ï¼Œæ¨¡ä»¿å®˜æ–¹ImageFolderç»“æ„"""
    
    def __init__(self, image_paths, transform=None):
        self.image_paths = image_paths
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # æ— æ¡ä»¶ç”Ÿæˆï¼Œæ‰€æœ‰æ ‡ç­¾ä¸º0
        label = 0
        
        return image, label

def load_image_paths(dataset_root, split_file, split_name):
    """åŠ è½½æŒ‡å®šsplitçš„å›¾åƒè·¯å¾„"""
    print(f"ğŸ“‚ åŠ è½½{split_name}æ•°æ®é›†è·¯å¾„...")
    
    with open(split_file, 'r') as f:
        splits = json.load(f)
    
    if split_name not in splits:
        raise ValueError(f"Split '{split_name}' not found in {split_file}")
    
    image_paths = []
    user_data = splits[split_name]
    
    for user_id, paths in user_data.items():
        for rel_path in paths:
            full_path = os.path.join(dataset_root, rel_path)
            if os.path.exists(full_path):
                image_paths.append(full_path)
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
    
    print(f"âœ… åŠ è½½äº†{len(image_paths)}å¼ å›¾åƒ")
    return image_paths

def create_transform(vae):
    """ä½¿ç”¨å®˜æ–¹VA-VAEçš„å›¾åƒå˜æ¢"""
    # ä½¿ç”¨å®˜æ–¹VA-VAEçš„é¢„å¤„ç†ç®¡é“
    return vae.img_transform(p_hflip=0.0)  # æ— æ°´å¹³ç¿»è½¬

def main(args):
    """
    æå–å¾®å¤šæ™®å‹’latentç‰¹å¾å¹¶ä¿å­˜ä¸ºsafetensorsæ ¼å¼
    å®Œå…¨éµå¾ªå®˜æ–¹extract_features.pyçš„é€»è¾‘
    """
    print("ğŸš€ å¼€å§‹æå–å¾®å¤šæ™®å‹’latentç‰¹å¾...")
    
    # è®¾ç½®è®¾å¤‡
    device = 0 if torch.cuda.is_available() else 'cpu'
    torch.cuda.set_device(device) if torch.cuda.is_available() else None
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = os.path.join(args.output_path, args.split)
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºVA-VAEæ¨¡å‹ï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEæ¥å£ï¼‰
    print("ğŸ”§ åŠ è½½VA-VAEæ¨¡å‹...")
    
    # æ£€æŸ¥LightningDiTç›®å½•æ˜¯å¦å­˜åœ¨
    lightningdit_check_path = '/kaggle/working/VA-VAE/LightningDiT'
    if not os.path.exists(lightningdit_check_path):
        lightningdit_check_path = './LightningDiT'
    
    if not os.path.exists(lightningdit_check_path):
        print(f"âŒ LightningDiTç›®å½•ä¸å­˜åœ¨: {lightningdit_check_path}")
        print("   è¯·å…ˆè¿è¡Œ: git clone https://github.com/Alpha-VLLM/LightningDiT.git")
        return
    
    # è°ƒè¯•ï¼šæ˜¾ç¤ºLightningDiTç›®å½•å†…å®¹
    print(f"ğŸ“‚ LightningDiTè·¯å¾„: {lightningdit_check_path}")
    datasets_dir = os.path.join(lightningdit_check_path, 'datasets')
    tokenizer_dir = os.path.join(lightningdit_check_path, 'tokenizer')
    
    if os.path.exists(datasets_dir):
        print("âœ… datasetsç›®å½•å­˜åœ¨")
        datasets_files = os.listdir(datasets_dir)
        print(f"   datasetsæ–‡ä»¶: {datasets_files}")
        if 'img_latent_dataset.py' in datasets_files:
            print("âœ… img_latent_dataset.pyå­˜åœ¨")
        else:
            print("âŒ img_latent_dataset.pyä¸å­˜åœ¨")
        if '__init__.py' in datasets_files:
            print("âœ… datasets/__init__.pyå­˜åœ¨")
        else:
            print("âŒ datasets/__init__.pyä¸å­˜åœ¨ - è¿™æ˜¯å¯¼å…¥å¤±è´¥çš„åŸå› !")
    else:
        print("âŒ datasetsç›®å½•ä¸å­˜åœ¨")
    
    if os.path.exists(tokenizer_dir):
        print("âœ… tokenizerç›®å½•å­˜åœ¨")
        tokenizer_files = os.listdir(tokenizer_dir)
        print(f"   tokenizeræ–‡ä»¶: {tokenizer_files}")
        if 'vavae.py' in tokenizer_files:
            print("âœ… vavae.pyå­˜åœ¨")
        else:
            print("âŒ vavae.pyä¸å­˜åœ¨")
        if '__init__.py' in tokenizer_files:
            print("âœ… tokenizer/__init__.pyå­˜åœ¨")
        else:
            print("âŒ tokenizer/__init__.pyä¸å­˜åœ¨ - è¿™æ˜¯å¯¼å…¥å¤±è´¥çš„åŸå› !")
    else:
        print("âŒ tokenizerç›®å½•ä¸å­˜åœ¨")
    
    # æ£€æŸ¥æ ¹ç›®å½•çš„__init__.py
    root_init = os.path.join(lightningdit_check_path, '__init__.py')
    if os.path.exists(root_init):
        print("âœ… LightningDiT/__init__.pyå­˜åœ¨")
    else:
        print("âŒ LightningDiT/__init__.pyä¸å­˜åœ¨")
    
    # è‡ªåŠ¨åˆ›å»ºç¼ºå¤±çš„__init__.pyæ–‡ä»¶
    init_files_to_create = [
        os.path.join(lightningdit_check_path, '__init__.py'),
        os.path.join(datasets_dir, '__init__.py'),
        os.path.join(tokenizer_dir, '__init__.py')
    ]
    
    for init_file in init_files_to_create:
        if not os.path.exists(init_file):
            print(f"ğŸ”§ åˆ›å»ºç¼ºå¤±çš„__init__.py: {init_file}")
            with open(init_file, 'w') as f:
                f.write("# Auto-generated __init__.py for package imports\n")
    
    # å¯¼å…¥å®˜æ–¹æ¨¡å— - ä½¿ç”¨ç›´æ¥å¯¼å…¥æ–¹å¼
    try:
        # æ–¹æ³•1: å°è¯•æ ‡å‡†åŒ…å¯¼å…¥
        from tokenizer.vavae import VA_VAE
        from datasets.img_latent_dataset import ImgLatentDataset
        print("âœ… æˆåŠŸå¯¼å…¥å®˜æ–¹æ¨¡å— (æ ‡å‡†åŒ…å¯¼å…¥)")
    except ImportError as e1:
        print(f"âŒ æ ‡å‡†å¯¼å…¥å¤±è´¥: {e1}")
        try:
            # æ–¹æ³•2: ä½¿ç”¨importlibç›´æ¥å¯¼å…¥
            import importlib.util
            
            # å¯¼å…¥vavaeæ¨¡å—
            vavae_path = os.path.join(tokenizer_dir, 'vavae.py')
            spec_vavae = importlib.util.spec_from_file_location("vavae", vavae_path)
            vavae_module = importlib.util.module_from_spec(spec_vavae)
            spec_vavae.loader.exec_module(vavae_module)
            VA_VAE = vavae_module.VA_VAE
            
            # å¯¼å…¥img_latent_datasetæ¨¡å—
            dataset_path = os.path.join(datasets_dir, 'img_latent_dataset.py')
            spec_dataset = importlib.util.spec_from_file_location("img_latent_dataset", dataset_path)
            dataset_module = importlib.util.module_from_spec(spec_dataset)
            spec_dataset.loader.exec_module(dataset_module)
            ImgLatentDataset = dataset_module.ImgLatentDataset
            
            print("âœ… æˆåŠŸå¯¼å…¥å®˜æ–¹æ¨¡å— (ç›´æ¥æ–‡ä»¶å¯¼å…¥)")
        except Exception as e2:
            print(f"âŒ ç›´æ¥å¯¼å…¥ä¹Ÿå¤±è´¥: {e2}")
            print(f"   å½“å‰Pythonè·¯å¾„: {sys.path[-3:]}")
            print("   è¯·ç¡®ä¿å·²æ­£ç¡®å…‹éš†LightningDiTä»“åº“åˆ°æ­£ç¡®ä½ç½®")
            return
    
    # åˆ›å»ºä¸å®˜æ–¹ä¸€è‡´çš„VA-VAEé…ç½®
    vae_config = {
        'model_name': 'vavae_f16d32',
        'downsample_ratio': 16,
        'checkpoint_path': args.vae_checkpoint
    }
    
    # ä½¿ç”¨å®˜æ–¹VA-VAEç±» - éœ€è¦å…ˆä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„checkpointè·¯å¾„
    vae_config_path = os.path.join(lightningdit_check_path, 'tokenizer/configs/vavae_f16d32.yaml')
    
    # è¯»å–å¹¶ä¿®æ”¹é…ç½®
    with open(vae_config_path, 'r') as f:
        vae_config_content = f.read()
    
    # æ›¿æ¢checkpointè·¯å¾„
    if args.vae_checkpoint:
        print(f"   è®¾ç½®æ£€æŸ¥ç‚¹è·¯å¾„: {args.vae_checkpoint}")
        vae_config_content = vae_config_content.replace('/path/to/checkpoint.pt', args.vae_checkpoint)
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    temp_config_path = './temp_vavae_config.yaml'
    with open(temp_config_path, 'w') as f:
        f.write(vae_config_content)
    
    # åˆå§‹åŒ–VA-VAE
    vae = VA_VAE(temp_config_path)
    
    # VA_VAEçš„modelå·²ç»åœ¨load()ä¸­è®¾ç½®ä¸º.cuda().eval()ï¼Œæ— éœ€å†æ¬¡è®¾ç½®
    print("âœ… VA-VAEæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½æ•°æ®
    image_paths = load_image_paths(args.data_path, args.split_file, args.split)
    
    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨ï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEå˜æ¢ï¼‰
    transform = create_transform(vae)
    dataset = MicrodopplerDataset(image_paths, transform=transform)
    
    loader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    total_data_in_loop = len(loader.dataset)
    print(f"ğŸ“Š æ€»è®¡å›¾åƒæ•°: {total_data_in_loop}")
    
    # æå–ç‰¹å¾ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰
    run_images = 0
    saved_files = 0
    latents = []
    labels = []
    
    print("ğŸ”„ å¼€å§‹æå–latentç‰¹å¾...")
    
    for batch_idx, data in enumerate(loader):
        x = data[0].to(device)  # (N, C, H, W)
        y = data[1]             # (N,) - æ ‡ç­¾
        
        run_images += x.shape[0]
        if run_images % 100 == 0:
            print(f'{datetime.now()} å¤„ç† {run_images}/{total_data_in_loop} å›¾åƒ')
        
        # ç¼–ç ä¸ºlatentï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEæ¥å£ï¼‰
        with torch.no_grad():
            z = vae.encode_images(x).detach().cpu()  # (N, 32, 16, 16)
        
        if batch_idx == 0:
            print(f'Latent shape: {z.shape}, dtype: {z.dtype}')
        
        latents.append(z)
        labels.append(y)
        
        # æ¯10000å¼ å›¾åƒä¿å­˜ä¸€æ¬¡ï¼ˆå®˜æ–¹è®¾ç½®ï¼‰
        if len(latents) >= 10000 // args.batch_size:
            # æ‹¼æ¥tensor
            latents = torch.cat(latents, dim=0)
            labels = torch.cat(labels, dim=0)
            
            # ä¿å­˜ä¸ºsafetensorsï¼ˆæ— æ•°æ®å¢å¼ºæ ¼å¼ï¼‰
            save_dict = {
                'latents': latents,
                'latents_flip': latents,  # ä½¿ç”¨ç›¸åŒæ•°æ®ï¼Œä¿æŒæ ¼å¼å…¼å®¹
                'labels': labels
            }
            
            print(f"ä¿å­˜æ‰¹æ¬¡ {saved_files}:")
            for key in save_dict:
                print(f"  {key}: {save_dict[key].shape}")
            
            save_filename = os.path.join(output_dir, f'latents_rank00_shard{saved_files:03d}.safetensors')
            save_file(
                save_dict,
                save_filename,
                metadata={'total_size': f'{latents.shape[0]}', 'dtype': f'{latents.dtype}'}
            )
            print(f'âœ… ä¿å­˜: {save_filename}')
            
            # é‡ç½®
            latents = []
            labels = []
            saved_files += 1
    
    # ä¿å­˜å‰©ä½™çš„latents
    if len(latents) > 0:
        latents = torch.cat(latents, dim=0)
        labels = torch.cat(labels, dim=0)
        
        save_dict = {
            'latents': latents,
            'latents_flip': latents,  # ä½¿ç”¨ç›¸åŒæ•°æ®ï¼Œä¿æŒæ ¼å¼å…¼å®¹
            'labels': labels
        }
        
        print(f"ä¿å­˜æœ€ç»ˆæ‰¹æ¬¡ {saved_files}:")
        for key in save_dict:
            print(f"  {key}: {save_dict[key].shape}")
        
        save_filename = os.path.join(output_dir, f'latents_rank00_shard{saved_files:03d}.safetensors')
        save_file(
            save_dict,
            save_filename,
            metadata={'total_size': f'{latents.shape[0]}', 'dtype': f'{latents.dtype}'}
        )
        print(f'âœ… ä¿å­˜: {save_filename}')
    
    # è®¡ç®—latentç»Ÿè®¡ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
    print("ğŸ“Š è®¡ç®—latentç»Ÿè®¡...")
    dataset = ImgLatentDataset(output_dir, latent_norm=True)
    print(f"âœ… ç»Ÿè®¡è®¡ç®—å®Œæˆï¼Œæ•°æ®é›†åŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
    
    print("ğŸ‰ ç‰¹å¾æå–å®Œæˆï¼")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¾®å¤šæ™®å‹’ç‰¹å¾æå–")
    parser.add_argument("--data_path", type=str, required=True, help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--split_file", type=str, required=True, help="æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶")
    parser.add_argument("--split", type=str, choices=['train', 'val'], required=True, help="è¦å¤„ç†çš„split")
    parser.add_argument("--vae_checkpoint", type=str, required=True, help="VA-VAEæ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output_path", type=str, default="./latents_safetensors", help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    
    args = parser.parse_args()
    main(args)

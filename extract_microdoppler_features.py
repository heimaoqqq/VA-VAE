"""
åŸºäºå®˜æ–¹extract_features.pyï¼Œé€‚é…å¾®å¤šæ™®å‹’æ•°æ®é›†
å®Œå…¨æŒ‰ç…§å®˜æ–¹æ ¼å¼ç”Ÿæˆsafetensorsæ–‡ä»¶
"""

import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
import argparse
import os
import json
import numpy as np
from pathlib import Path
from safetensors.torch import save_file
from datetime import datetime
from PIL import Image
import sys

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('./LightningDiT')

from datasets.img_latent_dataset import ImgLatentDataset
from simplified_vavae import SimplifiedVAVAE

class MicrodopplerDataset(torch.utils.data.Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›†ï¼Œæ¨¡ä»¿å®˜æ–¹ImageFolderç»“æ„"""
    
    def __init__(self, image_paths, transform=None):
        self.image_paths = image_paths
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # æ— æ¡ä»¶ç”Ÿæˆï¼Œæ‰€æœ‰æ ‡ç­¾ä¸º0
        label = 0
        
        return image, label

def load_image_paths(dataset_root, split_file, split_name):
    """åŠ è½½æŒ‡å®šsplitçš„å›¾åƒè·¯å¾„"""
    print(f"ğŸ“‚ åŠ è½½{split_name}æ•°æ®é›†è·¯å¾„...")
    
    with open(split_file, 'r') as f:
        splits = json.load(f)
    
    if split_name not in splits:
        raise ValueError(f"Split '{split_name}' not found in {split_file}")
    
    image_paths = []
    user_data = splits[split_name]
    
    for user_id, paths in user_data.items():
        for rel_path in paths:
            full_path = os.path.join(dataset_root, rel_path)
            if os.path.exists(full_path):
                image_paths.append(full_path)
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
    
    print(f"âœ… åŠ è½½äº†{len(image_paths)}å¼ å›¾åƒ")
    return image_paths

def create_transform():
    """åˆ›å»ºå›¾åƒå˜æ¢ï¼Œæ¨¡ä»¿VA-VAEçš„é¢„å¤„ç†"""
    from torchvision import transforms
    
    return transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

def main(args):
    """
    æå–å¾®å¤šæ™®å‹’latentç‰¹å¾å¹¶ä¿å­˜ä¸ºsafetensorsæ ¼å¼
    å®Œå…¨éµå¾ªå®˜æ–¹extract_features.pyçš„é€»è¾‘
    """
    print("ğŸš€ å¼€å§‹æå–å¾®å¤šæ™®å‹’latentç‰¹å¾...")
    
    # è®¾ç½®è®¾å¤‡
    device = 0 if torch.cuda.is_available() else 'cpu'
    torch.cuda.set_device(device) if torch.cuda.is_available() else None
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = os.path.join(args.output_path, args.split)
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºVA-VAEæ¨¡å‹
    print("ğŸ”§ åŠ è½½VA-VAEæ¨¡å‹...")
    vae = SimplifiedVAVAE(args.vae_checkpoint)
    vae.to(device)
    vae.eval()
    
    # åŠ è½½æ•°æ®
    image_paths = load_image_paths(args.data_path, args.split_file, args.split)
    
    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰
    transform = create_transform()
    dataset = MicrodopplerDataset(image_paths, transform=transform)
    
    loader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    total_data_in_loop = len(loader.dataset)
    print(f"ğŸ“Š æ€»è®¡å›¾åƒæ•°: {total_data_in_loop}")
    
    # æå–ç‰¹å¾ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰
    run_images = 0
    saved_files = 0
    latents = []
    labels = []
    
    print("ğŸ”„ å¼€å§‹æå–latentç‰¹å¾...")
    
    for batch_idx, data in enumerate(loader):
        x = data[0].to(device)  # (N, C, H, W)
        y = data[1]             # (N,) - æ ‡ç­¾
        
        run_images += x.shape[0]
        if run_images % 100 == 0:
            print(f'{datetime.now()} å¤„ç† {run_images}/{total_data_in_loop} å›¾åƒ')
        
        # ç¼–ç ä¸ºlatent
        with torch.no_grad():
            z = vae.encode(x).detach().cpu()  # (N, 32, 16, 16)
        
        if batch_idx == 0:
            print(f'Latent shape: {z.shape}, dtype: {z.dtype}')
        
        latents.append(z)
        labels.append(y)
        
        # æ¯10000å¼ å›¾åƒä¿å­˜ä¸€æ¬¡ï¼ˆå®˜æ–¹è®¾ç½®ï¼‰
        if len(latents) >= 10000 // args.batch_size:
            # æ‹¼æ¥tensor
            latents = torch.cat(latents, dim=0)
            labels = torch.cat(labels, dim=0)
            
            # ä¿å­˜ä¸ºsafetensorsï¼ˆæ— æ•°æ®å¢å¼ºæ ¼å¼ï¼‰
            save_dict = {
                'latents': latents,
                'latents_flip': latents,  # ä½¿ç”¨ç›¸åŒæ•°æ®ï¼Œä¿æŒæ ¼å¼å…¼å®¹
                'labels': labels
            }
            
            print(f"ä¿å­˜æ‰¹æ¬¡ {saved_files}:")
            for key in save_dict:
                print(f"  {key}: {save_dict[key].shape}")
            
            save_filename = os.path.join(output_dir, f'latents_rank00_shard{saved_files:03d}.safetensors')
            save_file(
                save_dict,
                save_filename,
                metadata={'total_size': f'{latents.shape[0]}', 'dtype': f'{latents.dtype}'}
            )
            print(f'âœ… ä¿å­˜: {save_filename}')
            
            # é‡ç½®
            latents = []
            labels = []
            saved_files += 1
    
    # ä¿å­˜å‰©ä½™çš„latents
    if len(latents) > 0:
        latents = torch.cat(latents, dim=0)
        labels = torch.cat(labels, dim=0)
        
        save_dict = {
            'latents': latents,
            'latents_flip': latents,  # ä½¿ç”¨ç›¸åŒæ•°æ®ï¼Œä¿æŒæ ¼å¼å…¼å®¹
            'labels': labels
        }
        
        print(f"ä¿å­˜æœ€ç»ˆæ‰¹æ¬¡ {saved_files}:")
        for key in save_dict:
            print(f"  {key}: {save_dict[key].shape}")
        
        save_filename = os.path.join(output_dir, f'latents_rank00_shard{saved_files:03d}.safetensors')
        save_file(
            save_dict,
            save_filename,
            metadata={'total_size': f'{latents.shape[0]}', 'dtype': f'{latents.dtype}'}
        )
        print(f'âœ… ä¿å­˜: {save_filename}')
    
    # è®¡ç®—latentç»Ÿè®¡ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
    print("ğŸ“Š è®¡ç®—latentç»Ÿè®¡...")
    dataset = ImgLatentDataset(output_dir, latent_norm=True)
    print(f"âœ… ç»Ÿè®¡è®¡ç®—å®Œæˆï¼Œæ•°æ®é›†åŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
    
    print("ğŸ‰ ç‰¹å¾æå–å®Œæˆï¼")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¾®å¤šæ™®å‹’ç‰¹å¾æå–")
    parser.add_argument("--data_path", type=str, required=True, help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--split_file", type=str, required=True, help="æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶")
    parser.add_argument("--split", type=str, choices=['train', 'val'], required=True, help="è¦å¤„ç†çš„split")
    parser.add_argument("--vae_checkpoint", type=str, required=True, help="VA-VAEæ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output_path", type=str, default="./latents_safetensors", help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    
    args = parser.parse_args()
    main(args)

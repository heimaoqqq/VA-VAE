"""
åŸºäºå®˜æ–¹extract_features.pyï¼Œé€‚é…å¾®å¤šæ™®å‹’æ•°æ®é›†
å®Œå…¨æŒ‰ç…§å®˜æ–¹æ ¼å¼ç”Ÿæˆsafetensorsæ–‡ä»¶
"""

import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
import argparse
import os
import json
import numpy as np
from pathlib import Path
from safetensors.torch import save_file
from datetime import datetime
from PIL import Image
import sys

# æ·»åŠ LightningDiTè·¯å¾„
lightningdit_path = '/kaggle/working/VA-VAE/LightningDiT'
if not os.path.exists(lightningdit_path):
    lightningdit_path = './LightningDiT'  # å¤‡ç”¨è·¯å¾„
sys.path.append(lightningdit_path)

class MicrodopplerDataset(torch.utils.data.Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›†ï¼Œæ¨¡ä»¿å®˜æ–¹ImageFolderç»“æ„"""
    
    def __init__(self, image_paths, transform=None):
        self.image_paths = image_paths
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # æ— æ¡ä»¶ç”Ÿæˆï¼Œæ‰€æœ‰æ ‡ç­¾ä¸º0
        label = 0
        
        return image, label

def load_image_paths(dataset_root, split_file, split_name):
    """åŠ è½½æŒ‡å®šsplitçš„å›¾åƒè·¯å¾„"""
    print(f"ğŸ“‚ åŠ è½½{split_name}æ•°æ®é›†è·¯å¾„...")
    
    with open(split_file, 'r') as f:
        splits = json.load(f)
    
    if split_name not in splits:
        raise ValueError(f"Split '{split_name}' not found in {split_file}")
    
    image_paths = []
    user_data = splits[split_name]
    
    for user_id, paths in user_data.items():
        for rel_path in paths:
            full_path = os.path.join(dataset_root, rel_path)
            if os.path.exists(full_path):
                image_paths.append(full_path)
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
    
    print(f"âœ… åŠ è½½äº†{len(image_paths)}å¼ å›¾åƒ")
    return image_paths

def create_transform(vae):
    """ä½¿ç”¨å®˜æ–¹VA-VAEçš„å›¾åƒå˜æ¢"""
    # ä½¿ç”¨å®˜æ–¹VA-VAEçš„é¢„å¤„ç†ç®¡é“
    return vae.img_transform(p_hflip=0.0)  # æ— æ°´å¹³ç¿»è½¬

def main(args):
    """
    æå–å¾®å¤šæ™®å‹’latentç‰¹å¾å¹¶ä¿å­˜ä¸ºsafetensorsæ ¼å¼
    å®Œå…¨éµå¾ªå®˜æ–¹extract_features.pyçš„é€»è¾‘
    """
    print("ğŸš€ å¼€å§‹æå–å¾®å¤šæ™®å‹’latentç‰¹å¾...")
    
    # è®¾ç½®è®¾å¤‡
    device = 0 if torch.cuda.is_available() else 'cpu'
    torch.cuda.set_device(device) if torch.cuda.is_available() else None
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = os.path.join(args.output_path, args.split)
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºVA-VAEæ¨¡å‹ï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEæ¥å£ï¼‰
    print("ğŸ”§ åŠ è½½VA-VAEæ¨¡å‹...")
    
    # æ£€æŸ¥LightningDiTç›®å½•æ˜¯å¦å­˜åœ¨
    lightningdit_check_path = '/kaggle/working/VA-VAE/LightningDiT'
    if not os.path.exists(lightningdit_check_path):
        lightningdit_check_path = './LightningDiT'
    
    if not os.path.exists(lightningdit_check_path):
        print(f"âŒ LightningDiTç›®å½•ä¸å­˜åœ¨: {lightningdit_check_path}")
        print("   è¯·å…ˆè¿è¡Œ: git clone https://github.com/Alpha-VLLM/LightningDiT.git")
        return
    
    # è°ƒè¯•ï¼šæ˜¾ç¤ºLightningDiTç›®å½•å†…å®¹
    print(f"ğŸ“‚ LightningDiTè·¯å¾„: {lightningdit_check_path}")
    if os.path.exists(os.path.join(lightningdit_check_path, 'datasets')):
        print("âœ… datasetsç›®å½•å­˜åœ¨")
    if os.path.exists(os.path.join(lightningdit_check_path, 'tokenizer')):
        print("âœ… tokenizerç›®å½•å­˜åœ¨")
    
    # å¯¼å…¥å®˜æ–¹æ¨¡å—
    try:
        from tokenizer.vavae import VA_VAE
        from datasets.img_latent_dataset import ImgLatentDataset
        print("âœ… æˆåŠŸå¯¼å…¥å®˜æ–¹æ¨¡å—")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å®˜æ–¹æ¨¡å—å¤±è´¥: {e}")
        print(f"   å½“å‰Pythonè·¯å¾„: {sys.path[-3:]}")
        print("   è¯·ç¡®ä¿å·²æ­£ç¡®å…‹éš†LightningDiTä»“åº“åˆ°æ­£ç¡®ä½ç½®")
        return
    
    # åˆ›å»ºä¸å®˜æ–¹ä¸€è‡´çš„VA-VAEé…ç½®
    vae_config = {
        'model_name': 'vavae_f16d32',
        'downsample_ratio': 16,
        'checkpoint_path': args.vae_checkpoint
    }
    
    # ä½¿ç”¨å®˜æ–¹VA-VAEç±»
    vae = VA_VAE('./LightningDiT/configs/lightningdit_xl_vavae_f16d32.yaml')
    # å¦‚æœæœ‰æˆ‘ä»¬çš„æ£€æŸ¥ç‚¹ï¼ŒåŠ è½½æƒé‡
    if args.vae_checkpoint:
        print(f"   åŠ è½½æ£€æŸ¥ç‚¹: {args.vae_checkpoint}")
        checkpoint = torch.load(args.vae_checkpoint, map_location='cpu', weights_only=False)
        if 'model_state_dict' in checkpoint:
            vae.model.load_state_dict(checkpoint['model_state_dict'])
        elif 'state_dict' in checkpoint:
            vae.model.load_state_dict(checkpoint['state_dict'])
        else:
            vae.model.load_state_dict(checkpoint)
    
    vae.to(device)
    vae.eval()
    
    # åŠ è½½æ•°æ®
    image_paths = load_image_paths(args.data_path, args.split_file, args.split)
    
    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨ï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEå˜æ¢ï¼‰
    transform = create_transform(vae)
    dataset = MicrodopplerDataset(image_paths, transform=transform)
    
    loader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    total_data_in_loop = len(loader.dataset)
    print(f"ğŸ“Š æ€»è®¡å›¾åƒæ•°: {total_data_in_loop}")
    
    # æå–ç‰¹å¾ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰
    run_images = 0
    saved_files = 0
    latents = []
    labels = []
    
    print("ğŸ”„ å¼€å§‹æå–latentç‰¹å¾...")
    
    for batch_idx, data in enumerate(loader):
        x = data[0].to(device)  # (N, C, H, W)
        y = data[1]             # (N,) - æ ‡ç­¾
        
        run_images += x.shape[0]
        if run_images % 100 == 0:
            print(f'{datetime.now()} å¤„ç† {run_images}/{total_data_in_loop} å›¾åƒ')
        
        # ç¼–ç ä¸ºlatentï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEæ¥å£ï¼‰
        with torch.no_grad():
            z = vae.encode_images(x).detach().cpu()  # (N, 32, 16, 16)
        
        if batch_idx == 0:
            print(f'Latent shape: {z.shape}, dtype: {z.dtype}')
        
        latents.append(z)
        labels.append(y)
        
        # æ¯10000å¼ å›¾åƒä¿å­˜ä¸€æ¬¡ï¼ˆå®˜æ–¹è®¾ç½®ï¼‰
        if len(latents) >= 10000 // args.batch_size:
            # æ‹¼æ¥tensor
            latents = torch.cat(latents, dim=0)
            labels = torch.cat(labels, dim=0)
            
            # ä¿å­˜ä¸ºsafetensorsï¼ˆæ— æ•°æ®å¢å¼ºæ ¼å¼ï¼‰
            save_dict = {
                'latents': latents,
                'latents_flip': latents,  # ä½¿ç”¨ç›¸åŒæ•°æ®ï¼Œä¿æŒæ ¼å¼å…¼å®¹
                'labels': labels
            }
            
            print(f"ä¿å­˜æ‰¹æ¬¡ {saved_files}:")
            for key in save_dict:
                print(f"  {key}: {save_dict[key].shape}")
            
            save_filename = os.path.join(output_dir, f'latents_rank00_shard{saved_files:03d}.safetensors')
            save_file(
                save_dict,
                save_filename,
                metadata={'total_size': f'{latents.shape[0]}', 'dtype': f'{latents.dtype}'}
            )
            print(f'âœ… ä¿å­˜: {save_filename}')
            
            # é‡ç½®
            latents = []
            labels = []
            saved_files += 1
    
    # ä¿å­˜å‰©ä½™çš„latents
    if len(latents) > 0:
        latents = torch.cat(latents, dim=0)
        labels = torch.cat(labels, dim=0)
        
        save_dict = {
            'latents': latents,
            'latents_flip': latents,  # ä½¿ç”¨ç›¸åŒæ•°æ®ï¼Œä¿æŒæ ¼å¼å…¼å®¹
            'labels': labels
        }
        
        print(f"ä¿å­˜æœ€ç»ˆæ‰¹æ¬¡ {saved_files}:")
        for key in save_dict:
            print(f"  {key}: {save_dict[key].shape}")
        
        save_filename = os.path.join(output_dir, f'latents_rank00_shard{saved_files:03d}.safetensors')
        save_file(
            save_dict,
            save_filename,
            metadata={'total_size': f'{latents.shape[0]}', 'dtype': f'{latents.dtype}'}
        )
        print(f'âœ… ä¿å­˜: {save_filename}')
    
    # è®¡ç®—latentç»Ÿè®¡ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
    print("ğŸ“Š è®¡ç®—latentç»Ÿè®¡...")
    dataset = ImgLatentDataset(output_dir, latent_norm=True)
    print(f"âœ… ç»Ÿè®¡è®¡ç®—å®Œæˆï¼Œæ•°æ®é›†åŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
    
    print("ğŸ‰ ç‰¹å¾æå–å®Œæˆï¼")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¾®å¤šæ™®å‹’ç‰¹å¾æå–")
    parser.add_argument("--data_path", type=str, required=True, help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--split_file", type=str, required=True, help="æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶")
    parser.add_argument("--split", type=str, choices=['train', 'val'], required=True, help="è¦å¤„ç†çš„split")
    parser.add_argument("--vae_checkpoint", type=str, required=True, help="VA-VAEæ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output_path", type=str, default="./latents_safetensors", help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    
    args = parser.parse_args()
    main(args)

"""
åŸºäºå®˜æ–¹extract_features.pyï¼Œé€‚é…å¾®å¤šæ™®å‹’æ•°æ®é›†
å®Œå…¨æŒ‰ç…§å®˜æ–¹æ ¼å¼ç”Ÿæˆsafetensorsæ–‡ä»¶
"""

import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
import argparse
import os
import json
import numpy as np
from pathlib import Path
from safetensors.torch import save_file
from datetime import datetime
from PIL import Image
import sys

# æ·»åŠ LightningDiTè·¯å¾„
lightningdit_path = '/kaggle/working/VA-VAE/LightningDiT'
if not os.path.exists(lightningdit_path):
    lightningdit_path = './LightningDiT'  # å¤‡ç”¨è·¯å¾„
sys.path.append(lightningdit_path)

class MicrodopplerDataset(torch.utils.data.Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›†ï¼Œæ¨¡ä»¿å®˜æ–¹ImageFolderç»“æ„"""
    
    def __init__(self, image_paths, transform=None):
        self.image_paths = image_paths
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # æ¡ä»¶ç”Ÿæˆï¼šä»æ–‡ä»¶è·¯å¾„æå–ç”¨æˆ·ID
        # è·¯å¾„æ ¼å¼: /path/to/ID_X/IDX_caseX_X_DopplerX.jpg
        import re
        path_parts = Path(image_path).parts
        user_folder = None
        for part in path_parts:
            if part.startswith('ID_'):
                user_folder = part
                break
                
        if user_folder:
            # æå–ç”¨æˆ·ID: ID_1->0, ID_2->1, ..., ID_31->30
            match = re.match(r'ID_(\d+)', user_folder)
            if match:
                user_id = int(match.group(1))
                label = user_id - 1  # ID_1->0, ID_2->1, etc.
                if label < 0 or label >= 31:
                    print(f"âš ï¸ ç”¨æˆ·IDè¶…å‡ºèŒƒå›´: {user_folder} -> {label}, ä½¿ç”¨0")
                    label = 0
            else:
                print(f"âš ï¸ æ— æ³•è§£æç”¨æˆ·ID: {user_folder}, ä½¿ç”¨0")
                label = 0
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·æ–‡ä»¶å¤¹: {image_path}, ä½¿ç”¨0")
            label = 0
        
        return image, label

def load_image_paths(dataset_root, split_file, split_name):
    """åŠ è½½æŒ‡å®šsplitçš„å›¾åƒè·¯å¾„"""
    print(f"ğŸ“‚ åŠ è½½{split_name}æ•°æ®é›†è·¯å¾„...")
    
    with open(split_file, 'r') as f:
        splits = json.load(f)
    
    if split_name not in splits:
        raise ValueError(f"Split '{split_name}' not found in {split_file}")
    
    image_paths = []
    user_data = splits[split_name]
    
    for user_id, paths in user_data.items():
        for rel_path in paths:
            full_path = os.path.join(dataset_root, rel_path)
            if os.path.exists(full_path):
                image_paths.append(full_path)
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
    
    print(f"âœ… åŠ è½½äº†{len(image_paths)}å¼ å›¾åƒ")
    return image_paths

def create_transform(vae):
    """ä½¿ç”¨å®˜æ–¹VA-VAEçš„å›¾åƒå˜æ¢"""
    # ä½¿ç”¨å®˜æ–¹VA-VAEçš„é¢„å¤„ç†ç®¡é“
    return vae.img_transform(p_hflip=0.0)  # æ— æ°´å¹³ç¿»è½¬

def main(args):
    """
    æå–å¾®å¤šæ™®å‹’latentç‰¹å¾å¹¶ä¿å­˜ä¸ºsafetensorsæ ¼å¼
    å®Œå…¨éµå¾ªå®˜æ–¹extract_features.pyçš„é€»è¾‘
    """
    print("ğŸš€ å¼€å§‹æå–å¾®å¤šæ™®å‹’latentç‰¹å¾...")
    
    # è®¾ç½®è®¾å¤‡ - æ”¯æŒå¤šGPU
    if torch.cuda.is_available():
        num_gpus = torch.cuda.device_count()
        print(f"ğŸ”§ æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU")
        device = 0
        torch.cuda.set_device(device)
    else:
        device = 'cpu'
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = os.path.join(args.output_path, args.split)
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºVA-VAEæ¨¡å‹ï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEæ¥å£ï¼‰
    print("ğŸ”§ åŠ è½½VA-VAEæ¨¡å‹...")
    
    # æ£€æŸ¥LightningDiTç›®å½•æ˜¯å¦å­˜åœ¨
    lightningdit_check_path = '/kaggle/working/VA-VAE/LightningDiT'
    if not os.path.exists(lightningdit_check_path):
        lightningdit_check_path = './LightningDiT'
    
    if not os.path.exists(lightningdit_check_path):
        print(f"âŒ LightningDiTç›®å½•ä¸å­˜åœ¨: {lightningdit_check_path}")
        print("   è¯·å…ˆè¿è¡Œ: git clone https://github.com/Alpha-VLLM/LightningDiT.git")
        return
    
    # è®¾ç½®è·¯å¾„
    datasets_dir = os.path.join(lightningdit_check_path, 'datasets')
    tokenizer_dir = os.path.join(lightningdit_check_path, 'tokenizer')
    
    # è‡ªåŠ¨åˆ›å»ºç¼ºå¤±çš„__init__.pyæ–‡ä»¶
    init_files_to_create = [
        os.path.join(lightningdit_check_path, '__init__.py'),
        os.path.join(datasets_dir, '__init__.py'),
        os.path.join(tokenizer_dir, '__init__.py')
    ]
    
    for init_file in init_files_to_create:
        if not os.path.exists(init_file):
            with open(init_file, 'w') as f:
                f.write("# Auto-generated __init__.py for package imports\n")
    
    # ç›´æ¥æ–‡ä»¶å¯¼å…¥å®˜æ–¹æ¨¡å—
    import importlib.util
    
    try:
        # å¯¼å…¥vavaeæ¨¡å—
        vavae_path = os.path.join(tokenizer_dir, 'vavae.py')
        spec_vavae = importlib.util.spec_from_file_location("vavae", vavae_path)
        vavae_module = importlib.util.module_from_spec(spec_vavae)
        spec_vavae.loader.exec_module(vavae_module)
        VA_VAE = vavae_module.VA_VAE
        
        # å¯¼å…¥img_latent_datasetæ¨¡å—
        dataset_path = os.path.join(datasets_dir, 'img_latent_dataset.py')
        spec_dataset = importlib.util.spec_from_file_location("img_latent_dataset", dataset_path)
        dataset_module = importlib.util.module_from_spec(spec_dataset)
        spec_dataset.loader.exec_module(dataset_module)
        ImgLatentDataset = dataset_module.ImgLatentDataset
        
        print("âœ… å®˜æ–¹æ¨¡å—å¯¼å…¥å®Œæˆ")
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print(f"   å½“å‰Pythonè·¯å¾„: {sys.path[-3:]}")
        print("   è¯·ç¡®ä¿å·²æ­£ç¡®å…‹éš†LightningDiTä»“åº“åˆ°æ­£ç¡®ä½ç½®")
        return
    
    # åˆ›å»ºä¸å®˜æ–¹ä¸€è‡´çš„VA-VAEé…ç½®
    vae_config = {
        'model_name': 'vavae_f16d32',
        'downsample_ratio': 16,
        'checkpoint_path': args.vae_checkpoint
    }
    
    # ä½¿ç”¨å®˜æ–¹VA-VAEç±» - éœ€è¦å…ˆä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„checkpointè·¯å¾„
    vae_config_path = os.path.join(lightningdit_check_path, 'tokenizer/configs/vavae_f16d32.yaml')
    
    # è¯»å–å¹¶ä¿®æ”¹é…ç½®
    with open(vae_config_path, 'r') as f:
        vae_config_content = f.read()
    
    # æ›¿æ¢checkpointè·¯å¾„
    if args.vae_checkpoint:
        vae_config_content = vae_config_content.replace('/path/to/checkpoint.pt', args.vae_checkpoint)
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    temp_config_path = './temp_vavae_config.yaml'
    with open(temp_config_path, 'w') as f:
        f.write(vae_config_content)
    
    # åˆå§‹åŒ–VA-VAE
    vae = VA_VAE(temp_config_path)
    
    # å¯ç”¨å¤šGPUæ”¯æŒ
    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        vae.model = torch.nn.DataParallel(vae.model)
        print(f"ğŸ“Š VA-VAEä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œå¹¶è¡Œå¤„ç†")
    
    # VA_VAEçš„modelå·²ç»åœ¨load()ä¸­è®¾ç½®ä¸º.cuda().eval()ï¼Œæ— éœ€å†æ¬¡è®¾ç½®
    print("âœ… VA-VAEæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½æ•°æ®
    image_paths = load_image_paths(args.data_path, args.split_file, args.split)
    
    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨ï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEå˜æ¢ï¼‰
    transform = create_transform(vae)
    dataset = MicrodopplerDataset(image_paths, transform=transform)
    
    # æ ¹æ®GPUæ•°é‡è°ƒæ•´batch_size
    effective_batch_size = args.batch_size
    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        # å¤šGPUæ—¶å¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch_size
        print(f"ğŸ“Š å¤šGPUç¯å¢ƒï¼Œbatch_sizeä¿æŒä¸º {effective_batch_size}")
    
    loader = DataLoader(
        dataset,
        batch_size=effective_batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=False,
        persistent_workers=True if args.num_workers > 0 else False  # æé«˜æ•°æ®åŠ è½½æ•ˆç‡
    )
    
    total_data_in_loop = len(loader.dataset)
    print(f"ğŸ“Š æ€»è®¡å›¾åƒæ•°: {total_data_in_loop}")
    
    # æå–ç‰¹å¾ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰
    run_images = 0
    saved_files = 0
    latents = []
    labels = []
    
    print("ğŸ”„ å¼€å§‹æå–latentç‰¹å¾...")
    
    for batch_idx, data in enumerate(loader):
        x = data[0].to(device)  # (N, C, H, W)
        y = data[1]             # (N,) - æ ‡ç­¾
        
        run_images += x.shape[0]
        if run_images % 1000 == 0:
            print(f'ğŸ“Š å¤„ç†è¿›åº¦: {run_images}/{total_data_in_loop} ({run_images/total_data_in_loop*100:.1f}%)')
        
        # ç¼–ç ä¸ºlatentï¼ˆä½¿ç”¨å®˜æ–¹VA-VAEæ¥å£ï¼‰
        with torch.no_grad():
            z = vae.encode_images(x).detach().cpu()  # (N, 32, 16, 16)
        
        # ç¬¬ä¸€æ¬¡æ˜¾ç¤ºlatentå½¢çŠ¶
        if batch_idx == 0:
            print(f'âœ… Latentå½¢çŠ¶: {z.shape}')
        
        latents.append(z)
        labels.append(y)
        
        # æ¯10000å¼ å›¾åƒä¿å­˜ä¸€æ¬¡ï¼ˆå®˜æ–¹è®¾ç½®ï¼‰
        if len(latents) >= 10000 // args.batch_size:
            # æ‹¼æ¥tensor
            latents = torch.cat(latents, dim=0)
            labels = torch.cat(labels, dim=0)
            
            # ä¿å­˜ä¸ºsafetensorsï¼ˆæ— æ•°æ®å¢å¼ºæ ¼å¼ï¼‰
            save_dict = {
                'latents': latents,
                'latents_flip': latents.clone(),  # ä½¿ç”¨ç‹¬ç«‹å†…å­˜æ‹·è´ï¼Œé¿å…å…±äº«å†…å­˜é”™è¯¯
                'labels': labels
            }
            
            save_filename = os.path.join(output_dir, f'latents_rank00_shard{saved_files:03d}.safetensors')
            save_file(
                save_dict,
                save_filename,
                metadata={'total_size': f'{latents.shape[0]}', 'dtype': f'{latents.dtype}'}
            )
            print(f'ğŸ’¾ ä¿å­˜æ‰¹æ¬¡ {saved_files}: {latents.shape[0]} æ ·æœ¬')
            
            # é‡ç½®
            latents = []
            labels = []
            saved_files += 1
    
    # ä¿å­˜å‰©ä½™çš„latents
    if len(latents) > 0:
        latents = torch.cat(latents, dim=0)
        labels = torch.cat(labels, dim=0)
        
        save_dict = {
            'latents': latents,
            'latents_flip': latents.clone(),  # ä½¿ç”¨ç›¸åŒæ•°æ®ï¼Œä¿æŒæ ¼å¼å…¼å®¹
            'labels': labels
        }
        
        save_filename = os.path.join(output_dir, f'latents_rank00_shard{saved_files:03d}.safetensors')
        save_file(
            save_dict,
            save_filename,
            metadata={'total_size': f'{latents.shape[0]}', 'dtype': f'{latents.dtype}'}
        )
        print(f'ğŸ’¾ ä¿å­˜æœ€ç»ˆæ‰¹æ¬¡: {latents.shape[0]} æ ·æœ¬')
    
    # è®¡ç®—latentç»Ÿè®¡ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
    print("ğŸ“Š è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
    dataset = ImgLatentDataset(output_dir, latent_norm=True)
    mean_tensor, std_tensor = dataset.get_latent_stats()  # æ­£ç¡®ï¼šè¿”å›tuple (mean, std)
    
    mean_range = f"[{mean_tensor.min():.3f}, {mean_tensor.max():.3f}]"
    std_range = f"[{std_tensor.min():.3f}, {std_tensor.max():.3f}]"
    print(f"   å‡å€¼èŒƒå›´: {mean_range}, æ ‡å‡†å·®èŒƒå›´: {std_range}")
    print(f'âœ… æ•°æ®é›†åŒ…å« {len(dataset)} ä¸ªæ ·æœ¬')
    print('ğŸ‰ ç‰¹å¾æå–å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¾®å¤šæ™®å‹’ç‰¹å¾æå–")
    parser.add_argument("--data_path", type=str, required=True, help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--split_file", type=str, required=True, help="æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶")
    parser.add_argument("--split", type=str, choices=['train', 'val'], required=True, help="è¦å¤„ç†çš„split")
    parser.add_argument("--vae_checkpoint", type=str, required=True, help="VA-VAEæ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output_path", type=str, default="./latents_safetensors", help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    
    args = parser.parse_args()
    main(args)

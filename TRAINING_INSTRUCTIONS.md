# ğŸ¯ å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®å¢å¹¿å®Œæ•´è®­ç»ƒæŒ‡ä»¤

## ğŸ“‹ **å®Œæ•´çš„7æ­¥è®­ç»ƒæµç¨‹**

### **ç¯å¢ƒå‡†å¤‡é˜¶æ®µ**

#### **Step 1: å…‹éš†é¡¹ç›®**
```bash
git clone https://github.com/heimaoqqq/VA-VAE.git
cd VA-VAE
```

#### **Step 2: ä¸€é”®å®‰è£…æ‰€æœ‰ä¾èµ–** âœ¨
```bash
# å®‰è£…å®Œæ•´ä¾èµ– (æ¨ç† + VA-VAEè®­ç»ƒ + Taming-Transformers)
python install_dependencies.py

# æˆ–è€…åªå®‰è£…æ¨ç†ä¾èµ–
python install_dependencies.py --inference-only
```

**åŒ…å«çš„ä¾èµ–ï¼š**
- âœ… LightningDiTåŸºç¡€æ¨ç†ä¾èµ–
- âœ… VA-VAEè®­ç»ƒä¸“ç”¨ä¾èµ– (pytorch-lightning, lpips, korniaç­‰)
- âœ… Taming-Transformersè‡ªåŠ¨å®‰è£…å’Œtorch 2.xå…¼å®¹æ€§ä¿®å¤
- âœ… å¾®å¤šæ™®å‹’è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰åŒ…

#### **Step 2.5: ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹**
```bash
# å¾®å¤šæ™®å‹’è®­ç»ƒä¸“ç”¨ (å¼ºçƒˆæ¨èï¼Œä»…800MB)
python step1_download_models.py --vae-only

# æˆ–å®Œæ•´ä¸‹è½½ (åŒ…å«ä¸å¿…è¦çš„6GBæ¨ç†æ¨¡å‹)
python step1_download_models.py
```

**æ¨¡å‹è¯´æ˜ï¼š**
- âœ… **VA-VAEæ¨¡å‹** (~800MB) - å¾®è°ƒè®­ç»ƒçš„åŸºç¡€ï¼Œ**å¿…éœ€**
- âŒ **æ½œåœ¨ç»Ÿè®¡** (~1KB) - ImageNetç»Ÿè®¡ï¼Œ**ä¸é€‚ç”¨**å¾®å¤šæ™®å‹’æ•°æ®
- âŒ **æ‰©æ•£æ¨¡å‹** (~6GB) - ImageNet 1000ç±»ï¼Œ**ä¸é€‚ç”¨**31ç”¨æˆ·æ¡ä»¶

**ä¸ºä»€ä¹ˆé€‰æ‹© --vae-onlyï¼Ÿ**
1. **latents_stats.pt**: åŸºäºImageNetè®¡ç®—ï¼Œå¯¹å¾®å¤šæ™®å‹’æ•°æ®ä¸å‡†ç¡®
2. **LightningDiT-XL**: 1000ç±»ImageNet vs 31ä¸ªç”¨æˆ·ï¼Œæ¶æ„ä¸åŒ¹é…
3. **é‡å¤´è®­ç»ƒ**: æˆ‘ä»¬ä¼šè®­ç»ƒä¸“é—¨çš„31ç”¨æˆ·æ¡ä»¶æ‰©æ•£æ¨¡å‹

---

### **æ•°æ®å‡†å¤‡é˜¶æ®µ**

#### **Step 3: å‡†å¤‡æ•°æ®é›†**
```bash
# ç¡®ä¿æ‚¨çš„æ•°æ®é›†ç»“æ„å¦‚ä¸‹ï¼š
# your_micro_doppler_data/
# â”œâ”€â”€ ID_1/  (ç”¨æˆ·1çš„150å¼ æ—¶é¢‘å›¾)
# â”œâ”€â”€ ID_2/  (ç”¨æˆ·2çš„150å¼ æ—¶é¢‘å›¾)
# â””â”€â”€ ... ID_31/ (ç”¨æˆ·31çš„150å¼ æ—¶é¢‘å›¾)

# è¿è¡Œæ•°æ®é›†åˆ’åˆ†å’Œå‡†å¤‡ (8:2æ¯”ä¾‹)
python step3_prepare_micro_doppler_dataset.py \
    --input_dir /path/to/your_micro_doppler_data \
    --output_dir micro_doppler_dataset

# éªŒè¯æ•°æ®é›†ç»“æ„
ls micro_doppler_dataset/
# åº”è¯¥çœ‹åˆ°ï¼š
# â”œâ”€â”€ train/ (31ä¸ªç”¨æˆ·ï¼Œæ¯ç”¨æˆ·çº¦120å¼ )
# â”‚   â”œâ”€â”€ user1/ user2/ ... user31/
# â”œâ”€â”€ val/ (31ä¸ªç”¨æˆ·ï¼Œæ¯ç”¨æˆ·çº¦30å¼ )
# â”‚   â”œâ”€â”€ user1/ user2/ ... user31/
# â”œâ”€â”€ dataset_config.yaml
# â””â”€â”€ split_info.txt
```

---

### **VA-VAEå¾®è°ƒé˜¶æ®µ**

#### **Step 4: VA-VAEå¾®è°ƒ** (2-3å¤©)
```bash
# å¯åŠ¨VA-VAEå¾®è°ƒï¼Œé’ˆå¯¹T4Ã—2 GPUä¼˜åŒ–
python step4_finetune_vavae.py \
    --dataset_dir micro_doppler_dataset \
    --output_dir vavae_finetuned

# ç›‘æ§è®­ç»ƒè¿›åº¦
tensorboard --logdir LightningDiT/vavae/logs/

# è®­ç»ƒå®Œæˆåï¼Œcheckpointä¿å­˜åœ¨ï¼š
# LightningDiT/vavae/logs/[timestamp]/checkpoints/last.ckpt
```

**å…³é”®é…ç½®ï¼š**
- å­¦ä¹ ç‡ï¼š1e-5 (å¾®è°ƒ)
- æ‰¹æ¬¡å¤§å°ï¼š1 (T4Ã—2ä¼˜åŒ–)
- æ¢¯åº¦ç´¯ç§¯ï¼š4
- è®­ç»ƒè½®æ•°ï¼š100è½®
- ä½¿ç”¨æ–¹æ¡ˆBå¢å¼ºç”¨æˆ·åµŒå…¥

---

### **ç‰¹å¾æå–é˜¶æ®µ**

#### **Step 5: æå–æ½œåœ¨ç‰¹å¾** (2-3å°æ—¶)
```bash
# ä½¿ç”¨å¾®è°ƒåçš„VA-VAEæå–32Ã—16Ã—16æ½œåœ¨ç‰¹å¾
python step5_extract_latent_features.py \
    --dataset_dir micro_doppler_dataset \
    --checkpoint_path LightningDiT/vavae/logs/[å®é™…æ—¶é—´æˆ³]/checkpoints/last.ckpt \
    --output_dir micro_doppler_latents

# éªŒè¯ç‰¹å¾æå–ç»“æœ
ls micro_doppler_latents/
# åº”è¯¥çœ‹åˆ°ï¼š
# â”œâ”€â”€ train/
# â”‚   â”œâ”€â”€ user1/ user2/ ... user31/ (æ¯ä¸ªåŒ…å«.ptç‰¹å¾æ–‡ä»¶)
# â”œâ”€â”€ val/
# â”‚   â”œâ”€â”€ user1/ user2/ ... user31/
# â””â”€â”€ latent_dataset_config.yaml
```

---

### **æ‰©æ•£æ¨¡å‹è®­ç»ƒé˜¶æ®µ**

#### **Step 6: è®­ç»ƒLightningDiTæ‰©æ•£æ¨¡å‹** (1-2å‘¨)
```bash
# è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œæ”¯æŒ31ä¸ªç”¨æˆ·æ¡ä»¶ç”Ÿæˆ
python step6_train_diffusion_model.py \
    --latent_dir micro_doppler_latents \
    --output_dir lightningdit_trained \
    --num_users 31 \
    --batch_size 2

# ç›‘æ§è®­ç»ƒ
tensorboard --logdir lightningdit_trained/logs/
```

---

### **ç”Ÿæˆæµ‹è¯•é˜¶æ®µ**

#### **Step 7: ç”Ÿæˆå¾®å¤šæ™®å‹’å›¾åƒ** (10åˆ†é’Ÿ)
```bash
# ç”ŸæˆæŒ‡å®šç”¨æˆ·çš„æ—¶é¢‘å›¾
python step7_generate_micro_doppler.py \
    --diffusion_model lightningdit_trained/checkpoints/best.ckpt \
    --vae_model LightningDiT/vavae/logs/[æ—¶é—´æˆ³]/checkpoints/last.ckpt \
    --user_id 5 \
    --num_samples 10 \
    --output_dir generated_samples

# æ‰¹é‡ç”Ÿæˆæ‰€æœ‰ç”¨æˆ· (æ•°æ®å¢å¹¿)
for user_id in {1..31}; do
    python step7_generate_micro_doppler.py \
        --diffusion_model lightningdit_trained/checkpoints/best.ckpt \
        --vae_model LightningDiT/vavae/logs/[æ—¶é—´æˆ³]/checkpoints/last.ckpt \
        --user_id $user_id \
        --num_samples 50 \
        --output_dir generated_samples/user_$user_id
done
```

---

## â±ï¸ **æ—¶é—´å’Œèµ„æºé¢„ä¼°**

| é˜¶æ®µ | æ­¥éª¤ | é¢„è®¡æ—¶é—´ | GPUä½¿ç”¨ | å…³é”®è¾“å‡º |
|------|------|----------|---------|----------|
| ç¯å¢ƒå‡†å¤‡ | Step 1-2 | 30åˆ†é’Ÿ | - | å®Œæ•´ä¾èµ–ç¯å¢ƒ |
| æ•°æ®å‡†å¤‡ | Step 3 | 10åˆ†é’Ÿ | - | è®­ç»ƒ/éªŒè¯æ•°æ®é›† |
| VA-VAEå¾®è°ƒ | Step 4 | **2-3å¤©** | T4Ã—2 | å¾®è°ƒVA-VAEæ¨¡å‹ |
| ç‰¹å¾æå– | Step 5 | 2-3å°æ—¶ | T4Ã—1 | æ½œåœ¨ç‰¹å¾æ•°æ®é›† |
| æ‰©æ•£è®­ç»ƒ | Step 6 | **1-2å‘¨** | T4Ã—2 | æ¡ä»¶æ‰©æ•£æ¨¡å‹ |
| ç”Ÿæˆæµ‹è¯• | Step 7 | 10åˆ†é’Ÿ | T4Ã—1 | å¢å¹¿æ—¶é¢‘å›¾ |

---

## ğŸ” **å…³é”®æ£€æŸ¥ç‚¹**

### **Step 3 å®Œæˆæ£€æŸ¥**
```bash
cat micro_doppler_dataset/split_info.txt
# åº”è¯¥æ˜¾ç¤º31ä¸ªç”¨æˆ·çš„8:2è®­ç»ƒ/éªŒè¯åˆ†å¸ƒ
```

### **Step 4 å®Œæˆæ£€æŸ¥**
```bash
# æ£€æŸ¥VA-VAEå¾®è°ƒæ•ˆæœ
python -c "
import torch
ckpt_path = 'LightningDiT/vavae/logs/[æ—¶é—´æˆ³]/checkpoints/last.ckpt'
ckpt = torch.load(ckpt_path, map_location='cpu')
print('VA-VAEå¾®è°ƒå®Œæˆï¼Œepoch:', ckpt.get('epoch', 'unknown'))
print('éªŒè¯æŸå¤±:', ckpt.get('val_loss', 'unknown'))
"
```

### **Step 5 å®Œæˆæ£€æŸ¥**
```bash
# æ£€æŸ¥æ½œåœ¨ç‰¹å¾
python -c "
import torch
from pathlib import Path
latent_files = list(Path('micro_doppler_latents/train/user1').glob('*.pt'))
if latent_files:
    sample = torch.load(latent_files[0])
    print('æ½œåœ¨ç‰¹å¾å½¢çŠ¶:', sample['latent'].shape)  # åº”è¯¥æ˜¯ [32, 16, 16]
    print('ç”¨æˆ·ID:', sample['user_id'])  # åº”è¯¥æ˜¯ 0-30
else:
    print('æœªæ‰¾åˆ°æ½œåœ¨ç‰¹å¾æ–‡ä»¶')
"
```

---

## ğŸš¨ **æ•…éšœæ’é™¤**

### **å†…å­˜ä¸è¶³**
```bash
# åœ¨step4_finetune_vavae.pyä¸­è°ƒæ•´ï¼š
# batch_size: 1 â†’ batch_size: 1 (å·²ç»æ˜¯æœ€å°)
# accumulate_grad_batches: 4 â†’ accumulate_grad_batches: 8
```

### **è®­ç»ƒä¸­æ–­æ¢å¤**
```bash
# VA-VAEè®­ç»ƒæ¢å¤
python step4_finetune_vavae.py \
    --dataset_dir micro_doppler_dataset \
    --resume_from_checkpoint /path/to/checkpoint

# æ‰©æ•£æ¨¡å‹è®­ç»ƒæ¢å¤
python step6_train_diffusion_model.py \
    --latent_dir micro_doppler_latents \
    --resume_from_checkpoint /path/to/checkpoint
```

---

## ğŸ¯ **æœ€ç»ˆç›®æ ‡**

å®Œæˆè®­ç»ƒåï¼Œæ‚¨å°†è·å¾—ï¼š
1. **å¾®è°ƒçš„VA-VAE** - ä¸“é—¨ç¼–ç /è§£ç å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾
2. **æ¡ä»¶æ‰©æ•£æ¨¡å‹** - å¯æŒ‡å®šç”¨æˆ·IDç”Ÿæˆè¯¥ç”¨æˆ·çš„æ–°æ—¶é¢‘å›¾
3. **æ•°æ®å¢å¹¿èƒ½åŠ›** - ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆå¤§é‡é«˜è´¨é‡çš„æ–°æ ·æœ¬
4. **ç”¨æˆ·ç‰¹å¾ä¿æŒ** - ç”Ÿæˆçš„å›¾åƒä¿æŒæŒ‡å®šç”¨æˆ·çš„æ­¥æ€ç‰¹å¾

è¿™æ ·å°±å®Œæˆäº†å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾çš„æ•°æ®å¢å¹¿ç³»ç»Ÿï¼ğŸš€

"""
æœ€å°åŒ–VA-VAEä¿®æ”¹ - ä»…æ·»åŠ ç”¨æˆ·æ¡ä»¶åŠŸèƒ½
åŸºäºLightningDiT/tokenizer/vavae.pyçš„æœ€å°ä¿®æ”¹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class UserConditionedVAVAE(nn.Module):
    """
    ç”¨æˆ·æ¡ä»¶åŒ–çš„VA-VAE
    åœ¨åŸæœ‰VA-VAEåŸºç¡€ä¸Šæ·»åŠ æœ€ç®€å•çš„ç”¨æˆ·æ¡ä»¶åŠŸèƒ½
    """
    
    def __init__(self, original_vavae, num_users=31, condition_dim=128):
        """
        åŸºäºåŸæœ‰VA-VAEæ·»åŠ ç”¨æˆ·æ¡ä»¶
        
        Args:
            original_vavae: åŸå§‹çš„VA-VAEæ¨¡å‹
            num_users: ç”¨æˆ·æ•°é‡
            condition_dim: æ¡ä»¶å‘é‡ç»´åº¦
        """
        super().__init__()
        
        # ä¿æŒåŸæœ‰çš„VA-VAEç»“æ„
        self.encoder = original_vavae.encoder
        self.decoder = original_vavae.decoder
        self.quant_conv = original_vavae.quant_conv
        self.post_quant_conv = original_vavae.post_quant_conv
        
        # ä»…æ·»åŠ ç”¨æˆ·åµŒå…¥å±‚
        self.num_users = num_users
        self.condition_dim = condition_dim
        self.user_embedding = nn.Embedding(num_users, condition_dim)
        
        # ç®€å•çš„æ¡ä»¶èåˆå±‚
        # è·å–ç¼–ç å™¨è¾“å‡ºçš„é€šé“æ•°
        encoder_out_channels = self._get_encoder_out_channels()
        self.condition_proj = nn.Linear(condition_dim, encoder_out_channels)
        
        print(f"æ·»åŠ ç”¨æˆ·æ¡ä»¶: {num_users}ä¸ªç”¨æˆ·, æ¡ä»¶ç»´åº¦: {condition_dim}")

        # è°ƒè¯•æ ‡å¿—
        self._debug_first_call = False
        self._debug_user_ids = False
    
    def _get_encoder_out_channels(self):
        """è·å–ç¼–ç å™¨è¾“å‡ºé€šé“æ•°"""
        # åˆ›å»ºä¸€ä¸ªdummyè¾“å…¥æ¥è·å–ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶
        dummy_input = torch.randn(1, 3, 256, 256)
        with torch.no_grad():
            encoder_out = self.encoder(dummy_input)
            return encoder_out.shape[1]
    
    def encode(self, x, user_ids=None):
        """
        ç¼–ç  - æ·»åŠ ç”¨æˆ·æ¡ä»¶

        Args:
            x: è¾“å…¥å›¾åƒ (B, 3, H, W)
            user_ids: ç”¨æˆ·ID (B,)

        Returns:
            posterior: åéªŒåˆ†å¸ƒ
        """
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥è¾“å…¥ç»´åº¦
        if hasattr(self, '_debug_first_call') and not self._debug_first_call:
            print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - è¾“å…¥å¼ é‡ç»´åº¦: {x.shape}")
            print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - è¾“å…¥å¼ é‡æ•°æ®ç±»å‹: {x.dtype}")
            print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - è¾“å…¥å¼ é‡èŒƒå›´: [{x.min():.3f}, {x.max():.3f}]")
            self._debug_first_call = True

        # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        if x.dim() == 4 and x.shape[1] != 3:
            if x.shape[3] == 3:
                # (B, H, W, C) -> (B, C, H, W)
                x = x.permute(0, 3, 1, 2)
                print(f"âš ï¸ ä¿®æ­£è¾“å…¥ç»´åº¦: {x.shape}")

        # åŸæœ‰çš„ç¼–ç è¿‡ç¨‹
        h = self.encoder(x)
        
        # æ·»åŠ ç”¨æˆ·æ¡ä»¶ (å¦‚æœæä¾›)
        if user_ids is not None:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç”¨æˆ·IDèŒƒå›´
            if hasattr(self, '_debug_user_ids') and not self._debug_user_ids:
                print(f"ğŸ” ç”¨æˆ·IDè°ƒè¯• - åŸå§‹ç”¨æˆ·IDèŒƒå›´: [{user_ids.min().item()}, {user_ids.max().item()}]")
                print(f"ğŸ” ç”¨æˆ·IDè°ƒè¯• - åµŒå…¥å±‚å¤§å°: {self.user_embedding.num_embeddings}")
                self._debug_user_ids = True

            # è·å–ç”¨æˆ·åµŒå…¥ (ç”¨æˆ·IDä»1å¼€å§‹ï¼Œéœ€è¦è½¬æ¢ä¸º0å¼€å§‹çš„ç´¢å¼•)
            user_indices = user_ids - 1  # å°†1-31è½¬æ¢ä¸º0-30

            # éªŒè¯ç´¢å¼•èŒƒå›´
            assert user_indices.min() >= 0, f"ç”¨æˆ·ç´¢å¼•è¿‡å°: {user_indices.min()}"
            assert user_indices.max() < self.user_embedding.num_embeddings, f"ç”¨æˆ·ç´¢å¼•è¿‡å¤§: {user_indices.max()}, åµŒå…¥å±‚å¤§å°: {self.user_embedding.num_embeddings}"

            user_emb = self.user_embedding(user_indices)  # (B, condition_dim)
            
            # æŠ•å½±åˆ°ç¼–ç å™¨è¾“å‡ºç©ºé—´
            user_cond = self.condition_proj(user_emb)  # (B, encoder_channels)
            
            # ç®€å•çš„ç‰¹å¾èåˆ: é€å…ƒç´ ç›¸åŠ 
            # æ‰©å±•ç»´åº¦ä»¥åŒ¹é…ç©ºé—´ç»´åº¦
            user_cond = user_cond.unsqueeze(-1).unsqueeze(-1)  # (B, C, 1, 1)
            h = h + user_cond  # å¹¿æ’­ç›¸åŠ 
        
        # åŸæœ‰çš„é‡åŒ–å·ç§¯
        moments = self.quant_conv(h)
        
        # è¿”å›åéªŒåˆ†å¸ƒ (ä¿æŒåŸæœ‰æ ¼å¼)
        return DiagonalGaussianDistribution(moments)
    
    def decode(self, z, user_ids=None):
        """
        è§£ç  - æ·»åŠ ç”¨æˆ·æ¡ä»¶
        
        Args:
            z: æ½œåœ¨å˜é‡
            user_ids: ç”¨æˆ·ID
            
        Returns:
            é‡å»ºå›¾åƒ
        """
        # åŸæœ‰çš„åé‡åŒ–å·ç§¯
        z = self.post_quant_conv(z)
        
        # æ·»åŠ ç”¨æˆ·æ¡ä»¶ (å¦‚æœæä¾›)
        if user_ids is not None:
            # è·å–ç”¨æˆ·åµŒå…¥
            user_emb = self.user_embedding(user_ids)
            
            # æŠ•å½±åˆ°æ½œåœ¨ç©ºé—´
            user_cond = self.condition_proj(user_emb)
            user_cond = user_cond.unsqueeze(-1).unsqueeze(-1)
            z = z + user_cond
        
        # åŸæœ‰çš„è§£ç è¿‡ç¨‹
        dec = self.decoder(z)
        return dec
    
    def forward(self, input, user_ids=None, sample_posterior=True):
        """
        å‰å‘ä¼ æ’­ - ä¿æŒåŸæœ‰æ¥å£ï¼Œæ·»åŠ ç”¨æˆ·æ¡ä»¶
        
        Args:
            input: è¾“å…¥å›¾åƒ
            user_ids: ç”¨æˆ·ID
            sample_posterior: æ˜¯å¦ä»åéªŒé‡‡æ ·
            
        Returns:
            é‡å»ºå›¾åƒå’Œå…¶ä»–ä¿¡æ¯
        """
        posterior = self.encode(input, user_ids)
        
        if sample_posterior:
            z = posterior.sample()
        else:
            z = posterior.mode()
        
        dec = self.decode(z, user_ids)
        
        return dec, posterior
    
    def get_last_layer(self):
        """è·å–æœ€åä¸€å±‚ - ä¿æŒåŸæœ‰æ¥å£"""
        return self.decoder.conv_out.weight
    
    def sample(self, user_ids, num_samples=1, device='cuda'):
        """
        ä»å…ˆéªŒåˆ†å¸ƒé‡‡æ ·ç”Ÿæˆ
        
        Args:
            user_ids: ç”¨æˆ·ID tensor (B,)
            num_samples: æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•°
            device: è®¾å¤‡
            
        Returns:
            ç”Ÿæˆçš„å›¾åƒ
        """
        batch_size = user_ids.size(0)
        
        # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·
        # éœ€è¦çŸ¥é“æ½œåœ¨ç©ºé—´çš„å½¢çŠ¶ï¼Œè¿™é‡Œå‡è®¾æ˜¯ (B, C, H, W)
        # å¯ä»¥é€šè¿‡ç¼–ç ä¸€ä¸ªæ ·æœ¬æ¥è·å–å½¢çŠ¶
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, 256, 256, device=device)
            dummy_posterior = self.encode(dummy_input)
            latent_shape = dummy_posterior.sample().shape[1:]  # (C, H, W)
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆæ ·æœ¬
        all_samples = []
        for i in range(num_samples):
            z = torch.randn(batch_size, *latent_shape, device=device)
            samples = self.decode(z, user_ids)
            all_samples.append(samples)
        
        return torch.cat(all_samples, dim=0)


# ç”¨äºå…¼å®¹åŸæœ‰ä»£ç çš„åéªŒåˆ†å¸ƒç±»
class DiagonalGaussianDistribution:
    """å¯¹è§’é«˜æ–¯åˆ†å¸ƒ - ä¿æŒä¸åŸæœ‰ä»£ç å…¼å®¹"""
    
    def __init__(self, parameters):
        self.parameters = parameters
        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)
        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)
        self.std = torch.exp(0.5 * self.logvar)
        self.var = torch.exp(self.logvar)
    
    def sample(self):
        x = self.mean + self.std * torch.randn_like(self.mean)
        return x
    
    def mode(self):
        return self.mean
    
    def kl(self, other=None):
        if other is None:
            return 0.5 * torch.sum(torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar, dim=[1, 2, 3])
        else:
            return 0.5 * torch.sum(
                torch.pow(self.mean - other.mean, 2) / other.var + 
                self.var / other.var - 1.0 - self.logvar + other.logvar,
                dim=[1, 2, 3]
            )


def create_user_conditioned_vavae(original_vavae_path, num_users=31, condition_dim=128):
    """
    åˆ›å»ºç”¨æˆ·æ¡ä»¶åŒ–çš„VA-VAE
    
    Args:
        original_vavae_path: åŸå§‹VA-VAEæ¨¡å‹è·¯å¾„
        num_users: ç”¨æˆ·æ•°é‡
        condition_dim: æ¡ä»¶ç»´åº¦
        
    Returns:
        ç”¨æˆ·æ¡ä»¶åŒ–çš„VA-VAEæ¨¡å‹
    """
    # åŠ è½½åŸå§‹VA-VAE
    print(f"åŠ è½½åŸå§‹VA-VAEæ¨¡å‹: {original_vavae_path}")
    checkpoint = torch.load(original_vavae_path, map_location='cpu')
    
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„LightningDiTæ¨¡å‹ç»“æ„æ¥è°ƒæ•´
    # å‡è®¾åŸå§‹æ¨¡å‹åœ¨checkpoint['model']ä¸­
    original_vavae = checkpoint['model']  # æˆ–è€…å…¶ä»–é”®å
    
    # åˆ›å»ºæ¡ä»¶åŒ–æ¨¡å‹
    conditioned_model = UserConditionedVAVAE(
        original_vavae=original_vavae,
        num_users=num_users,
        condition_dim=condition_dim
    )
    
    print(f"æˆåŠŸåˆ›å»ºç”¨æˆ·æ¡ä»¶åŒ–VA-VAEï¼Œæ”¯æŒ{num_users}ä¸ªç”¨æˆ·")
    
    return conditioned_model


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•ç”¨æˆ·æ¡ä»¶åŒ–VA-VAE
    print("æµ‹è¯•ç”¨æˆ·æ¡ä»¶åŒ–VA-VAE...")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®é™…çš„VA-VAEæ¨¡å‹æ¥æµ‹è¯•
    
    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 4
    num_users = 31
    
    # æ¨¡æ‹Ÿå›¾åƒå’Œç”¨æˆ·ID
    images = torch.randn(batch_size, 3, 256, 256)
    user_ids = torch.randint(0, num_users, (batch_size,))
    
    print(f"è¾“å…¥å›¾åƒå½¢çŠ¶: {images.shape}")
    print(f"ç”¨æˆ·ID: {user_ids}")
    
    # è¿™é‡Œéœ€è¦å®é™…çš„æ¨¡å‹æ¥æµ‹è¯•
    # conditioned_model = create_user_conditioned_vavae("path/to/vavae.pth")
    # output, posterior = conditioned_model(images, user_ids)
    # print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    print("æµ‹è¯•å®Œæˆï¼")

"""
åˆ†å¸ƒå¼æ¡ä»¶æ‰©æ•£æ ·æœ¬ç”Ÿæˆè„šæœ¬ (torchrunç‰ˆæœ¬)
ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆ200å¼ æ¡ä»¶æ ·æœ¬ï¼Œç”¨äºè¯„ä¼°ç”¨æˆ·åŒºåˆ†åº¦
"""
import torch
import torch.distributed as dist
import sys
import os
import yaml
import numpy as np
from PIL import Image
from tqdm import tqdm
import argparse
from pathlib import Path

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from models.lightningdit import LightningDiT_models
from transport import create_transport, Sampler
from simplified_vavae import SimplifiedVAVAE

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if 'RANK' in os.environ:
        rank = int(os.environ['RANK'])
        local_rank = int(os.environ['LOCAL_RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend='nccl')
        
        return rank, local_rank, world_size
    else:
        return 0, 0, 1

def load_weights_with_shape_check(model, checkpoint, rank=0):
    """ä½¿ç”¨å½¢çŠ¶æ£€æŸ¥åŠ è½½æƒé‡ï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹å®ç°ï¼‰"""
    model_state_dict = model.state_dict()
    # check shape and load weights
    for name, param in checkpoint['model'].items():
        if name in model_state_dict:
            if param.shape == model_state_dict[name].shape:
                model_state_dict[name].copy_(param)
            elif name == 'x_embedder.proj.weight':
                # special case for x_embedder.proj.weight
                # the pretrained model is trained with 256x256 images
                # we can load the weights by resizing the weights
                # and keep the first 3 channels the same
                weight = torch.zeros_like(model_state_dict[name])
                weight[:, :16] = param[:, :16]
                model_state_dict[name] = weight
            else:
                if rank == 0:
                    print(f"Skipping loading parameter '{name}' due to shape mismatch: "
                        f"checkpoint shape {param.shape}, model shape {model_state_dict[name].shape}")
        else:
            if rank == 0:
                print(f"Parameter '{name}' not found in model, skipping.")
    # load state dict
    model.load_state_dict(model_state_dict, strict=False)
    
    return model

def load_model_and_config(checkpoint_path, config_path, local_rank):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®ï¼ˆæŒ‰ç…§å®˜æ–¹æ–¹å¼ï¼‰"""
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device(f'cuda:{local_rank}')
    
    # åˆ›å»ºDiTæ¨¡å‹
    latent_size = config['data']['image_size'] // config['vae']['downsample_ratio']
    model = LightningDiT_models[config['model']['model_type']](
        input_size=latent_size,
        num_classes=config['data']['num_classes'],
        class_dropout_prob=config['model'].get('class_dropout_prob', 0.1),
        use_qknorm=config['model']['use_qknorm'],
        use_swiglu=config['model'].get('use_swiglu', False),
        use_rope=config['model'].get('use_rope', False),
        use_rmsnorm=config['model'].get('use_rmsnorm', False),
        wo_shift=config['model'].get('wo_shift', False),
        in_channels=config['model'].get('in_chans', 4),
        use_checkpoint=config['model'].get('use_checkpoint', False),
    ).to(device)
    
    # æŒ‰ç…§å®˜æ–¹æ–¹å¼åŠ è½½æƒé‡
    if os.path.exists(checkpoint_path):
        if local_rank == 0:
            print(f"ğŸ“¦ ä»checkpointåŠ è½½æƒé‡: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)
        
        # å¤„ç†æƒé‡é”®åï¼ˆæŒ‰ç…§å®˜æ–¹æ–¹å¼ï¼‰
        if 'ema' in checkpoint:
            # ä¼˜å…ˆä½¿ç”¨EMAæƒé‡ï¼ˆæ¨ç†æ—¶æ›´ç¨³å®šï¼‰
            checkpoint_weights = {'model': checkpoint['ema']}
            if local_rank == 0:
                print("ğŸ“¦ ä½¿ç”¨EMAæƒé‡è¿›è¡Œæ¨ç†")
        elif 'model' in checkpoint:
            checkpoint_weights = checkpoint
            if local_rank == 0:
                print("ğŸ“¦ ä½¿ç”¨æ¨¡å‹æƒé‡è¿›è¡Œæ¨ç†")
        else:
            checkpoint_weights = {'model': checkpoint}
            if local_rank == 0:
                print("ğŸ“¦ ä½¿ç”¨ç›´æ¥æƒé‡è¿›è¡Œæ¨ç†")
        
        # æ¸…ç†é”®åï¼ˆremove the prefix 'module.'ï¼‰
        checkpoint_weights['model'] = {k.replace('module.', ''): v for k, v in checkpoint_weights['model'].items()}
        
        # ä½¿ç”¨å®˜æ–¹æƒé‡åŠ è½½å‡½æ•°
        model = load_weights_with_shape_check(model, checkpoint_weights, rank=local_rank)
        
        if local_rank == 0:
            print("âœ… æƒé‡åŠ è½½å®Œæˆ")
    else:
        if local_rank == 0:
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°checkpointæ–‡ä»¶ {checkpoint_path}")
            print("âš ï¸ ä½¿ç”¨æœªè®­ç»ƒçš„éšæœºæƒé‡ï¼Œç”Ÿæˆç»“æœå°†æ˜¯å™ªå£°ï¼")
    
    model.eval()
    
    # åˆ›å»ºVAEï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹train_dit_s_official.pyæ–¹å¼ï¼‰
    vae = None
    try:
        # æ·»åŠ LightningDiTè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
        import sys
        lightningdit_path = os.path.join(os.getcwd(), 'LightningDiT')
        if lightningdit_path not in sys.path:
            sys.path.insert(0, lightningdit_path)
        
        from tokenizer.vavae import VA_VAE
        import tempfile
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„VAEæ¨¡å‹è·¯å¾„
        custom_vae_checkpoint = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
        
        # åˆ›å»ºä¸train_dit_s_official.pyå®Œå…¨ä¸€è‡´çš„é…ç½®
        vae_config = {
            'ckpt_path': custom_vae_checkpoint,
            'model': {
                'base_learning_rate': 2.0e-05,
                'target': 'ldm.models.autoencoder.AutoencoderKL',
                'params': {
                    'monitor': 'val/rec_loss',
                    'embed_dim': 32,
                    'use_vf': 'dinov2',
                    'reverse_proj': True,
                    'ddconfig': {
                        'double_z': True, 'z_channels': 32, 'resolution': 256,
                        'in_channels': 3, 'out_ch': 3, 'ch': 128,
                        'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2,
                        'attn_resolutions': [16], 'dropout': 0.0
                    },
                    'lossconfig': {
                        'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                        'params': {
                            'disc_start': 1, 'disc_num_layers': 3, 'disc_weight': 0.5,
                            'disc_factor': 1.0, 'disc_in_channels': 3, 'disc_conditional': False,
                            'disc_loss': 'hinge', 'pixelloss_weight': 1.0, 'perceptual_weight': 1.0,
                            'kl_weight': 1e-6, 'logvar_init': 0.0, 'use_actnorm': False,
                            'pp_style': False, 'vf_weight': 0.1, 'adaptive_vf': False,
                            'distmat_weight': 1.0, 'cos_weight': 1.0,
                            'distmat_margin': 0.25, 'cos_margin': 0.5
                        }
                    }
                }
            }
        }
        
        # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
        temp_config_fd, temp_config_path = tempfile.mkstemp(suffix='.yaml')
        with open(temp_config_path, 'w') as f:
            yaml.dump(vae_config, f, default_flow_style=False)
        os.close(temp_config_fd)
        
        try:
            # ä½¿ç”¨å®˜æ–¹VA_VAEç±»åŠ è½½
            vae = VA_VAE(temp_config_path)
            vae = vae.to(device)
            vae.eval()
            if local_rank == 0:
                print(f"âœ… VAEåŠ è½½å®Œæˆ: {custom_vae_checkpoint}")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_config_path)
            
    except Exception as e:
        if local_rank == 0:
            print(f"âš ï¸ VAEåŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print("âš ï¸ å°è¯•ä½¿ç”¨ç®€åŒ–VAEä½œä¸ºå¤‡ç”¨")
        # å¤‡ç”¨æ–¹æ¡ˆ
        try:
            vae = SimplifiedVAVAE(config['vae']['model_name']).to(device)
            vae.eval()
            if local_rank == 0:
                print(f"âœ… å¤‡ç”¨VAEåŠ è½½å®Œæˆ: {config['vae']['model_name']}")
        except Exception as e2:
            if local_rank == 0:
                print(f"âš ï¸ å¤‡ç”¨VAEä¹ŸåŠ è½½å¤±è´¥: {e2}")
            vae = None
    
    # åˆ›å»ºtransport
    transport = create_transport(
        config['transport']['path_type'],
        config['transport']['prediction'],
        config['transport']['loss_weight'],
        config['transport']['train_eps'],
        config['transport']['sample_eps'],
        use_cosine_loss=config['transport'].get('use_cosine_loss', False),
        use_lognorm=config['transport'].get('use_lognorm', False),
        partitial_train=config['transport'].get('partitial_train', None),
        partial_ratio=config['transport'].get('partial_ratio', 1.0),
        shift_lg=config['transport'].get('shift_lg', False),
    )
    
    return model, vae, transport, config, device

def generate_samples_for_user_distributed(model, vae, transport, sampler, user_id, num_samples, 
                                        output_dir, cfg_scale=10.0, seed=42, batch_size=16, 
                                        rank=0, world_size=1):
    """åˆ†å¸ƒå¼ç”ŸæˆæŒ‡å®šç”¨æˆ·çš„æ¡ä»¶æ ·æœ¬"""
    # åˆ›å»ºé‡‡æ ·å™¨å’Œé‡‡æ ·å‡½æ•° - å®Œå…¨æŒ‰ç…§é…ç½®æ–‡ä»¶
    sampler = Sampler(transport)
    sample_fn = sampler.sample_ode(
        sampling_method="dopri5",      # é«˜ç²¾åº¦ODEæ±‚è§£å™¨
        num_steps=300,                 # é‡‡æ ·æ­¥æ•°ï¼ˆä¸é…ç½®æ–‡ä»¶ä¸€è‡´ï¼‰
        atol=1e-6,                     # ç»å¯¹è¯¯å·®å®¹é™
        rtol=1e-3,                     # ç›¸å¯¹è¯¯å·®å®¹é™
        reverse=False,                 # ä¸åå‘é‡‡æ ·
        timestep_shift=0.1             # æ—¶é—´æ­¥åç§»ï¼ˆå®˜æ–¹è®¾ç½®ï¼‰
    )
    
    # è®¡ç®—æ¯ä¸ªè¿›ç¨‹å¤„ç†çš„æ ·æœ¬æ•°
    samples_per_rank = num_samples // world_size
    start_idx = rank * samples_per_rank
    end_idx = start_idx + samples_per_rank
    if rank == world_size - 1:  # æœ€åä¸€ä¸ªè¿›ç¨‹å¤„ç†å‰©ä½™æ ·æœ¬
        end_idx = num_samples
    
    actual_samples = end_idx - start_idx
    
    torch.manual_seed(seed + user_id + rank * 1000)
    np.random.seed(seed + user_id + rank * 1000)
    
    device = next(model.parameters()).device
    user_dir = Path(output_dir) / f"user_{user_id:02d}"
    user_dir.mkdir(parents=True, exist_ok=True)
    
    if rank == 0:
        print(f"ğŸ¨ ç”Ÿæˆç”¨æˆ· {user_id} çš„æ ·æœ¬ (æ€»è®¡{num_samples}å¼ , {world_size}å¡å¹¶è¡Œ)...")
    
    with torch.no_grad():
        num_batches = (actual_samples + batch_size - 1) // batch_size
        sample_idx = start_idx
        
        for batch_idx in tqdm(range(num_batches), desc=f"Rank{rank}-User{user_id}", 
                             disable=(rank != 0)):
            # è®¡ç®—å½“å‰batchå¤§å°
            current_batch_size = min(batch_size, end_idx - sample_idx)
            if current_batch_size <= 0:
                break
            
            # å‡†å¤‡æ¡ä»¶
            y = torch.tensor([user_id] * current_batch_size, device=device)
            
            # åˆ›å»ºéšæœºå™ªå£°
            z = torch.randn(current_batch_size, 32, 16, 16, device=device)
            
            # CFGé‡‡æ · - å®Œå…¨æŒ‰ç…§å®˜æ–¹train_dit_s_official.pyå®ç°
            if cfg_scale > 1.0:
                # æ„å»ºCFG batch
                z_cfg = torch.cat([z, z], 0)
                y_null = torch.tensor([31] * current_batch_size, device=device)  # null class
                y_cfg = torch.cat([y, y_null], 0)
                
                # ä½¿ç”¨å®˜æ–¹CFGé…ç½®
                cfg_interval_start = 0.11  # ä¸å®˜æ–¹ä¿æŒä¸€è‡´
                model_kwargs = dict(y=y_cfg, cfg_scale=cfg_scale, cfg_interval=True, cfg_interval_start=cfg_interval_start)
                
                # ä½¿ç”¨CFGå‰å‘ä¼ æ’­ï¼ˆä¸å®˜æ–¹å®Œå…¨ä¸€è‡´ï¼‰
                if hasattr(model, 'forward_with_cfg'):
                    samples = sample_fn(z_cfg, model.forward_with_cfg, **model_kwargs)
                else:
                    # å¦‚æœæ¨¡å‹æ²¡æœ‰forward_with_cfgæ–¹æ³•ï¼Œä½¿ç”¨æ‰‹åŠ¨CFG
                    def model_fn_cfg(x, t, **kwargs):
                        pred = model(x, t, **kwargs)
                        pred_cond, pred_uncond = pred.chunk(2, dim=0)
                        return pred_uncond + cfg_scale * (pred_cond - pred_uncond)
                    samples = sample_fn(z_cfg, model_fn_cfg, **model_kwargs)
                
                samples = samples[-1]  # è·å–æœ€ç»ˆæ—¶é—´æ­¥çš„æ ·æœ¬
                samples, _ = samples.chunk(2, dim=0)  # å»æ‰null classæ ·æœ¬
            else:
                # æ ‡å‡†é‡‡æ ·
                samples = sample_fn(z, model, **dict(y=y))
                samples = samples[-1]
            
            # æ£€æŸ¥latentèŒƒå›´ (ä»…rank 0è¾“å‡º)
            if rank == 0 and batch_idx == 0:
                print(f"ğŸ” ç”Ÿæˆçš„LatentèŒƒå›´: [{samples.min():.3f}, {samples.max():.3f}], æ ‡å‡†å·®: {samples.std():.3f}")
            
            # ğŸ”´ å…³é”®æ­¥éª¤ï¼šåå½’ä¸€åŒ–ï¼ï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹train_dit_s_official.pyå®ç°ï¼‰
            # å®˜æ–¹å…¬å¼ï¼šsamples_denorm = (samples * std) / latent_multiplier + mean
            # å› ä¸ºè®­ç»ƒæ—¶åšäº†ï¼šfeature = (feature - mean) / std * latent_multiplier
            latent_stats_path = '/kaggle/working/VA-VAE/latents_safetensors/train/latent_stats.pt'
            if os.path.exists(latent_stats_path):
                stats = torch.load(latent_stats_path, map_location=device)
                mean = stats['mean'].to(device)  # [32, 1, 1]
                std = stats['std'].to(device)     # [32, 1, 1]
                latent_multiplier = 1.0  # VA-VAEä½¿ç”¨1.0ï¼Œä¸æ˜¯0.18215
                
                # å®˜æ–¹åå½’ä¸€åŒ–å…¬å¼ï¼ˆä¸train_dit_s_official.pyç¬¬549è¡Œå®Œå…¨ä¸€è‡´ï¼‰
                samples_denorm = (samples * std) / latent_multiplier + mean
                
                if rank == 0 and batch_idx == 0:
                    print(f"ğŸ” åå½’ä¸€åŒ–åèŒƒå›´: [{samples_denorm.min():.3f}, {samples_denorm.max():.3f}], æ ‡å‡†å·®: {samples_denorm.std():.3f}")
                    print(f"ğŸ“Š ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯: mean shape={mean.shape}, std shape={std.shape}")
            else:
                if rank == 0:
                    print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°latentç»Ÿè®¡æ–‡ä»¶ {latent_stats_path}")
                    print(f"âš ï¸ è·³è¿‡åå½’ä¸€åŒ–æ­¥éª¤ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆå™ªå£°ï¼")
                    print(f"ğŸ’¡ å°è¯•ä»æ•°æ®é›†ç›´æ¥è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
                    # å¦‚æœç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ•°æ®é›†ç›´æ¥è®¡ç®—
                    try:
                        from LightningDiT.datasets.img_latent_dataset import ImgLatentDataset
                        train_dataset = ImgLatentDataset('./latents_safetensors/train', latent_norm=True)
                        stats = train_dataset.compute_latent_stats()
                        mean = stats['mean'].to(device)  # [1, 32, 1, 1]
                        std = stats['std'].to(device)    # [1, 32, 1, 1]
                        # å»æ‰batchç»´åº¦
                        mean = mean.squeeze(0)  # [32, 1, 1]
                        std = std.squeeze(0)    # [32, 1, 1]
                        
                        # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶ä¾›ä¸‹æ¬¡ä½¿ç”¨
                        os.makedirs('./latents_safetensors/train', exist_ok=True)
                        torch.save({'mean': mean, 'std': std}, './latents_safetensors/train/latents_stats.pt')
                        print(f"âœ… ä»æ•°æ®é›†è®¡ç®—ç»Ÿè®¡å®Œæˆï¼Œå·²ä¿å­˜åˆ° ./latents_safetensors/train/latents_stats.pt")
                        
                        # åå½’ä¸€åŒ–ï¼ˆä¸å®˜æ–¹å…¬å¼ä¿æŒä¸€è‡´ï¼‰
                        latent_multiplier = 1.0  # VA-VAEä½¿ç”¨1.0
                        samples_denorm = (samples * std) / latent_multiplier + mean
                        
                        if rank == 0 and batch_idx == 0:
                            print(f"ğŸ” åå½’ä¸€åŒ–åèŒƒå›´: [{samples_denorm.min():.3f}, {samples_denorm.max():.3f}], æ ‡å‡†å·®: {samples_denorm.std():.3f}")
                            print(f"ğŸ“Š ä½¿ç”¨latentç»Ÿè®¡ä¿¡æ¯: mean shape={mean.shape}, std shape={std.shape}")
                            print(f"âœ… åå½’ä¸€åŒ–å…¬å¼ä¸å®˜æ–¹train_dit_s_official.pyå®Œå…¨ä¸€è‡´")
                    except Exception as e:
                        if rank == 0:
                            print(f"âŒ æ— æ³•è®¡ç®—ç»Ÿè®¡ä¿¡æ¯: {e}")
                            print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ: python prepare_latent_stats.py --data_dir ./latents_safetensors/train")
                        samples_denorm = samples
                else:
                    samples_denorm = samples
            
            # VAEè§£ç ï¼ˆä½¿ç”¨åå½’ä¸€åŒ–åçš„latentï¼‰
            if vae is not None:
                try:
                    # ç¡®ä¿VAEåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                    if hasattr(vae, 'to'):
                        vae = vae.to(device)
                    
                    # ä½¿ç”¨VA-VAEè§£ç latentä¸ºå›¾åƒï¼ˆä¸train_dit_s_official.pyç¬¬568è¡Œä¸€è‡´ï¼‰
                    decoded_images = vae.decode_to_images(samples_denorm)
                    
                    # æŒ‰ç…§å®˜æ–¹æ–¹å¼ä¿å­˜å•ä¸ªå›¾åƒæ–‡ä»¶
                    from PIL import Image
                    for i, image in enumerate(decoded_images):
                        save_path = user_dir / f"sample_{sample_idx + i:06d}.png"
                        Image.fromarray(image).save(save_path)
                except Exception as e:
                    if rank == 0:
                        print(f"âŒ VAEè§£ç å¤±è´¥: {e}")
                sample_idx += 1
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if world_size > 1:
        dist.barrier()
    
    if rank == 0:
        print(f"âœ… å®Œæˆç”¨æˆ· {user_id}: {num_samples} ä¸ªæ ·æœ¬å·²ä¿å­˜åˆ° {user_dir}")

def main():
    parser = argparse.ArgumentParser(description='Distributed conditional sample generation')
    parser.add_argument('--checkpoint', type=str, 
                       default='/kaggle/input/50000-pt/0050000.pt', 
                       help='Model checkpoint path')
    parser.add_argument('--config', type=str, 
                       default='configs/dit_s_microdoppler.yaml', 
                       help='Config file path')
    parser.add_argument('--output_dir', type=str, default='./generated_samples', help='Output directory')
    parser.add_argument('--num_samples', '--samples_per_user', type=int, default=200, help='Samples per user')
    parser.add_argument('--cfg_scale', type=float, default=10.0, help='CFG scaleï¼ˆä¸é…ç½®æ–‡ä»¶ä¸€è‡´ï¼‰')
    parser.add_argument('--batch_size', type=int, default=16, help='Batch size')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    args = parser.parse_args()
    
    # è®¾ç½®åˆ†å¸ƒå¼
    rank, local_rank, world_size = setup_distributed()
    
    if rank == 0:
        print(f"ğŸš€ åˆ†å¸ƒå¼æ¨ç†: {world_size} GPUs")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # åŠ è½½æ¨¡å‹å’Œé…ç½®ï¼ˆä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
    if rank == 0:
        print(f"ğŸš€ ä½¿ç”¨è®­ç»ƒå¥½çš„DiTæ¨¡å‹: {args.checkpoint}")
        print(f"ğŸ“‹ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    
    model, vae, transport, config, device = load_model_and_config(
        args.checkpoint, args.config, local_rank
    )
    
    # åˆ›å»ºé‡‡æ ·å™¨
    sampler = Sampler(transport)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if rank == 0:
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    if world_size > 1:
        dist.barrier()  # ç­‰å¾…ç›®å½•åˆ›å»ºå®Œæˆ
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆæ ·æœ¬
    for user_id in range(31):
        generate_samples_for_user_distributed(
            model, vae, transport, sampler, 
            user_id, args.num_samples, args.output_dir,
            cfg_scale=args.cfg_scale, seed=args.seed, batch_size=args.batch_size,
            rank=rank, world_size=world_size
        )
    
    if rank == 0:
        print(f"ğŸ¯ åˆ†å¸ƒå¼æ¡ä»¶æ ·æœ¬ç”Ÿæˆå®Œæˆï¼")
        print(f"âœ… æ‰€æœ‰ç”¨æˆ·çš„æ ·æœ¬å·²ç”Ÿæˆåˆ°: {args.output_dir}")
        print(f"ğŸ“ˆ æ¯ç”¨æˆ·ç”Ÿæˆäº† {args.num_samples} ä¸ªæ ·æœ¬")
    
    # æ¸…ç†åˆ†å¸ƒå¼
    if world_size > 1:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()

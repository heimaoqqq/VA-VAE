#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯åˆ†å¸ƒå¯¹é½é›†æˆæ•ˆæœ
æµ‹è¯•VA-VAE latentåˆ†å¸ƒå’Œæ‰©æ•£æ¨¡å‹è®­ç»ƒ/ç”Ÿæˆçš„å…¼å®¹æ€§
"""

import torch
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from simplified_vavae import SimplifiedVAVAE
from distribution_aligned_diffusion import DistributionAlignedDiffusion
from microdoppler_data_loader import create_balanced_dataloader
import argparse


def test_vae_latent_distribution(vae, dataloader, device, num_batches=10):
    """æµ‹è¯•VAEç¼–ç çš„latentåˆ†å¸ƒ"""
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•1: VA-VAE Latentåˆ†å¸ƒåˆ†æ")
    print("="*60)
    
    all_latents = []
    vae.eval()
    
    with torch.no_grad():
        for idx, (images, _) in enumerate(dataloader):
            if idx >= num_batches:
                break
            
            images = images.to(device)
            # å¦‚æœè¾“å…¥æ˜¯latentï¼Œè·³è¿‡ç¼–ç 
            if images.shape[1] == 32:  # latent channels
                latents = images
            else:  # å›¾åƒè¾“å…¥
                latents = vae.encode(images)
            
            all_latents.append(latents.cpu())
            print(f"   Batch {idx+1}: shape={latents.shape}, std={latents.std():.4f}")
    
    all_latents = torch.cat(all_latents, dim=0)
    
    # ç»Ÿè®¡åˆ†æ
    stats = {
        'mean': all_latents.mean().item(),
        'std': all_latents.std().item(),
        'min': all_latents.min().item(),
        'max': all_latents.max().item(),
        'shape': all_latents.shape
    }
    
    print(f"\nğŸ“ˆ Latentåˆ†å¸ƒç»Ÿè®¡:")
    print(f"   å‡å€¼(Mean): {stats['mean']:.6f} (æœŸæœ›â‰ˆ0)")
    print(f"   æ ‡å‡†å·®(Std): {stats['std']:.6f} (æœŸæœ›â‰ˆ1.5Â±0.3)")
    print(f"   æœ€å°å€¼(Min): {stats['min']:.3f}")
    print(f"   æœ€å¤§å€¼(Max): {stats['max']:.3f}")
    print(f"   å½¢çŠ¶(Shape): {stats['shape']}")
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å¸ƒå¯¹é½
    if abs(stats['std'] - 1.0) > 0.3:
        print(f"\nâš ï¸ æ£€æµ‹åˆ°åˆ†å¸ƒåç¦»ï¼éœ€è¦åˆ†å¸ƒå¯¹é½")
        print(f"   å®é™…std={stats['std']:.4f}, æœŸæœ›stdâ‰ˆ1.0")
        print(f"   å»ºè®®ä½¿ç”¨åˆ†å¸ƒå¯¹é½æ–¹æ¡ˆ")
    else:
        print(f"\nâœ… åˆ†å¸ƒæ¥è¿‘æ ‡å‡†æ­£æ€ï¼Œå¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨å¯¹é½")
    
    return stats


def test_distribution_alignment(vae, dataloader, device):
    """æµ‹è¯•åˆ†å¸ƒå¯¹é½æ¨¡å—åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ”§ æµ‹è¯•2: åˆ†å¸ƒå¯¹é½åŠŸèƒ½éªŒè¯")
    print("="*60)
    
    # åˆ›å»ºåˆ†å¸ƒå¯¹é½çš„æ‰©æ•£æ¨¡å‹
    model = DistributionAlignedDiffusion(
        vae=vae,
        num_users=31,
        prototype_dim=768,
        enable_alignment=True,
        track_statistics=True
    ).to(device)
    
    print("âœ… åˆ†å¸ƒå¯¹é½æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # æµ‹è¯•å½’ä¸€åŒ–å’Œåå½’ä¸€åŒ–
    with torch.no_grad():
        for idx, (latents, user_ids) in enumerate(dataloader):
            if idx >= 1:  # åªæµ‹è¯•ä¸€ä¸ªbatch
                break
            
            latents = latents.to(device)
            
            print(f"\nğŸ“Š æµ‹è¯•Batch:")
            print(f"   åŸå§‹latent: mean={latents.mean():.4f}, std={latents.std():.4f}")
            
            # æ›´æ–°ç»Ÿè®¡
            model.update_statistics(latents)
            
            # æµ‹è¯•å½’ä¸€åŒ–
            normalized = model.normalize_latents(latents)
            print(f"   å½’ä¸€åŒ–å: mean={normalized.mean():.4f}, std={normalized.std():.4f}")
            
            # æµ‹è¯•åå½’ä¸€åŒ–
            denormalized = model.denormalize_latents(normalized)
            print(f"   åå½’ä¸€åŒ–å: mean={denormalized.mean():.4f}, std={denormalized.std():.4f}")
            
            # éªŒè¯å¯é€†æ€§
            error = (denormalized - latents).abs().mean()
            print(f"   å¯é€†æ€§è¯¯å·®: {error:.6f}")
            
            if error < 1e-5:
                print("   âœ… å½’ä¸€åŒ–/åå½’ä¸€åŒ–å¯é€†æ€§éªŒè¯é€šè¿‡")
            else:
                print(f"   âš ï¸ å¯é€†æ€§è¯¯å·®è¾ƒå¤§: {error}")
    
    return model


def test_training_step(model, dataloader, device):
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
    print("\n" + "="*60)
    print("ğŸš€ æµ‹è¯•3: è®­ç»ƒæ­¥éª¤éªŒè¯")
    print("="*60)
    
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    for idx, (latents, user_ids) in enumerate(dataloader):
        if idx >= 3:  # æµ‹è¯•3ä¸ªbatch
            break
        
        latents = latents.to(device)
        user_ids = user_ids.to(device)
        
        # è®­ç»ƒæ­¥éª¤ (ç›´æ¥ä¼ é€’user_idsï¼Œè®©training_stepå†…éƒ¨å¤„ç†)
        loss_dict = model.training_step(latents, user_ids)
        total_loss = loss_dict['total_loss']
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        print(f"   Batch {idx+1}: Loss={total_loss.item():.4f} "
              f"(Diff={loss_dict['diffusion_loss'].item():.4f}, "
              f"Cont={loss_dict['contrastive_loss'].item():.4f})")
    
    print("\nâœ… è®­ç»ƒæ­¥éª¤æµ‹è¯•é€šè¿‡")


def test_generation(model, vae, device, num_samples=4):
    """æµ‹è¯•ç”ŸæˆåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ¨ æµ‹è¯•4: ç”ŸæˆåŠŸèƒ½éªŒè¯")
    print("="*60)
    
    model.eval()
    
    with torch.no_grad():
        # ç”Ÿæˆlatents
        print(f"   ç”Ÿæˆ{num_samples}ä¸ªæ ·æœ¬...")
        user_ids = [0] * num_samples  # ä½¿ç”¨ç”¨æˆ·0
        
        latents = model.generate(
            user_ids=user_ids,
            num_samples=num_samples,
            num_inference_steps=50,
            guidance_scale=7.5
        )
        
        print(f"   ç”Ÿæˆlatentåˆ†å¸ƒ: mean={latents.mean():.4f}, std={latents.std():.4f}")
        
        # å¦‚æœå¯ç”¨äº†åˆ†å¸ƒå¯¹é½ï¼Œæ£€æŸ¥æ˜¯å¦å·²åå½’ä¸€åŒ–
        if model.enable_alignment:
            expected_std = model.latent_std.item() if model.latent_std > 0 else 1.5
            if abs(latents.std().item() - expected_std) < 0.3:
                print(f"   âœ… Latentå·²æ­£ç¡®åå½’ä¸€åŒ–åˆ°VAEåˆ†å¸ƒ (stdâ‰ˆ{expected_std:.2f})")
            else:
                print(f"   âš ï¸ Latentåˆ†å¸ƒå¯èƒ½æœ‰é—®é¢˜: std={latents.std().item():.4f}, æœŸæœ›â‰ˆ{expected_std:.2f}")
        
        # è§£ç åˆ°å›¾åƒ
        print(f"   è§£ç latentsåˆ°å›¾åƒ...")
        images = vae.decode(latents)
        
        print(f"   å›¾åƒèŒƒå›´: [{images.min():.3f}, {images.max():.3f}]")
        
        if images.min() >= -0.1 and images.max() <= 1.1:
            print("   âœ… ç”Ÿæˆå›¾åƒåœ¨åˆç†èŒƒå›´å†…")
        else:
            print(f"   âš ï¸ å›¾åƒå€¼è¶…å‡ºé¢„æœŸèŒƒå›´")
    
    print("\nâœ… ç”ŸæˆåŠŸèƒ½æµ‹è¯•å®Œæˆ")


def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•åˆ†å¸ƒå¯¹é½é›†æˆ')
    
    parser.add_argument('--vae_checkpoint', type=str, 
                      default='/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt',
                      help='VA-VAEæ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--latent_dir', type=str, 
                      default='/kaggle/working/latents',
                      help='Latentæ•°æ®ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=8,
                      help='æµ‹è¯•æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--device', type=str, default='cuda',
                      help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½VAE
    print("\nğŸ“¦ åŠ è½½VA-VAE...")
    vae = SimplifiedVAVAE(args.vae_checkpoint, use_vf='dinov2')
    vae.eval()
    vae = vae.to(device)
    print(f"âœ… VAEåŠ è½½æˆåŠŸ")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    try:
        dataloader = create_balanced_dataloader(
            latent_dir=args.latent_dir,
            batch_size=args.batch_size,
            num_users_per_batch=4,
            split='train',
            num_workers=0  # æµ‹è¯•æ—¶ä½¿ç”¨å•çº¿ç¨‹
        )
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åˆ›å»ºlatentæ•°æ®åŠ è½½å™¨: {e}")
        print("   ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        class DummyDataset(torch.utils.data.Dataset):
            def __init__(self, size=100):
                self.size = size
                # æ¨¡æ‹Ÿæœ‰åçš„latentåˆ†å¸ƒ (meanâ‰ˆ0.1, stdâ‰ˆ1.5)
                self.latents = torch.randn(size, 32, 16, 16) * 1.5 + 0.1
                self.user_ids = torch.randint(0, 31, (size,))
            
            def __len__(self):
                return self.size
            
            def __getitem__(self, idx):
                return self.latents[idx], self.user_ids[idx]
        
        dataset = DummyDataset()
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=args.batch_size, shuffle=True
        )
    
    # 3. è¿è¡Œæµ‹è¯•
    print("\n" + "="*60)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•åˆ†å¸ƒå¯¹é½é›†æˆ")
    print("="*60)
    
    # æµ‹è¯•1: åˆ†æVAE latentåˆ†å¸ƒ
    latent_stats = test_vae_latent_distribution(vae, dataloader, device)
    
    # æµ‹è¯•2: åˆ†å¸ƒå¯¹é½åŠŸèƒ½
    model = test_distribution_alignment(vae, dataloader, device)
    
    # æµ‹è¯•3: è®­ç»ƒæ­¥éª¤
    test_training_step(model, dataloader, device)
    
    # æµ‹è¯•4: ç”ŸæˆåŠŸèƒ½
    test_generation(model, vae, device)
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    if abs(latent_stats['std'] - 1.0) > 0.3:
        print("âœ… åˆ†å¸ƒå¯¹é½æ–¹æ¡ˆå·²æˆåŠŸé›†æˆ")
        print(f"   - VAE latent std={latent_stats['std']:.4f} (åç¦»æ ‡å‡†åˆ†å¸ƒ)")
        print("   - åˆ†å¸ƒå¯¹é½æ¨¡å—æ­£å¸¸å·¥ä½œ")
        print("   - è®­ç»ƒå’Œç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        print("\nå»ºè®®ï¼šä½¿ç”¨train_distribution_aligned.pyè¿›è¡Œè®­ç»ƒ")
    else:
        print("âœ… ç³»ç»Ÿæ­£å¸¸ï¼Œlatentåˆ†å¸ƒæ¥è¿‘æ ‡å‡†")
        print("   - å¯é€‰æ‹©æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¯¹é½")
        print("   - æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
é˜¶æ®µ1: ç‰¹å¾æå–
åŸºäºLightningDiTåŸé¡¹ç›®çš„extract_features.py
ä½¿ç”¨é¢„è®­ç»ƒVA-VAEæå–å¾®å¤šæ™®å‹’å›¾åƒçš„æ½œåœ¨ç‰¹å¾
"""

import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
import torch.distributed as dist
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torchvision.datasets import ImageFolder
import argparse
import os
from safetensors.torch import save_file
from datetime import datetime
from pathlib import Path
import numpy as np
from PIL import Image
import torchvision.transforms as transforms

# å¯¼å…¥LightningDiTç»„ä»¶
import sys
sys.path.append('LightningDiT')
from tokenizer.vavae import VA_VAE

class MicroDopplerDataset(torch.utils.data.Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›†"""
    
    def __init__(self, data_dir, transform=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        
        # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶å’Œç”¨æˆ·ID
        self.samples = []
        for user_dir in sorted(self.data_dir.iterdir()):
            if user_dir.is_dir() and user_dir.name.startswith('user'):
                user_id = int(user_dir.name.replace('user', ''))
                for img_file in user_dir.glob('*.png'):
                    self.samples.append((str(img_file), user_id))
        
        print(f"åŠ è½½äº† {len(self.samples)} ä¸ªå¾®å¤šæ™®å‹’æ ·æœ¬")
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, user_id = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, user_id

def main(args):
    """
    åŸºäºåŸé¡¹ç›®extract_features.pyçš„ç‰¹å¾æå–ä¸»å‡½æ•°
    """
    assert torch.cuda.is_available(), "ç‰¹å¾æå–éœ€è¦è‡³å°‘ä¸€ä¸ªGPU"

    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ (å‚è€ƒåŸé¡¹ç›®)
    try:
        dist.init_process_group("nccl")
        rank = dist.get_rank()
        device = rank % torch.cuda.device_count()
        world_size = dist.get_world_size()
        seed = args.seed + rank
        if rank == 0:
            print(f"å¯åŠ¨ rank={rank}, seed={seed}, world_size={world_size}")
    except:
        print("åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å¼")
        rank = 0
        device = 0
        world_size = 1
        seed = args.seed
    
    torch.manual_seed(seed)
    torch.cuda.set_device(device)

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(args.output_path)
    if rank == 0:
        output_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºVA-VAEæ¨¡å‹ (VA-VAEåœ¨åˆå§‹åŒ–æ—¶å·²ç»è®¾ç½®ä¸ºevalæ¨¡å¼å¹¶ç§»åˆ°GPU)
    tokenizer = VA_VAE(args.vavae_config)

    # æ•°æ®é¢„å¤„ç† (ä¸åŸé¡¹ç›®ä¸€è‡´)
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    # å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    for split in ['train', 'val']:
        split_dir = Path(args.data_dir) / split
        if not split_dir.exists():
            print(f"è·³è¿‡ä¸å­˜åœ¨çš„åˆ†å‰²: {split}")
            continue
            
        print(f"\nğŸ“Š å¤„ç† {split} æ•°æ®...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = MicroDopplerDataset(split_dir, transform=transform)
        
        # åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
        if world_size > 1:
            sampler = DistributedSampler(
                dataset,
                num_replicas=world_size,
                rank=rank,
                shuffle=False
            )
        else:
            sampler = None
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=args.batch_size,
            sampler=sampler,
            num_workers=4,
            pin_memory=True,
            drop_last=False
        )
        
        # æå–ç‰¹å¾
        all_latents = []
        all_user_ids = []
        
        with torch.no_grad():
            for batch_idx, (images, user_ids) in enumerate(tqdm(
                dataloader, 
                desc=f"Rank {rank}",
                disable=(rank != 0)
            )):
                images = images.cuda()
                
                # ä½¿ç”¨VA-VAEç¼–ç  (ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•)
                latents = tokenizer.encode_images(images)  # (B, 32, 16, 16)
                
                all_latents.append(latents.cpu())
                all_user_ids.extend(user_ids.tolist())
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        if all_latents:
            latents_tensor = torch.cat(all_latents, dim=0)
            user_ids_tensor = torch.tensor(all_user_ids, dtype=torch.long)
            
            print(f"Rank {rank}: æå–äº† {len(latents_tensor)} ä¸ªç‰¹å¾")
            print(f"ç‰¹å¾å½¢çŠ¶: {latents_tensor.shape}")
            print(f"ç”¨æˆ·IDèŒƒå›´: [{min(all_user_ids)}, {max(all_user_ids)}]")
            
            # ä¿å­˜ç‰¹å¾ (å‚è€ƒåŸé¡¹ç›®çš„safetensorsæ ¼å¼)
            save_dict = {
                'latents': latents_tensor,
                'user_ids': user_ids_tensor,
                'num_samples': torch.tensor(len(latents_tensor)),
                'num_users': torch.tensor(len(set(all_user_ids))),
            }
            
            # ä¿å­˜åˆ°rankç‰¹å®šçš„æ–‡ä»¶
            rank_file = output_dir / f"{split}_rank{rank:02d}.safetensors"
            save_file(save_dict, rank_file)
            print(f"âœ… ç‰¹å¾å·²ä¿å­˜åˆ°: {rank_file}")

    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if world_size > 1:
        dist.barrier()
    
    # ä¸»è¿›ç¨‹åˆå¹¶æ‰€æœ‰rankçš„ç»“æœ
    if rank == 0:
        print("\nğŸ”„ åˆå¹¶æ‰€æœ‰rankçš„ç‰¹å¾...")
        merge_features(output_dir, world_size)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ (å‚è€ƒåŸé¡¹ç›®)
        print("\nğŸ“Š è®¡ç®—æ½œåœ¨ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯...")
        compute_latent_stats(output_dir)
    
    print("âœ… ç‰¹å¾æå–å®Œæˆ!")

def merge_features(output_dir, world_size):
    """åˆå¹¶æ‰€æœ‰rankçš„ç‰¹å¾æ–‡ä»¶"""
    for split in ['train', 'val']:
        all_latents = []
        all_user_ids = []
        
        # æ”¶é›†æ‰€æœ‰rankçš„æ•°æ®
        for rank in range(world_size):
            rank_file = output_dir / f"{split}_rank{rank:02d}.safetensors"
            if rank_file.exists():
                from safetensors import safe_open
                with safe_open(rank_file, framework="pt", device="cpu") as f:
                    latents = f.get_tensor('latents')
                    user_ids = f.get_tensor('user_ids')
                    all_latents.append(latents)
                    all_user_ids.append(user_ids)
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                rank_file.unlink()
        
        if all_latents:
            # åˆå¹¶æ•°æ®
            merged_latents = torch.cat(all_latents, dim=0)
            merged_user_ids = torch.cat(all_user_ids, dim=0)
            
            print(f"åˆå¹¶åçš„ {split} ç‰¹å¾:")
            print(f"  æ ·æœ¬æ•°é‡: {len(merged_latents)}")
            print(f"  ç‰¹å¾å½¢çŠ¶: {merged_latents.shape}")
            print(f"  ç”¨æˆ·æ•°é‡: {len(set(merged_user_ids.tolist()))}")
            
            # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
            save_dict = {
                'latents': merged_latents,
                'user_ids': merged_user_ids,
                'num_samples': torch.tensor(len(merged_latents)),
                'num_users': torch.tensor(len(set(merged_user_ids.tolist()))),
            }
            
            final_file = output_dir / f"{split}.safetensors"
            save_file(save_dict, final_file)
            print(f"âœ… æœ€ç»ˆç‰¹å¾ä¿å­˜åˆ°: {final_file}")

def compute_latent_stats(output_dir):
    """è®¡ç®—æ½œåœ¨ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ (å‚è€ƒåŸé¡¹ç›®)"""
    from safetensors import safe_open
    
    train_file = output_dir / "train.safetensors"
    if not train_file.exists():
        print("âŒ è®­ç»ƒé›†ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # åŠ è½½è®­ç»ƒé›†ç‰¹å¾
    with safe_open(train_file, framework="pt", device="cpu") as f:
        latents = f.get_tensor('latents')  # (N, 32, 16, 16)
    
    print(f"è®¡ç®— {len(latents)} ä¸ªæ ·æœ¬çš„ç»Ÿè®¡ä¿¡æ¯...")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ (å‚è€ƒåŸé¡¹ç›®æ–¹å¼)
    mean = latents.mean(dim=[0, 2, 3], keepdim=True)  # (1, 32, 1, 1)
    std = latents.std(dim=[0, 2, 3], keepdim=True)    # (1, 32, 1, 1)
    
    print(f"æ½œåœ¨ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  å‡å€¼èŒƒå›´: [{mean.min():.4f}, {mean.max():.4f}]")
    print(f"  æ ‡å‡†å·®èŒƒå›´: [{std.min():.4f}, {std.max():.4f}]")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ (ä¸åŸé¡¹ç›®æ ¼å¼ä¸€è‡´)
    stats = {
        'mean': mean,
        'std': std
    }
    
    stats_file = output_dir / "latents_stats.pt"
    torch.save(stats, stats_file)
    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯ä¿å­˜åˆ°: {stats_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='å¾®å¤šæ™®å‹’ç‰¹å¾æå–')
    parser.add_argument('--data_dir', type=str, required=True, help='æ•°æ®ç›®å½•')
    parser.add_argument('--vavae_config', type=str, required=True, help='VA-VAEé…ç½®æ–‡ä»¶')
    parser.add_argument('--output_path', type=str, required=True, help='è¾“å‡ºè·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    main(args)

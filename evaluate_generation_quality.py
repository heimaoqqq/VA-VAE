"""
æ¡ä»¶ç”Ÿæˆè´¨é‡è¯„ä¼°è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„åˆ†ç±»å™¨è¯„ä¼°ç”Ÿæˆæ ·æœ¬ï¼Œç­›é€‰ç½®ä¿¡åº¦>90%çš„æ ·æœ¬
"""
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import timm
import os
import argparse
from pathlib import Path
import numpy as np
from PIL import Image
import json
import shutil
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import defaultdict

class GeneratedSampleDataset(Dataset):
    """ç”Ÿæˆæ ·æœ¬æ•°æ®é›†"""
    def __init__(self, data_dir, transform=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.samples = []
        
        # æ‰«ææ‰€æœ‰ç”¨æˆ·ç›®å½•
        for user_dir in sorted(self.data_dir.glob("user_*")):
            if user_dir.is_dir():
                user_id = int(user_dir.name.split('_')[1])
                
                # æ‰«æè¯¥ç”¨æˆ·çš„æ‰€æœ‰ç”Ÿæˆå›¾åƒ
                for img_path in sorted(user_dir.glob("*.png")):
                    self.samples.append((str(img_path), user_id))
        
        print(f"Found {len(self.samples)} generated samples from {len(set([s[1] for s in self.samples]))} users")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, true_label = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, true_label, img_path

def load_classifier(model_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„åˆ†ç±»å™¨"""
    checkpoint = torch.load(model_path, map_location=device)
    
    # åˆ›å»ºæ¨¡å‹
    model = timm.create_model('resnet18', pretrained=False, num_classes=checkpoint['num_classes'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    print(f"Loaded classifier from {model_path}")
    print(f"Best validation accuracy: {checkpoint['best_val_acc']:.2f}%")
    
    return model

def create_transforms():
    """åˆ›å»ºæ•°æ®å˜æ¢ï¼ˆä¸åˆ†ç±»å™¨è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰"""
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    return transform

def evaluate_samples(model, dataloader, device, confidence_threshold=0.9):
    """è¯„ä¼°ç”Ÿæˆæ ·æœ¬"""
    model.eval()
    
    results = {
        'predictions': [],
        'true_labels': [],
        'confidences': [],
        'max_probs': [],
        'image_paths': [],
        'correct': [],
        'high_confidence': []
    }
    
    user_stats = defaultdict(lambda: {
        'total': 0, 'correct': 0, 'high_conf': 0, 'high_conf_correct': 0
    })
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Evaluating")
        for images, true_labels, img_paths in pbar:
            images = images.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            probabilities = F.softmax(outputs, dim=1)
            max_probs, predictions = torch.max(probabilities, dim=1)
            
            # è½¬æ¢ä¸ºnumpy
            predictions_np = predictions.cpu().numpy()
            true_labels_np = true_labels.numpy()
            max_probs_np = max_probs.cpu().numpy()
            
            # è®°å½•ç»“æœ
            for i in range(len(predictions_np)):
                pred = predictions_np[i]
                true_label = true_labels_np[i]
                max_prob = max_probs_np[i]
                img_path = img_paths[i]
                
                is_correct = (pred == true_label)
                is_high_conf = (max_prob >= confidence_threshold)
                
                results['predictions'].append(pred)
                results['true_labels'].append(true_label)
                results['confidences'].append(probabilities[i].cpu().numpy())
                results['max_probs'].append(max_prob)
                results['image_paths'].append(img_path)
                results['correct'].append(is_correct)
                results['high_confidence'].append(is_high_conf)
                
                # æ›´æ–°ç”¨æˆ·ç»Ÿè®¡
                user_stats[true_label]['total'] += 1
                if is_correct:
                    user_stats[true_label]['correct'] += 1
                if is_high_conf:
                    user_stats[true_label]['high_conf'] += 1
                    if is_correct:
                        user_stats[true_label]['high_conf_correct'] += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            total_correct = sum(results['correct'])
            total_high_conf = sum(results['high_confidence'])
            accuracy = total_correct / len(results['correct']) if results['correct'] else 0
            high_conf_ratio = total_high_conf / len(results['high_confidence']) if results['high_confidence'] else 0
            
            pbar.set_postfix({
                'Acc': f'{accuracy*100:.1f}%',
                'HighConf': f'{high_conf_ratio*100:.1f}%'
            })
    
    return results, user_stats

def save_high_confidence_samples(results, output_dir, confidence_threshold=0.9):
    """åªä¿å­˜é«˜ç½®ä¿¡åº¦ä¸”æ­£ç¡®è¯†åˆ«çš„æ ·æœ¬ï¼ŒæŒ‰ç”¨æˆ·IDåˆ†ç±»"""
    output_dir = Path(output_dir)
    
    # åˆ›å»ºæˆåŠŸè¯†åˆ«æ ·æœ¬ç›®å½•
    success_dir = output_dir / f"successful_samples_conf_{confidence_threshold}"
    success_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç›®å½•
    for user_id in range(31):
        user_dir = success_dir / f"user_{user_id:02d}"
        user_dir.mkdir(exist_ok=True)
    
    saved_count = 0
    per_user_stats = {i: {'total': 0, 'success': 0, 'high_conf_correct': 0} for i in range(31)}
    
    for i, (is_high_conf, is_correct, img_path, true_label, pred, confidence) in enumerate(
        zip(results['high_confidence'], results['correct'], results['image_paths'], 
            results['true_labels'], results['predictions'], results['confidences'])
    ):
        # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬
        per_user_stats[true_label]['total'] += 1
        
        # åªä¿å­˜é«˜ç½®ä¿¡åº¦ä¸”æ­£ç¡®è¯†åˆ«çš„æ ·æœ¬
        if is_high_conf and is_correct:
            per_user_stats[true_label]['success'] += 1
            per_user_stats[true_label]['high_conf_correct'] += 1
            
            # ç¡®å®šä¿å­˜è·¯å¾„
            user_dir = success_dir / f"user_{true_label:02d}"
            original_filename = Path(img_path).name
            name_parts = original_filename.split('.')
            
            # ç®€åŒ–æ–‡ä»¶åï¼ŒåªåŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯
            new_filename = f"{name_parts[0]}_conf{confidence:.3f}.{name_parts[1]}"
            new_path = user_dir / new_filename
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(img_path, new_path)
            saved_count += 1
        elif is_correct:
            # ç»Ÿè®¡æ­£ç¡®ä½†ä½ç½®ä¿¡åº¦çš„
            per_user_stats[true_label]['success'] += 1
    
    # æ‰“å°æ¯ç”¨æˆ·æˆåŠŸè¯†åˆ«ç»Ÿè®¡
    print(f"\nğŸ“Š æ¯ç”¨æˆ·æˆåŠŸè¯†åˆ«ç»Ÿè®¡ (ç½®ä¿¡åº¦>{confidence_threshold}):")
    print("-" * 60)
    total_samples = 0
    total_success = 0
    for user_id in range(31):
        stats = per_user_stats[user_id]
        if stats['total'] > 0:
            success_rate = stats['high_conf_correct'] / stats['total'] * 100
            total_samples += stats['total']
            total_success += stats['high_conf_correct']
            print(f"User_{user_id:02d}: {stats['high_conf_correct']}/{stats['total']} "
                  f"({success_rate:.1f}%)")
    
    overall_rate = total_success / total_samples * 100 if total_samples > 0 else 0
    print(f"\nğŸ“ˆ æ€»ä½“æˆåŠŸè¯†åˆ«ç‡: {total_success}/{total_samples} ({overall_rate:.1f}%)")
    print(f"ğŸ’¾ æˆåŠŸè¯†åˆ«æ ·æœ¬ä¿å­˜åˆ°: {success_dir}")
    print(f"âœ… å…±ä¿å­˜ {saved_count} ä¸ªæˆåŠŸè¯†åˆ«çš„æ ·æœ¬")
    
    return saved_count, saved_count  # è¿”å›ä¿å­˜æ•°å’ŒæˆåŠŸæ•°ï¼ˆç›¸åŒï¼‰

def generate_evaluation_report(results, user_stats, output_dir, confidence_threshold=0.9):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    output_dir = Path(output_dir)
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_samples = len(results['predictions'])
    total_correct = sum(results['correct'])
    total_high_conf = sum(results['high_confidence'])
    high_conf_correct = sum(c and h for c, h in zip(results['correct'], results['high_confidence']))
    
    overall_accuracy = total_correct / total_samples
    high_conf_ratio = total_high_conf / total_samples
    high_conf_accuracy = high_conf_correct / total_high_conf if total_high_conf > 0 else 0
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'evaluation_summary': {
            'total_samples': total_samples,
            'total_correct': total_correct,
            'overall_accuracy': overall_accuracy,
            'high_confidence_samples': total_high_conf,
            'high_confidence_ratio': high_conf_ratio,
            'high_confidence_correct': high_conf_correct,
            'high_confidence_accuracy': high_conf_accuracy,
            'confidence_threshold': confidence_threshold
        },
        'per_user_stats': {}
    }
    
    # æ¯ç”¨æˆ·ç»Ÿè®¡
    for user_id in sorted(user_stats.keys()):
        stats = user_stats[user_id]
        user_accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        user_high_conf_ratio = stats['high_conf'] / stats['total'] if stats['total'] > 0 else 0
        user_high_conf_accuracy = stats['high_conf_correct'] / stats['high_conf'] if stats['high_conf'] > 0 else 0
        
        report['per_user_stats'][f'user_{user_id:02d}'] = {
            'total_samples': stats['total'],
            'correct_predictions': stats['correct'],
            'accuracy': user_accuracy,
            'high_confidence_samples': stats['high_conf'],
            'high_confidence_ratio': user_high_conf_ratio,
            'high_confidence_correct': stats['high_conf_correct'],
            'high_confidence_accuracy': user_high_conf_accuracy
        }
    
    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼ˆç”¨äºJSONåºåˆ—åŒ–ï¼‰
    def convert_numpy_types(obj):
        if isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(v) for v in obj]
        elif hasattr(obj, 'item'):  # numpyæ ‡é‡
            return obj.item()
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        else:
            return obj
    
    # ä¿å­˜JSONæŠ¥å‘Š  
    report_path = output_dir / 'evaluation_report.json'
    with open(report_path, 'w') as f:
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œç„¶åä¿å­˜
        converted_report = convert_numpy_types(report)
        json.dump(converted_report, f, indent=2)
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    text_report_path = output_dir / 'evaluation_report.txt'
    with open(text_report_path, 'w') as f:
        f.write("=== CONDITIONAL GENERATION EVALUATION REPORT ===\n\n")
        f.write(f"Confidence Threshold: {confidence_threshold}\n\n")
        
        f.write("OVERALL STATISTICS:\n")
        f.write(f"  Total Samples: {total_samples}\n")
        f.write(f"  Correct Predictions: {total_correct}\n")
        f.write(f"  Overall Accuracy: {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)\n")
        f.write(f"  High-Confidence Samples: {total_high_conf}\n")
        f.write(f"  High-Confidence Ratio: {high_conf_ratio:.3f} ({high_conf_ratio*100:.1f}%)\n")
        f.write(f"  High-Confidence Accuracy: {high_conf_accuracy:.3f} ({high_conf_accuracy*100:.1f}%)\n\n")
        
        f.write("PER-USER STATISTICS:\n")
        f.write("User | Total | Correct | Acc(%) | HighConf | HCRatio(%) | HCAcc(%)\n")
        f.write("-" * 70 + "\n")
        
        for user_id in sorted(user_stats.keys()):
            stats = user_stats[user_id]
            user_accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
            user_high_conf_ratio = stats['high_conf'] / stats['total'] if stats['total'] > 0 else 0
            user_high_conf_accuracy = stats['high_conf_correct'] / stats['high_conf'] if stats['high_conf'] > 0 else 0
            
            f.write(f"{user_id:4d} | {stats['total']:5d} | {stats['correct']:7d} | "
                   f"{user_accuracy*100:6.1f} | {stats['high_conf']:8d} | "
                   f"{user_high_conf_ratio*100:10.1f} | {user_high_conf_accuracy*100:7.1f}\n")
    
    # ç»˜åˆ¶ç»Ÿè®¡å›¾è¡¨
    plt.figure(figsize=(15, 10))
    
    # å‡†å¤‡æ•°æ®
    user_ids = sorted(user_stats.keys())
    accuracies = [user_stats[uid]['correct'] / user_stats[uid]['total'] for uid in user_ids]
    high_conf_ratios = [user_stats[uid]['high_conf'] / user_stats[uid]['total'] for uid in user_ids]
    high_conf_accuracies = [user_stats[uid]['high_conf_correct'] / user_stats[uid]['high_conf'] 
                           if user_stats[uid]['high_conf'] > 0 else 0 for uid in user_ids]
    
    # å­å›¾1: æ¯ç”¨æˆ·å‡†ç¡®ç‡
    plt.subplot(2, 2, 1)
    plt.bar(user_ids, accuracies)
    plt.axhline(y=overall_accuracy, color='r', linestyle='--', label=f'Overall: {overall_accuracy:.3f}')
    plt.title('Accuracy per User')
    plt.xlabel('User ID')
    plt.ylabel('Accuracy')
    plt.legend()
    
    # å­å›¾2: æ¯ç”¨æˆ·é«˜ç½®ä¿¡åº¦æ¯”ä¾‹
    plt.subplot(2, 2, 2)
    plt.bar(user_ids, high_conf_ratios)
    plt.axhline(y=high_conf_ratio, color='r', linestyle='--', label=f'Overall: {high_conf_ratio:.3f}')
    plt.title('High-Confidence Ratio per User')
    plt.xlabel('User ID')
    plt.ylabel('High-Confidence Ratio')
    plt.legend()
    
    # å­å›¾3: æ¯ç”¨æˆ·é«˜ç½®ä¿¡åº¦æ ·æœ¬å‡†ç¡®ç‡
    plt.subplot(2, 2, 3)
    plt.bar(user_ids, high_conf_accuracies)
    plt.axhline(y=high_conf_accuracy, color='r', linestyle='--', label=f'Overall: {high_conf_accuracy:.3f}')
    plt.title('High-Confidence Sample Accuracy per User')
    plt.xlabel('User ID')
    plt.ylabel('Accuracy')
    plt.legend()
    
    # å­å›¾4: ç½®ä¿¡åº¦åˆ†å¸ƒ
    plt.subplot(2, 2, 4)
    plt.hist(results['max_probs'], bins=50, alpha=0.7, edgecolor='black')
    plt.axvline(x=confidence_threshold, color='r', linestyle='--', 
               label=f'Threshold: {confidence_threshold}')
    plt.title('Confidence Score Distribution')
    plt.xlabel('Max Probability')
    plt.ylabel('Count')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(output_dir / 'evaluation_charts.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Evaluation report saved to {report_path}")
    print(f"Text report saved to {text_report_path}")
    print(f"Charts saved to {output_dir / 'evaluation_charts.png'}")
    
    return report

def main():
    parser = argparse.ArgumentParser(description='Evaluate generated samples')
    parser.add_argument('--generated_dir', type=str, required=True, help='Directory with generated samples')
    parser.add_argument('--classifier_path', type=str, required=True, help='Path to trained classifier')
    parser.add_argument('--output_dir', type=str, default='./evaluation_results', help='Output directory')
    parser.add_argument('--confidence_threshold', type=float, default=0.9, help='Confidence threshold')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for evaluation')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½åˆ†ç±»å™¨
    print("Loading classifier...")
    classifier = load_classifier(args.classifier_path, device)
    
    # åˆ›å»ºæ•°æ®å˜æ¢
    transform = create_transforms()
    
    # åŠ è½½ç”Ÿæˆæ ·æœ¬æ•°æ®é›†
    print("Loading generated samples...")
    dataset = GeneratedSampleDataset(args.generated_dir, transform=transform)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)
    
    # è¯„ä¼°æ ·æœ¬
    print(f"Evaluating samples with confidence threshold {args.confidence_threshold}...")
    results, user_stats = evaluate_samples(classifier, dataloader, device, args.confidence_threshold)
    
    # ä¿å­˜é«˜ç½®ä¿¡åº¦æ ·æœ¬
    print("Saving high-confidence samples...")
    saved_count, saved_correct = save_high_confidence_samples(results, args.output_dir, args.confidence_threshold)
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    print("Generating evaluation report...")
    report = generate_evaluation_report(results, user_stats, args.output_dir, args.confidence_threshold)
    
    # æ‰“å°æ€»ç»“
    print(f"\n=== EVALUATION SUMMARY ===")
    print(f"Total samples evaluated: {len(results['predictions'])}")
    print(f"Overall accuracy: {report['evaluation_summary']['overall_accuracy']:.3f} ({report['evaluation_summary']['overall_accuracy']*100:.1f}%)")
    print(f"High-confidence samples: {report['evaluation_summary']['high_confidence_samples']} ({report['evaluation_summary']['high_confidence_ratio']*100:.1f}%)")
    print(f"High-confidence accuracy: {report['evaluation_summary']['high_confidence_accuracy']:.3f} ({report['evaluation_summary']['high_confidence_accuracy']*100:.1f}%)")
    print(f"High-confidence samples saved: {saved_count}")

if __name__ == "__main__":
    main()

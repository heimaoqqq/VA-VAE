"""
è®­ç»ƒå…·æœ‰è‰¯å¥½æ ¡å‡†åº¦çš„åˆ†ç±»å™¨
å…³é”®æ”¹è¿›ï¼šæ ‡ç­¾å¹³æ»‘ã€æ··åˆå¢å¼ºã€ç„¦ç‚¹æŸå¤±ã€æ¸©åº¦é€€ç«
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset, Subset
from torchvision.datasets import ImageFolder
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
import json
import os
from PIL import Image


class LabelSmoothingLoss(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤± - é˜²æ­¢è¿‡åº¦è‡ªä¿¡"""
    def __init__(self, num_classes, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        self.num_classes = num_classes
    
    def forward(self, pred, target):
        n = pred.size(0)
        one_hot = torch.zeros_like(pred).scatter(1, target.view(-1, 1), 1)
        one_hot = one_hot * (1 - self.smoothing) + self.smoothing / self.num_classes
        log_prb = F.log_softmax(pred, dim=1)
        loss = -(one_hot * log_prb).sum(dim=1).mean()
        return loss


class FocalLoss(nn.Module):
    """ç„¦ç‚¹æŸå¤± - å‡å°‘æ˜“åˆ†ç±»æ ·æœ¬æƒé‡ï¼Œä¸“æ³¨å›°éš¾æ ·æœ¬"""
    def __init__(self, gamma=2.0, alpha=None):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
    
    def forward(self, pred, target):
        ce_loss = F.cross_entropy(pred, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        
        if self.alpha is not None:
            alpha_t = self.alpha.gather(0, target)
            focal_loss = alpha_t * focal_loss
        
        return focal_loss.mean()


class MixupAugmentation:
    """Mixupæ•°æ®å¢å¼º - æ”¹å–„æ³›åŒ–å’Œæ ¡å‡†"""
    def __init__(self, alpha=0.2):
        self.alpha = alpha
    
    def __call__(self, images, labels):
        batch_size = images.size(0)
        
        if self.alpha > 0:
            lam = np.random.beta(self.alpha, self.alpha)
        else:
            lam = 1
        
        index = torch.randperm(batch_size).to(images.device)
        mixed_images = lam * images + (1 - lam) * images[index]
        
        return mixed_images, labels, labels[index], lam


class DomainAdaptiveClassifier(nn.Module):
    """é’ˆå¯¹å°æ•°æ®é›†å’ŒåŸŸé€‚åº”è®¾è®¡çš„åˆ†ç±»å™¨"""
    def __init__(self, num_classes=31, dropout_rate=0.3, feature_dim=512):
        super().__init__()
        
        # ä½¿ç”¨ResNet18ä½œä¸ºbackboneï¼ˆé¢„è®­ç»ƒæƒé‡å¾ˆé‡è¦ï¼‰
        self.backbone = models.resnet18(pretrained=True)
        
        # å†»ç»“å‰é¢çš„å±‚ï¼Œåªå¾®è°ƒåé¢çš„å±‚ï¼ˆé˜²æ­¢å°æ•°æ®é›†è¿‡æ‹Ÿåˆï¼‰
        for param in list(self.backbone.parameters())[:-20]:
            param.requires_grad = False
        
        # ç‰¹å¾æå–ç»´åº¦
        backbone_dim = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()
        
        # ç‰¹å¾æŠ•å½±å±‚ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        self.feature_projector = nn.Sequential(
            nn.Linear(backbone_dim, feature_dim),
            nn.BatchNorm1d(feature_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate)
        )
        
        # åˆ†ç±»å¤´ï¼ˆç®€å•ä½†æœ‰æ•ˆï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
        
        # èº«ä»½ç‰¹å¾è®°å¿†åº“ï¼ˆç”¨äºç­›é€‰æ—¶çš„ç‰¹å¾åŒ¹é…ï¼‰
        self.register_buffer('feature_bank', torch.zeros(num_classes, feature_dim))
        self.register_buffer('feature_count', torch.zeros(num_classes))
        
        # æ¸©åº¦å‚æ•°
        self.temperature = 1.0
    
    def forward(self, x, labels=None, update_bank=False):
        # ç‰¹å¾æå–
        backbone_features = self.backbone(x)
        features = self.feature_projector(backbone_features)
        
        # æ›´æ–°ç‰¹å¾åº“ï¼ˆè®­ç»ƒæ—¶ï¼‰
        if update_bank and labels is not None:
            with torch.no_grad():
                for i, label in enumerate(labels):
                    self.feature_bank[label] = self.feature_bank[label] * 0.95 + features[i] * 0.05
                    self.feature_count[label] += 1
        
        # åˆ†ç±»
        logits = self.classifier(features)
        
        return logits, features
    
    def compute_feature_similarity(self, features):
        """è®¡ç®—ç‰¹å¾ä¸å„ç±»åˆ«åŸå‹çš„ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–ç‰¹å¾
        features_norm = F.normalize(features, dim=1)
        prototypes_norm = F.normalize(self.feature_bank, dim=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = torch.matmul(features_norm, prototypes_norm.T)
        return similarity


def train_epoch(model, train_loader, criterion, optimizer, device, epoch, rank=0, use_contrastive=True):
    """è®­ç»ƒä¸€ä¸ªepochï¼ŒåŒ…å«å¯¹æ¯”å­¦ä¹ """
    model.train()
    total_loss = 0
    total_ce_loss = 0
    total_contrast_loss = 0
    correct = 0
    total = 0
    
    # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
    iterator = tqdm(train_loader, desc="Training") if rank == 0 else train_loader
    
    for batch_idx, (images, labels) in enumerate(iterator):
        images, labels = images.to(device), labels.to(device)
        
        # å‰å‘ä¼ æ’­ï¼ˆæ›´æ–°ç‰¹å¾åº“ï¼‰
        logits, features = model(images, labels, update_bank=True)
        
        # åˆ†ç±»æŸå¤±
        ce_loss = criterion(logits, labels)
        
        # å¯¹æ¯”æŸå¤±ï¼ˆå¢å¼ºèº«ä»½ç‰¹å¾å­¦ä¹ ï¼‰
        contrast_loss = torch.tensor(0.0).to(device)
        if use_contrastive and epoch > 5:  # å‰5ä¸ªepochåªç”¨åˆ†ç±»æŸå¤±
            # è·å–å®é™…æ¨¡å‹ï¼ˆå¤„ç†DDP wrapperï¼‰
            actual_model = model.module if hasattr(model, 'module') else model
            
            # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
            feature_sim = actual_model.compute_feature_similarity(features)
            
            # åŠ¨æ€è·å–ç±»åˆ«æ•°é‡
            num_classes = feature_sim.size(1)
            
            # å¯¹æ¯”æŸå¤±ï¼šåŒç±»ç‰¹å¾ç›¸ä¼¼ï¼Œå¼‚ç±»ç‰¹å¾åˆ†ç¦»
            labels_one_hot = F.one_hot(labels, num_classes=num_classes).float()
            positive_sim = (feature_sim * labels_one_hot).sum(dim=1)
            negative_sim = (feature_sim * (1 - labels_one_hot)).max(dim=1)[0]
            
            # Margin loss
            margin = 0.3
            contrast_loss = F.relu(negative_sim - positive_sim + margin).mean()
        
        # æ€»æŸå¤±
        loss = ce_loss + 0.1 * contrast_loss
        
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        total_ce_loss += ce_loss.item()
        total_contrast_loss += contrast_loss.item()
        
        _, predicted = logits.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    avg_loss = total_loss / len(train_loader)
    avg_ce_loss = total_ce_loss / len(train_loader)
    avg_contrast_loss = total_contrast_loss / len(train_loader)
    accuracy = correct / total
    
    return avg_loss, accuracy, avg_ce_loss, avg_contrast_loss


def evaluate(model, val_loader, criterion, device, rank=0):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    all_probs = []
    all_labels = []
    all_features = []
    
    # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
    iterator = tqdm(val_loader, desc="Evaluating") if rank == 0 else val_loader
    
    with torch.no_grad():
        for images, labels in iterator:
            images, labels = images.to(device), labels.to(device)
            
            logits, features = model(images)
            loss = criterion(logits, labels)
            
            total_loss += loss.item()
            
            probs = F.softmax(logits, dim=1)
            _, predicted = probs.max(1)
            
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_features.append(features.cpu())
            
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = correct / total
    
    # è®¡ç®—ECE
    ece = compute_ece(np.array(all_probs), np.array(all_labels))
    
    # è®¡ç®—ç‰¹å¾å¤šæ ·æ€§ï¼ˆç”¨äºè¯„ä¼°èº«ä»½ä¿ç•™ï¼‰
    all_features = torch.cat(all_features, dim=0)
    feature_std = all_features.std(dim=0).mean().item()
    
    return avg_loss, accuracy, ece, feature_std


def compute_ece(probs, labels, n_bins=10):
    """è®¡ç®—æœŸæœ›æ ¡å‡†è¯¯å·®"""
    confidences = np.max(probs, axis=1)
    predictions = np.argmax(probs, axis=1)
    accuracies = (predictions == labels)
    
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    ece = 0
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
        prop_in_bin = in_bin.mean()
        
        if prop_in_bin > 0:
            accuracy_in_bin = accuracies[in_bin].mean()
            avg_confidence_in_bin = confidences[in_bin].mean()
            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
    
    return ece


class SplitDataset(Dataset):
    """åŸºäºdataset_split.jsonçš„æ•°æ®é›†ç±»"""
    def __init__(self, split_data, transform=None, debug=False):
        self.transform = transform
        
        if debug:
            print(f"Split data type: {type(split_data)}")
            print(f"Split data length: {len(split_data)}")
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶è½¬æ¢
        self.data = []
        unique_classes = set()
        
        if isinstance(split_data, dict):
            # æ ¼å¼: {"User_01": ["/path1.jpg", "/path2.jpg"], "User_02": [...]}
            if debug:
                print(f"Detected user-grouped format with {len(split_data)} users")
                print(f"Users: {list(split_data.keys())[:5]}...")
            
            for user_id, image_paths in split_data.items():
                if debug and user_id == list(split_data.keys())[0]:
                    print(f"First user {user_id} has {len(image_paths)} images")
                    if len(image_paths) > 0:
                        print(f"First image: {image_paths[0]}")
                
                for img_path in image_paths:
                    self.data.append({'path': img_path, 'class': user_id})
                    unique_classes.add(user_id)
        
        elif isinstance(split_data, list):
            # å…¶ä»–æ ¼å¼çš„åˆ—è¡¨
            if debug:
                print(f"Detected list format")
                if len(split_data) > 0:
                    print(f"First item type: {type(split_data[0])}")
                    print(f"First item: {split_data[0]}")
            
            for item in split_data:
                if isinstance(item, dict):
                    # æ ¼å¼1: [{'path': '...', 'class': '...'}]
                    if 'class' in item and 'path' in item:
                        self.data.append(item)
                        unique_classes.add(item['class'])
                    elif 'label' in item and 'path' in item:
                        # å¯èƒ½ç”¨çš„æ˜¯labelè€Œä¸class
                        self.data.append({'path': item['path'], 'class': item['label']})
                        unique_classes.add(item['label'])
                    else:
                        if debug:
                            print(f"Unknown dict format: {item}")
                elif isinstance(item, str):
                    # æ ¼å¼2: ['/path/to/User_01/image1.jpg', ...]
                    # ä»è·¯å¾„ä¸­æå–ç±»åˆ«
                    path_parts = Path(item).parts
                    # å¯»æ‰¾User_XXæ¨¡å¼
                    user_class = None
                    for part in path_parts:
                        if part.startswith('User_'):
                            user_class = part
                            break
                    
                    if user_class:
                        self.data.append({'path': item, 'class': user_class})
                        unique_classes.add(user_class)
                    else:
                        if debug:
                            print(f"Cannot extract class from path: {item}")
                elif isinstance(item, list) and len(item) == 2:
                    # æ ¼å¼3: [['/path', 'class'], ...]
                    path, class_name = item
                    self.data.append({'path': path, 'class': class_name})
                    unique_classes.add(class_name)
                else:
                    if debug:
                        print(f"Unknown item format: {type(item)} - {item}")
        else:
            raise ValueError(f"Unsupported split_data format: {type(split_data)}")
        
        # åˆ›å»ºç±»åˆ«æ˜ å°„ï¼ˆæ•°å€¼æ’åºï¼‰
        def sort_ids(class_list):
            """æ•°å€¼æ’åºID_1, ID_2, ..., ID_31"""
            def extract_number(class_name):
                if class_name.startswith('ID_'):
                    try:
                        return int(class_name.split('_')[1])
                    except (IndexError, ValueError):
                        return float('inf')
                elif class_name.startswith('User_'):
                    try:
                        return int(class_name.split('_')[1])
                    except (IndexError, ValueError):
                        return float('inf')
                else:
                    return float('inf')
            
            return sorted(class_list, key=extract_number)
        
        self.classes = sort_ids(list(unique_classes))
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}
        
        if debug:
            print(f"Processed {len(self.data)} items")
            print(f"Found {len(self.classes)} classes: {self.classes[:5]}...")
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = item['path']
        class_name = item['class']
        
        # åŠ è½½å›¾åƒ
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # è¿”å›ä¸€ä¸ªé»‘è‰²å›¾åƒä½œä¸ºå¤‡é€‰
            image = Image.new('RGB', (256, 256), color='black')
        
        if self.transform:
            image = self.transform(image)
        
        # è¿”å›å›¾åƒå’Œç±»åˆ«ç´¢å¼•
        label = self.class_to_idx[class_name]
        return image, label


def setup_ddp():
    """åˆå§‹åŒ–DDPç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        print("Not using distributed training")
        return 0, 1, 0
    
    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend='nccl')
    
    return rank, world_size, local_rank


def cleanup_ddp():
    """æ¸…ç†DDPç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def load_dataset_split(split_file):
    """åŠ è½½æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶"""
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    return split_data


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--split_file', type=str, default='/kaggle/working/dataset_split.json',
                       help='æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./calibrated_classifier')
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--batch_size', type=int, default=16)  # å°æ•°æ®é›†ç”¨å°batch
    parser.add_argument('--lr', type=float, default=5e-4)  # æ›´å°çš„å­¦ä¹ ç‡
    parser.add_argument('--weight_decay', type=float, default=1e-4)
    parser.add_argument('--label_smoothing', type=float, default=0.05)  # é€‚åº¦å¹³æ»‘
    parser.add_argument('--dropout', type=float, default=0.3)  # é€‚åº¦dropout
    parser.add_argument('--mixup_alpha', type=float, default=0.0)  # å°æ•°æ®é›†ä¸ç”¨mixup
    parser.add_argument('--use_focal_loss', action='store_true')
    parser.add_argument('--early_stop_patience', type=int, default=8,
                       help='è¿ç»­å¤šå°‘ä¸ªepochä¸æ”¹å–„åˆ™æ—©åœ')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–DDP
    rank, world_size, local_rank = setup_ddp()
    is_distributed = world_size > 1
    
    device = torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')
    
    # åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºè¾“å‡ºç›®å½•
    if rank == 0:
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    if is_distributed:
        dist.barrier()  # ç­‰å¾…ä¸»è¿›ç¨‹åˆ›å»ºå®Œç›®å½•
    
    # åŠ è½½æ•°æ®é›†åˆ’åˆ†
    if rank == 0:
        print(f"Loading dataset split from: {args.split_file}")
    
    split_data = load_dataset_split(args.split_file)
    
    # æ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼‰
    debug_mode = (rank == 0)  # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    train_dataset = SplitDataset(split_data['train'], 
                                transform=train_transform, 
                                debug=debug_mode)
    val_dataset = SplitDataset(split_data['val'], 
                              transform=val_transform, 
                              debug=debug_mode)
    
    if rank == 0:
        print(f"Train samples: {len(train_dataset)}")
        print(f"Val samples: {len(val_dataset)}")
        print(f"Classes: {len(train_dataset.classes)}")
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    if is_distributed:
        train_sampler = DistributedSampler(train_dataset, 
                                         num_replicas=world_size, 
                                         rank=rank, 
                                         shuffle=True)
        val_sampler = DistributedSampler(val_dataset, 
                                       num_replicas=world_size, 
                                       rank=rank, 
                                       shuffle=False)
    else:
        train_sampler = None
        val_sampler = None
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, 
                             batch_size=args.batch_size, 
                             sampler=train_sampler,
                             shuffle=(train_sampler is None),
                             num_workers=2,  # åˆ†å¸ƒå¼è®­ç»ƒç”¨è¾ƒå°‘worker
                             pin_memory=True)
    
    val_loader = DataLoader(val_dataset, 
                           batch_size=args.batch_size, 
                           sampler=val_sampler,
                           shuffle=False,
                           num_workers=2,
                           pin_memory=True)
    
    # æ¨¡å‹åˆå§‹åŒ–
    num_classes = len(train_dataset.classes)
    model = DomainAdaptiveClassifier(num_classes=num_classes, dropout_rate=args.dropout)
    model.to(device)
    
    # åŒ…è£…ä¸ºDDPæ¨¡å‹
    if is_distributed:
        model = DDP(model, device_ids=[local_rank], find_unused_parameters=False)  # å…³é—­æœªä½¿ç”¨å‚æ•°æ£€æŸ¥
    
    # è®¡ç®—å‚æ•°é‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ‰“å°ï¼‰
    if rank == 0:
        model_for_counting = model.module if is_distributed else model
        total_params = sum(p.numel() for p in model_for_counting.parameters())
        trainable_params = sum(p.numel() for p in model_for_counting.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    
    # æŸå¤±å‡½æ•°é€‰æ‹©
    if args.use_focal_loss:
        criterion = FocalLoss(gamma=2.0)
    else:
        criterion = LabelSmoothingLoss(num_classes=num_classes, smoothing=args.label_smoothing)
    
    val_criterion = nn.CrossEntropyLoss()  # éªŒè¯æ—¶ç”¨æ ‡å‡†CE
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, 
                                  weight_decay=args.weight_decay)
    
    # å­¦ä¹ ç‡è°ƒåº¦ - ä½™å¼¦é€€ç«
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.epochs, eta_min=1e-6
    )
    
    # å­¦ä¹ ç‡é¢„çƒ­ï¼ˆå°æ•°æ®é›†é‡è¦ï¼‰
    warmup_epochs = 5
    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
        optimizer, start_factor=0.1, total_iters=warmup_epochs
    )
    
    # è®­ç»ƒå¾ªç¯
    best_ece = float('inf')
    best_score = -float('inf')  # ç”¨äºæ—©åœçš„æœ€ä½³åˆ†æ•° (val_acc - val_ece)
    patience_counter = 0
    history = []
    
    for epoch in range(args.epochs):
        # è®¾ç½®é‡‡æ ·å™¨çš„epochï¼ˆç”¨äºshuffleï¼‰
        if is_distributed and train_sampler is not None:
            train_sampler.set_epoch(epoch)
        
        if rank == 0:
            print(f"\nEpoch {epoch+1}/{args.epochs}")
            print(f"Learning rate: {scheduler.get_last_lr()[0]:.6f}")
        
        # è®­ç»ƒ
        train_loss, train_acc, ce_loss, contrast_loss = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch, rank
        )
        
        # éªŒè¯
        val_loss, val_acc, val_ece, feature_std = evaluate(
            model, val_loader, val_criterion, device, rank
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        if epoch < warmup_epochs:
            warmup_scheduler.step()
        else:
            scheduler.step()
        
        # è®°å½•å†å²
        history.append({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'val_ece': val_ece,
            'score': val_acc - val_ece,
            'patience_counter': patience_counter if rank == 0 else 0
        })
        
        if rank == 0:
            print(f"Train Loss: {train_loss:.4f} (CE: {ce_loss:.4f}, Contrast: {contrast_loss:.4f})")
            print(f"Train Acc: {train_acc:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            print(f"Val ECE: {val_ece:.4f}, Feature Std: {feature_std:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ—©åœæ£€æŸ¥ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if rank == 0:
            score = val_acc - val_ece  # ç»¼åˆåˆ†æ•°ï¼šå‡†ç¡®ç‡é«˜ä¸”æ ¡å‡†å¥½
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
            if score > best_score:
                best_score = score
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_ece < best_ece and val_acc > 0.85:
                    best_ece = val_ece
                    
                    model_to_save = model.module if is_distributed else model
                    torch.save({
                        'model_state_dict': model_to_save.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'epoch': epoch + 1,
                        'val_acc': val_acc,
                        'val_ece': val_ece,
                        'num_classes': num_classes,
                        'class_names': train_dataset.classes,
                        'args': vars(args)
                    }, Path(args.output_dir) / 'best_calibrated_model.pth')
                    print(f"Best model saved with ECE: {val_ece:.4f}")
                
                print(f"Score improved: {score:.4f} (best: {best_score:.4f})")
            else:
                patience_counter += 1
                print(f"No improvement for {patience_counter}/{args.early_stop_patience} epochs")
                
                # æ—©åœæ£€æŸ¥
                if patience_counter >= args.early_stop_patience:
                    print(f"\nğŸ›‘ Early stopping triggered after {patience_counter} epochs without improvement")
                    print(f"Best score: {best_score:.4f}, Best ECE: {best_ece:.4f}")
                    break
    
    # ä¿å­˜è®­ç»ƒå†å²ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    if rank == 0:
        with open(Path(args.output_dir) / 'training_history.json', 'w') as f:
            json.dump(history, f, indent=2)
        
        final_epoch = len(history)
        if patience_counter >= args.early_stop_patience:
            print(f"\nğŸ›‘ è®­ç»ƒæå‰åœæ­¢åœ¨ç¬¬ {final_epoch} epoch")
        else:
            print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ€»å…± {final_epoch} epochs")
        
        print(f"æœ€ä½³ç»¼åˆåˆ†æ•°: {best_score:.4f}")
        print(f"æœ€ä½³ECE: {best_ece:.4f}")
        print(f"æ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}")
    
    # æ¸…ç†DDP
    cleanup_ddp()


if __name__ == "__main__":
    main()

"""
å°†é¢„ç¼–ç çš„latentæ•°æ®è½¬æ¢ä¸ºLightningDiTå®˜æ–¹çš„safetensorsæ ¼å¼
ä¿æŒä¸å®˜æ–¹æ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´
"""

import torch
import numpy as np
from pathlib import Path
from safetensors.torch import save_file
from tqdm import tqdm
import argparse

def convert_to_safetensors(input_path, output_dir, split='train'):
    """
    è½¬æ¢latentæ•°æ®åˆ°safetensorsæ ¼å¼
    
    å®˜æ–¹æ ¼å¼:
    - latents: [N, C, H, W] çš„latentå‘é‡
    - latents_flip: æ°´å¹³ç¿»è½¬çš„latentï¼ˆæ•°æ®å¢å¼ºï¼‰
    - labels: ç±»åˆ«æ ‡ç­¾ï¼ˆæ— æ¡ä»¶ç”Ÿæˆæ—¶ä¸º0ï¼‰
    """
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½é¢„ç¼–ç çš„latents
    print(f"ğŸ“‚ åŠ è½½ {split} latents...")
    latent_file = Path(input_path) / f"{split}_latents.pt"
    
    if not latent_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {latent_file}")
        return
    
    data = torch.load(latent_file, map_location='cpu', weights_only=False)
    latents = data['latents']  # [N, C, H, W]
    user_ids = data.get('user_ids', None)  # ç”¨æˆ·IDæ ‡ç­¾
    
    print(f"âœ… åŠ è½½äº† {len(latents)} ä¸ªlatents")
    print(f"   Shape: {latents[0].shape}")
    
    if user_ids is not None:
        print(f"   ç”¨æˆ·æ ‡ç­¾: {len(user_ids)} ä¸ªï¼ŒèŒƒå›´ {user_ids.min()}-{user_ids.max()}")
    else:
        print("   âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·æ ‡ç­¾ï¼Œå°†ä½¿ç”¨0ä½œä¸ºé»˜è®¤æ ‡ç­¾")
    
    # è®¡ç®—channel-wiseç»Ÿè®¡ï¼ˆä¸å®˜æ–¹ä¸€è‡´ï¼‰
    print("ğŸ“Š è®¡ç®—channel-wiseç»Ÿè®¡...")
    mean = latents.mean(dim=[0, 2, 3], keepdim=True)  # [1, 32, 1, 1]
    std = latents.std(dim=[0, 2, 3], keepdim=True)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆå®˜æ–¹æ ¼å¼ï¼‰
    stats = {
        'mean': mean.squeeze(0),  # [32, 1, 1]
        'std': std.squeeze(0)      # [32, 1, 1]
    }
    stats_file = output_dir / 'latents_stats.pt'
    torch.save(stats, stats_file)
    print(f"âœ… ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° {stats_file}")
    
    # åˆ†æ‰¹ä¿å­˜ä¸ºsafetensorsï¼ˆå®˜æ–¹æ¯ä¸ªæ–‡ä»¶1000ä¸ªæ ·æœ¬ï¼‰
    batch_size = 1000
    num_batches = (len(latents) + batch_size - 1) // batch_size
    
    print(f"ğŸ“¦ è½¬æ¢ä¸ºsafetensorsæ ¼å¼...")
    for batch_idx in tqdm(range(num_batches), desc="æ‰¹æ¬¡"):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(latents))
        
        batch_latents = latents[start_idx:end_idx]
        
        # æ— æ•°æ®å¢å¼º - ä½¿ç”¨ç›¸åŒçš„latents
        batch_latents_flip = batch_latents  # ä¸åšç¿»è½¬
        
        # æ— æ¡ä»¶ç”Ÿæˆï¼Œlabelså…¨ä¸º0
        batch_labels = torch.zeros(len(batch_latents), dtype=torch.long)
        
        # ä¿å­˜ä¸ºsafetensors
        safetensors_data = {
            'latents': batch_latents,
            'latents_flip': batch_latents_flip,
            'labels': batch_labels
        }
        
        output_file = output_dir / f'latents_rank00_shard{batch_idx:03d}.safetensors'
        save_file(safetensors_data, output_file)
    
    print(f"âœ… å®Œæˆï¼ä¿å­˜äº† {num_batches} ä¸ªsafetensorsæ–‡ä»¶åˆ° {output_dir}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_dir', type=str, default='./latents',
                        help='åŒ…å«train_latents.ptå’Œval_latents.ptçš„ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='./latents_safetensors',
                        help='è¾“å‡ºsafetensorsæ ¼å¼çš„ç›®å½•')
    args = parser.parse_args()
    
    # è½¬æ¢è®­ç»ƒé›†
    print("\n=== è½¬æ¢è®­ç»ƒé›† ===")
    train_output = Path(args.output_dir) / 'train'
    convert_to_safetensors(args.input_dir, train_output, 'train')
    
    # è½¬æ¢éªŒè¯é›†
    print("\n=== è½¬æ¢éªŒè¯é›† ===")
    val_output = Path(args.output_dir) / 'val'
    convert_to_safetensors(args.input_dir, val_output, 'val')
    
    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"è¯·æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„data_pathä¸º: {train_output}")
    print(f"è¯·æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„valid_pathä¸º: {val_output}")

if __name__ == "__main__":
    main()

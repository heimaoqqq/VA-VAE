#!/usr/bin/env python3
"""
SimplifiedVAVAE: ç®€åŒ–ç‰ˆVA-VAEï¼Œç¦ç”¨VFåŠŸèƒ½ï¼Œä»…ä¿ç•™é‡å»ºèƒ½åŠ›
ä¸“é—¨ç”¨äºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„VAEç¼–ç å™¨/è§£ç å™¨
"""

import torch
import torch.nn as nn
from pathlib import Path
import sys
import yaml

# æ·»åŠ LightningDiTè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))

# è®¾ç½®tamingè·¯å¾„
def setup_taming_path():
    taming_locations = [
        Path('/kaggle/working/taming-transformers'),
        Path.cwd().parent / 'taming-transformers',
    ]
    for location in taming_locations:
        if location.exists() and str(location) not in sys.path:
            sys.path.insert(0, str(location))
            return True
    return False

setup_taming_path()

from omegaconf import OmegaConf
from ldm.models.autoencoder import AutoencoderKL
from ldm.util import instantiate_from_config


class SimplifiedVAVAE(nn.Module):
    """ç®€åŒ–VA-VAEï¼šæ”¯æŒVFåŠŸèƒ½ä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹"""
    
    def __init__(self, checkpoint_path=None, use_vf='dinov2'):
        super().__init__()
        self.scale_factor = 1.0  # é»˜è®¤å€¼ï¼Œä»checkpointä¸­è¯»å–çœŸå®å€¼
        self.use_vf = use_vf
        
        # åˆ›å»ºVA-VAEé…ç½®ï¼ˆå¯ç”¨VFä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹ï¼‰
        config = OmegaConf.create({
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'monitor': 'val/rec_loss',
                'embed_dim': 32,
                'ddconfig': {
                    'double_z': True, 
                    'z_channels': 32, 
                    'resolution': 256,
                    'in_channels': 3, 
                    'out_ch': 3, 
                    'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4], 
                    'num_res_blocks': 2,
                    'attn_resolutions': [16], 
                    'dropout': 0.0
                },
                'lossconfig': {
                    'target': 'ldm.modules.losses.LPIPSWithDiscriminator',
                    'params': {
                        'disc_start': 50001,  # æ¨ç†æ—¶ä¸ä½¿ç”¨åˆ¤åˆ«å™¨
                        'kl_weight': 1e-6,
                        'pixelloss_weight': 1.0,
                        'perceptual_weight': 1.0,
                        'disc_weight': 0.0,  # æ¨ç†æ—¶ç¦ç”¨åˆ¤åˆ«å™¨
                    }
                },
                'use_vf': use_vf,  # âœ… å¯ç”¨VFä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹é…ç½®
                'reverse_proj': False,
            }
        })
        
        # åˆ›å»ºæ¨¡å‹
        self.vae = instantiate_from_config(config)
        
        # åŠ è½½æƒé‡ï¼ˆå¦‚æœæä¾›ï¼‰
        if checkpoint_path and Path(checkpoint_path).exists():
            self.load_checkpoint(checkpoint_path)
        
        # å†»ç»“å‚æ•°
        self.freeze()
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½VA-VAEæƒé‡"""
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # å¤„ç†ä¸åŒæ ¼å¼çš„checkpoint
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # æå–çœŸå®çš„ç¼©æ”¾å› å­
        if 'scale_factor' in checkpoint:
            self.scale_factor = float(checkpoint['scale_factor'])
            print(f"ğŸ”§ ä»checkpointè¯»å–ç¼©æ”¾å› å­: {self.scale_factor}")
        elif 'state_dict' in checkpoint and any('scale_factor' in k for k in checkpoint['state_dict'].keys()):
            # å¯»æ‰¾åŒ…å«scale_factorçš„é”®
            for k, v in checkpoint['state_dict'].items():
                if 'scale_factor' in k and isinstance(v, torch.Tensor):
                    self.scale_factor = float(v.item())
                    print(f"ğŸ”§ ä»state_dictè¯»å–ç¼©æ”¾å› å­: {self.scale_factor}")
                    break
        else:
            # åŠ¨æ€è®¡ç®—ç¼©æ”¾å› å­çš„å¤‡ç”¨æ–¹æ¡ˆ
            print(f"âš ï¸ æœªåœ¨checkpointä¸­æ‰¾åˆ°scale_factorï¼Œä½¿ç”¨é»˜è®¤å€¼1.0")
            print(f"   å»ºè®®ï¼šè®­ç»ƒæ—¶æ·»åŠ scale_by_std=Trueæ¥åŠ¨æ€è®¡ç®—")
        
        # æ ¹æ®VFé…ç½®å†³å®šæ˜¯å¦åŒ…å«VFæƒé‡
        filtered_state_dict = {}
        if self.use_vf:
            # å¯ç”¨VFæ—¶ï¼Œä¿ç•™VFç›¸å…³æƒé‡ï¼Œä»…æ’é™¤foundation_model
            excluded_prefixes = ['foundation_model']
        else:
            # ç¦ç”¨VFæ—¶ï¼Œæ’é™¤æ‰€æœ‰VFç›¸å…³æƒé‡
            excluded_prefixes = ['vf_proj', 'vf_model', 'foundation_model']
        
        print(f"ğŸ” VFæ¨¡å¼: {'å¯ç”¨' if self.use_vf else 'ç¦ç”¨'}, æ’é™¤å‰ç¼€: {excluded_prefixes}")
        
        for k, v in state_dict.items():
            # ä¿®å¤ï¼šä½¿ç”¨ç²¾ç¡®å‰ç¼€åŒ¹é…ï¼Œé¿å…å­å­—ç¬¦ä¸²è¯¯åŒ¹é…
            should_exclude = False
            for prefix in excluded_prefixes:
                # æ£€æŸ¥æ˜¯å¦ä»¥å‰ç¼€å¼€å¤´ï¼Œæˆ–è€…å‰ç¼€å‰é¢æœ‰åˆ†éš”ç¬¦
                if k.startswith(prefix) or f'.{prefix}' in k or f'_{prefix}' in k:
                    # ç‰¹æ®Šå¤„ç†ï¼šlinear_projä¸åº”è¯¥è¢«vf_projæ’é™¤
                    if prefix == 'vf_proj' and 'linear_proj' in k:
                        continue  # ä¸æ’é™¤linear_proj
                    should_exclude = True
                    break
            
            if not should_exclude:
                # ç§»é™¤å‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
                clean_key = k.replace('module.', '').replace('vae.', '')
                filtered_state_dict[clean_key] = v
        
        print(f"ğŸ“Š è¿‡æ»¤åæƒé‡æ•°é‡: {len(filtered_state_dict)}")
        all_keys = list(filtered_state_dict.keys())
        linear_keys = [k for k in all_keys if 'linear' in k.lower()]
        if linear_keys:
            print(f"ğŸ” åŒ…å«linearçš„é”®: {linear_keys}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•åŒ…å«linearçš„é”®")
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰åŒ…å«linear_projçš„é”®
        linear_proj_keys = [k for k in filtered_state_dict.keys() if 'linear_proj' in k]
        if linear_proj_keys:
            print(f"ğŸ” å‘ç°linear_projç›¸å…³é”®: {linear_proj_keys}")
            for key in linear_proj_keys:
                if 'weight' in key:
                    shape = filtered_state_dict[key].shape
                    print(f"   {key}: {list(shape)}")
                    # ä¿®å¤å½¢çŠ¶ä¸åŒ¹é…
                    if shape == torch.Size([1024, 32, 1, 1]):
                        filtered_state_dict[key] = filtered_state_dict[key].transpose(0, 1)
                        print(f"   ğŸ”§ å·²ä¿®å¤ {key}: [1024,32,1,1] -> [32,1024,1,1]")
        
        # åŠ è½½æƒé‡
        missing, unexpected = self.vae.load_state_dict(filtered_state_dict, strict=False)
        
        if missing:
            print(f"âš ï¸ ç¼ºå¤±çš„æƒé‡: {missing[:5]}...")  # ä»…æ˜¾ç¤ºå‰5ä¸ª
        if unexpected:
            print(f"âš ï¸ æœªé¢„æœŸçš„æƒé‡: {unexpected[:5]}...")
        
        print(f"âœ… æˆåŠŸåŠ è½½VA-VAEæƒé‡: {checkpoint_path}")
        print(f"ğŸ“ ä½¿ç”¨ç¼©æ”¾å› å­: {self.scale_factor}")
    
    def freeze(self):
        """å†»ç»“VAEå‚æ•°"""
        for param in self.vae.parameters():
            param.requires_grad = False
        self.vae.eval()
    
    @torch.no_grad()
    def encode(self, x):
        """
        ç¼–ç å›¾åƒåˆ°latentç©ºé—´
        Args:
            x: [B, 3, 256, 256] å›¾åƒå¼ é‡
        Returns:
            z: [B, 32, 16, 16] latentå¼ é‡ï¼ˆå·²ç¼©æ”¾ï¼‰
        """
        # ç¡®ä¿è¾“å…¥èŒƒå›´æ­£ç¡®
        if x.min() >= 0 and x.max() <= 1:
            x = 2.0 * x - 1.0  # [0,1] -> [-1,1]
        
        # ç¼–ç 
        posterior = self.vae.encode(x)
        z = posterior.sample()
        
        # åº”ç”¨ç¼©æ”¾
        z = z * self.scale_factor
        
        return z
    
    @torch.no_grad()
    def decode(self, z):
        """
        è§£ç latentåˆ°å›¾åƒ
        Args:
            z: [B, 32, 16, 16] latentå¼ é‡ï¼ˆå·²ç¼©æ”¾ï¼‰
        Returns:
            x: [B, 3, 256, 256] å›¾åƒå¼ é‡
        """
        # è¿˜åŸç¼©æ”¾
        z = z / self.scale_factor
        
        # è§£ç 
        x = self.vae.decode(z)
        
        # è½¬æ¢åˆ°[0,1]
        x = (x + 1.0) / 2.0
        x = torch.clamp(x, 0, 1)
        
        return x
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼šç¼–ç åè§£ç """
        z = self.encode(x)
        x_recon = self.decode(z)
        return x_recon, z

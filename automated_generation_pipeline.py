#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æ¡ä»¶ç”Ÿæˆ-ç­›é€‰ç®¡é“
ç»“åˆæ¡ä»¶æ‰©æ•£ç”Ÿæˆå’Œè´¨é‡ç­›é€‰ï¼Œè‡ªåŠ¨ä¿å­˜åˆ°ç›®æ ‡æ•°é‡
"""

import os
import sys
import time
import json
import argparse
import torch
import torch.distributed as dist
from pathlib import Path
from tqdm import tqdm
import numpy as np
from PIL import Image
from collections import defaultdict
import logging

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å¿…è¦æ¨¡å—
sys.path.append(str(Path(__file__).parent / "LightningDiT"))

from LightningDiT.sample import create_model_and_diffusion, sample_from_model
from LightningDiT.utils import set_logger, set_seed
from improved_classifier_training import ImprovedClassifier
from prepare_safetensors_dataset import MicroDopplerDataset
import torchvision.transforms as transforms


class AutomatedGenerationPipeline:
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.setup_logging()
        self.setup_directories()
        self.load_models()
        self.setup_transforms()
        self.user_progress = defaultdict(int)  # è·Ÿè¸ªæ¯ä¸ªç”¨æˆ·å·²ä¿å­˜çš„æ ·æœ¬æ•°
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = Path(self.args.output_dir) / 'generation_log.txt'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        self.output_dir = Path(self.args.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºè¾“å‡ºç›®å½•
        self.user_dirs = {}
        for user_id in range(self.args.num_users):
            user_dir = self.output_dir / f"ID_{user_id:02d}"
            user_dir.mkdir(exist_ok=True)
            self.user_dirs[user_id] = user_dir
            
        self.logger.info(f"è®¾ç½®è¾“å‡ºç›®å½•: {self.output_dir}")
        
    def load_models(self):
        """åŠ è½½æ‰©æ•£æ¨¡å‹å’Œåˆ†ç±»å™¨"""
        # åŠ è½½æ‰©æ•£æ¨¡å‹
        self.logger.info("åŠ è½½æ‰©æ•£æ¨¡å‹...")
        self.model, self.diffusion = create_model_and_diffusion(self.args.config)
        
        # åŠ è½½checkpoint
        if self.args.checkpoint:
            checkpoint = torch.load(self.args.checkpoint, map_location='cpu')
            if 'model' in checkpoint:
                self.model.load_state_dict(checkpoint['model'])
            else:
                self.model.load_state_dict(checkpoint)
            self.logger.info(f"åŠ è½½checkpoint: {self.args.checkpoint}")
            
        self.model = self.model.to(self.device)
        self.model.eval()
        
        # åŠ è½½åˆ†ç±»å™¨
        self.logger.info("åŠ è½½åˆ†ç±»å™¨...")
        self.classifier = ImprovedClassifier(num_classes=self.args.num_users)
        classifier_checkpoint = torch.load(self.args.classifier_path, map_location='cpu')
        self.classifier.load_state_dict(classifier_checkpoint)
        self.classifier = self.classifier.to(self.device)
        self.classifier.eval()
        
    def setup_transforms(self):
        """è®¾ç½®å›¾åƒé¢„å¤„ç†"""
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
    def generate_batch(self, user_ids, batch_size):
        """ç”Ÿæˆä¸€ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬"""
        with torch.no_grad():
            # åˆ›å»ºæ¡ä»¶æ ‡ç­¾
            y = torch.tensor(user_ids, device=self.device)
            
            # ç”Ÿæˆæ ·æœ¬
            samples = sample_from_model(
                self.model, 
                self.diffusion,
                batch_size=len(user_ids),
                class_labels=y,
                cfg_scale=self.args.cfg_scale,
                device=self.device
            )
            
            return samples
            
    def evaluate_samples(self, samples, expected_user_ids):
        """è¯„ä¼°ç”Ÿæˆæ ·æœ¬çš„è´¨é‡"""
        batch_results = []
        
        with torch.no_grad():
            # è½¬æ¢ä¸ºPILå›¾åƒå¹¶é¢„å¤„ç†
            processed_samples = []
            for sample in samples:
                # å‡è®¾sampleæ˜¯CHWæ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºHWC
                if len(sample.shape) == 3:
                    sample = sample.permute(1, 2, 0)
                
                # å½’ä¸€åŒ–åˆ°0-255èŒƒå›´
                sample = ((sample + 1) * 127.5).clamp(0, 255).cpu().numpy().astype(np.uint8)
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                if sample.shape[2] == 1:
                    sample = np.repeat(sample, 3, axis=2)  # ç°åº¦è½¬RGB
                pil_image = Image.fromarray(sample)
                
                # åº”ç”¨å˜æ¢
                tensor_image = self.transform(pil_image).unsqueeze(0).to(self.device)
                processed_samples.append(tensor_image)
            
            # æ‰¹é‡å¤„ç†
            if processed_samples:
                batch_tensor = torch.cat(processed_samples, dim=0)
                logits = self.classifier(batch_tensor)
                probabilities = torch.softmax(logits, dim=1)
                predicted_classes = torch.argmax(logits, dim=1)
                max_probabilities = torch.max(probabilities, dim=1)[0]
                
                # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
                for i, (pred_class, confidence, expected_id) in enumerate(zip(
                    predicted_classes.cpu().numpy(),
                    max_probabilities.cpu().numpy(), 
                    expected_user_ids
                )):
                    is_correct = (pred_class == expected_id)
                    is_high_confidence = (confidence >= self.args.confidence_threshold)
                    
                    result = {
                        'sample_idx': i,
                        'expected_id': expected_id,
                        'predicted_id': pred_class,
                        'confidence': confidence,
                        'is_correct': is_correct,
                        'is_high_confidence': is_high_confidence,
                        'accept': is_correct and is_high_confidence,
                        'pil_image': Image.fromarray(
                            ((samples[i].permute(1, 2, 0) + 1) * 127.5)
                            .clamp(0, 255).cpu().numpy().astype(np.uint8)
                        )
                    }
                    batch_results.append(result)
                    
        return batch_results
        
    def save_accepted_samples(self, batch_results):
        """ä¿å­˜é€šè¿‡ç­›é€‰çš„æ ·æœ¬"""
        saved_count = 0
        
        for result in batch_results:
            if result['accept']:
                user_id = result['expected_id']
                
                # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡æ•°é‡
                if self.user_progress[user_id] >= self.args.target_per_user:
                    continue
                    
                # ä¿å­˜æ ·æœ¬
                filename = f"generated_{self.user_progress[user_id]:04d}_conf_{result['confidence']:.3f}.png"
                save_path = self.user_dirs[user_id] / filename
                
                # ç¡®ä¿æ˜¯RGBæ ¼å¼
                pil_image = result['pil_image']
                if pil_image.mode != 'RGB':
                    if pil_image.mode == 'L':
                        pil_image = pil_image.convert('RGB')
                        
                pil_image.save(save_path)
                self.user_progress[user_id] += 1
                saved_count += 1
                
                # è®°å½•ä¿å­˜ä¿¡æ¯
                self.logger.info(
                    f"ä¿å­˜æ ·æœ¬: User_{user_id:02d} -> {filename} "
                    f"(ç½®ä¿¡åº¦: {result['confidence']:.3f}, "
                    f"è¿›åº¦: {self.user_progress[user_id]}/{self.args.target_per_user})"
                )
                
        return saved_count
        
    def check_completion(self):
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç”¨æˆ·éƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡"""
        completed_users = sum(1 for count in self.user_progress.values() 
                            if count >= self.args.target_per_user)
        
        return completed_users >= self.args.num_users
        
    def print_progress(self):
        """æ‰“å°å½“å‰è¿›åº¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ç”Ÿæˆè¿›åº¦ç»Ÿè®¡")
        print(f"{'='*60}")
        
        for user_id in range(self.args.num_users):
            progress = self.user_progress[user_id]
            percentage = (progress / self.args.target_per_user) * 100
            bar_length = 20
            filled_length = int(bar_length * progress / self.args.target_per_user)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            
            print(f"User_{user_id:02d}: [{bar}] {progress:3d}/{self.args.target_per_user} ({percentage:5.1f}%)")
            
        total_saved = sum(self.user_progress.values())
        total_target = self.args.num_users * self.args.target_per_user
        print(f"\nğŸ¯ æ€»è¿›åº¦: {total_saved}/{total_target} ({total_saved/total_target*100:.1f}%)")
        
    def run(self):
        """è¿è¡Œè‡ªåŠ¨åŒ–ç®¡é“"""
        self.logger.info("ğŸš€ å¯åŠ¨è‡ªåŠ¨åŒ–ç”Ÿæˆ-ç­›é€‰ç®¡é“")
        
        total_generated = 0
        total_accepted = 0
        batch_count = 0
        
        try:
            while not self.check_completion():
                batch_count += 1
                
                # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„ç”¨æˆ·IDåˆ—è¡¨
                current_batch_ids = []
                for user_id in range(self.args.num_users):
                    if self.user_progress[user_id] < self.args.target_per_user:
                        # æ ¹æ®å½“å‰è¿›åº¦å†³å®šè¯¥ç”¨æˆ·åœ¨æ‰¹æ¬¡ä¸­çš„æ ·æœ¬æ•°
                        remaining = self.args.target_per_user - self.user_progress[user_id]
                        batch_size_for_user = min(self.args.batch_size // self.args.num_users + 1, 
                                                remaining * 2)  # ç”Ÿæˆ2å€æ•°é‡ä»¥æé«˜ç­›é€‰æ•ˆç‡
                        current_batch_ids.extend([user_id] * batch_size_for_user)
                
                if not current_batch_ids:
                    break
                    
                # é™åˆ¶æ‰¹æ¬¡å¤§å°
                if len(current_batch_ids) > self.args.batch_size:
                    current_batch_ids = current_batch_ids[:self.args.batch_size]
                
                self.logger.info(f"æ‰¹æ¬¡ {batch_count}: ç”Ÿæˆ {len(current_batch_ids)} ä¸ªæ ·æœ¬...")
                
                # ç”Ÿæˆæ ·æœ¬
                samples = self.generate_batch(current_batch_ids, len(current_batch_ids))
                total_generated += len(samples)
                
                # è¯„ä¼°æ ·æœ¬
                results = self.evaluate_samples(samples, current_batch_ids)
                
                # ä¿å­˜é€šè¿‡ç­›é€‰çš„æ ·æœ¬
                saved_count = self.save_accepted_samples(results)
                total_accepted += saved_count
                
                # è®¡ç®—æ‰¹æ¬¡ç»Ÿè®¡
                batch_accuracy = sum(1 for r in results if r['is_correct']) / len(results)
                batch_acceptance = saved_count / len(results)
                
                self.logger.info(
                    f"æ‰¹æ¬¡ {batch_count} å®Œæˆ: "
                    f"å‡†ç¡®ç‡ {batch_accuracy:.1%}, "
                    f"æ¥å—ç‡ {batch_acceptance:.1%}, "
                    f"ä¿å­˜ {saved_count} ä¸ªæ ·æœ¬"
                )
                
                # æ¯10ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡è¿›åº¦
                if batch_count % 10 == 0:
                    self.print_progress()
                    
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­ç”Ÿæˆè¿‡ç¨‹")
            
        # æœ€ç»ˆç»Ÿè®¡
        self.print_progress()
        self.logger.info(f"ğŸ‰ ç”Ÿæˆå®Œæˆ!")
        self.logger.info(f"ğŸ“Š æ€»ç»Ÿè®¡: ç”Ÿæˆ {total_generated} ä¸ªæ ·æœ¬, æ¥å— {total_accepted} ä¸ªæ ·æœ¬")
        self.logger.info(f"ğŸ“Š æ€»ä½“æ¥å—ç‡: {total_accepted/total_generated:.1%}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_generated': total_generated,
            'total_accepted': total_accepted,
            'acceptance_rate': total_accepted / total_generated,
            'user_progress': dict(self.user_progress),
            'batch_count': batch_count
        }
        
        stats_file = self.output_dir / 'generation_stats.json'
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
            
        self.logger.info(f"ğŸ“„ ç»Ÿè®¡ä¿¡æ¯ä¿å­˜åˆ°: {stats_file}")


def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–æ¡ä»¶ç”Ÿæˆ-ç­›é€‰ç®¡é“')
    
    # æ‰©æ•£æ¨¡å‹å‚æ•°
    parser.add_argument('--checkpoint', required=True, help='æ‰©æ•£æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--config', required=True, help='æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--cfg_scale', type=float, default=10.0, help='CFG scale')
    
    # åˆ†ç±»å™¨å‚æ•°  
    parser.add_argument('--classifier_path', required=True, help='åˆ†ç±»å™¨æ¨¡å‹è·¯å¾„')
    parser.add_argument('--confidence_threshold', type=float, default=0.9, 
                       help='ç½®ä¿¡åº¦é˜ˆå€¼')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--num_users', type=int, default=31, help='ç”¨æˆ·æ•°é‡')
    parser.add_argument('--target_per_user', type=int, default=300, 
                       help='æ¯ä¸ªç”¨æˆ·ç›®æ ‡æ ·æœ¬æ•°é‡')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', default='./automated_samples', 
                       help='è¾“å‡ºç›®å½•')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºå¹¶è¿è¡Œç®¡é“
    pipeline = AutomatedGenerationPipeline(args)
    pipeline.run()


if __name__ == '__main__':
    main()

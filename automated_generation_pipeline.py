#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æ¡ä»¶ç”Ÿæˆ-ç­›é€‰ç®¡é“
ç»“åˆæ¡ä»¶æ‰©æ•£ç”Ÿæˆå’Œè´¨é‡ç­›é€‰ï¼Œè‡ªåŠ¨ä¿å­˜åˆ°ç›®æ ‡æ•°é‡
"""

import os
import sys
import time
import json
import argparse
import torch
import torch.distributed as dist
from pathlib import Path
from tqdm import tqdm
import numpy as np
from PIL import Image
from collections import defaultdict
import logging

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å¿…è¦æ¨¡å—
sys.path.append(str(Path(__file__).parent / "LightningDiT"))
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥ç”Ÿæˆç›¸å…³æ¨¡å— - ä½¿ç”¨ç°æœ‰çš„ç”Ÿæˆè„šæœ¬ä¸­çš„å‡½æ•°
import yaml
from omegaconf import OmegaConf
import torchvision.transforms as transforms
from improved_classifier_training import ImprovedClassifier

# å¯¼å…¥LightningDiTç›¸å…³æ¨¡å—
from models.lightningdit import LightningDiT_models
from transport import create_transport, Sampler
from simplified_vavae import SimplifiedVAVAE

# æ³¨æ„ï¼šVA_VAEä¼šåœ¨load_modelsä¸­åŠ¨æ€å¯¼å…¥ï¼Œè¿™é‡Œä¸éœ€è¦æå‰å¯¼å…¥

import tempfile

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    import random
    random.seed(seed)

class AutomatedGenerationPipeline:
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.rank = 0
        self.local_rank = 0
        self.world_size = 1
        self.setup_logging()
        self.setup_directories()
        self.load_models()
        self.setup_transforms()
        self.user_progress = defaultdict(int)  # è·Ÿè¸ªæ¯ä¸ªç”¨æˆ·å·²ä¿å­˜çš„æ ·æœ¬æ•°
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(self.args.output_dir).mkdir(parents=True, exist_ok=True)
        log_file = Path(self.args.output_dir) / 'generation_log.txt'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        self.output_dir = Path(self.args.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºè¾“å‡ºç›®å½•
        self.user_dirs = {}
        for user_id in range(self.args.num_users):
            user_dir = self.output_dir / f"ID_{user_id:02d}"
            user_dir.mkdir(exist_ok=True)
            self.user_dirs[user_id] = user_dir
            
        self.logger.info(f"è®¾ç½®è¾“å‡ºç›®å½•: {self.output_dir}")
        
    def load_models(self):
        """åŠ è½½æ‰©æ•£æ¨¡å‹å’Œåˆ†ç±»å™¨"""
        # åŠ è½½é…ç½®
        with open(self.args.config, 'r', encoding='utf-8') as f:
            self.config = yaml.load(f, Loader=yaml.FullLoader)
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        sys.path.append('LightningDiT')
        from models.lightningdit import LightningDiT_models
        from transport import create_transport, Sampler
        from simplified_vavae import SimplifiedVAVAE
        
        # åŠ è½½æ‰©æ•£æ¨¡å‹
        self.logger.info("åŠ è½½æ‰©æ•£æ¨¡å‹...")
        latent_size = self.config['data']['image_size'] // self.config['vae']['downsample_ratio']
        self.model = LightningDiT_models[self.config['model']['model_type']](
            input_size=latent_size,
            num_classes=self.config['data']['num_classes'],
            class_dropout_prob=self.config['model'].get('class_dropout_prob', 0.1),
            use_qknorm=self.config['model']['use_qknorm'],
            use_swiglu=self.config['model'].get('use_swiglu', False),
            use_rope=self.config['model'].get('use_rope', False),
            use_rmsnorm=self.config['model'].get('use_rmsnorm', False),
            wo_shift=self.config['model'].get('wo_shift', False),
            in_channels=self.config['model'].get('in_chans', 4),
            use_checkpoint=self.config['model'].get('use_checkpoint', False),
        ).to(self.device)
        
        # åŠ è½½checkpoint
        if self.args.checkpoint and os.path.exists(self.args.checkpoint):
            self.logger.info(f"åŠ è½½checkpoint: {self.args.checkpoint}")
            checkpoint = torch.load(self.args.checkpoint, map_location='cpu')
            
            # å¤„ç†æƒé‡é”®å
            if 'ema' in checkpoint:
                checkpoint_weights = {'model': checkpoint['ema']}
                self.logger.info("ä½¿ç”¨EMAæƒé‡è¿›è¡Œæ¨ç†")
            elif 'model' in checkpoint:
                checkpoint_weights = checkpoint
                self.logger.info("ä½¿ç”¨æ¨¡å‹æƒé‡è¿›è¡Œæ¨ç†")
            else:
                checkpoint_weights = {'model': checkpoint}
                
            # æ¸…ç†é”®å
            checkpoint_weights['model'] = {k.replace('module.', ''): v for k, v in checkpoint_weights['model'].items()}
            
            # åŠ è½½æƒé‡
            self.load_weights_with_shape_check(self.model, checkpoint_weights)
            
        self.model.eval()
        
        # åˆå§‹åŒ–transportå’Œsampler
        self.transport = create_transport(
            self.config['transport']['path_type'],
            self.config['transport']['prediction'],
            self.config['transport']['loss_weight'],
            self.config['transport']['train_eps'],
            self.config['transport']['sample_eps'],
            use_cosine_loss=self.config['transport'].get('use_cosine_loss', False),
            use_lognorm=self.config['transport'].get('use_lognorm', False),
            partitial_train=self.config['transport'].get('partitial_train', None),
            partial_ratio=self.config['transport'].get('partial_ratio', 1.0),
            shift_lg=self.config['transport'].get('shift_lg', False),
        )
        self.sampler = Sampler(self.transport)
        
        # åŠ è½½VAEï¼ˆå®Œå…¨æŒ‰ç…§generate_conditional_samples_distributed.pyæ–¹å¼ï¼‰
        self.vae = None
        try:
            # æ·»åŠ LightningDiTè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
            lightningdit_path = os.path.join(os.getcwd(), 'LightningDiT')
            if lightningdit_path not in sys.path:
                sys.path.insert(0, lightningdit_path)
            
            from tokenizer.vavae import VA_VAE
            
            # ä½¿ç”¨è®­ç»ƒå¥½çš„VAEæ¨¡å‹è·¯å¾„
            custom_vae_checkpoint = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
            
            # åˆ›å»ºä¸train_dit_s_official.pyå®Œå…¨ä¸€è‡´çš„é…ç½®
            vae_config = {
                'ckpt_path': custom_vae_checkpoint,
                'model': {
                    'base_learning_rate': 2.0e-05,
                    'target': 'ldm.models.autoencoder.AutoencoderKL',
                    'params': {
                        'monitor': 'val/rec_loss',
                        'embed_dim': 32,
                        'use_vf': 'dinov2',
                        'reverse_proj': True,
                        'ddconfig': {
                            'double_z': True, 'z_channels': 32, 'resolution': 256,
                            'in_channels': 3, 'out_ch': 3, 'ch': 128,
                            'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2,
                            'attn_resolutions': [16], 'dropout': 0.0
                        },
                        'lossconfig': {
                            'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                            'params': {
                                'disc_start': 1, 'disc_num_layers': 3, 'disc_weight': 0.5,
                                'disc_factor': 1.0, 'disc_in_channels': 3, 'disc_conditional': False,
                                'disc_loss': 'hinge', 'pixelloss_weight': 1.0, 'perceptual_weight': 1.0,
                                'kl_weight': 1e-6, 'logvar_init': 0.0, 'use_actnorm': False,
                                'pp_style': False, 'vf_weight': 0.1, 'adaptive_vf': False,
                                'distmat_weight': 1.0, 'cos_weight': 1.0,
                                'distmat_margin': 0.25, 'cos_margin': 0.5
                            }
                        }
                    }
                }
            }
            
            # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
            temp_config_fd, temp_config_path = tempfile.mkstemp(suffix='.yaml')
            with open(temp_config_path, 'w') as f:
                yaml.dump(vae_config, f, default_flow_style=False)
            os.close(temp_config_fd)
            
            try:
                # ä½¿ç”¨å®˜æ–¹VA_VAEç±»åŠ è½½
                self.vae = VA_VAE(temp_config_path)
                # æ£€æŸ¥æ˜¯å¦æœ‰.to()æ–¹æ³•ï¼ˆä¸å®˜æ–¹train_dit_s_official.pyä¸€è‡´ï¼‰
                if hasattr(self.vae, 'to'):
                    self.vae = self.vae.to(self.device)
                if hasattr(self.vae, 'eval'):
                    self.vae.eval()
                self.logger.info(f"âœ… VAEåŠ è½½å®Œæˆ: {custom_vae_checkpoint}")
                print(f"âœ… VAEåŠ è½½æˆåŠŸ: ä½¿ç”¨VA-VAE {custom_vae_checkpoint}")
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_config_path)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ VAEåŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.logger.warning("âš ï¸ å°è¯•ä½¿ç”¨ç®€åŒ–VAEä½œä¸ºå¤‡ç”¨")
            # å¤‡ç”¨æ–¹æ¡ˆ
            try:
                self.vae = SimplifiedVAVAE(self.config['vae']['model_name']).to(self.device)
                self.vae.eval()
                self.logger.info(f"âœ… å¤‡ç”¨VAEåŠ è½½å®Œæˆ: {self.config['vae']['model_name']}")
            except Exception as e2:
                self.logger.error(f"âš ï¸ å¤‡ç”¨VAEä¹ŸåŠ è½½å¤±è´¥: {e2}")
                self.vae = None
        
        # åŠ è½½latentç»Ÿè®¡ä¿¡æ¯
        self.latent_stats = None
        latent_stats_path = 'latents_safetensors/train/latent_stats.pt'
        if os.path.exists(latent_stats_path):
            self.latent_stats = torch.load(latent_stats_path, map_location='cpu')
            print(f"âœ… å·²åŠ è½½latentç»Ÿè®¡ä¿¡æ¯: {latent_stats_path}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°latentç»Ÿè®¡æ–‡ä»¶: {latent_stats_path}")
        
        # åŠ è½½åˆ†ç±»å™¨
        self.logger.info("åŠ è½½åˆ†ç±»å™¨...")
        self.classifier = ImprovedClassifier(num_classes=self.args.num_users)
        classifier_checkpoint = torch.load(self.args.classifier_path, map_location='cpu')
        
        # å¤„ç†checkpointæ ¼å¼
        if 'model_state_dict' in classifier_checkpoint:
            state_dict = classifier_checkpoint['model_state_dict']
        else:
            state_dict = classifier_checkpoint
            
        self.classifier.load_state_dict(state_dict)
        self.classifier = self.classifier.to(self.device)
        self.classifier.eval()
        
    def load_weights_with_shape_check(self, model, checkpoint):
        """ä½¿ç”¨å½¢çŠ¶æ£€æŸ¥åŠ è½½æƒé‡"""
        model_state_dict = model.state_dict()
        for name, param in checkpoint['model'].items():
            if name in model_state_dict:
                if param.shape == model_state_dict[name].shape:
                    model_state_dict[name].copy_(param)
                elif name == 'x_embedder.proj.weight':
                    weight = torch.zeros_like(model_state_dict[name])
                    weight[:, :16] = param[:, :16]
                    model_state_dict[name] = weight
                else:
                    self.logger.warning(f"è·³è¿‡å‚æ•° '{name}' å½¢çŠ¶ä¸åŒ¹é…: "
                                      f"checkpoint {param.shape}, model {model_state_dict[name].shape}")
            else:
                self.logger.warning(f"å‚æ•° '{name}' åœ¨æ¨¡å‹ä¸­æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
        model.load_state_dict(model_state_dict, strict=False)
        
    def setup_transforms(self):
        """è®¾ç½®å›¾åƒé¢„å¤„ç†"""
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
    def generate_batch(self, user_ids, batch_size):
        """ç”Ÿæˆä¸€ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬"""
        with torch.no_grad():
            current_batch_size = len(user_ids)
            
            # åˆ›å»ºæ¡ä»¶æ ‡ç­¾
            y = torch.tensor(user_ids, device=self.device, dtype=torch.long)
            
            # åˆ›å»ºéšæœºå™ªå£° (VA-VAEä½¿ç”¨32é€šé“ï¼Œ16x16ç©ºé—´åˆ†è¾¨ç‡)
            z = torch.randn(current_batch_size, 32, 16, 16, device=self.device)
            
            # åˆ›å»ºé‡‡æ ·å‡½æ•° (å®Œå…¨æŒ‰ç…§generate_conditional_samples_distributed.pyçš„å®ç°)
            sample_fn = self.sampler.sample_ode(
                sampling_method="dopri5",
                num_steps=300,
                atol=1e-6,
                rtol=1e-3,
                reverse=False,
                timestep_shift=0.1
            )
            
            # CFGé‡‡æ · - å®Œå…¨æŒ‰ç…§generate_conditional_samples_distributed.pyå®ç°
            if self.args.cfg_scale > 1.0:
                # æ„å»ºCFG batch
                z_cfg = torch.cat([z, z], 0)
                y_null = torch.tensor([31] * current_batch_size, device=self.device)  # null class
                y_cfg = torch.cat([y, y_null], 0)
                
                # ä½¿ç”¨å®˜æ–¹CFGé…ç½®
                cfg_interval_start = 0.11
                model_kwargs = dict(y=y_cfg, cfg_scale=self.args.cfg_scale, 
                                  cfg_interval=True, cfg_interval_start=cfg_interval_start)
                
                # ä½¿ç”¨CFGå‰å‘ä¼ æ’­ï¼ˆä¸å®˜æ–¹å®Œå…¨ä¸€è‡´ï¼‰
                if hasattr(self.model, 'forward_with_cfg'):
                    samples = sample_fn(z_cfg, self.model.forward_with_cfg, **model_kwargs)
                else:
                    # å¦‚æœæ¨¡å‹æ²¡æœ‰forward_with_cfgæ–¹æ³•ï¼Œä½¿ç”¨æ‰‹åŠ¨CFG
                    def model_fn_cfg(x, t, **kwargs):
                        pred = self.model(x, t, **kwargs)
                        pred_cond, pred_uncond = pred.chunk(2, dim=0)
                        return pred_uncond + self.args.cfg_scale * (pred_cond - pred_uncond)
                    samples = sample_fn(z_cfg, model_fn_cfg, **model_kwargs)
                
                samples = samples[-1]  # è·å–æœ€ç»ˆæ—¶é—´æ­¥çš„æ ·æœ¬
                samples, _ = samples.chunk(2, dim=0)  # å»æ‰null classæ ·æœ¬
            else:
                # æ ‡å‡†é‡‡æ ·
                samples = sample_fn(z, self.model, **dict(y=y))
                samples = samples[-1]
            
            # åå½’ä¸€åŒ–å¤„ç† (å®Œå…¨æŒ‰ç…§generate_conditional_samples_distributed.pyå®ç°)
            if self.latent_stats is not None:
                mean = self.latent_stats['mean'].to(self.device)
                std = self.latent_stats['std'].to(self.device)
                latent_multiplier = 1.0  # VA-VAEä½¿ç”¨1.0ï¼Œä¸æ˜¯0.18215
                
                # å®˜æ–¹åå½’ä¸€åŒ–å…¬å¼ï¼ˆä¸train_dit_s_official.pyç¬¬549è¡Œå®Œå…¨ä¸€è‡´ï¼‰
                samples_denorm = (samples * std) / latent_multiplier + mean
            else:
                print("âš ï¸ æ— latentç»Ÿè®¡ä¿¡æ¯ï¼Œè·³è¿‡åå½’ä¸€åŒ–")
                samples_denorm = samples
            
            # VAEè§£ç  (ä½¿ç”¨VA-VAEè§£ç latentä¸ºå›¾åƒ)
            decoded_images = self.vae.decode_to_images(samples_denorm)
            
            return decoded_images
            
    def evaluate_samples(self, samples, expected_user_ids):
        """è¯„ä¼°ç”Ÿæˆæ ·æœ¬çš„è´¨é‡"""
        batch_results = []
        
        with torch.no_grad():
            # è½¬æ¢ä¸ºPILå›¾åƒå¹¶é¢„å¤„ç†
            processed_samples = []
            pil_images = []
            
            for sample in samples:
                # VAEè¾“å‡ºçš„å›¾åƒå·²ç»åœ¨0-1èŒƒå›´å†…ï¼Œè½¬æ¢ä¸º0-255
                if isinstance(sample, torch.Tensor):
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    if sample.dim() == 3:  # CHWæ ¼å¼
                        sample_np = sample.permute(1, 2, 0).cpu().numpy()
                    else:  # HWCæ ¼å¼
                        sample_np = sample.cpu().numpy()
                else:
                    sample_np = sample
                
                # ç¡®ä¿åœ¨0-1èŒƒå›´å†…ï¼Œç„¶åè½¬æ¢ä¸º0-255
                sample_np = np.clip(sample_np, 0, 1)
                sample_uint8 = (sample_np * 255).astype(np.uint8)
                
                # å¤„ç†ç°åº¦æˆ–å•é€šé“å›¾åƒ
                if len(sample_uint8.shape) == 2:
                    sample_uint8 = np.stack([sample_uint8] * 3, axis=2)
                elif sample_uint8.shape[2] == 1:
                    sample_uint8 = np.repeat(sample_uint8, 3, axis=2)
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                pil_image = Image.fromarray(sample_uint8)
                pil_images.append(pil_image)
                
                # åº”ç”¨åˆ†ç±»å™¨é¢„å¤„ç†
                tensor_image = self.transform(pil_image).unsqueeze(0).to(self.device)
                processed_samples.append(tensor_image)
            
            # æ‰¹é‡å¤„ç†
            if processed_samples:
                batch_tensor = torch.cat(processed_samples, dim=0)
                logits = self.classifier(batch_tensor)
                probabilities = torch.softmax(logits, dim=1)
                predicted_classes = torch.argmax(logits, dim=1)
                max_probabilities = torch.max(probabilities, dim=1)[0]
                
                # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
                for i, (pred_class, confidence, expected_id) in enumerate(zip(
                    predicted_classes.cpu().numpy(),
                    max_probabilities.cpu().numpy(), 
                    expected_user_ids
                )):
                    is_correct = (pred_class == expected_id)
                    is_high_confidence = (confidence >= self.args.confidence_threshold)
                    
                    result = {
                        'sample_idx': i,
                        'expected_id': expected_id,
                        'predicted_id': pred_class,
                        'confidence': confidence,
                        'is_correct': is_correct,
                        'is_high_confidence': is_high_confidence,
                        'accept': is_correct and is_high_confidence,
                        'pil_image': pil_images[i]
                    }
                    batch_results.append(result)
                    
        return batch_results
        
    def save_accepted_samples(self, batch_results):
        """ä¿å­˜é€šè¿‡ç­›é€‰çš„æ ·æœ¬"""
        saved_count = 0
        
        for result in batch_results:
            if result['accept']:
                user_id = result['expected_id']
                
                # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡æ•°é‡
                if self.user_progress[user_id] >= self.args.target_per_user:
                    continue
                    
                # ä¿å­˜æ ·æœ¬
                filename = f"generated_{self.user_progress[user_id]:04d}_conf_{result['confidence']:.3f}.png"
                save_path = self.user_dirs[user_id] / filename
                
                # ç¡®ä¿æ˜¯RGBæ ¼å¼
                pil_image = result['pil_image']
                if pil_image.mode != 'RGB':
                    if pil_image.mode == 'L':
                        pil_image = pil_image.convert('RGB')
                        
                pil_image.save(save_path)
                self.user_progress[user_id] += 1
                saved_count += 1
                
                # è®°å½•ä¿å­˜ä¿¡æ¯
                self.logger.info(
                    f"ä¿å­˜æ ·æœ¬: User_{user_id:02d} -> {filename} "
                    f"(ç½®ä¿¡åº¦: {result['confidence']:.3f}, "
                    f"è¿›åº¦: {self.user_progress[user_id]}/{self.args.target_per_user})"
                )
                
        return saved_count
        
    def check_completion(self):
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç”¨æˆ·éƒ½è¾¾åˆ°ç›®æ ‡æ•°é‡"""
        completed_users = sum(1 for count in self.user_progress.values() 
                            if count >= self.args.target_per_user)
        
        return completed_users >= self.args.num_users
        
    def print_progress(self):
        """æ‰“å°å½“å‰è¿›åº¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ç”Ÿæˆè¿›åº¦ç»Ÿè®¡")
        print(f"{'='*60}")
        
        for user_id in range(self.args.num_users):
            progress = self.user_progress[user_id]
            percentage = (progress / self.args.target_per_user) * 100
            bar_length = 20
            filled_length = int(bar_length * progress / self.args.target_per_user)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            
            print(f"User_{user_id:02d}: [{bar}] {progress:3d}/{self.args.target_per_user} ({percentage:5.1f}%)")
            
        total_saved = sum(self.user_progress.values())
        total_target = self.args.num_users * self.args.target_per_user
        print(f"\nğŸ¯ æ€»è¿›åº¦: {total_saved}/{total_target} ({total_saved/total_target*100:.1f}%)")
        
    def run(self):
        """è¿è¡Œè‡ªåŠ¨åŒ–ç®¡é“"""
        self.logger.info("ğŸš€ å¯åŠ¨è‡ªåŠ¨åŒ–ç”Ÿæˆ-ç­›é€‰ç®¡é“")
        
        total_generated = 0
        total_accepted = 0
        batch_count = 0
        
        try:
            while not self.check_completion():
                batch_count += 1
                
                # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„ç”¨æˆ·IDåˆ—è¡¨
                current_batch_ids = []
                for user_id in range(self.args.num_users):
                    if self.user_progress[user_id] < self.args.target_per_user:
                        # æ ¹æ®å½“å‰è¿›åº¦å†³å®šè¯¥ç”¨æˆ·åœ¨æ‰¹æ¬¡ä¸­çš„æ ·æœ¬æ•°
                        remaining = self.args.target_per_user - self.user_progress[user_id]
                        batch_size_for_user = min(self.args.batch_size // self.args.num_users + 1, 
                                                remaining * 2)  # ç”Ÿæˆ2å€æ•°é‡ä»¥æé«˜ç­›é€‰æ•ˆç‡
                        current_batch_ids.extend([user_id] * batch_size_for_user)
                
                if not current_batch_ids:
                    break
                    
                # é™åˆ¶æ‰¹æ¬¡å¤§å°
                if len(current_batch_ids) > self.args.batch_size:
                    current_batch_ids = current_batch_ids[:self.args.batch_size]
                
                self.logger.info(f"æ‰¹æ¬¡ {batch_count}: ç”Ÿæˆ {len(current_batch_ids)} ä¸ªæ ·æœ¬...")
                
                # ç”Ÿæˆæ ·æœ¬
                samples = self.generate_batch(current_batch_ids, len(current_batch_ids))
                total_generated += len(samples)
                
                # è¯„ä¼°æ ·æœ¬
                results = self.evaluate_samples(samples, current_batch_ids)
                
                # ä¿å­˜é€šè¿‡ç­›é€‰çš„æ ·æœ¬
                saved_count = self.save_accepted_samples(results)
                total_accepted += saved_count
                
                # è®¡ç®—æ‰¹æ¬¡ç»Ÿè®¡
                batch_accuracy = sum(1 for r in results if r['is_correct']) / len(results)
                batch_acceptance = saved_count / len(results)
                
                self.logger.info(
                    f"æ‰¹æ¬¡ {batch_count} å®Œæˆ: "
                    f"å‡†ç¡®ç‡ {batch_accuracy:.1%}, "
                    f"æ¥å—ç‡ {batch_acceptance:.1%}, "
                    f"ä¿å­˜ {saved_count} ä¸ªæ ·æœ¬"
                )
                
                # æ¯10ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡è¿›åº¦
                if batch_count % 10 == 0:
                    self.print_progress()
                    
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­ç”Ÿæˆè¿‡ç¨‹")
            
        # æœ€ç»ˆç»Ÿè®¡
        self.print_progress()
        self.logger.info(f"ğŸ‰ ç”Ÿæˆå®Œæˆ!")
        self.logger.info(f"ğŸ“Š æ€»ç»Ÿè®¡: ç”Ÿæˆ {total_generated} ä¸ªæ ·æœ¬, æ¥å— {total_accepted} ä¸ªæ ·æœ¬")
        self.logger.info(f"ğŸ“Š æ€»ä½“æ¥å—ç‡: {total_accepted/total_generated:.1%}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_generated': total_generated,
            'total_accepted': total_accepted,
            'acceptance_rate': total_accepted / total_generated,
            'user_progress': dict(self.user_progress),
            'batch_count': batch_count
        }
        
        stats_file = self.output_dir / 'generation_stats.json'
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
            
        self.logger.info(f"ğŸ“„ ç»Ÿè®¡ä¿¡æ¯ä¿å­˜åˆ°: {stats_file}")


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if 'RANK' in os.environ:
        rank = int(os.environ['RANK'])
        local_rank = int(os.environ['LOCAL_RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend='nccl')
        
        return rank, local_rank, world_size
    else:
        return 0, 0, 1

def main():
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, local_rank, world_size = setup_distributed()
    
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–æ¡ä»¶ç”Ÿæˆ-ç­›é€‰ç®¡é“')
    
    # æ‰©æ•£æ¨¡å‹å‚æ•°
    parser.add_argument('--checkpoint', required=True, help='æ‰©æ•£æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--config', required=True, help='æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--cfg_scale', type=float, default=10.0, help='CFG scale')
    
    # åˆ†ç±»å™¨å‚æ•°  
    parser.add_argument('--classifier_path', required=True, help='åˆ†ç±»å™¨æ¨¡å‹è·¯å¾„')
    parser.add_argument('--confidence_threshold', type=float, default=0.9, 
                       help='ç½®ä¿¡åº¦é˜ˆå€¼')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--num_users', type=int, default=31, help='ç”¨æˆ·æ•°é‡')
    parser.add_argument('--target_per_user', type=int, default=300, 
                       help='æ¯ä¸ªç”¨æˆ·ç›®æ ‡æ ·æœ¬æ•°é‡')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', default='./automated_samples', 
                       help='è¾“å‡ºç›®å½•')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­ï¼ˆæ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„ç§å­ï¼‰
    set_seed(args.seed + rank * 1000)
    
    # åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šåˆ›å»ºå’Œè¿è¡Œç®¡é“
    if rank == 0 and world_size > 1:
        print(f"ğŸš€ ä½¿ç”¨ {world_size} ä¸ªGPUè¿›è¡Œåˆ†å¸ƒå¼ç”Ÿæˆ")
    
    # åˆ›å»ºå¹¶è¿è¡Œç®¡é“ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½è¿è¡Œï¼‰
    pipeline = AutomatedGenerationPipeline(args)
    pipeline.rank = rank
    pipeline.local_rank = local_rank 
    pipeline.world_size = world_size
    
    # è®¾ç½®æ­£ç¡®çš„è®¾å¤‡
    if world_size > 1:
        pipeline.device = torch.device(f'cuda:{local_rank}')
    
    pipeline.run()
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    if world_size > 1:
        dist.destroy_process_group()


if __name__ == '__main__':
    main()

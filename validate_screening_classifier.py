"""
éªŒè¯ç­›é€‰åˆ†ç±»å™¨çš„å¯é æ€§
è¯„ä¼°åªåœ¨çœŸå®æ•°æ®è®­ç»ƒçš„åˆ†ç±»å™¨æ˜¯å¦èƒ½èƒœä»»åˆæˆæ•°æ®ç­›é€‰å·¥ä½œ
"""
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import json
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss, log_loss
from scipy.stats import entropy
import argparse
from tqdm import tqdm
from PIL import Image
import torchvision.transforms as transforms

class ScreeningClassifierValidator:
    """ç­›é€‰åˆ†ç±»å™¨å¯é æ€§éªŒè¯å™¨"""
    
    def __init__(self, classifier, device):
        self.classifier = classifier
        self.device = device
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def evaluate_calibration(self, test_data_dir):
        """
        è¯„ä¼°åˆ†ç±»å™¨çš„æ¦‚ç‡æ ¡å‡†åº¦
        è‰¯å¥½æ ¡å‡†çš„åˆ†ç±»å™¨ï¼šå½“å®ƒè¯´90%ç½®ä¿¡åº¦æ—¶ï¼ŒçœŸçš„æœ‰90%å‡†ç¡®ç‡
        """
        print("\nğŸ“Š è¯„ä¼°åˆ†ç±»å™¨æ ¡å‡†åº¦...")
        
        all_probs = []
        all_labels = []
        
        # æ”¶é›†é¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾
        total_samples = 0
        correct_predictions = 0
        prediction_samples = []
        
        for class_id in range(31):  # class_id: 0-30
            # çœŸå®æ•°æ®æ ¼å¼ï¼šID_1åˆ°ID_31ï¼Œjpgæ ¼å¼
            # ID_1å¯¹åº”class_id=0, ID_2å¯¹åº”class_id=1, ..., ID_31å¯¹åº”class_id=30
            user_dir = Path(test_data_dir) / f"ID_{class_id + 1}"
            if not user_dir.exists():
                print(f"   âš ï¸ ç›®å½•ä¸å­˜åœ¨: {user_dir}")
                continue
            
            user_samples = 0
            user_correct = 0
            
            for img_file in list(user_dir.glob("*.jpg"))[:50]:  # æ¯ç”¨æˆ·å–50ä¸ªæ ·æœ¬
                img = Image.open(img_file).convert('RGB')
                img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    output = self.classifier(img_tensor)
                    prob = torch.softmax(output, dim=1)
                    
                    # å¤šåˆ†ç±»æ ¡å‡†ï¼šç”¨æœ€å¤§æ¦‚ç‡ï¼ˆæ¨¡å‹ç½®ä¿¡åº¦ï¼‰
                    max_prob, pred = torch.max(prob, dim=1)  # æœ€é«˜ç½®ä¿¡åº¦å’Œé¢„æµ‹ç±»åˆ«
                    pred_label = pred.item()
                    is_correct = (pred_label == class_id)
                    
                    all_probs.append(max_prob.item())  # ç”¨æœ€å¤§æ¦‚ç‡åšæ ¡å‡†
                    all_labels.append(int(is_correct))
                    
                    # ç»Ÿè®¡
                    total_samples += 1
                    user_samples += 1
                    if is_correct:
                        correct_predictions += 1
                        user_correct += 1
                    
                    # è®°å½•å‰å‡ ä¸ªæ ·æœ¬ç”¨äºè°ƒè¯•
                    if len(prediction_samples) < 10:
                        prediction_samples.append({
                            'class_id': class_id,
                            'file': img_file.name,
                            'predicted': pred_label,
                            'confidence': max_prob.item(),
                            'correct': is_correct
                        })
            
            if user_samples > 0 and class_id < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªç”¨æˆ·
                user_acc = user_correct / user_samples
                print(f"   ID_{class_id + 1}: {user_samples}æ ·æœ¬, å‡†ç¡®ç‡={user_acc:.3f}")
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        overall_acc = correct_predictions / total_samples if total_samples > 0 else 0
        print(f"   ğŸ“ˆ æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   ğŸ“ˆ æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.3f}")
        print(f"   ğŸ“ˆ å‰10ä¸ªé¢„æµ‹æ ·ä¾‹:")
        
        for sample in prediction_samples:
            print(f"      ID_{sample['class_id']+1}/{sample['file']}: é¢„æµ‹={sample['predicted']}, ç½®ä¿¡åº¦={sample['confidence']:.3f}, æ­£ç¡®={sample['correct']}")
        
        # è®¡ç®—æ ¡å‡†æ›²çº¿
        fraction_positives, mean_predicted = calibration_curve(
            all_labels, all_probs, n_bins=10
        )
        
        # è®¡ç®—ECE (Expected Calibration Error)
        ece = np.abs(fraction_positives - mean_predicted).mean()
        
        # Brier Score (è¶Šå°è¶Šå¥½ï¼Œ0æ˜¯å®Œç¾æ ¡å‡†)
        brier = brier_score_loss(all_labels, all_probs)
        
        return {
            'ece': ece,
            'brier_score': brier,
            'calibration_curve': (mean_predicted, fraction_positives),
            'is_well_calibrated': ece < 0.1  # ECE < 0.1è®¤ä¸ºæ ¡å‡†è‰¯å¥½
        }
    
    def evaluate_confidence_distribution(self, real_data_dir, synthetic_data_dir):
        """
        æ¯”è¾ƒåˆ†ç±»å™¨åœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šçš„ç½®ä¿¡åº¦åˆ†å¸ƒ
        å¦‚æœåˆ†å¸ƒå·®å¼‚è¿‡å¤§ï¼Œè¯´æ˜åˆ†ç±»å™¨ä¸é€‚åˆç­›é€‰
        """
        print("\nğŸ“Š æ¯”è¾ƒç½®ä¿¡åº¦åˆ†å¸ƒ...")
        
        real_confidences = self._get_confidence_distribution(real_data_dir, "çœŸå®æ•°æ®")
        synthetic_confidences = self._get_confidence_distribution(synthetic_data_dir, "åˆæˆæ•°æ®")
        
        # è®¡ç®—KLæ•£åº¦
        # å°†ç½®ä¿¡åº¦ç¦»æ•£åŒ–åˆ°bins
        bins = np.linspace(0, 1, 21)
        real_hist, _ = np.histogram(real_confidences, bins, density=True)
        synth_hist, _ = np.histogram(synthetic_confidences, bins, density=True)
        
        # é¿å…0å€¼
        real_hist = real_hist + 1e-10
        synth_hist = synth_hist + 1e-10
        
        # KLæ•£åº¦
        kl_divergence = entropy(real_hist, synth_hist)
        
        # å‡å€¼å’Œæ ‡å‡†å·®
        real_mean, real_std = np.mean(real_confidences), np.std(real_confidences)
        synth_mean, synth_std = np.mean(synthetic_confidences), np.std(synthetic_confidences)
        
        return {
            'real_confidence': {'mean': real_mean, 'std': real_std},
            'synthetic_confidence': {'mean': synth_mean, 'std': synth_std},
            'kl_divergence': kl_divergence,
            'distribution_shift': abs(real_mean - synth_mean),
            'is_distribution_similar': kl_divergence < 0.5  # KL < 0.5è®¤ä¸ºåˆ†å¸ƒç›¸ä¼¼
        }
    
    def _get_confidence_distribution(self, data_dir, desc=""):
        """è·å–æ•°æ®é›†çš„ç½®ä¿¡åº¦åˆ†å¸ƒ"""
        confidences = []
        
        # åˆ¤æ–­æ˜¯çœŸå®æ•°æ®è¿˜æ˜¯åˆæˆæ•°æ®
        is_real_data = "çœŸå®" in desc
        
        for class_id in range(31):  # class_id: 0-30
            if is_real_data:
                # çœŸå®æ•°æ®æ ¼å¼ï¼šID_1åˆ°ID_31ï¼Œjpgæ ¼å¼
                # ID_1å¯¹åº”class_id=0, ID_2å¯¹åº”class_id=1, ...
                user_dir = Path(data_dir) / f"ID_{class_id + 1}"
                file_pattern = "*.jpg"
            else:
                # åˆæˆæ•°æ®æ ¼å¼ï¼šUser_00åˆ°User_30ï¼Œpngæ ¼å¼
                # User_00å¯¹åº”class_id=0, User_01å¯¹åº”class_id=1, ...
                user_dir = Path(data_dir) / f"User_{class_id:02d}"
                file_pattern = "*.png"
            
            if not user_dir.exists():
                continue
            
            for img_file in tqdm(list(user_dir.glob(file_pattern))[:50], 
                               desc=f"å¤„ç†{desc}", leave=False):
                img = Image.open(img_file).convert('RGB')
                img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    output = self.classifier(img_tensor)
                    prob = torch.softmax(output, dim=1)
                    max_prob, _ = torch.max(prob, dim=1)
                    confidences.append(max_prob.item())
        
        return np.array(confidences)
    
    def evaluate_decision_boundary_stability(self, synthetic_samples, num_augmentations=10):
        """
        æµ‹è¯•å†³ç­–è¾¹ç•Œç¨³å®šæ€§
        å¯¹åˆæˆæ ·æœ¬è¿›è¡Œè½»å¾®æ‰°åŠ¨ï¼Œçœ‹é¢„æµ‹æ˜¯å¦ç¨³å®š
        """
        print("\nğŸ“Š è¯„ä¼°å†³ç­–è¾¹ç•Œç¨³å®šæ€§...")
        
        stability_scores = []
        
        augment = transforms.Compose([
            transforms.RandomAffine(degrees=3, translate=(0.02, 0.02)),
            transforms.ColorJitter(brightness=0.05, contrast=0.05)
        ])
        
        for sample_path in synthetic_samples[:100]:  # æµ‹è¯•100ä¸ªæ ·æœ¬
            img = Image.open(sample_path).convert('RGB')
            
            predictions = []
            confidences = []
            
            for _ in range(num_augmentations):
                # åº”ç”¨è½»å¾®å¢å¼º
                aug_img = augment(img)
                img_tensor = self.transform(aug_img).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    output = self.classifier(img_tensor)
                    prob = torch.softmax(output, dim=1)
                    conf, pred = torch.max(prob, dim=1)
                    
                    predictions.append(pred.item())
                    confidences.append(conf.item())
            
            # è®¡ç®—ç¨³å®šæ€§ï¼ˆé¢„æµ‹ä¸€è‡´æ€§ï¼‰
            unique_preds = len(set(predictions))
            stability = 1.0 / unique_preds if unique_preds > 0 else 0
            stability_scores.append(stability)
        
        avg_stability = np.mean(stability_scores)
        
        return {
            'average_stability': avg_stability,
            'stable_samples_ratio': np.mean(np.array(stability_scores) == 1.0),
            'is_stable': avg_stability > 0.8  # 80%ä»¥ä¸Šç¨³å®šè®¤ä¸ºå¯é 
        }
    
    def evaluate_error_analysis(self, synthetic_data_dir):
        """
        é”™è¯¯åˆ†æï¼šåˆ†æåˆ†ç±»å™¨åœ¨åˆæˆæ•°æ®ä¸Šçš„é”™è¯¯æ¨¡å¼
        """
        print("\nğŸ“Š åˆ†æé”™è¯¯æ¨¡å¼...")
        
        error_patterns = {
            'low_confidence_errors': [],  # ä½ç½®ä¿¡åº¦é”™è¯¯
            'high_confidence_errors': [],  # é«˜ç½®ä¿¡åº¦é”™è¯¯
            'confusion_pairs': {}  # æ˜“æ··æ·†çš„ç”¨æˆ·å¯¹
        }
        
        for class_id in range(31):  # class_id: 0-30
            # åˆæˆæ•°æ®æ ¼å¼ï¼šUser_00åˆ°User_30ï¼Œpngæ ¼å¼
            # User_00å¯¹åº”class_id=0, User_01å¯¹åº”class_id=1, ...
            user_dir = Path(synthetic_data_dir) / f"User_{class_id:02d}"
            if not user_dir.exists():
                continue
            
            for img_file in list(user_dir.glob("*.png"))[:50]:
                img = Image.open(img_file).convert('RGB')
                img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    output = self.classifier(img_tensor)
                    prob = torch.softmax(output, dim=1)
                    conf, pred = torch.max(prob, dim=1)
                    
                    if pred.item() != class_id:
                        # è®°å½•é”™è¯¯
                        error_info = {
                            'true_label': class_id,
                            'predicted': pred.item(),
                            'confidence': conf.item()
                        }
                        
                        if conf.item() < 0.5:
                            error_patterns['low_confidence_errors'].append(error_info)
                        else:
                            error_patterns['high_confidence_errors'].append(error_info)
                        
                        # è®°å½•æ··æ·†å¯¹
                        pair = (class_id, pred.item())
                        if pair not in error_patterns['confusion_pairs']:
                            error_patterns['confusion_pairs'][pair] = 0
                        error_patterns['confusion_pairs'][pair] += 1
        
        # åˆ†æç»“æœ
        total_errors = len(error_patterns['low_confidence_errors']) + \
                      len(error_patterns['high_confidence_errors'])
        
        if total_errors > 0:
            low_conf_ratio = len(error_patterns['low_confidence_errors']) / total_errors
        else:
            low_conf_ratio = 0
        
        return {
            'total_errors': total_errors,
            'low_confidence_error_ratio': low_conf_ratio,
            'high_confidence_errors_count': len(error_patterns['high_confidence_errors']),
            'top_confusion_pairs': sorted(
                error_patterns['confusion_pairs'].items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:5],
            'is_error_pattern_acceptable': low_conf_ratio > 0.7  # 70%é”™è¯¯æ˜¯ä½ç½®ä¿¡åº¦
        }
    
    def generate_reliability_report(self, results, output_path):
        """ç”Ÿæˆå¯é æ€§è¯„ä¼°æŠ¥å‘Š"""
        
        report = {
            'overall_reliability': self._compute_overall_reliability(results),
            'calibration': results['calibration'],
            'distribution_similarity': results['distribution'],
            'decision_stability': results['stability'],
            'error_analysis': results['errors'],
            'recommendations': self._generate_recommendations(results)
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(output_path / 'screening_classifier_reliability.json', 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_reliability_metrics(results, output_path)
        
        return report
    
    def _compute_overall_reliability(self, results):
        """è®¡ç®—æ€»ä½“å¯é æ€§åˆ†æ•°"""
        scores = [
            results['calibration']['is_well_calibrated'] * 25,
            results['distribution']['is_distribution_similar'] * 25,
            results['stability']['is_stable'] * 25,
            results['errors']['is_error_pattern_acceptable'] * 25
        ]
        
        total_score = sum(scores)
        
        return {
            'score': total_score,
            'rating': 'Excellent' if total_score >= 80 else 
                     'Good' if total_score >= 60 else 
                     'Fair' if total_score >= 40 else 'Poor',
            'is_reliable': total_score >= 60
        }
    
    def _generate_recommendations(self, results):
        """åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        if not results['calibration']['is_well_calibrated']:
            recommendations.append("åˆ†ç±»å™¨éœ€è¦æ¦‚ç‡æ ¡å‡†ï¼ˆå¦‚æ¸©åº¦ç¼©æ”¾ï¼‰")
        
        if not results['distribution']['is_distribution_similar']:
            recommendations.append("çœŸå®/åˆæˆæ•°æ®ç½®ä¿¡åº¦åˆ†å¸ƒå·®å¼‚å¤§ï¼Œè€ƒè™‘åŸŸé€‚åº”æŠ€æœ¯")
        
        if not results['stability']['is_stable']:
            recommendations.append("å†³ç­–è¾¹ç•Œä¸ç¨³å®šï¼Œå¢åŠ æ•°æ®å¢å¼ºæˆ–æ­£åˆ™åŒ–")
        
        if not results['errors']['is_error_pattern_acceptable']:
            recommendations.append("é«˜ç½®ä¿¡åº¦é”™è¯¯è¿‡å¤šï¼Œéœ€è¦é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´é˜ˆå€¼")
        
        if results['distribution']['synthetic_confidence']['mean'] < 0.5:
            recommendations.append("åˆæˆæ•°æ®æ•´ä½“ç½®ä¿¡åº¦è¿‡ä½ï¼Œè€ƒè™‘æé«˜ç”Ÿæˆè´¨é‡")
        
        return recommendations
    
    def _plot_reliability_metrics(self, results, output_path):
        """ç»˜åˆ¶å¯é æ€§æŒ‡æ ‡å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. æ ¡å‡†æ›²çº¿
        mean_pred, fraction_pos = results['calibration']['calibration_curve']
        axes[0, 0].plot([0, 1], [0, 1], 'k--', label='å®Œç¾æ ¡å‡†')
        axes[0, 0].plot(mean_pred, fraction_pos, 'b-', label='å®é™…æ ¡å‡†')
        axes[0, 0].set_xlabel('å¹³å‡é¢„æµ‹æ¦‚ç‡')
        axes[0, 0].set_ylabel('å®é™…å‡†ç¡®ç‡')
        axes[0, 0].set_title(f'æ ¡å‡†æ›²çº¿ (ECE={results["calibration"]["ece"]:.3f})')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”
        categories = ['çœŸå®æ•°æ®', 'åˆæˆæ•°æ®']
        means = [results['distribution']['real_confidence']['mean'],
                results['distribution']['synthetic_confidence']['mean']]
        stds = [results['distribution']['real_confidence']['std'],
               results['distribution']['synthetic_confidence']['std']]
        
        x = np.arange(len(categories))
        axes[0, 1].bar(x, means, yerr=stds, capsize=5, color=['blue', 'orange'])
        axes[0, 1].set_ylabel('å¹³å‡ç½®ä¿¡åº¦')
        axes[0, 1].set_title(f'ç½®ä¿¡åº¦åˆ†å¸ƒ (KL={results["distribution"]["kl_divergence"]:.3f})')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(categories)
        axes[0, 1].grid(True, alpha=0.3, axis='y')
        
        # 3. ç¨³å®šæ€§è¯„åˆ†
        stability_data = [
            results['stability']['average_stability'],
            results['stability']['stable_samples_ratio']
        ]
        labels = ['å¹³å‡ç¨³å®šæ€§', 'ç¨³å®šæ ·æœ¬æ¯”ä¾‹']
        
        axes[1, 0].barh(labels, stability_data, color=['green', 'lightgreen'])
        axes[1, 0].set_xlim([0, 1])
        axes[1, 0].set_xlabel('åˆ†æ•°')
        axes[1, 0].set_title('å†³ç­–è¾¹ç•Œç¨³å®šæ€§')
        axes[1, 0].grid(True, alpha=0.3, axis='x')
        
        # 4. é”™è¯¯æ¨¡å¼åˆ†æ
        if results['errors']['total_errors'] > 0:
            sizes = [
                len(results['errors']['low_confidence_errors']),
                len(results['errors']['high_confidence_errors'])
            ]
            labels = ['ä½ç½®ä¿¡åº¦é”™è¯¯', 'é«˜ç½®ä¿¡åº¦é”™è¯¯']
            colors = ['lightblue', 'lightcoral']
            
            axes[1, 1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
            axes[1, 1].set_title('é”™è¯¯ç±»å‹åˆ†å¸ƒ')
        else:
            axes[1, 1].text(0.5, 0.5, 'æ— é”™è¯¯', ha='center', va='center', fontsize=16)
            axes[1, 1].set_title('é”™è¯¯ç±»å‹åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig(output_path / 'screening_reliability_metrics.png', dpi=150)
        plt.close()


def load_classifier(checkpoint_path, device):
    """åŠ è½½åˆ†ç±»å™¨æ¨¡å‹"""
    import torchvision.models as models
    
    # åˆ›å»ºä¸improved_classifier_training.pyå®Œå…¨ä¸€è‡´çš„æ¨¡å‹ç»“æ„
    class MicroDopplerModel(nn.Module):
        def __init__(self, num_classes=31, dropout_rate=0.3):
            super().__init__()
            
            # ä½¿ç”¨ResNet18ä½œä¸ºbackbone
            self.backbone = models.resnet18(pretrained=False)
            self.backbone.fc = nn.Identity()
            feature_dim = 512
            
            # åˆ†ç±»å¤´
            self.classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(feature_dim, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(inplace=False),
                nn.Dropout(dropout_rate * 0.5),
                nn.Linear(256, num_classes)
            )
            
            # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
            self.projection_head = nn.Sequential(
                nn.Linear(feature_dim, 128),
                nn.ReLU(inplace=False),
                nn.Linear(128, 64)
            )
        
        def forward(self, x):
            features = self.backbone(x)
            return self.classifier(features)
    
    # åˆ›å»ºæ¨¡å‹
    model = MicroDopplerModel(num_classes=31)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    print(f"âœ… åˆ†ç±»å™¨åŠ è½½å®Œæˆ: {checkpoint_path}")
    return model


def main():
    parser = argparse.ArgumentParser(description='Validate screening classifier reliability')
    parser.add_argument('--classifier_path', type=str, required=True,
                       help='Path to the screening classifier checkpoint')
    parser.add_argument('--real_test_data', type=str, required=True,
                       help='Real test data directory')
    parser.add_argument('--synthetic_data', type=str, required=True,
                       help='Synthetic data directory for validation')
    parser.add_argument('--output_dir', type=str, default='./reliability_analysis',
                       help='Output directory for results')
    
    args = parser.parse_args()
    
    print("ğŸ” ç­›é€‰åˆ†ç±»å™¨å¯é æ€§éªŒè¯")
    print(f"ğŸ“¦ åˆ†ç±»å™¨: {args.classifier_path}")
    print(f"ğŸ“‚ çœŸå®æµ‹è¯•æ•°æ®: {args.real_test_data}")
    print(f"ğŸ“‚ åˆæˆæ•°æ®: {args.synthetic_data}")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½åˆ†ç±»å™¨
    try:
        classifier = load_classifier(args.classifier_path, device)
        validator = ScreeningClassifierValidator(classifier, device)
    except Exception as e:
        print(f"âŒ åˆ†ç±»å™¨åŠ è½½å¤±è´¥: {e}")
        return
    
    # æ‰§è¡ŒéªŒè¯
    print("\nğŸš€ å¼€å§‹éªŒè¯...")
    
    try:
        # 1. æ ¡å‡†åº¦è¯„ä¼°
        print("\nğŸ“Š 1. è¯„ä¼°åˆ†ç±»å™¨æ ¡å‡†åº¦...")
        calibration_results = validator.evaluate_calibration(args.real_test_data)
        print(f"   ECE: {calibration_results['ece']:.4f} {'âœ…' if calibration_results['is_well_calibrated'] else 'âŒ'}")
        print(f"   Brier Score: {calibration_results['brier_score']:.4f}")
        
        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒæ¯”è¾ƒ
        print("\nğŸ“Š 2. æ¯”è¾ƒç½®ä¿¡åº¦åˆ†å¸ƒ...")
        distribution_results = validator.evaluate_confidence_distribution(
            args.real_test_data, args.synthetic_data
        )
        print(f"   çœŸå®æ•°æ®ç½®ä¿¡åº¦: {distribution_results['real_confidence']['mean']:.3f} Â± {distribution_results['real_confidence']['std']:.3f}")
        print(f"   åˆæˆæ•°æ®ç½®ä¿¡åº¦: {distribution_results['synthetic_confidence']['mean']:.3f} Â± {distribution_results['synthetic_confidence']['std']:.3f}")
        print(f"   KLæ•£åº¦: {distribution_results['kl_divergence']:.4f} {'âœ…' if distribution_results['is_distribution_similar'] else 'âŒ'}")
        
        # 3. å†³ç­–è¾¹ç•Œç¨³å®šæ€§
        print("\nğŸ“Š 3. è¯„ä¼°å†³ç­–è¾¹ç•Œç¨³å®šæ€§...")
        synthetic_samples = list(Path(args.synthetic_data).glob("**/*.png"))
        if synthetic_samples:
            stability_results = validator.evaluate_decision_boundary_stability(synthetic_samples)
            print(f"   å¹³å‡ç¨³å®šæ€§: {stability_results['average_stability']:.3f}")
            print(f"   ç¨³å®šæ ·æœ¬æ¯”ä¾‹: {stability_results['stable_samples_ratio']:.3f} {'âœ…' if stability_results['is_stable'] else 'âŒ'}")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°åˆæˆæ ·æœ¬")
            stability_results = {'is_stable': False, 'average_stability': 0, 'stable_samples_ratio': 0}
        
        # 4. é”™è¯¯æ¨¡å¼åˆ†æ
        print("\nğŸ“Š 4. åˆ†æé”™è¯¯æ¨¡å¼...")
        error_results = validator.evaluate_error_analysis(args.synthetic_data)
        print(f"   æ€»é”™è¯¯æ•°: {error_results['total_errors']}")
        print(f"   ä½ç½®ä¿¡åº¦é”™è¯¯æ¯”ä¾‹: {error_results['low_confidence_error_ratio']:.3f} {'âœ…' if error_results['is_error_pattern_acceptable'] else 'âŒ'}")
        print(f"   é«˜ç½®ä¿¡åº¦é”™è¯¯æ•°: {error_results['high_confidence_errors_count']}")
        
        # ç»¼åˆè¯„ä¼°
        print("\nğŸ“‹ ç»¼åˆè¯„ä¼°ç»“æœ:")
        print("=" * 50)
        
        # è®¡ç®—æ€»ä½“å¯é æ€§åˆ†æ•°
        results = {
            'calibration': calibration_results,
            'distribution': distribution_results,
            'stability': stability_results,
            'errors': error_results
        }
        
        overall = validator._compute_overall_reliability(results)
        print(f"ğŸ“ˆ æ€»ä½“å¯é æ€§è¯„åˆ†: {overall['score']}/100 ({overall['rating']})")
        print(f"ğŸ¯ æ˜¯å¦å¯ç”¨äºç­›é€‰: {'âœ… å¯ç”¨' if overall['is_reliable'] else 'âŒ ä¸å¯ç”¨'}")
        
        # ç”Ÿæˆå»ºè®®
        recommendations = validator._generate_recommendations(results)
        if recommendations:
            print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        else:
            print("\nğŸ’¡ åˆ†ç±»å™¨è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€æ”¹è¿›")
            
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ… éªŒè¯å®Œæˆï¼")

if __name__ == "__main__":
    main()

"""
åˆ†æç”Ÿæˆæ ·æœ¬çš„ç­›é€‰æŒ‡æ ‡åˆ†å¸ƒ
å¸®åŠ©ç¡®å®šåˆç†çš„ç­›é€‰é˜ˆå€¼
"""

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import json


def load_classifier(model_path, device):
    """åŠ è½½åˆ†ç±»å™¨"""
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    
    # æ ¹æ®checkpointåˆ¤æ–­æ¨¡å‹ç±»å‹
    if 'feature_projector.0.weight' in checkpoint['model_state_dict']:
        from train_calibrated_classifier import DomainAdaptiveClassifier
        model = DomainAdaptiveClassifier(
            num_classes=checkpoint['num_classes'],
            dropout_rate=0.3,
            feature_dim=512
        )
    else:
        from improved_classifier_training import ImprovedClassifier
        model = ImprovedClassifier(
            num_classes=checkpoint['num_classes'],
            backbone='resnet18',
            dropout_rate=0.5,
            freeze_layers='minimal'
        )
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    return model


def extract_features(img_tensor, classifier):
    """æå–ç‰¹å¾"""
    with torch.no_grad():
        if hasattr(classifier, 'backbone'):
            features = classifier.backbone(img_tensor)
        else:
            # å¦‚æœæ²¡æœ‰backboneå±æ€§ï¼Œä½¿ç”¨æ¨¡å‹çš„ç‰¹å¾æå–éƒ¨åˆ†
            features = classifier.features(img_tensor)
        return features.cpu().numpy().flatten()


def compute_sample_metrics(image_path, classifier, user_id, device):
    """è®¡ç®—å•ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç­›é€‰æŒ‡æ ‡"""
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½å›¾åƒ
    img = Image.open(image_path).convert('RGB')
    img_tensor = transform(img).unsqueeze(0).to(device)
    
    with torch.no_grad():
        # è·å–åˆ†ç±»å™¨è¾“å‡º
        outputs = classifier(img_tensor)
        if isinstance(outputs, tuple):
            logits = outputs[0]
        else:
            logits = outputs
        
        probs = F.softmax(logits, dim=1)
        
        # 1. ç½®ä¿¡åº¦
        confidence, pred = torch.max(probs, dim=1)
        confidence = confidence.item()
        
        # 2. å†³ç­–è¾¹ç•Œ (margin)
        sorted_probs, _ = torch.sort(probs, dim=1, descending=True)
        margin = (sorted_probs[0, 0] - sorted_probs[0, 1]).item()
        
        # 3. ç”¨æˆ·ç‰¹å¼‚æ€§
        user_prob = probs[0, user_id].item()
        other_probs = torch.cat([probs[0, :user_id], probs[0, user_id+1:]])
        max_other_prob = torch.max(other_probs).item()
        user_specificity = user_prob - max_other_prob
        
        # 4. æå–ç‰¹å¾ç”¨äºå¤šæ ·æ€§è®¡ç®—
        features = extract_features(img_tensor, classifier)
        
    return {
        'confidence': confidence,
        'margin': margin,
        'user_specificity': user_specificity,
        'predicted_user': pred.item(),
        'correct': pred.item() == user_id,
        'features': features,
        'image_path': str(image_path)
    }


def compute_batch_diversity(features_list):
    """è®¡ç®—æ‰¹æ¬¡å†…ç‰¹å¾å¤šæ ·æ€§"""
    if len(features_list) < 2:
        return 0.0
    
    features_array = np.array(features_list)
    cosine_sim_matrix = cosine_similarity(features_array)
    
    # è®¡ç®—ä¸Šä¸‰è§’çŸ©é˜µçš„å¹³å‡ç›¸ä¼¼åº¦
    upper_triangle = np.triu(cosine_sim_matrix, k=1)
    n = len(features_list)
    avg_similarity = np.sum(upper_triangle) / (n * (n - 1) / 2)
    
    diversity_score = 1.0 - avg_similarity
    return diversity_score


def analyze_user_samples(sample_dir, classifier, user_id, device):
    """åˆ†æå•ä¸ªç”¨æˆ·çš„æ‰€æœ‰æ ·æœ¬"""
    
    user_dir = Path(sample_dir) / f"user_{user_id:02d}"
    if not user_dir.exists():
        user_dir = Path(sample_dir) / f"User_{user_id:02d}"
    if not user_dir.exists():
        print(f"âŒ æœªæ‰¾åˆ°ç”¨æˆ· {user_id} çš„æ ·æœ¬ç›®å½•")
        return None
    
    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    image_files = []
    for ext in ['*.png', '*.jpg', '*.jpeg']:
        image_files.extend(list(user_dir.glob(ext)))
    
    if not image_files:
        print(f"âŒ ç”¨æˆ· {user_id} ç›®å½•ä¸­æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶")
        return None
    
    print(f"ğŸ“Š åˆ†æç”¨æˆ· {user_id} çš„ {len(image_files)} ä¸ªæ ·æœ¬...")
    
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
    sample_metrics = []
    features_list = []
    
    for img_path in tqdm(image_files, desc=f"User {user_id}"):
        try:
            metrics = compute_sample_metrics(img_path, classifier, user_id, device)
            sample_metrics.append(metrics)
            features_list.append(metrics['features'])
        except Exception as e:
            print(f"âš ï¸  å¤„ç† {img_path} æ—¶å‡ºé”™: {e}")
            continue
    
    if not sample_metrics:
        return None
    
    # è®¡ç®—æ‰¹æ¬¡å¤šæ ·æ€§
    batch_diversity = compute_batch_diversity(features_list)
    
    # ç»Ÿè®¡ç»“æœ
    confidences = [m['confidence'] for m in sample_metrics]
    margins = [m['margin'] for m in sample_metrics]
    user_specificities = [m['user_specificity'] for m in sample_metrics]
    stabilities = [m['stability'] for m in sample_metrics]
    correct_predictions = [m['correct'] for m in sample_metrics]
    
    results = {
        'user_id': user_id,
        'total_samples': len(sample_metrics),
        'accuracy': np.mean(correct_predictions),
        'metrics': {
            'confidence': {
                'mean': np.mean(confidences),
                'std': np.std(confidences),
                'min': np.min(confidences),
                'max': np.max(confidences),
                'percentiles': {
                    '25': np.percentile(confidences, 25),
                    '50': np.percentile(confidences, 50),
                    '75': np.percentile(confidences, 75),
                    '90': np.percentile(confidences, 90),
                    '95': np.percentile(confidences, 95)
                }
            },
            'margin': {
                'mean': np.mean(margins),
                'std': np.std(margins),
                'min': np.min(margins),
                'max': np.max(margins),
                'percentiles': {
                    '25': np.percentile(margins, 25),
                    '50': np.percentile(margins, 50),
                    '75': np.percentile(margins, 75)
                }
            },
            'user_specificity': {
                'mean': np.mean(user_specificities),
                'std': np.std(user_specificities),
                'min': np.min(user_specificities),
                'max': np.max(user_specificities),
                'percentiles': {
                    '25': np.percentile(user_specificities, 25),
                    '50': np.percentile(user_specificities, 50),
                    '75': np.percentile(user_specificities, 75)
                }
            },
            'stability': {
                'mean': np.mean(stabilities),
                'std': np.std(stabilities),
                'min': np.min(stabilities),
                'max': np.max(stabilities),
                'percentiles': {
                    '25': np.percentile(stabilities, 25),
                    '50': np.percentile(stabilities, 50),
                    '75': np.percentile(stabilities, 75)
                }
            },
            'batch_diversity': batch_diversity
        },
        'raw_data': {
            'confidences': confidences,
            'margins': margins,
            'user_specificities': user_specificities,
            'stabilities': stabilities
        }
    }
    
    return results


def plot_metrics_distribution(results_list, output_dir):
    """ç»˜åˆ¶æŒ‡æ ‡åˆ†å¸ƒå›¾"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰æ•°æ®
    all_confidences = []
    all_margins = []
    all_user_specificities = []
    all_stabilities = []
    user_ids = []
    
    for result in results_list:
        if result is None:
            continue
        raw_data = result['raw_data']
        n_samples = len(raw_data['confidences'])
        
        all_confidences.extend(raw_data['confidences'])
        all_margins.extend(raw_data['margins'])
        all_user_specificities.extend(raw_data['user_specificities'])
        all_stabilities.extend(raw_data['stabilities'])
        user_ids.extend([result['user_id']] * n_samples)
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. ç½®ä¿¡åº¦åˆ†å¸ƒ
    axes[0, 0].hist(all_confidences, bins=50, alpha=0.7, edgecolor='black')
    axes[0, 0].axvline(0.75, color='red', linestyle='--', label='æ¨èé˜ˆå€¼: 0.75')
    axes[0, 0].set_xlabel('ç½®ä¿¡åº¦')
    axes[0, 0].set_ylabel('é¢‘æ¬¡')
    axes[0, 0].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
    axes[0, 0].legend()
    
    # 2. å†³ç­–è¾¹ç•Œåˆ†å¸ƒ
    axes[0, 1].hist(all_margins, bins=50, alpha=0.7, edgecolor='black')
    axes[0, 1].axvline(0.3, color='red', linestyle='--', label='æ¨èé˜ˆå€¼: 0.3')
    axes[0, 1].set_xlabel('å†³ç­–è¾¹ç•Œ (Margin)')
    axes[0, 1].set_ylabel('é¢‘æ¬¡')
    axes[0, 1].set_title('å†³ç­–è¾¹ç•Œåˆ†å¸ƒ')
    axes[0, 1].legend()
    
    # 3. ç”¨æˆ·ç‰¹å¼‚æ€§åˆ†å¸ƒ
    axes[1, 0].hist(all_user_specificities, bins=50, alpha=0.7, edgecolor='black')
    axes[1, 0].axvline(0.2, color='red', linestyle='--', label='æ¨èé˜ˆå€¼: 0.2')
    axes[1, 0].set_xlabel('ç”¨æˆ·ç‰¹å¼‚æ€§')
    axes[1, 0].set_ylabel('é¢‘æ¬¡')
    axes[1, 0].set_title('ç”¨æˆ·ç‰¹å¼‚æ€§åˆ†å¸ƒ')
    axes[1, 0].legend()
    
    # 4. ç¨³å®šæ€§åˆ†å¸ƒ
    axes[1, 1].hist(all_stabilities, bins=20, alpha=0.7, edgecolor='black')
    axes[1, 1].axvline(0.6, color='red', linestyle='--', label='æ¨èé˜ˆå€¼: 0.6')
    axes[1, 1].set_xlabel('ç¨³å®šæ€§')
    axes[1, 1].set_ylabel('é¢‘æ¬¡')
    axes[1, 1].set_title('ç¨³å®šæ€§åˆ†å¸ƒ')
    axes[1, 1].legend()
    
    plt.tight_layout()
    plt.savefig(output_dir / 'metrics_distribution.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š æŒ‡æ ‡åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {output_dir / 'metrics_distribution.png'}")


def print_summary_report(results_list):
    """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š ç­›é€‰æŒ‡æ ‡åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    valid_results = [r for r in results_list if r is not None]
    
    if not valid_results:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ")
        return
    
    # è®¡ç®—å„æŒ‡æ ‡çš„æ€»ä½“ç»Ÿè®¡
    all_metrics = {
        'confidence': [],
        'margin': [],
        'user_specificity': [],
        'batch_diversity': []
    }
    
    for result in valid_results:
        all_metrics['confidence'].extend(result['raw_data']['confidences'])
        all_metrics['margin'].extend(result['raw_data']['margins'])
        all_metrics['user_specificity'].extend(result['raw_data']['user_specificities'])
        all_metrics['batch_diversity'].append(result['metrics']['batch_diversity'])
    
    print(f"\nğŸ“ˆ æ€»ä½“æŒ‡æ ‡ç»Ÿè®¡ (åŸºäº {sum(r['total_samples'] for r in valid_results)} ä¸ªæ ·æœ¬):")
    
    for metric_name, values in all_metrics.items():
        if not values:
            continue
        print(f"\nğŸ¯ {metric_name}:")
        print(f"   å¹³å‡å€¼: {np.mean(values):.3f} Â± {np.std(values):.3f}")
        print(f"   èŒƒå›´: [{np.min(values):.3f}, {np.max(values):.3f}]")
        print(f"   25%åˆ†ä½: {np.percentile(values, 25):.3f}")
        print(f"   50%åˆ†ä½: {np.percentile(values, 50):.3f}")
        print(f"   75%åˆ†ä½: {np.percentile(values, 75):.3f}")
    
    # æ¨¡æ‹Ÿç­›é€‰é€šè¿‡ç‡
    print(f"\nğŸ” ç­›é€‰é€šè¿‡ç‡é¢„æµ‹:")
    
    thresholds = {
        'confidence': [0.7, 0.75, 0.8, 0.85, 0.9],
        'margin': [0.2, 0.3, 0.4, 0.5],
        'user_specificity': [0.1, 0.2, 0.3, 0.4]
    }
    
    for metric_name, threshold_list in thresholds.items():
        values = all_metrics[metric_name]
        print(f"\nğŸ“Š {metric_name} é€šè¿‡ç‡:")
        for threshold in threshold_list:
            pass_rate = np.mean(np.array(values) >= threshold) * 100
            print(f"   é˜ˆå€¼ {threshold}: {pass_rate:.1f}%")


def main():
    parser = argparse.ArgumentParser(description='åˆ†æç”Ÿæˆæ ·æœ¬çš„ç­›é€‰æŒ‡æ ‡')
    parser.add_argument('--sample_dir', type=str, default='./generated_samples', 
                       help='ç”Ÿæˆæ ·æœ¬ç›®å½•')
    parser.add_argument('--classifier_path', type=str, 
                       default='/kaggle/working/VA-VAE/domain_adaptive_classifier/best_calibrated_model.pth',
                       help='åˆ†ç±»å™¨è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./metrics_analysis',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--user_ids', type=str, default='all',
                       help='è¦åˆ†æçš„ç”¨æˆ·IDï¼Œé€—å·åˆ†éš”ï¼Œæˆ–"all"åˆ†ææ‰€æœ‰')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½åˆ†ç±»å™¨
    print("ğŸ“¦ åŠ è½½åˆ†ç±»å™¨...")
    classifier = load_classifier(args.classifier_path, device)
    
    # ç¡®å®šè¦åˆ†æçš„ç”¨æˆ·
    if args.user_ids == 'all':
        sample_dir = Path(args.sample_dir)
        user_dirs = list(sample_dir.glob('user_*')) + list(sample_dir.glob('User_*'))
        user_ids = []
        for user_dir in user_dirs:
            if user_dir.name.startswith('user_'):
                user_id = int(user_dir.name.split('_')[1])
            elif user_dir.name.startswith('User_'):
                user_id = int(user_dir.name.split('_')[1])
            else:
                continue
            user_ids.append(user_id)
        user_ids = sorted(user_ids)
    else:
        user_ids = [int(x.strip()) for x in args.user_ids.split(',')]
    
    print(f"ğŸ¯ åˆ†æç”¨æˆ·: {user_ids}")
    
    # åˆ†ææ¯ä¸ªç”¨æˆ·
    results_list = []
    for user_id in user_ids:
        result = analyze_user_samples(args.sample_dir, classifier, user_id, device)
        if result:
            results_list.append(result)
            print(f"âœ… ç”¨æˆ· {user_id}: å‡†ç¡®ç‡ {result['accuracy']:.1%}")
        else:
            results_list.append(None)
    
    # ç”ŸæˆæŠ¥å‘Š
    print_summary_report(results_list)
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    if results_list:
        plot_metrics_distribution(results_list, args.output_dir)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_dir / 'detailed_results.json', 'w') as f:
        # ç§»é™¤featuresæ•°æ®ä»¥å‡å°æ–‡ä»¶å¤§å°
        save_results = []
        for result in results_list:
            if result:
                save_result = result.copy()
                save_result['raw_data'] = {k: v for k, v in result['raw_data'].items() if k != 'features'}
                save_results.append(save_result)
            else:
                save_results.append(None)
        json.dump(save_results, f, indent=2)
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_dir / 'detailed_results.json'}")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
VA-VAEå¾®è°ƒè„šæœ¬ - é€‚é…å¾®å¤šæ™®å‹’æ•°æ®
åŸºäºåŸé¡¹ç›®çš„è®­ç»ƒæ¡†æ¶ï¼Œé’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–
"""

import os
import sys
import torch
import yaml
import numpy as np
from pathlib import Path
from PIL import Image
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from tokenizer.vavae import VA_VAE

class MicroDopplerDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›† - ç”¨äºVA-VAEå¾®è°ƒ"""
    
    def __init__(self, data_dir, transform=None, max_images_per_user=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.image_files = []
        self.user_labels = []
        
        # æ”¶é›†æ‰€æœ‰ç”¨æˆ·ç›®å½•ä¸‹çš„å›¾åƒ
        user_dirs = [d for d in self.data_dir.iterdir() if d.is_dir() and d.name.startswith('ID_')]
        user_dirs.sort()
        
        for user_dir in user_dirs:
            user_id = user_dir.name
            images = list(user_dir.glob('*.png')) + list(user_dir.glob('*.jpg'))
            
            # é™åˆ¶æ¯ä¸ªç”¨æˆ·çš„å›¾åƒæ•°é‡
            if max_images_per_user and len(images) > max_images_per_user:
                images = images[:max_images_per_user]
            
            self.image_files.extend(images)
            self.user_labels.extend([user_id] * len(images))
        
        print(f"ğŸ“ å¾®è°ƒæ•°æ®é›†: {len(self.image_files)} å¼ å›¾åƒï¼Œæ¥è‡ª {len(set(self.user_labels))} ä¸ªç”¨æˆ·")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        user_id = self.user_labels[idx]
        
        try:
            image = Image.open(image_path)
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            return image, user_id
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾åƒå¤±è´¥ {image_path}: {e}")
            # è¿”å›é»‘è‰²å›¾åƒä½œä¸ºfallback
            black_image = Image.new('RGB', (256, 256), (0, 0, 0))
            if self.transform:
                black_image = self.transform(black_image)
            return black_image, user_id

class VAEFineTuner:
    """VA-VAEå¾®è°ƒå™¨"""
    
    def __init__(self, vae_model_path, device='cuda'):
        self.device = device
        self.vae_model_path = vae_model_path
        
        # åŠ è½½é¢„è®­ç»ƒVA-VAE
        print("ğŸ”§ åŠ è½½é¢„è®­ç»ƒVA-VAEæ¨¡å‹...")
        self.vae = self.load_vae_model()
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    
    def load_vae_model(self):
        """åŠ è½½VA-VAEæ¨¡å‹"""
        try:
            config_path = "LightningDiT/tokenizer/configs/vavae_f16d32.yaml"
            
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            config['ckpt_path'] = self.vae_model_path
            
            temp_config = "temp_finetune_vavae_config.yaml"
            with open(temp_config, 'w') as f:
                yaml.dump(config, f)
            
            vae = VA_VAE(config=temp_config)
            print("âœ… VA-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
            return vae
        except Exception as e:
            print(f"âŒ VA-VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
    
    def freeze_encoder(self, freeze=True):
        """å†»ç»“/è§£å†»ç¼–ç å™¨"""
        for param in self.vae.model.encoder.parameters():
            param.requires_grad = not freeze
        
        status = "å†»ç»“" if freeze else "è§£å†»"
        print(f"ğŸ”’ ç¼–ç å™¨å·²{status}")
    
    def create_optimizer(self, learning_rate, freeze_encoder=False):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if freeze_encoder:
            # åªä¼˜åŒ–è§£ç å™¨
            params = list(self.vae.model.decoder.parameters()) + list(self.vae.model.quant_conv.parameters()) + list(self.vae.model.post_quant_conv.parameters())
        else:
            # ä¼˜åŒ–å…¨æ¨¡å‹
            params = self.vae.model.parameters()
        
        optimizer = torch.optim.AdamW(params, lr=learning_rate, weight_decay=1e-4)
        return optimizer
    
    def compute_loss(self, images):
        """è®¡ç®—é‡å»ºæŸå¤±"""
        with torch.cuda.amp.autocast():
            # ç¼–ç 
            latents = self.vae.model.encode(images).latent_dist.sample()
            
            # è§£ç 
            reconstructed = self.vae.model.decode(latents).sample
            
            # é‡å»ºæŸå¤± (L1 + L2)
            l1_loss = F.l1_loss(reconstructed, images)
            l2_loss = F.mse_loss(reconstructed, images)
            recon_loss = l1_loss + 0.1 * l2_loss
            
            # KLæ•£åº¦æŸå¤±
            kl_loss = torch.mean(torch.sum(latents ** 2, dim=[1, 2, 3]))
            
            # æ€»æŸå¤±
            total_loss = recon_loss + 1e-6 * kl_loss
            
            return total_loss, recon_loss, kl_loss, reconstructed
    
    def train_epoch(self, dataloader, optimizer, epoch, freeze_encoder=False):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.vae.model.train()
        self.freeze_encoder(freeze_encoder)
        
        total_loss = 0
        total_recon_loss = 0
        total_kl_loss = 0
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
        
        for batch_idx, (images, _) in enumerate(pbar):
            images = images.to(self.device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            loss, recon_loss, kl_loss, _ = self.compute_loss(images)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.vae.model.parameters(), 1.0)
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            total_recon_loss += recon_loss.item()
            total_kl_loss += kl_loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss.item():.6f}',
                'Recon': f'{recon_loss.item():.6f}',
                'KL': f'{kl_loss.item():.8f}'
            })
        
        avg_loss = total_loss / len(dataloader)
        avg_recon_loss = total_recon_loss / len(dataloader)
        avg_kl_loss = total_kl_loss / len(dataloader)
        
        return avg_loss, avg_recon_loss, avg_kl_loss
    
    def validate(self, dataloader):
        """éªŒè¯"""
        self.vae.model.eval()
        
        total_loss = 0
        total_recon_loss = 0
        
        with torch.no_grad():
            for images, _ in dataloader:
                images = images.to(self.device)
                loss, recon_loss, _, _ = self.compute_loss(images)
                
                total_loss += loss.item()
                total_recon_loss += recon_loss.item()
        
        avg_loss = total_loss / len(dataloader)
        avg_recon_loss = total_recon_loss / len(dataloader)
        
        return avg_loss, avg_recon_loss
    
    def save_checkpoint(self, epoch, optimizer, loss, save_path):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.vae.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
        }
        torch.save(checkpoint, save_path)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")
    
    def finetune(self, data_dir, output_dir, config):
        """æ‰§è¡Œå¾®è°ƒ"""
        print("ğŸš€ å¼€å§‹VA-VAEå¾®è°ƒ")
        print("="*60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = MicroDopplerDataset(data_dir, transform=self.transform)
        
        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        train_size = int(0.9 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
        
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)
        
        print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾åƒ")
        print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} å¼ å›¾åƒ")
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        
        # é˜¶æ®µ1: è§£ç å™¨å¾®è°ƒ
        if config.get('stage1_epochs', 0) > 0:
            print(f"\nğŸ”¥ é˜¶æ®µ1: è§£ç å™¨å¾®è°ƒ ({config['stage1_epochs']} epochs)")
            optimizer = self.create_optimizer(config['stage1_lr'], freeze_encoder=True)
            
            for epoch in range(1, config['stage1_epochs'] + 1):
                train_loss, train_recon, train_kl = self.train_epoch(train_loader, optimizer, epoch, freeze_encoder=True)
                val_loss, val_recon = self.validate(val_loader)
                
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                
                print(f"Epoch {epoch}: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if epoch % 2 == 0:
                    self.save_checkpoint(epoch, optimizer, train_loss, output_path / f"stage1_epoch_{epoch}.pt")
        
        # é˜¶æ®µ2: å…¨æ¨¡å‹å¾®è°ƒ
        if config.get('stage2_epochs', 0) > 0:
            print(f"\nğŸ”¥ é˜¶æ®µ2: å…¨æ¨¡å‹å¾®è°ƒ ({config['stage2_epochs']} epochs)")
            optimizer = self.create_optimizer(config['stage2_lr'], freeze_encoder=False)
            
            for epoch in range(1, config['stage2_epochs'] + 1):
                train_loss, train_recon, train_kl = self.train_epoch(train_loader, optimizer, epoch, freeze_encoder=False)
                val_loss, val_recon = self.validate(val_loader)
                
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                
                print(f"Epoch {epoch}: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(epoch, optimizer, train_loss, output_path / f"stage2_epoch_{epoch}.pt")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = output_path / "finetuned_vavae.pt"
        torch.save(self.vae.model.state_dict(), final_model_path)
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(train_losses, val_losses, output_path / "training_curves.png")
        
        return final_model_path

    def plot_training_curves(self, train_losses, val_losses, save_path):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(10, 6))
        plt.plot(train_losses, label='Training Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('VA-VAE Fine-tuning Loss Curves')
        plt.legend()
        plt.grid(True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ VA-VAEå¾®è°ƒå·¥å…·")
    print("="*50)
    
    # é…ç½®
    config = {
        'batch_size': 4,
        'stage1_epochs': 2,  # è§£ç å™¨å¾®è°ƒ
        'stage1_lr': 5e-5,
        'stage2_epochs': 3,  # å…¨æ¨¡å‹å¾®è°ƒ
        'stage2_lr': 1e-5,
    }
    
    # è·¯å¾„
    data_dir = "/kaggle/input/dataset"
    vae_model_path = "models/vavae-imagenet256-f16d32-dinov2.pt"
    output_dir = "vavae_finetuned"
    
    print("ğŸ“‹ ä½¿ç”¨è¯´æ˜:")
    print("1. ç¡®ä¿æ•°æ®åœ¨ /kaggle/input/dataset/ ç›®å½•")
    print("2. ç¡®ä¿é¢„è®­ç»ƒæ¨¡å‹åœ¨ models/ ç›®å½•")
    print("3. è°ƒç”¨å¾®è°ƒå‡½æ•°:")
    print("   from finetune_vavae import VAEFineTuner")
    print("   tuner = VAEFineTuner('models/vavae-imagenet256-f16d32-dinov2.pt')")
    print("   tuner.finetune('/kaggle/input/dataset', 'vavae_finetuned', config)")

if __name__ == "__main__":
    main()

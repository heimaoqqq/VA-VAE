#!/usr/bin/env python3
"""
VA-VAEå¾®è°ƒè„šæœ¬ - å®Œæ•´ç‰ˆæœ¬
åŸºäºç ”ç©¶è¯æ®çš„æœ€ä½³å®è·µï¼šåŒæ—¶è®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨
æ”¯æŒæ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ã€å®Œæ•´çš„è®­ç»ƒç›‘æ§
"""

import os
import sys
import torch
import yaml
import numpy as np
from pathlib import Path
from PIL import Image
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import time

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from tokenizer.vavae import VA_VAE

class MicroDopplerDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›† - ç”¨äºVA-VAEå¾®è°ƒ"""
    
    def __init__(self, data_dir, transform=None, max_images_per_user=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.image_files = []
        self.user_labels = []
        
        # æ”¶é›†æ‰€æœ‰ç”¨æˆ·ç›®å½•ä¸‹çš„å›¾åƒ
        user_dirs = [d for d in self.data_dir.iterdir() if d.is_dir() and d.name.startswith('ID_')]
        user_dirs.sort()
        
        for user_dir in user_dirs:
            user_id = user_dir.name
            images = list(user_dir.glob('*.png')) + list(user_dir.glob('*.jpg'))
            
            # é™åˆ¶æ¯ä¸ªç”¨æˆ·çš„å›¾åƒæ•°é‡
            if max_images_per_user and len(images) > max_images_per_user:
                images = images[:max_images_per_user]
            
            self.image_files.extend(images)
            self.user_labels.extend([user_id] * len(images))
        
        print(f"ğŸ“ å¾®è°ƒæ•°æ®é›†: {len(self.image_files)} å¼ å›¾åƒï¼Œæ¥è‡ª {len(set(self.user_labels))} ä¸ªç”¨æˆ·")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        user_id = self.user_labels[idx]
        
        try:
            image = Image.open(image_path)
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            return image, user_id
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾åƒå¤±è´¥ {image_path}: {e}")
            # è¿”å›é»‘è‰²å›¾åƒä½œä¸ºfallback
            black_image = Image.new('RGB', (256, 256), (0, 0, 0))
            if self.transform:
                black_image = self.transform(black_image)
            return black_image, user_id

class VAEFineTuner:
    """VA-VAEå¾®è°ƒå™¨"""
    
    def __init__(self, vae_model_path, device='cuda'):
        self.device = device
        self.vae_model_path = vae_model_path
        
        # åŠ è½½é¢„è®­ç»ƒVA-VAE
        print("ğŸ”§ åŠ è½½é¢„è®­ç»ƒVA-VAEæ¨¡å‹...")
        self.vae = self.load_vae_model()
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    
    def load_vae_model(self):
        """åŠ è½½VA-VAEæ¨¡å‹"""
        try:
            config_path = "LightningDiT/tokenizer/configs/vavae_f16d32.yaml"
            
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            config['ckpt_path'] = self.vae_model_path
            
            temp_config = "temp_finetune_vavae_config.yaml"
            with open(temp_config, 'w') as f:
                yaml.dump(config, f)
            
            vae = VA_VAE(config=temp_config)
            print("âœ… VA-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
            return vae
        except Exception as e:
            print(f"âŒ VA-VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
    
    def create_optimizer(self, learning_rate):
        """åˆ›å»ºä¼˜åŒ–å™¨ - åŒæ—¶è®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨"""
        # åŸºäºç ”ç©¶è¯æ®ï¼šåŒæ—¶ä¼˜åŒ–å…¨æ¨¡å‹
        optimizer = torch.optim.AdamW(
            self.vae.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )
        return optimizer

    def create_scheduler(self, optimizer, total_steps):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # Cosine annealing with warmup
        warmup_steps = int(0.1 * total_steps)  # 10% warmup

        def lr_lambda(step):
            if step < warmup_steps:
                return step / warmup_steps
            else:
                progress = (step - warmup_steps) / (total_steps - warmup_steps)
                return 0.5 * (1 + np.cos(np.pi * progress))

        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        return scheduler
    
    def compute_loss(self, images):
        """è®¡ç®—é‡å»ºæŸå¤±"""
        with torch.cuda.amp.autocast():
            # ç¼–ç 
            latents = self.vae.model.encode(images).latent_dist.sample()
            
            # è§£ç 
            reconstructed = self.vae.model.decode(latents).sample
            
            # é‡å»ºæŸå¤± (L1 + L2)
            l1_loss = F.l1_loss(reconstructed, images)
            l2_loss = F.mse_loss(reconstructed, images)
            recon_loss = l1_loss + 0.1 * l2_loss
            
            # KLæ•£åº¦æŸå¤±
            kl_loss = torch.mean(torch.sum(latents ** 2, dim=[1, 2, 3]))
            
            # æ€»æŸå¤±
            total_loss = recon_loss + 1e-6 * kl_loss
            
            return total_loss, recon_loss, kl_loss, reconstructed
    
    def train_epoch(self, dataloader, optimizer, scheduler, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.vae.model.train()

        total_loss = 0
        total_recon_loss = 0
        total_kl_loss = 0

        pbar = tqdm(dataloader, desc=f"Epoch {epoch}")

        for batch_idx, (images, _) in enumerate(pbar):
            images = images.to(self.device)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            loss, recon_loss, kl_loss, _ = self.compute_loss(images)

            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.vae.model.parameters(), 1.0)
            optimizer.step()

            # æ›´æ–°å­¦ä¹ ç‡
            if scheduler:
                scheduler.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            total_recon_loss += recon_loss.item()
            total_kl_loss += kl_loss.item()

            # æ›´æ–°è¿›åº¦æ¡
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'Loss': f'{loss.item():.6f}',
                'Recon': f'{recon_loss.item():.6f}',
                'KL': f'{kl_loss.item():.8f}',
                'LR': f'{current_lr:.2e}'
            })

        avg_loss = total_loss / len(dataloader)
        avg_recon_loss = total_recon_loss / len(dataloader)
        avg_kl_loss = total_kl_loss / len(dataloader)

        return avg_loss, avg_recon_loss, avg_kl_loss
    
    def validate(self, dataloader):
        """éªŒè¯"""
        self.vae.model.eval()
        
        total_loss = 0
        total_recon_loss = 0
        
        with torch.no_grad():
            for images, _ in dataloader:
                images = images.to(self.device)
                loss, recon_loss, _, _ = self.compute_loss(images)
                
                total_loss += loss.item()
                total_recon_loss += recon_loss.item()
        
        avg_loss = total_loss / len(dataloader)
        avg_recon_loss = total_recon_loss / len(dataloader)
        
        return avg_loss, avg_recon_loss
    
    def save_checkpoint(self, epoch, optimizer, loss, save_path):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.vae.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
        }
        torch.save(checkpoint, save_path)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")
    
    def finetune(self, data_dir, output_dir, config):
        """æ‰§è¡Œå¾®è°ƒ - å®Œæ•´ç‰ˆæœ¬"""
        print("ğŸš€ å¼€å§‹VA-VAEå¾®è°ƒ (åŒæ—¶è®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨)")
        print("="*60)
        print(f"ğŸ“Š é…ç½®: {config['epochs']} epochs, lr={config['learning_rate']:.2e}, æ—©åœpatience={config['patience']}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ•°æ®é›†
        dataset = MicroDopplerDataset(data_dir, transform=self.transform)

        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        train_size = int(0.9 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size],
            generator=torch.Generator().manual_seed(42)  # å›ºå®šéšæœºç§å­
        )

        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)

        print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾åƒ")
        print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} å¼ å›¾åƒ")

        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = self.create_optimizer(config['learning_rate'])
        total_steps = len(train_loader) * config['epochs']
        scheduler = self.create_scheduler(optimizer, total_steps)

        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience_counter = 0

        # è®­ç»ƒå¾ªç¯
        start_time = time.time()

        for epoch in range(1, config['epochs'] + 1):
            print(f"\nğŸ”¥ Epoch {epoch}/{config['epochs']}")

            # è®­ç»ƒ
            train_loss, train_recon, train_kl = self.train_epoch(train_loader, optimizer, scheduler, epoch)

            # éªŒè¯
            val_loss, val_recon = self.validate(val_loader)

            # è®°å½•å†å²
            train_losses.append(train_loss)
            val_losses.append(val_loss)

            # æ‰“å°ç»“æœ
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {current_lr:.2e}")

            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_model_path = output_path / "best_model.pt"
                torch.save(self.vae.model.state_dict(), best_model_path)
                print(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Val Loss: {val_loss:.6f})")
            else:
                patience_counter += 1
                print(f"â³ æ—©åœè®¡æ•°: {patience_counter}/{config['patience']}")

            # æ—©åœ
            if patience_counter >= config['patience']:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
                break

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 10 == 0:
                checkpoint_path = output_path / f"checkpoint_epoch_{epoch}.pt"
                self.save_checkpoint(epoch, optimizer, train_loss, checkpoint_path)

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {total_time/3600:.2f} å°æ—¶")
        print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = output_path / "finetuned_vavae.pt"
        torch.save(self.vae.model.state_dict(), final_model_path)

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(train_losses, val_losses, output_path / "training_curves.png")

        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        self.save_training_log(train_losses, val_losses, best_val_loss, total_time, output_path / "training_log.txt")

        return best_model_path

    def plot_training_curves(self, train_losses, val_losses, save_path):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(12, 8))

        # ä¸»å›¾ï¼šæŸå¤±æ›²çº¿
        plt.subplot(2, 1, 1)
        epochs = range(1, len(train_losses) + 1)
        plt.plot(epochs, train_losses, label='Training Loss', color='blue', alpha=0.7)
        plt.plot(epochs, val_losses, label='Validation Loss', color='red', alpha=0.7)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('VA-VAE Fine-tuning Loss Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # å­å›¾ï¼šæœ€å50%çš„æŸå¤±æ›²çº¿ï¼ˆæ”¾å¤§æ˜¾ç¤ºï¼‰
        plt.subplot(2, 1, 2)
        start_idx = len(train_losses) // 2
        epochs_zoom = range(start_idx + 1, len(train_losses) + 1)
        plt.plot(epochs_zoom, train_losses[start_idx:], label='Training Loss (Zoomed)', color='blue', alpha=0.7)
        plt.plot(epochs_zoom, val_losses[start_idx:], label='Validation Loss (Zoomed)', color='red', alpha=0.7)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Loss Curves (Last 50% of Training)')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")

    def save_training_log(self, train_losses, val_losses, best_val_loss, total_time, save_path):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        with open(save_path, 'w') as f:
            f.write("VA-VAEå¾®è°ƒè®­ç»ƒæ—¥å¿—\n")
            f.write("=" * 40 + "\n")
            f.write(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶\n")
            f.write(f"è®­ç»ƒè½®æ•°: {len(train_losses)} epochs\n")
            f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\n")
            f.write(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}\n")
            f.write(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}\n")
            f.write(f"æŸå¤±æ”¹å–„: {(train_losses[0] - train_losses[-1])/train_losses[0]*100:.1f}%\n")
            f.write("\nè¯¦ç»†è®­ç»ƒå†å²:\n")
            f.write("Epoch\tTrain Loss\tVal Loss\n")
            for i, (train_loss, val_loss) in enumerate(zip(train_losses, val_losses)):
                f.write(f"{i+1}\t{train_loss:.6f}\t{val_loss:.6f}\n")

        print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {save_path}")

def run_complete_finetune():
    """å®Œæ•´çš„å¾®è°ƒæµç¨‹ - ä¸€é”®è¿è¡Œ"""
    print("ğŸš€ VA-VAEå®Œæ•´å¾®è°ƒæµç¨‹")
    print("="*60)

    # æ£€æŸ¥ç¯å¢ƒ
    data_dir = "/kaggle/input/dataset"
    vae_model_path = "models/vavae-imagenet256-f16d32-dinov2.pt"
    output_dir = "vavae_finetuned"

    if not Path(data_dir).exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return False

    if not Path(vae_model_path).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {vae_model_path}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ step2_download_models.py")
        return False

    # åŸºäºç ”ç©¶è¯æ®çš„é…ç½®
    config = {
        'batch_size': 4,           # é€‚åˆKaggle GPUå†…å­˜
        'epochs': 100,             # åŸºäºç ”ç©¶è¯æ®çš„åˆç†epochæ•°
        'learning_rate': 2e-5,     # åŸŸé€‚åº”çš„æœ€ä½³å­¦ä¹ ç‡
        'patience': 10,            # æ—©åœpatience
    }

    print("âš™ï¸ å¾®è°ƒé…ç½® (åŸºäºç ”ç©¶è¯æ®):")
    print(f"   åŒæ—¶è®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨: âœ…")
    print(f"   æœ€å¤§è®­ç»ƒè½®æ•°: {config['epochs']} epochs")
    print(f"   å­¦ä¹ ç‡: {config['learning_rate']:.2e}")
    print(f"   æ—©åœpatience: {config['patience']}")
    print(f"   æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"   é¢„è®¡æ—¶é—´: 3-8å°æ—¶ (å–å†³äºæ”¶æ•›é€Ÿåº¦)")

    # åˆ›å»ºå¾®è°ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")

    tuner = VAEFineTuner(vae_model_path, device)
    if tuner.vae is None:
        print("âŒ VA-VAEæ¨¡å‹åŠ è½½å¤±è´¥")
        return False

    # å¼€å§‹å¾®è°ƒ
    try:
        print(f"\nğŸš€ å¼€å§‹å¾®è°ƒ...")
        start_time = time.time()

        best_model_path = tuner.finetune(data_dir, output_dir, config)

        total_time = time.time() - start_time
        print(f"\nğŸ‰ å¾®è°ƒå®Œæˆï¼æ€»æ—¶é—´: {total_time/3600:.2f} å°æ—¶")
        print(f"ğŸ“ æœ€ä½³æ¨¡å‹: {best_model_path}")
        print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—: {output_dir}/training_log.txt")
        print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿: {output_dir}/training_curves.png")

        # å»ºè®®ä¸‹ä¸€æ­¥
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print(f"1. è¿è¡Œè¯„ä¼°è„šæœ¬éªŒè¯å¾®è°ƒæ•ˆæœ:")
        print(f"   !python evaluate_finetuned_vae.py")
        print(f"2. å¦‚æœæ•ˆæœæ»¡æ„ï¼Œè¿›å…¥é˜¶æ®µ2 UNetæ‰©æ•£æ¨¡å‹è®­ç»ƒ")

        return True

    except Exception as e:
        print(f"âŒ å¾®è°ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ VA-VAEå¾®è°ƒå·¥å…· - å®Œæ•´ç‰ˆæœ¬")
    print("="*50)

    print("ğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
    print("1. ç›´æ¥è¿è¡Œå®Œæ•´å¾®è°ƒ:")
    print("   python finetune_vavae.py")
    print("2. æˆ–è€…åœ¨ä»£ç ä¸­è°ƒç”¨:")
    print("   from finetune_vavae import VAEFineTuner")
    print("   tuner = VAEFineTuner('models/vavae-imagenet256-f16d32-dinov2.pt')")
    print("   config = {'batch_size': 4, 'epochs': 100, 'learning_rate': 2e-5, 'patience': 10}")
    print("   tuner.finetune('/kaggle/input/dataset', 'vavae_finetuned', config)")

    # è¿è¡Œå®Œæ•´å¾®è°ƒ
    success = run_complete_finetune()
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

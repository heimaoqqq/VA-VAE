"""
åˆ†ææ˜¯å¦éœ€è¦é‡å¤´è®­ç»ƒVA-VAEå’ŒVFMçš„è¯Šæ–­è„šæœ¬
"""

import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from pathlib import Path
import sys
import torchvision.transforms as T
from skimage.metrics import structural_similarity as compare_ssim
from skimage.metrics import peak_signal_noise_ratio as compare_psnr
import cv2

sys.path.append('/kaggle/working/VA-VAE/LightningDiT/vavae')
sys.path.append('/kaggle/working/taming-transformers')  # æ·»åŠ tamingè·¯å¾„
sys.path.append('/kaggle/working/VA-VAE')  # æ·»åŠ VA-VAEæ ¹è·¯å¾„

# ç¡®ä¿tamingæ¨¡å—å¯ä»¥æ­£ç¡®å¯¼å…¥
import os
os.environ['PYTHONPATH'] = '/kaggle/working/taming-transformers:' + os.environ.get('PYTHONPATH', '')

def analyze_vf_semantic_relevance():
    """åˆ†æVFç‰¹å¾å¯¹å¾®å¤šæ™®å‹’æ•°æ®çš„è¯­ä¹‰ç›¸å…³æ€§"""
    
    print("="*60)
    print("VFè¯­ä¹‰ç›¸å…³æ€§åˆ†æ")
    print("="*60)
    
    # 1. åŠ è½½ä¸åŒç”¨æˆ·çš„å¾®å¤šæ™®å‹’å›¾åƒ
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®è·¯å¾„
    possible_paths = [
        Path('/kaggle/input/dataset'),
        Path('/kaggle/input/microdoppler-dataset'),
        Path('/kaggle/input/microdoppler-dataset/train'),
        Path('/kaggle/input/dataset/train')
    ]
    
    data_dir = None
    for path in possible_paths:
        if path.exists():
            print(f"ğŸ” æ£€æŸ¥è·¯å¾„: {path}")
            # åˆ—å‡ºæ‰€æœ‰å­ç›®å½•è¿›è¡Œè°ƒè¯•
            subdirs = [d for d in path.iterdir() if d.is_dir()]
            print(f"   å­ç›®å½•: {[d.name for d in subdirs[:10]]}")  # åªæ˜¾ç¤ºå‰10ä¸ª
            
            user_dirs = list(path.glob('ID_*')) or list(path.glob('user*'))
            if user_dirs:
                data_dir = path
                print(f"ğŸ“‚ æ‰¾åˆ°æ•°æ®ç›®å½•: {data_dir}")
                print(f"   ç”¨æˆ·ç›®å½•: {[d.name for d in user_dirs[:5]]}")
                break
    
    if not data_dir:
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®ç›®å½•ï¼Œå°è¯•çš„è·¯å¾„:")
        for path in possible_paths:
            if path.exists():
                subdirs = [d.name for d in path.iterdir() if d.is_dir()][:5]
                print(f"   {path} - å­˜åœ¨ï¼Œå­ç›®å½•: {subdirs}")
            else:
                print(f"   {path} - ä¸å­˜åœ¨")
        return {}
    
    user_samples = {}
    # æ¯ä¸ªç”¨æˆ·å–5å¼ å›¾åƒ
    user_dirs = list(data_dir.glob('ID_*')) or list(data_dir.glob('user*'))
    for user_dir in sorted(user_dirs)[:5]:  # åªåˆ†æå‰5ä¸ªç”¨æˆ·
        user_id = user_dir.name
        print(f"ğŸ” æ£€æŸ¥ç”¨æˆ·ç›®å½•: {user_dir}")
        
        # å°è¯•å¤šç§å›¾åƒæ ¼å¼ï¼Œjpgä¼˜å…ˆ
        image_patterns = ['*.jpg', '*.jpeg', '*.JPG', '*.JPEG', '*.png', '*.PNG']
        images = []
        for pattern in image_patterns:
            found_images = list(user_dir.glob(pattern))
            if found_images:
                images.extend(found_images)
                print(f"   æ‰¾åˆ° {len(found_images)} ä¸ª {pattern} æ–‡ä»¶")
                break
        
        if not images:
            # è°ƒè¯•ï¼šåˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            all_files = list(user_dir.iterdir())
            print(f"   ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶: {[f.name for f in all_files[:10]]}")
        
        if images:
            user_samples[user_id] = images[:5]  # åªå–å‰5å¼ 
            print(f"ç”¨æˆ· {user_id}: {len(user_samples[user_id])} å¼ å›¾åƒ")
    
    if not user_samples:
        print(f"âŒ åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•ç”¨æˆ·å›¾åƒ")
        return {}
    
    return user_samples

def load_dinov2_features(image_path, model):
    """æå–DINOv2ç‰¹å¾"""
    from PIL import Image
    import torchvision.transforms as T
    
    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    image = Image.open(image_path).convert('RGB')
    x = transform(image).unsqueeze(0)
    
    # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
    if torch.cuda.is_available():
        x = x.cuda()
    
    with torch.no_grad():
        features = model(x)
    
    return features.squeeze(0).cpu()  # è¿”å›CPUä¸Šçš„ç‰¹å¾

def calculate_fid_score(real_images, fake_images, device='cuda'):
    """è®¡ç®—FIDåˆ†æ•° - é’ˆå¯¹é‡å»ºä»»åŠ¡ä¼˜åŒ–"""
    
    # æ£€æŸ¥æ ·æœ¬æ•°é‡
    n_samples = real_images.shape[0]
    if n_samples < 50:
        print(f"âš ï¸ FIDè®¡ç®—ï¼šæ ·æœ¬æ•°é‡å¤ªå°‘({n_samples})ï¼ŒFIDéœ€è¦å¤§é‡æ ·æœ¬(>50)æ‰å‡†ç¡®")
        print("   å»ºè®®ï¼šå¢åŠ æµ‹è¯•å›¾åƒæ•°é‡æˆ–ä½¿ç”¨å…¶ä»–é‡å»ºè´¨é‡æŒ‡æ ‡")
        return None
    
    try:
        from torchvision.models import inception_v3
        
        # åŠ è½½é¢„è®­ç»ƒçš„Inceptionç½‘ç»œ
        inception = inception_v3(pretrained=True, transform_input=False).to(device)
        inception.eval()
        
        def get_inception_features(images):
            with torch.no_grad():
                # è°ƒæ•´å›¾åƒå°ºå¯¸åˆ°299x299 (Inceptionè¾“å…¥è¦æ±‚)
                if images.shape[-1] != 299:
                    images = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)
                
                # è·å–ç‰¹å¾
                features = inception(images)
                return features.cpu().numpy()
        
        # è®¡ç®—çœŸå®å›¾åƒå’Œé‡å»ºå›¾åƒçš„ç‰¹å¾
        real_features = get_inception_features(real_images)
        fake_features = get_inception_features(fake_images)
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        if real_features.shape[0] != fake_features.shape[0]:
            print("âš ï¸ FIDè®¡ç®—ï¼šç‰¹å¾æ•°é‡ä¸åŒ¹é…")
            return None
        
        # è®¡ç®—å‡å€¼å’Œåæ–¹å·®
        mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)
        mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)
        
        # æ·»åŠ æ­£åˆ™åŒ–é˜²æ­¢å¥‡å¼‚çŸ©é˜µ
        eps = 1e-6
        sigma1 += eps * np.eye(sigma1.shape[0])
        sigma2 += eps * np.eye(sigma2.shape[0])
        
        # è®¡ç®—FID - ä¿®å¤è´Ÿæ•°é—®é¢˜
        diff = mu1 - mu2
        
        # ä½¿ç”¨scipyçš„çŸ©é˜µå¹³æ–¹æ ¹æ¥é¿å…æ•°å€¼ä¸ç¨³å®š
        try:
            from scipy.linalg import sqrtm
            covmean = sqrtm(sigma1.dot(sigma2))
            if np.iscomplexobj(covmean):
                covmean = covmean.real
        except ImportError:
            # é€€å›åˆ°ç®€åŒ–è®¡ç®—
            print("âš ï¸ scipyä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–FIDè®¡ç®—")
            eigvals = np.linalg.eigvals(sigma1.dot(sigma2))
            covmean_trace = np.sqrt(np.abs(eigvals)).sum()
            fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * covmean_trace
            return max(0, fid)  # ç¡®ä¿éè´Ÿ
        
        fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)
        return max(0, fid)  # ç¡®ä¿FIDéè´Ÿ
        
    except Exception as e:
        print(f"FIDè®¡ç®—å¤±è´¥: {e}")
        return None


def calculate_feature_similarity(real_images, fake_images, device='cuda'):
    """è®¡ç®—ç‰¹å¾å±‚é¢çš„ç›¸ä¼¼åº¦ - æ›´é€‚åˆé‡å»ºä»»åŠ¡"""
    try:
        from torchvision.models import inception_v3
        
        inception = inception_v3(pretrained=True, transform_input=False).to(device)
        inception.eval()
        
        def get_inception_features(images):
            with torch.no_grad():
                if images.shape[-1] != 299:
                    images = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)
                features = inception(images)
                return features
        
        # è·å–ç‰¹å¾
        real_features = get_inception_features(real_images)
        fake_features = get_inception_features(fake_images)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cos_sim = F.cosine_similarity(real_features, fake_features, dim=1).mean().item()
        
        # è®¡ç®—ç‰¹å¾è·ç¦»
        feature_dist = F.mse_loss(real_features, fake_features).item()
        
        return {
            'cosine_similarity': cos_sim,
            'feature_mse': feature_dist
        }
        
    except Exception as e:
        print(f"ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
        return None

def calculate_lpips_score(real_images, fake_images):
    """è®¡ç®—LPIPSæ„ŸçŸ¥è·ç¦»"""
    try:
        import lpips
        
        # åˆå§‹åŒ–LPIPSç½‘ç»œ
        lpips_net = lpips.LPIPS(net='alex')  # ä½¿ç”¨AlexNet
        if torch.cuda.is_available():
            lpips_net = lpips_net.cuda()
        
        with torch.no_grad():
            # ç¡®ä¿å›¾åƒåœ¨[-1,1]èŒƒå›´å†…
            real_norm = (real_images - real_images.min()) / (real_images.max() - real_images.min()) * 2 - 1
            fake_norm = (fake_images - fake_images.min()) / (fake_images.max() - fake_images.min()) * 2 - 1
            
            # è®¡ç®—LPIPSè·ç¦»
            lpips_scores = []
            for i in range(real_norm.shape[0]):
                score = lpips_net(real_norm[i:i+1], fake_norm[i:i+1])
                lpips_scores.append(score.item())
            
            return np.mean(lpips_scores)
            
    except Exception as e:
        print(f"LPIPSè®¡ç®—å¤±è´¥: {e}")
        return None

def calculate_traditional_metrics(real_images, fake_images):
    """è®¡ç®—ä¼ ç»Ÿå›¾åƒè´¨é‡æŒ‡æ ‡ï¼šPSNRå’ŒSSIM"""
    psnr_scores = []
    ssim_scores = []
    
    for i in range(real_images.shape[0]):
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ [0,1]
        real_np = real_images[i].cpu().numpy().transpose(1, 2, 0)
        fake_np = fake_images[i].cpu().numpy().transpose(1, 2, 0)
        
        # ç¡®ä¿å€¼åœ¨[0,1]èŒƒå›´
        real_np = np.clip((real_np + 1) / 2, 0, 1)
        fake_np = np.clip((fake_np + 1) / 2, 0, 1)
        
        # è®¡ç®—PSNR
        psnr = compare_psnr(real_np, fake_np, data_range=1.0)
        psnr_scores.append(psnr)
        
        # è®¡ç®—SSIM
        if real_np.shape[-1] == 3:  # RGBå›¾åƒ
            ssim = compare_ssim(real_np, fake_np, multichannel=True, data_range=1.0, channel_axis=2)
        else:  # ç°åº¦å›¾åƒ
            ssim = compare_ssim(real_np, fake_np, data_range=1.0)
        ssim_scores.append(ssim)
    
    return np.mean(psnr_scores), np.mean(ssim_scores)

def analyze_inter_vs_intra_user_similarity():
    """åˆ†æç”¨æˆ·å†…vsç”¨æˆ·é—´çš„VFç‰¹å¾ç›¸ä¼¼æ€§"""
    
    print("\n" + "="*60)
    print("ç”¨æˆ·é—´/ç”¨æˆ·å†…VFç‰¹å¾ç›¸ä¼¼æ€§åˆ†æ")
    print("="*60)
    
    try:
        # åŠ è½½DINOv2æ¨¡å‹
        print("æ­£åœ¨ä¸‹è½½DINOv2æ¨¡å‹...")
        dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
        dinov2_model.eval()
        if torch.cuda.is_available():
            dinov2_model = dinov2_model.cuda()
        print("âœ“ DINOv2æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        user_samples = analyze_vf_semantic_relevance()
        
        if not user_samples:
            print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·æ ·æœ¬ï¼Œè·³è¿‡VFç‰¹å¾åˆ†æ")
            return None
        
        # æå–æ‰€æœ‰ç‰¹å¾
        all_features = {}
        for user_id, image_paths in user_samples.items():
            user_features = []
            for img_path in image_paths:
                try:
                    features = load_dinov2_features(img_path, dinov2_model)
                    user_features.append(features)
                except Exception as e:
                    print(f"è·³è¿‡å›¾åƒ {img_path}: {e}")
                    
            if user_features:
                all_features[user_id] = torch.stack(user_features)
                print(f"{user_id}: {len(user_features)} ä¸ªç‰¹å¾å‘é‡")
        
        # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
        intra_user_similarities = []
        inter_user_similarities = []
        
        users = list(all_features.keys())
        
        # ç”¨æˆ·å†…ç›¸ä¼¼æ€§
        for user_id in users:
            features = all_features[user_id]
            n_samples = features.shape[0]
            
            for i in range(n_samples):
                for j in range(i+1, n_samples):
                    sim = F.cosine_similarity(features[i], features[j], dim=0)
                    intra_user_similarities.append(sim.item())
        
        # ç”¨æˆ·é—´ç›¸ä¼¼æ€§
        for i, user1 in enumerate(users):
            for j, user2 in enumerate(users):
                if i < j:  # é¿å…é‡å¤
                    features1 = all_features[user1]
                    features2 = all_features[user2]
                    
                    # è®¡ç®—æ‰€æœ‰ç»„åˆçš„ç›¸ä¼¼æ€§
                    for f1 in features1:
                        for f2 in features2:
                            sim = F.cosine_similarity(f1, f2, dim=0)
                            inter_user_similarities.append(sim.item())
        
        # ç»Ÿè®¡åˆ†æ
        intra_mean = np.mean(intra_user_similarities)
        intra_std = np.std(intra_user_similarities)
        inter_mean = np.mean(inter_user_similarities)
        inter_std = np.std(inter_user_similarities)
        
        print(f"\nğŸ“Š VFç‰¹å¾ç›¸ä¼¼æ€§ç»Ÿè®¡:")
        print(f"ç”¨æˆ·å†…ç›¸ä¼¼æ€§: {intra_mean:.4f} Â± {intra_std:.4f}")
        print(f"ç”¨æˆ·é—´ç›¸ä¼¼æ€§: {inter_mean:.4f} Â± {inter_std:.4f}")
        print(f"åˆ¤åˆ«èƒ½åŠ› (å·®å€¼): {intra_mean - inter_mean:.4f}")
        
        # åˆ¤æ–­VFç‰¹å¾çš„åˆ¤åˆ«èƒ½åŠ›
        if intra_mean - inter_mean > 0.05:
            print("\nâœ… VFç‰¹å¾å¯¹ç”¨æˆ·æœ‰è¾ƒå¥½åˆ¤åˆ«èƒ½åŠ›ï¼Œå½“å‰VA-VAEå¯ç”¨")
        elif intra_mean - inter_mean > 0.02:
            print("\nâš ï¸ VFç‰¹å¾åˆ¤åˆ«èƒ½åŠ›ä¸€èˆ¬ï¼Œå¯è€ƒè™‘ä¼˜åŒ–")
        else:
            print("\nâŒ VFç‰¹å¾åˆ¤åˆ«èƒ½åŠ›å·®ï¼Œå»ºè®®é‡å¤´è®­ç»ƒVFM")
            
        return {
            'intra_similarities': intra_user_similarities,
            'inter_similarities': inter_user_similarities,
            'discrimination_power': intra_mean - inter_mean
        }
        
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        return None

def analyze_latent_space_quality():
    """åˆ†æå½“å‰VA-VAEæ½œåœ¨ç©ºé—´çš„è´¨é‡"""
    
    print("\n" + "="*60) 
    print("VA-VAEæ½œåœ¨ç©ºé—´è´¨é‡åˆ†æ")
    print("="*60)
    
    try:
        # åŠ è½½VA-VAE - å¤„ç†tamingå¯¼å…¥é—®é¢˜
        sys.path.insert(0, '/kaggle/working/VA-VAE/LightningDiT/vavae')
        sys.path.insert(0, '/kaggle/working/taming-transformers')
        
        # æ‰‹åŠ¨å¯¼å…¥tamingæ¨¡å—ä»¥é¿å…å¯¼å…¥é”™è¯¯
        try:
            import taming
        except ImportError:
            print("è­¦å‘Š: tamingæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°è¯•è·³è¿‡VA-VAEåˆ†æ")
            return None
            
        from ldm.models.autoencoder import AutoencoderKL
        
        # å…ˆåŠ è½½checkpointæ£€æŸ¥å®é™…æ¶æ„
        checkpoint = torch.load('/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt', 
                               map_location='cpu')
        
        # ä½¿ç”¨step4_train_vavae.pyä¸­çš„æ­£ç¡®é…ç½®
        vae = AutoencoderKL(
            embed_dim=32,
            use_vf='dinov2',
            reverse_proj=True,
            ddconfig=dict(
                double_z=True,
                z_channels=32,
                resolution=256,
                in_channels=3,
                out_ch=3,
                ch=128,
                ch_mult=[1, 1, 2, 2, 4],  # ä½¿ç”¨è®­ç»ƒæ—¶çš„æ­£ç¡®é…ç½®
                num_res_blocks=2,
                attn_resolutions=[16],
                dropout=0.0
            ),
            lossconfig=dict(
                target="ldm.modules.losses.contperceptual.LPIPSWithDiscriminator",
                params=dict(
                    disc_start=50001,
                    disc_num_layers=3,
                    disc_weight=0.5,
                    disc_factor=1.0,
                    disc_in_channels=3,
                    disc_conditional=False,
                    disc_loss='hinge',
                    pixelloss_weight=1.0,
                    perceptual_weight=1.0,
                    kl_weight=1e-6,
                    logvar_init=0.0,
                    use_actnorm=False,
                    pp_style=False,
                    vf_weight=1.0,
                    adaptive_vf=False,
                    distmat_weight=1.0,
                    cos_weight=1.0,
                    distmat_margin=0.0,
                    cos_margin=0.0
                )
            )
        )
        
        # åŠ è½½checkpoint  
        state_dict = checkpoint['state_dict']
        state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}
        vae.load_state_dict(state_dict, strict=False)
        vae.eval()
        
        # å°†VAEç§»åˆ°GPU
        if torch.cuda.is_available():
            vae = vae.cuda()
            print("âœ“ VA-VAEåŠ è½½æˆåŠŸå¹¶ç§»åˆ°GPU")
        else:
            print("âœ“ VA-VAEåŠ è½½æˆåŠŸ (CPUæ¨¡å¼)")
        
        # åˆ†ææ½œåœ¨ç©ºé—´çš„ç”¨æˆ·åˆ¤åˆ«æ€§ - ç›´æ¥åœ¨è¿™é‡ŒæŸ¥æ‰¾æ•°æ®
        possible_paths = [
            Path('/kaggle/input/dataset'),
            Path('/kaggle/input/microdoppler-dataset'),
            Path('/kaggle/input/microdoppler-dataset/train'),
            Path('/kaggle/input/dataset/train')
        ]
        
        data_dir = None
        for path in possible_paths:
            if path.exists():
                user_dirs = list(path.glob('ID_*')) or list(path.glob('user*'))
                if user_dirs:
                    data_dir = path
                    print(f"ğŸ“‚ æ½œåœ¨ç©ºé—´åˆ†ææ‰¾åˆ°æ•°æ®: {data_dir}")
                    break
        
        if not data_dir:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®ç›®å½•ï¼Œè·³è¿‡æ½œåœ¨ç©ºé—´åˆ†æ")
            return None
            
        user_latents = {}
        
        # è·å–ç”¨æˆ·ç›®å½•å’Œå›¾åƒ
        user_dirs = list(data_dir.glob('ID_*')) or list(data_dir.glob('user*'))
        for user_dir in sorted(user_dirs)[:3]:  # åˆ†æå‰3ä¸ªç”¨æˆ·
            user_id = user_dir.name
            # æŸ¥æ‰¾jpgå›¾åƒæ–‡ä»¶
            image_paths = list(user_dir.glob('*.jpg')) or list(user_dir.glob('*.JPG'))
            image_paths = image_paths[:3]  # æ¯ç”¨æˆ·3å¼ å›¾
            
            if not image_paths:
                continue
            latents = []
            print(f"å¤„ç†ç”¨æˆ· {user_id} çš„ {len(image_paths)} å¼ å›¾åƒ...")
            for i, img_path in enumerate(image_paths):
                try:
                    print(f"  å¤„ç†å›¾åƒ {i+1}/{len(image_paths)}: {img_path.name}")
                    # åŠ è½½å›¾åƒ
                    img = Image.open(img_path).convert('RGB').resize((256, 256))
                    img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0
                    img_tensor = img_tensor * 2.0 - 1.0  # å½’ä¸€åŒ–åˆ°[-1,1]
                    img_tensor = img_tensor.unsqueeze(0)
                    
                    if torch.cuda.is_available():
                        img_tensor = img_tensor.cuda()
                    
                    # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
                    with torch.no_grad():
                        posterior = vae.encode(img_tensor)
                        latent = posterior.sample()
                        latents.append(latent.squeeze(0).cpu())
                        print(f"    âœ“ ç¼–ç å®Œæˆï¼Œæ½œåœ¨å‘é‡å½¢çŠ¶: {latent.squeeze(0).shape}")
                        
                except Exception as e:
                    print(f"    âŒ å¤„ç†å›¾åƒå¤±è´¥ {img_path}: {e}")
                    import traceback
                    traceback.print_exc()
            
            if latents:
                user_latents[user_id] = torch.stack(latents)
                print(f"{user_id}: {len(latents)} ä¸ªæ½œåœ¨å‘é‡")
        
        # è®¡ç®—æ½œåœ¨ç©ºé—´çš„ç”¨æˆ·åˆ¤åˆ«æ€§
        if len(user_latents) >= 2:
            users = list(user_latents.keys())
            
            # ç”¨æˆ·å†…è·ç¦»
            intra_distances = []
            for user_id in users:
                latents = user_latents[user_id]
                for i in range(len(latents)):
                    for j in range(i+1, len(latents)):
                        dist = torch.norm(latents[i] - latents[j]).item()
                        intra_distances.append(dist)
            
            # ç”¨æˆ·é—´è·ç¦»  
            inter_distances = []
            for i, user1 in enumerate(users):
                for j, user2 in enumerate(users):
                    if i < j:
                        latents1 = user_latents[user1]
                        latents2 = user_latents[user2]
                        for l1 in latents1:
                            for l2 in latents2:
                                dist = torch.norm(l1 - l2).item()
                                inter_distances.append(dist)
            
            intra_mean = np.mean(intra_distances)
            inter_mean = np.mean(inter_distances)
            
            print(f"\nğŸ“Š æ½œåœ¨ç©ºé—´è·ç¦»ç»Ÿè®¡:")
            print(f"ç”¨æˆ·å†…è·ç¦»: {intra_mean:.4f}")
            print(f"ç”¨æˆ·é—´è·ç¦»: {inter_mean:.4f}") 
            print(f"åˆ†ç¦»åº¦: {inter_mean / intra_mean:.4f}")
            
            if inter_mean / intra_mean > 1.2:
                print("\nâœ… æ½œåœ¨ç©ºé—´ç”¨æˆ·åˆ†ç¦»åº¦è‰¯å¥½")
                return True
            else:
                print("\nâŒ æ½œåœ¨ç©ºé—´ç”¨æˆ·åˆ†ç¦»åº¦ä¸è¶³")
                return False
        
    except Exception as e:
        print(f"æ½œåœ¨ç©ºé—´åˆ†æå¤±è´¥: {e}")
        return None

def analyze_reconstruction_quality():
    """åˆ†æVA-VAEé‡å»ºè´¨é‡"""
    
    print("\n" + "="*60)
    print("VA-VAEé‡å»ºè´¨é‡åˆ†æ")
    print("="*60)
    
    try:
        # åŠ è½½VA-VAE (å¤ç”¨ä¹‹å‰çš„ä»£ç )
        sys.path.insert(0, '/kaggle/working/VA-VAE/LightningDiT/vavae')
        sys.path.insert(0, '/kaggle/working/taming-transformers')
        
        try:
            import taming
        except ImportError:
            print("è­¦å‘Š: tamingæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè·³è¿‡é‡å»ºè´¨é‡åˆ†æ")
            return None
            
        from ldm.models.autoencoder import AutoencoderKL
        
        # åŠ è½½checkpoint
        checkpoint = torch.load('/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt', 
                               map_location='cpu')
        
        # åˆ›å»ºVAEæ¨¡å‹
        vae = AutoencoderKL(
            embed_dim=32,
            use_vf='dinov2',
            reverse_proj=True,
            ddconfig=dict(
                double_z=True, z_channels=32, resolution=256, in_channels=3, out_ch=3,
                ch=128, ch_mult=[1, 1, 2, 2, 4], num_res_blocks=2, 
                attn_resolutions=[16], dropout=0.0
            ),
            lossconfig=dict(
                target="ldm.modules.losses.contperceptual.LPIPSWithDiscriminator",
                params=dict(
                    disc_start=50001, disc_num_layers=3, disc_weight=0.5, disc_factor=1.0,
                    disc_in_channels=3, disc_conditional=False, disc_loss='hinge',
                    pixelloss_weight=1.0, perceptual_weight=1.0, kl_weight=1e-6, logvar_init=0.0,
                    use_actnorm=False, pp_style=False, vf_weight=1.0, adaptive_vf=False,
                    distmat_weight=1.0, cos_weight=1.0, distmat_margin=0.0, cos_margin=0.0
                )
            )
        )
        
        # åŠ è½½æƒé‡
        state_dict = checkpoint['state_dict']
        state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}
        vae.load_state_dict(state_dict, strict=False)
        vae.eval()
        
        if torch.cuda.is_available():
            vae = vae.cuda()
        
        print("âœ“ VA-VAEåŠ è½½æˆåŠŸ")
        
        # æ”¶é›†æµ‹è¯•å›¾åƒ
        data_dir = Path('/kaggle/input/dataset')
        if not data_dir.exists():
            print("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return None
        
        test_images = []
        user_dirs = list(data_dir.glob('ID_*'))[:3]  # å‰3ä¸ªç”¨æˆ·
        
        for user_dir in user_dirs:
            images = list(user_dir.glob('*.jpg'))[:5]  # æ¯ç”¨æˆ·5å¼ å›¾
            for img_path in images:
                try:
                    img = Image.open(img_path).convert('RGB').resize((256, 256))
                    img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0
                    img_tensor = img_tensor * 2.0 - 1.0  # å½’ä¸€åŒ–åˆ°[-1,1]
                    test_images.append(img_tensor)
                except Exception as e:
                    continue
        
        if len(test_images) < 5:
            print(f"âŒ æµ‹è¯•å›¾åƒä¸è¶³: åªæ‰¾åˆ°{len(test_images)}å¼ ")
            return None
        
        # æ‰¹å¤„ç†é‡å»º
        batch_size = min(8, len(test_images))
        test_batch = torch.stack(test_images[:batch_size])
        
        if torch.cuda.is_available():
            test_batch = test_batch.cuda()
        
        print(f"ğŸ–¼ï¸ å¤„ç† {batch_size} å¼ æµ‹è¯•å›¾åƒ...")
        
        with torch.no_grad():
            # ç¼–ç -è§£ç é‡å»º
            posterior = vae.encode(test_batch)
            latents = posterior.sample()
            reconstructed = vae.decode(latents)
        
        print("âœ“ é‡å»ºå®Œæˆï¼Œå¼€å§‹è´¨é‡è¯„ä¼°...")
        
        # è®¡ç®—å„ç§è´¨é‡æŒ‡æ ‡
        results = {}
        
        # 1. ä¼ ç»ŸæŒ‡æ ‡ (PSNR, SSIM)
        try:
            psnr, ssim = calculate_traditional_metrics(test_batch, reconstructed)
            results['PSNR'] = psnr
            results['SSIM'] = ssim
            print(f"ğŸ“Š PSNR: {psnr:.2f} dB")
            print(f"ğŸ“Š SSIM: {ssim:.4f}")
        except Exception as e:
            print(f"âš ï¸ ä¼ ç»ŸæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        
        # 2. LPIPSæ„ŸçŸ¥è·ç¦»
        try:
            lpips_score = calculate_lpips_score(test_batch, reconstructed)
            if lpips_score is not None:
                results['LPIPS'] = lpips_score
                print(f"ğŸ“Š LPIPS: {lpips_score:.4f}")
        except Exception as e:
            print(f"âš ï¸ LPIPSè®¡ç®—å¤±è´¥: {e}")
        
        # 3. FIDåˆ†æ•° (é€‚ç”¨äºå¤§æ ·æœ¬åˆ†å¸ƒæ¯”è¾ƒ)
        try:
            fid_score = calculate_fid_score(test_batch, reconstructed)
            if fid_score is not None:
                results['FID'] = fid_score
                print(f"ğŸ“Š FID: {fid_score:.2f}")
            else:
                print("ğŸ“Š FID: æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œè·³è¿‡FIDè®¡ç®—")
        except Exception as e:
            print(f"âš ï¸ FIDè®¡ç®—å¤±è´¥: {e}")
        
        # 4. ç‰¹å¾ç›¸ä¼¼åº¦ (æ›´é€‚åˆé‡å»ºä»»åŠ¡)
        try:
            feat_sim = calculate_feature_similarity(test_batch, reconstructed)
            if feat_sim is not None:
                results['Feature_Cosine_Sim'] = feat_sim['cosine_similarity']
                results['Feature_MSE'] = feat_sim['feature_mse']
                print(f"ğŸ“Š ç‰¹å¾ä½™å¼¦ç›¸ä¼¼åº¦: {feat_sim['cosine_similarity']:.4f}")
                print(f"ğŸ“Š ç‰¹å¾MSE: {feat_sim['feature_mse']:.6f}")
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
        
        # 5. åƒç´ çº§è¯¯å·®
        mse = F.mse_loss(test_batch, reconstructed).item()
        mae = F.l1_loss(test_batch, reconstructed).item()
        results['MSE'] = mse
        results['MAE'] = mae
        print(f"ğŸ“Š MSE: {mse:.6f}")
        print(f"ğŸ“Š MAE: {mae:.6f}")
        
        # è¯„ä¼°ç»“æœ
        print(f"\nğŸ¯ é‡å»ºè´¨é‡è¯„ä¼°:")
        
        # PSNRè¯„ä¼° (è¶Šé«˜è¶Šå¥½)
        if 'PSNR' in results:
            if results['PSNR'] > 25:
                print(f"   âœ… PSNR {results['PSNR']:.1f}dB - é‡å»ºè´¨é‡ä¼˜ç§€")
            elif results['PSNR'] > 20:
                print(f"   âš ï¸ PSNR {results['PSNR']:.1f}dB - é‡å»ºè´¨é‡è‰¯å¥½")
            else:
                print(f"   âŒ PSNR {results['PSNR']:.1f}dB - é‡å»ºè´¨é‡è¾ƒå·®")
        
        # SSIMè¯„ä¼° (è¶Šé«˜è¶Šå¥½)
        if 'SSIM' in results:
            if results['SSIM'] > 0.9:
                print(f"   âœ… SSIM {results['SSIM']:.3f} - ç»“æ„ç›¸ä¼¼æ€§ä¼˜ç§€")
            elif results['SSIM'] > 0.8:
                print(f"   âš ï¸ SSIM {results['SSIM']:.3f} - ç»“æ„ç›¸ä¼¼æ€§è‰¯å¥½")
            else:
                print(f"   âŒ SSIM {results['SSIM']:.3f} - ç»“æ„ç›¸ä¼¼æ€§è¾ƒå·®")
        
        # LPIPSè¯„ä¼° (è¶Šä½è¶Šå¥½)
        if 'LPIPS' in results:
            if results['LPIPS'] < 0.1:
                print(f"   âœ… LPIPS {results['LPIPS']:.3f} - æ„ŸçŸ¥è´¨é‡ä¼˜ç§€")
            elif results['LPIPS'] < 0.2:
                print(f"   âš ï¸ LPIPS {results['LPIPS']:.3f} - æ„ŸçŸ¥è´¨é‡è‰¯å¥½")
            else:
                print(f"   âŒ LPIPS {results['LPIPS']:.3f} - æ„ŸçŸ¥è´¨é‡è¾ƒå·®")
        
        return results
        
    except Exception as e:
        print(f"âŒ é‡å»ºè´¨é‡åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("ğŸ” VA-VAEå’ŒVFMé‡è®­ç»ƒå¿…è¦æ€§åˆ†æ")
    print("="*70)
    
    # åˆ†æ1: VFç‰¹å¾åˆ¤åˆ«èƒ½åŠ›
    vf_analysis = analyze_inter_vs_intra_user_similarity()
    
    # åˆ†æ2: æ½œåœ¨ç©ºé—´è´¨é‡
    latent_quality = analyze_latent_space_quality()
    
    # åˆ†æ3: é‡å»ºè´¨é‡è¯„ä¼°
    reconstruction_quality = analyze_reconstruction_quality()
    
    # ç»¼åˆå»ºè®®
    print("\n" + "="*70)
    print("ğŸ¯ ç»¼åˆå»ºè®®")
    print("="*70)
    
    if vf_analysis and latent_quality:
        discrimination = vf_analysis['discrimination_power']
        
        if discrimination > 0.05 and latent_quality:
            print("âœ… å»ºè®®ï¼šç»§ç»­ä½¿ç”¨å½“å‰VA-VAE")
            print("   ç†ç”±ï¼šVFç‰¹å¾å’Œæ½œåœ¨ç©ºé—´éƒ½æœ‰è¶³å¤Ÿçš„åˆ¤åˆ«èƒ½åŠ›")
            
        elif discrimination > 0.02:
            print("âš ï¸ å»ºè®®ï¼šå¯å°è¯•æ”¹è¿›å½“å‰VA-VAE")
            print("   ç†ç”±ï¼šæ€§èƒ½å°šå¯ï¼Œä½†æœ‰æ”¹è¿›ç©ºé—´")
            print("   æ–¹æ¡ˆï¼šè°ƒæ•´VF lossæƒé‡æˆ–æ·»åŠ å¯¹æ¯”å­¦ä¹ ")
            
        else:
            print("âŒ å»ºè®®ï¼šè€ƒè™‘é‡å¤´è®­ç»ƒä¸“é—¨çš„VFM")
            print("   ç†ç”±ï¼šå½“å‰VFç‰¹å¾å¯¹å¾®å¤šæ™®å‹’æ•°æ®åˆ¤åˆ«èƒ½åŠ›ä¸è¶³")
            print("   æ–¹æ¡ˆï¼šè®¾è®¡å¾®å¤šæ™®å‹’ä¸“ç”¨çš„è¯­ä¹‰ç‰¹å¾æå–å™¨")
    
    print("\nğŸ’¡ å¦‚æœé€‰æ‹©é‡å¤´è®­ç»ƒï¼Œå»ºè®®çš„æ–¹å‘:")
    print("1. è‡ªç›‘ç£å­¦ä¹ ï¼šå¯¹æ¯”å­¦ä¹ è®­ç»ƒå¾®å¤šæ™®å‹’ç‰¹å¾æå–å™¨")
    print("2. è¿åŠ¨æ¨¡å¼åˆ†ç±»ï¼šåŸºäºç‰©ç†è¿åŠ¨ç±»åˆ«è®­ç»ƒåˆ¤åˆ«å™¨")
    print("3. æ—¶é¢‘æ¨¡å¼å­¦ä¹ ï¼šä¸“é—¨çš„æ—¶é¢‘åŸŸç‰¹å¾å­¦ä¹ ")

if __name__ == '__main__':
    main()

"""
æµ‹è¯•åˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½
éªŒè¯åˆ†ç±»å™¨æ˜¯å¦çœŸæ­£å­¦ä¹ äº†å¾®å¤šæ™®å‹’ç‰¹å¾è€Œéè¿‡æ‹Ÿåˆ
"""

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import timm
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
import json
from tqdm import tqdm
from collections import defaultdict
import matplotlib.pyplot as plt


class RealMicroDopplerDataset(Dataset):
    """çœŸå®å¾®å¤šæ™®å‹’æ•°æ®é›†ï¼Œé€‚é…ID_*ç›®å½•æ ¼å¼"""
    
    def __init__(self, data_dir, transform=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.samples = []
        
        # æ‰«ææ‰€æœ‰ID_*æˆ–User_*ç›®å½•
        print(f"Scanning directory: {self.data_dir}")
        id_dirs = list(self.data_dir.glob("ID_*"))
        user_dirs = list(self.data_dir.glob("User_*"))
        all_dirs = id_dirs + user_dirs
        print(f"Found directories: {[d.name for d in all_dirs]}")
        
        for user_dir in sorted(all_dirs):
            if user_dir.is_dir():
                if user_dir.name.startswith("ID_"):
                    # ID_1 -> user_id 0, ID_2 -> user_id 1, etc.
                    user_id = int(user_dir.name.split('_')[1]) - 1
                elif user_dir.name.startswith("User_"):
                    # User_00 -> user_id 0, User_01 -> user_id 1, etc.
                    user_id = int(user_dir.name.split('_')[1])
                else:
                    continue
                print(f"Processing {user_dir.name} -> user_id {user_id}")
                
                # æ‰«æè¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                found_files = []
                for ext in ['*.png', '*.jpg', '*.jpeg']:
                    files = list(user_dir.glob(ext))
                    found_files.extend(files)
                    if files:
                        print(f"  Found {len(files)} {ext} files")
                
                for img_path in sorted(found_files):
                    self.samples.append((str(img_path), user_id))
                    
                if not found_files:
                    print(f"  WARNING: No image files found in {user_dir}")
                    # List all files to debug
                    all_files = list(user_dir.iterdir())
                    print(f"  Available files: {[f.name for f in all_files[:5]]}...")  # Show first 5
        
        print(f"Found {len(self.samples)} real samples from {len(set([s[1] for s in self.samples]))} users")
        
        # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬æ•°
        user_counts = defaultdict(int)
        for _, user_id in self.samples:
            user_counts[user_id] += 1
        
        print("\nSamples per user:")
        for user_id in sorted(user_counts.keys()):
            print(f"  User {user_id}: {user_counts[user_id]} samples")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, true_label = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, true_label, img_path


def load_classifier(model_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„åˆ†ç±»å™¨"""
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # å¯¼å…¥ImprovedClassifierç±»
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    from improved_classifier_training import ImprovedClassifier
    
    # åˆ›å»ºImprovedClassifieræ¨¡å‹ - åŒ¹é…è®­ç»ƒæ—¶çš„æ¶æ„
    model = ImprovedClassifier(
        num_classes=checkpoint['num_classes'],
        backbone='resnet18',
        dropout_rate=0.5,
        freeze_layers='minimal'
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    print(f"Loaded classifier from {model_path}")
    print(f"Training validation accuracy: {checkpoint['best_val_acc']:.2f}%")
    
    return model, checkpoint


def test_on_real_data(model, dataloader, device):
    """åœ¨çœŸå®æ•°æ®ä¸Šæµ‹è¯•åˆ†ç±»å™¨"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    all_confidences = []
    all_paths = []
    
    user_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'confidences': []})
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Testing on real data")
        for images, true_labels, img_paths in pbar:
            images = images.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            probabilities = F.softmax(outputs, dim=1)
            max_probs, predictions = torch.max(probabilities, dim=1)
            
            # è½¬æ¢ä¸ºnumpy
            predictions_np = predictions.cpu().numpy()
            true_labels_np = true_labels.numpy()
            max_probs_np = max_probs.cpu().numpy()
            
            # è®°å½•ç»“æœ
            for i in range(len(predictions_np)):
                pred = predictions_np[i]
                true_label = true_labels_np[i]
                confidence = max_probs_np[i]
                
                all_predictions.append(pred)
                all_labels.append(true_label)
                all_confidences.append(confidence)
                all_paths.append(img_paths[i])
                
                # æ›´æ–°ç”¨æˆ·ç»Ÿè®¡
                user_stats[true_label]['total'] += 1
                user_stats[true_label]['confidences'].append(confidence)
                if pred == true_label:
                    user_stats[true_label]['correct'] += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            correct_so_far = sum([1 for p, l in zip(all_predictions, all_labels) if p == l])
            accuracy = correct_so_far / len(all_predictions) if all_predictions else 0
            avg_confidence = np.mean(all_confidences) if all_confidences else 0
            
            pbar.set_postfix({
                'Acc': f'{accuracy*100:.1f}%',
                'AvgConf': f'{avg_confidence:.3f}'
            })
    
    return all_predictions, all_labels, all_confidences, all_paths, user_stats


def analyze_results(predictions, labels, confidences, user_stats):
    """åˆ†ææµ‹è¯•ç»“æœ"""
    
    # æ€»ä½“å‡†ç¡®ç‡
    correct = sum([1 for p, l in zip(predictions, labels) if p == l])
    total = len(predictions)
    
    if total == 0:
        print("âŒ ERROR: No samples found for testing!")
        print("Please check the dataset path and file formats.")
        return None
        
    overall_accuracy = correct / total
    
    # ç½®ä¿¡åº¦åˆ†æ
    avg_confidence = np.mean(confidences)
    confidence_std = np.std(confidences)
    
    # é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼ˆ>0.9ï¼‰
    high_conf_mask = np.array(confidences) > 0.9
    high_conf_count = np.sum(high_conf_mask)
    high_conf_ratio = high_conf_count / total
    
    # é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„å‡†ç¡®ç‡
    if high_conf_count > 0:
        high_conf_correct = sum([1 for i, (p, l) in enumerate(zip(predictions, labels)) 
                                if high_conf_mask[i] and p == l])
        high_conf_accuracy = high_conf_correct / high_conf_count
    else:
        high_conf_accuracy = 0
    
    print("\n" + "="*60)
    print("REAL DATA TEST RESULTS")
    print("="*60)
    
    print(f"\nğŸ“Š Overall Statistics:")
    print(f"  Total samples: {total}")
    print(f"  Correct predictions: {correct}")
    print(f"  Overall accuracy: {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)")
    print(f"  Average confidence: {avg_confidence:.3f} Â± {confidence_std:.3f}")
    
    print(f"\nğŸ¯ High-confidence samples (>0.9):")
    print(f"  Count: {high_conf_count} ({high_conf_ratio*100:.1f}% of total)")
    print(f"  Accuracy: {high_conf_accuracy:.3f} ({high_conf_accuracy*100:.1f}%)")
    
    # æ¯ç”¨æˆ·å‡†ç¡®ç‡
    print(f"\nğŸ‘¥ Per-user accuracy:")
    user_accuracies = []
    for user_id in sorted(user_stats.keys()):
        stats = user_stats[user_id]
        if stats['total'] > 0:
            acc = stats['correct'] / stats['total']
            avg_conf = np.mean(stats['confidences'])
            user_accuracies.append(acc)
            print(f"  User {user_id:2d}: {stats['correct']:3d}/{stats['total']:3d} "
                  f"({acc*100:5.1f}%) | Avg conf: {avg_conf:.3f}")
    
    # ç”¨æˆ·å‡†ç¡®ç‡åˆ†å¸ƒ
    if user_accuracies:
        print(f"\nğŸ“ˆ User accuracy distribution:")
        print(f"  Min: {min(user_accuracies)*100:.1f}%")
        print(f"  Max: {max(user_accuracies)*100:.1f}%")
        print(f"  Mean: {np.mean(user_accuracies)*100:.1f}%")
        print(f"  Std: {np.std(user_accuracies)*100:.1f}%")
    
    return {
        'overall_accuracy': overall_accuracy,
        'avg_confidence': avg_confidence,
        'high_conf_ratio': high_conf_ratio,
        'high_conf_accuracy': high_conf_accuracy,
        'user_accuracies': user_accuracies
    }


def evaluate_classifier_reliability(results):
    """æ ¹æ®æµ‹è¯•ç»“æœè¯„ä¼°åˆ†ç±»å™¨å¯é æ€§"""
    
    print("\n" + "="*60)
    print("CLASSIFIER RELIABILITY ASSESSMENT")
    print("="*60)
    
    overall_acc = results['overall_accuracy']
    high_conf_acc = results['high_conf_accuracy']
    user_acc_std = np.std(results['user_accuracies']) if results['user_accuracies'] else 0
    
    # åˆ¤æ–­æ ‡å‡†
    if overall_acc >= 0.95:
        print("âœ… EXCELLENT: Overall accuracy â‰¥ 95%")
        print("   â†’ Classifier learned correct features")
        print("   â†’ Generated sample evaluation is reliable")
        reliability = "HIGHLY RELIABLE"
        
    elif overall_acc >= 0.85:
        print("âœ… GOOD: Overall accuracy 85-95%")
        print("   â†’ Classifier is generally reliable")
        print("   â†’ Minor overfitting possible but acceptable")
        reliability = "RELIABLE"
        
    elif overall_acc >= 0.70:
        print("âš ï¸ MODERATE: Overall accuracy 70-85%")
        print("   â†’ Some overfitting likely")
        print("   â†’ Generated sample evaluation should be cautious")
        reliability = "MODERATELY RELIABLE"
        
    else:
        print("âŒ POOR: Overall accuracy < 70%")
        print("   â†’ Significant overfitting detected")
        print("   â†’ Generated sample evaluation NOT reliable")
        reliability = "UNRELIABLE"
    
    # é¢å¤–æ£€æŸ¥
    if user_acc_std > 0.2:
        print("\nâš ï¸ WARNING: High variance in per-user accuracy")
        print("   â†’ Classifier may be biased toward certain users")
    
    if high_conf_acc < overall_acc - 0.1:
        print("\nâš ï¸ WARNING: High-confidence samples less accurate")
        print("   â†’ Confidence calibration issues")
    
    print(f"\nğŸ FINAL VERDICT: Classifier is {reliability}")
    
    return reliability


def plot_results(predictions, labels, confidences, user_stats, output_dir):
    """å¯è§†åŒ–æµ‹è¯•ç»“æœ"""
    
    output_dir = Path(output_dir)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. ç½®ä¿¡åº¦åˆ†å¸ƒ
    axes[0, 0].hist(confidences, bins=50, alpha=0.7, edgecolor='black')
    axes[0, 0].axvline(x=0.9, color='red', linestyle='--', label='High conf threshold')
    axes[0, 0].set_xlabel('Confidence Score')
    axes[0, 0].set_ylabel('Count')
    axes[0, 0].set_title('Confidence Distribution on Real Data')
    axes[0, 0].legend()
    
    # 2. æ¯ç”¨æˆ·å‡†ç¡®ç‡
    user_ids = sorted(user_stats.keys())
    user_accs = [user_stats[uid]['correct']/user_stats[uid]['total'] 
                 if user_stats[uid]['total'] > 0 else 0 
                 for uid in user_ids]
    
    axes[0, 1].bar(user_ids, user_accs)
    axes[0, 1].axhline(y=np.mean(user_accs), color='red', linestyle='--', 
                      label=f'Mean: {np.mean(user_accs):.3f}')
    axes[0, 1].set_xlabel('User ID')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].set_title('Per-User Accuracy on Real Data')
    axes[0, 1].legend()
    
    # 3. æ··æ·†çŸ©é˜µï¼ˆç®€åŒ–ç‰ˆï¼Œåªæ˜¾ç¤ºå‰10ä¸ªç”¨æˆ·ï¼‰
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(labels[:1000], predictions[:1000])  # å‰1000ä¸ªæ ·æœ¬
    im = axes[1, 0].imshow(cm[:10, :10], cmap='Blues')
    axes[1, 0].set_xlabel('Predicted')
    axes[1, 0].set_ylabel('True')
    axes[1, 0].set_title('Confusion Matrix (First 10 Users)')
    plt.colorbar(im, ax=axes[1, 0])
    
    # 4. å‡†ç¡®ç‡vsç½®ä¿¡åº¦
    conf_bins = np.linspace(0, 1, 11)
    bin_accs = []
    bin_counts = []
    
    for i in range(len(conf_bins)-1):
        mask = (np.array(confidences) >= conf_bins[i]) & (np.array(confidences) < conf_bins[i+1])
        if np.sum(mask) > 0:
            bin_preds = [predictions[j] for j in range(len(predictions)) if mask[j]]
            bin_labels = [labels[j] for j in range(len(labels)) if mask[j]]
            bin_acc = sum([1 for p, l in zip(bin_preds, bin_labels) if p == l]) / len(bin_preds)
            bin_accs.append(bin_acc)
            bin_counts.append(np.sum(mask))
        else:
            bin_accs.append(0)
            bin_counts.append(0)
    
    bin_centers = (conf_bins[:-1] + conf_bins[1:]) / 2
    axes[1, 1].bar(bin_centers, bin_accs, width=0.08, alpha=0.7)
    axes[1, 1].set_xlabel('Confidence Bin')
    axes[1, 1].set_ylabel('Accuracy')
    axes[1, 1].set_title('Accuracy vs Confidence')
    axes[1, 1].set_ylim(0, 1)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'real_data_test_results.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"\nPlots saved to {output_dir / 'real_data_test_results.png'}")


def main():
    parser = argparse.ArgumentParser(description='Test classifier on real data')
    parser.add_argument('--data_dir', type=str, required=True, help='Real data directory (with ID_* folders)')
    parser.add_argument('--classifier_path', type=str, required=True, help='Path to trained classifier')
    parser.add_argument('--output_dir', type=str, default='./real_data_test', help='Output directory')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½åˆ†ç±»å™¨
    print("\nLoading classifier...")
    model, checkpoint = load_classifier(args.classifier_path, device)
    
    # åˆ›å»ºæ•°æ®å˜æ¢ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½çœŸå®æ•°æ®
    print("\nLoading real data...")
    dataset = RealMicroDopplerDataset(args.data_dir, transform=transform)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)
    
    # æµ‹è¯•
    print("\nTesting classifier on real data...")
    predictions, labels, confidences, paths, user_stats = test_on_real_data(model, dataloader, device)
    
    # åˆ†æç»“æœ
    results = analyze_results(predictions, labels, confidences, user_stats)
    
    # è¯„ä¼°å¯é æ€§
    reliability = evaluate_classifier_reliability(results)
    
    # å¯è§†åŒ–
    plot_results(predictions, labels, confidences, user_stats, output_dir)
    
    # ä¿å­˜ç»“æœ
    results_file = output_dir / 'test_results.json'
    with open(results_file, 'w') as f:
        json.dump({
            'overall_accuracy': float(results['overall_accuracy']),
            'avg_confidence': float(results['avg_confidence']),
            'high_conf_ratio': float(results['high_conf_ratio']),
            'high_conf_accuracy': float(results['high_conf_accuracy']),
            'reliability': reliability,
            'total_samples': len(predictions)
        }, f, indent=2)
    
    print(f"\nResults saved to {results_file}")
    
    return results, reliability


if __name__ == "__main__":
    main()

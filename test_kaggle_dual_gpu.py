#!/usr/bin/env python3
"""
KaggleåŒGPUæµ‹è¯•è„šæœ¬
åœ¨å¹²å‡€çš„ç¯å¢ƒä¸­æµ‹è¯•notebook_launcher
å¿…é¡»åœ¨æ–°çš„Pythonè¿›ç¨‹ä¸­è¿è¡Œï¼Œé¿å…CUDAåˆå§‹åŒ–å†²çª
"""

def test_dual_gpu():
    """æµ‹è¯•åŒGPUåŠŸèƒ½"""
    import torch
    from accelerate import Accelerator
    
    accelerator = Accelerator()
    
    print(f"ğŸ”§ è¿›ç¨‹ {accelerator.process_index}/{accelerator.num_processes}")
    print(f"ğŸ”§ è®¾å¤‡: {accelerator.device}")
    print(f"ğŸ”§ åˆ†å¸ƒå¼ç±»å‹: {accelerator.distributed_type}")
    print(f"ğŸ”§ æ··åˆç²¾åº¦: {accelerator.mixed_precision}")
    
    # æµ‹è¯•GPUé€šä¿¡
    if accelerator.is_main_process:
        print("âœ… ä¸»è¿›ç¨‹å¯åŠ¨æˆåŠŸ")
    
    # ç®€å•çš„å¼ é‡æ“ä½œæµ‹è¯•
    x = torch.randn(100, 100).to(accelerator.device)
    y = x @ x.T
    
    print(f"âœ… å¼ é‡è®¡ç®—æˆåŠŸï¼Œè®¾å¤‡: {y.device}")
    print(f"âœ… å¼ é‡å½¢çŠ¶: {y.shape}")
    
    # æµ‹è¯•å†…å­˜ä½¿ç”¨
    memory_used = torch.cuda.memory_allocated(accelerator.device) / 1e6
    print(f"âœ… GPUå†…å­˜ä½¿ç”¨: {memory_used:.1f}MB")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ KaggleåŒGPUæµ‹è¯•")
    print("=" * 50)
    
    # ä½¿ç”¨notebook_launcherå¯åŠ¨åŒGPUæµ‹è¯•
    from accelerate import notebook_launcher
    
    try:
        print("ğŸš€ å¯åŠ¨åŒGPUæµ‹è¯•...")
        notebook_launcher(test_dual_gpu, num_processes=2)
        print("âœ… åŒGPUæµ‹è¯•æˆåŠŸ!")
        return True
    except Exception as e:
        print(f"âŒ åŒGPUæµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ KaggleåŒGPUç¯å¢ƒå·¥ä½œæ­£å¸¸!")
    else:
        print("\nâŒ åŒGPUç¯å¢ƒæœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥é…ç½®")

#!/usr/bin/env python3
"""
VA-VAEå®˜æ–¹å¾®è°ƒè„šæœ¬ - å®Œå…¨åŸºäºåŸé¡¹ç›®æ¡†æ¶
ä½¿ç”¨åŸé¡¹ç›®çš„3é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œå®˜æ–¹é…ç½®
"""

import os
import sys
import yaml
import shutil
from pathlib import Path

def create_stage_configs():
    """åˆ›å»º3é˜¶æ®µè®­ç»ƒé…ç½®æ–‡ä»¶ - åŸºäºåŸé¡¹ç›®å®˜æ–¹ç­–ç•¥"""
    
    # åŸºç¡€é…ç½®æ¨¡æ¿
    base_config = {
        'ckpt_path': '/path/to/ckpt',
        'weight_init': None,  # å°†åœ¨å„é˜¶æ®µä¸­è®¾ç½®
        'model': {
            'base_learning_rate': 1.0e-04,
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'monitor': 'val/rec_loss',
                'embed_dim': 32,
                'use_vf': 'dinov2',
                'reverse_proj': True,
                'lossconfig': {
                    'target': 'ldm.modules.losses.LPIPSWithDiscriminator',
                    'params': {
                        'kl_weight': 1.0e-06,
                        'disc_weight': 0.5,
                        'adaptive_vf': True,
                        # è¿™äº›å‚æ•°å°†åœ¨å„é˜¶æ®µä¸­è®¾ç½®
                        'disc_start': None,
                        'vf_weight': None,
                        'distmat_margin': None,
                        'cos_margin': None,
                    }
                },
                'ddconfig': {
                    'double_z': True,
                    'z_channels': 32,
                    'resolution': 256,
                    'in_channels': 3,
                    'out_ch': 3,
                    'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4],
                    'num_res_blocks': 2,
                    'attn_resolutions': [16],
                    'dropout': 0.0
                }
            }
        },
        'data': {
            'target': 'main.DataModuleFromConfig',
            'params': {
                'batch_size': 4,  # é€‚åˆKaggle GPU
                'wrap': True,
                'train': {
                    'target': 'ldm.data.custom.CustomDataset',  # éœ€è¦å®ç°
                    'params': {
                        'data_root': '/kaggle/input/dataset'
                    }
                },
                'validation': {
                    'target': 'ldm.data.custom.CustomDataset',
                    'params': {
                        'data_root': '/kaggle/input/dataset'
                    }
                }
            }
        },
        'lightning': {
            'trainer': {
                'devices': 1,
                'num_nodes': 1,
                'strategy': 'auto',
                'accelerator': 'gpu',
                'precision': 32,
                'max_epochs': None  # å°†åœ¨å„é˜¶æ®µä¸­è®¾ç½®
            }
        }
    }
    
    # é˜¶æ®µ1é…ç½® (100 epochs - å¯¹é½é˜¶æ®µ)
    stage1_config = base_config.copy()
    stage1_config['weight_init'] = 'models/vavae-imagenet256-f16d32-dinov2.pt'
    stage1_config['model']['params']['lossconfig']['params'].update({
        'disc_start': 5001,
        'vf_weight': 0.5,
        'distmat_margin': 0,
        'cos_margin': 0,
    })
    stage1_config['lightning']['trainer']['max_epochs'] = 50  # é€‚åº”å°æ•°æ®é›†
    
    # é˜¶æ®µ2é…ç½® (15 epochs - é‡å»ºä¼˜åŒ–)
    stage2_config = base_config.copy()
    stage2_config['weight_init'] = 'vavae_finetuned/stage1_final.pt'
    stage2_config['model']['params']['lossconfig']['params'].update({
        'disc_start': 1,
        'vf_weight': 0.1,
        'distmat_margin': 0,
        'cos_margin': 0,
    })
    stage2_config['lightning']['trainer']['max_epochs'] = 15
    
    # é˜¶æ®µ3é…ç½® (15 epochs - è¾¹è·ä¼˜åŒ–)
    stage3_config = base_config.copy()
    stage3_config['weight_init'] = 'vavae_finetuned/stage2_final.pt'
    stage3_config['model']['params']['lossconfig']['params'].update({
        'disc_start': 1,
        'vf_weight': 0.1,
        'distmat_margin': 0.25,
        'cos_margin': 0.5,
    })
    stage3_config['lightning']['trainer']['max_epochs'] = 15
    
    return stage1_config, stage2_config, stage3_config

def save_configs():
    """ä¿å­˜3é˜¶æ®µé…ç½®æ–‡ä»¶"""
    stage1, stage2, stage3 = create_stage_configs()
    
    # åˆ›å»ºé…ç½®ç›®å½•
    config_dir = Path("vavae_finetune_configs")
    config_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    configs = [
        (stage1, "stage1_alignment.yaml"),
        (stage2, "stage2_reconstruction.yaml"), 
        (stage3, "stage3_margin.yaml")
    ]
    
    for config, filename in configs:
        config_path = config_dir / filename
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        print(f"âœ… ä¿å­˜é…ç½®: {config_path}")
    
    return config_dir

def run_official_finetune():
    """è¿è¡Œå®˜æ–¹3é˜¶æ®µå¾®è°ƒ"""
    print("ğŸš€ VA-VAEå®˜æ–¹3é˜¶æ®µå¾®è°ƒ")
    print("="*60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not Path("/kaggle/input/dataset").exists():
        print("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: /kaggle/input/dataset")
        return False
    
    if not Path("models/vavae-imagenet256-f16d32-dinov2.pt").exists():
        print("âŒ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨")
        return False
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    print("ğŸ“ åˆ›å»ºå®˜æ–¹3é˜¶æ®µé…ç½®...")
    config_dir = save_configs()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("vavae_finetuned")
    output_dir.mkdir(exist_ok=True)
    
    print("âš™ï¸ å®˜æ–¹3é˜¶æ®µå¾®è°ƒç­–ç•¥:")
    print("   é˜¶æ®µ1 (50 epochs): å¯¹é½é˜¶æ®µ, vf_weight=0.5, disc_start=5001")
    print("   é˜¶æ®µ2 (15 epochs): é‡å»ºä¼˜åŒ–, vf_weight=0.1, disc_start=1")
    print("   é˜¶æ®µ3 (15 epochs): è¾¹è·ä¼˜åŒ–, margin=0.25/0.5")
    print("   æ€»è®¡: 80 epochs")
    print("   åŸºäº: åŸé¡¹ç›®f16d32_vfdinov2_long.yaml")
    
    print("\nğŸ”§ ä½¿ç”¨åŸé¡¹ç›®è®­ç»ƒæ¡†æ¶:")
    print("   cd LightningDiT/vavae")
    print("   bash run_train.sh ../../vavae_finetune_configs/stage1_alignment.yaml")
    print("   # ç„¶åä¾æ¬¡è¿è¡Œstage2å’Œstage3")
    
    print("\nğŸ’¡ æ³¨æ„äº‹é¡¹:")
    print("   1. éœ€è¦å®ç°CustomDatasetç±»æ¥åŠ è½½å¾®å¤šæ™®å‹’æ•°æ®")
    print("   2. ä½¿ç”¨åŸé¡¹ç›®çš„å®Œæ•´LDMæ¡†æ¶")
    print("   3. åŒ…å«åˆ¤åˆ«å™¨ã€LPIPSæŸå¤±ã€DINOv2å¯¹é½")
    print("   4. è¿™æ˜¯çœŸæ­£çš„VA-VAEå®˜æ–¹å¾®è°ƒæ–¹æ¡ˆ")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ VA-VAEå®˜æ–¹å¾®è°ƒå·¥å…·")
    print("="*50)
    
    print("ğŸ“š åŸºäºåŸé¡¹ç›®çš„å®Œæ•´3é˜¶æ®µè®­ç»ƒç­–ç•¥:")
    print("   - ä½¿ç”¨åŸé¡¹ç›®çš„LDMè®­ç»ƒæ¡†æ¶")
    print("   - å®Œæ•´çš„æŸå¤±å‡½æ•° (LPIPS + åˆ¤åˆ«å™¨ + DINOv2)")
    print("   - å®˜æ–¹çš„3é˜¶æ®µå‚æ•°è®¾ç½®")
    print("   - é¿å…è‡ªå·±å®ç°å¸¦æ¥çš„é”™è¯¯")
    
    success = run_official_finetune()
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

#!/usr/bin/env python3
"""
VA-VAEå®˜æ–¹å¾®è°ƒè„šæœ¬ - å®Œå…¨åŸºäºåŸé¡¹ç›®æ¡†æ¶
ä½¿ç”¨åŸé¡¹ç›®çš„3é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œå®˜æ–¹é…ç½®
"""

import os
import sys
import yaml
import shutil
from pathlib import Path

def create_stage_configs():
    """åˆ›å»º3é˜¶æ®µè®­ç»ƒé…ç½®æ–‡ä»¶ - åŸºäºåŸé¡¹ç›®å®˜æ–¹ç­–ç•¥"""

    # åŸºç¡€é…ç½®æ¨¡æ¿ - å®Œå…¨åŸºäºf16d32_vfdinov2_long.yaml
    base_config = {
        'ckpt_path': '/path/to/ckpt',  # ä»…ç”¨äºæµ‹è¯•
        # weight_init å°†åœ¨å„é˜¶æ®µä¸­è®¾ç½®
        'model': {
            'base_learning_rate': 1.0e-04,
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'monitor': 'val/rec_loss',
                'embed_dim': 32,
                'use_vf': 'dinov2',
                'reverse_proj': True,
                'lossconfig': {
                    'target': 'ldm.modules.losses.LPIPSWithDiscriminator',
                    'params': {
                        'kl_weight': 1.0e-06,
                        'disc_weight': 0.5,
                        'adaptive_vf': True,
                        # è¿™äº›å‚æ•°å°†åœ¨å„é˜¶æ®µä¸­è®¾ç½®
                        'disc_start': None,
                        'vf_weight': None,
                        'distmat_margin': None,
                        'cos_margin': None,
                        # VA-VAEå…³é”®å‚æ•° - ä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´
                        'distmat_weight': 1.0,  # è·ç¦»çŸ©é˜µæŸå¤±æƒé‡
                        'cos_weight': 1.0,      # ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±æƒé‡
                        'vf_loss_type': 'combined_v3',  # åŸé¡¹ç›®çš„æŸå¤±ç±»å‹
                    }
                },
                'ddconfig': {
                    'double_z': True,
                    'z_channels': 32,
                    'resolution': 256,
                    'in_channels': 3,
                    'out_ch': 3,
                    'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4],
                    'num_res_blocks': 2,
                    'attn_resolutions': [16],
                    'dropout': 0.0
                }
            }
        },
        'data': {
            'target': 'main.DataModuleFromConfig',
            'params': {
                'batch_size': 8,  # æ¯GPUæ‰¹æ¬¡å¤§å°ï¼ŒåŒGPUæ€»å…±16 (ä¸åŸé¡¹ç›®ä¸€è‡´)
                'wrap': True,
                'train': {
                    'target': 'ldm.data.microdoppler.MicroDopplerDataset',  # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
                    'params': {
                        'data_root': '/kaggle/input/dataset',
                        'size': 256,
                        'interpolation': 'bicubic',
                        'flip_p': 0.5
                    }
                },
                'validation': {
                    'target': 'ldm.data.microdoppler.MicroDopplerDataset',
                    'params': {
                        'data_root': '/kaggle/input/dataset',
                        'size': 256,
                        'interpolation': 'bicubic',
                        'flip_p': 0.0  # éªŒè¯é›†ä¸ç¿»è½¬
                    }
                }
            }
        },
        'lightning': {
            'trainer': {
                'devices': 2,  # åŒGPUé…ç½®
                'num_nodes': 1,
                'strategy': 'ddp_find_unused_parameters_true',  # ä¸åŸé¡¹ç›®ä¸€è‡´çš„DDPç­–ç•¥
                'accelerator': 'gpu',
                'precision': 32,
                'max_epochs': None  # å°†åœ¨å„é˜¶æ®µä¸­è®¾ç½®
            }
        }
    }
    
    # é˜¶æ®µ1é…ç½® (100 epochs -> 50 epochs - å¯¹é½é˜¶æ®µ)
    import copy
    stage1_config = copy.deepcopy(base_config)
    stage1_config['weight_init'] = 'models/vavae-imagenet256-f16d32-dinov2.pt'
    stage1_config['model']['params']['lossconfig']['params'].update({
        'disc_start': 5001,
        'vf_weight': 0.5,
        'distmat_margin': 0,
        'cos_margin': 0,
    })
    stage1_config['lightning']['trainer']['max_epochs'] = 50  # é€‚åº”å°æ•°æ®é›†

    # é˜¶æ®µ2é…ç½® (15 epochs - é‡å»ºä¼˜åŒ–)
    stage2_config = copy.deepcopy(base_config)
    stage2_config['weight_init'] = 'vavae_finetuned/stage1_final.ckpt'  # ä½¿ç”¨.ckptæ‰©å±•å
    stage2_config['model']['params']['lossconfig']['params'].update({
        'disc_start': 1,
        'vf_weight': 0.1,
        'distmat_margin': 0,
        'cos_margin': 0,
    })
    stage2_config['lightning']['trainer']['max_epochs'] = 15

    # é˜¶æ®µ3é…ç½® (15 epochs - è¾¹è·ä¼˜åŒ–)
    stage3_config = copy.deepcopy(base_config)
    stage3_config['weight_init'] = 'vavae_finetuned/stage2_final.ckpt'  # ä½¿ç”¨.ckptæ‰©å±•å
    stage3_config['model']['params']['lossconfig']['params'].update({
        'disc_start': 1,
        'vf_weight': 0.1,
        'distmat_margin': 0.25,
        'cos_margin': 0.5,
    })
    stage3_config['lightning']['trainer']['max_epochs'] = 15
    
    return stage1_config, stage2_config, stage3_config

def save_configs():
    """ä¿å­˜3é˜¶æ®µé…ç½®æ–‡ä»¶"""
    stage1, stage2, stage3 = create_stage_configs()
    
    # åˆ›å»ºé…ç½®ç›®å½•
    config_dir = Path("vavae_finetune_configs")
    config_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    configs = [
        (stage1, "stage1_alignment.yaml"),
        (stage2, "stage2_reconstruction.yaml"), 
        (stage3, "stage3_margin.yaml")
    ]
    
    for config, filename in configs:
        config_path = config_dir / filename
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        print(f"âœ… ä¿å­˜é…ç½®: {config_path}")
    
    return config_dir

def create_custom_dataset():
    """åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ç±»æ–‡ä»¶"""
    dataset_code = '''import os
import torch
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms

class MicroDopplerDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åƒæ•°æ®é›†"""

    def __init__(self, data_root, size=256, interpolation="bicubic", flip_p=0.5):
        self.data_root = data_root
        self.size = size
        self.interpolation = {"linear": Image.LINEAR,
                            "bilinear": Image.BILINEAR,
                            "bicubic": Image.BICUBIC,
                            "lanczos": Image.LANCZOS,}[interpolation]
        self.flip = transforms.RandomHorizontalFlip(p=flip_p)

        # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶
        self.image_paths = []
        for root, dirs, files in os.walk(data_root):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    self.image_paths.append(os.path.join(root, file))

        print(f"Found {len(self.image_paths)} images in {data_root}")

        # æ•°æ®é¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize(size, interpolation=self.interpolation),
            transforms.CenterCrop(size),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # å½’ä¸€åŒ–åˆ°[-1,1]
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert('RGB')
        image = self.transform(image)
        image = self.flip(image)
        return {"image": image}
'''

    # ä¿å­˜åˆ°LightningDiT/vavae/ldm/data/ç›®å½•
    dataset_path = Path("LightningDiT/vavae/ldm/data/microdoppler.py")
    with open(dataset_path, 'w', encoding='utf-8') as f:
        f.write(dataset_code)

    print(f"âœ… åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†: {dataset_path}")
    return dataset_path

def run_official_finetune():
    """è¿è¡Œå®˜æ–¹3é˜¶æ®µå¾®è°ƒ"""
    print("ğŸš€ VA-VAEå®˜æ–¹3é˜¶æ®µå¾®è°ƒ")
    print("="*60)

    # æ£€æŸ¥ç¯å¢ƒ (ä½†ä¸é˜»æ­¢é…ç½®æ–‡ä»¶ç”Ÿæˆ)
    data_dir = Path("/kaggle/input/dataset")
    model_path = Path("models/vavae-imagenet256-f16d32-dinov2.pt")

    env_issues = []
    if not data_dir.exists():
        env_issues.append("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: /kaggle/input/dataset")

    if not model_path.exists():
        env_issues.append("âŒ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨")

    if env_issues:
        print("âš ï¸ ç¯å¢ƒæ£€æŸ¥å‘ç°é—®é¢˜:")
        for issue in env_issues:
            print(f"   {issue}")
        print("ğŸ’¡ ç»§ç»­ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼Œè¯·åœ¨è®­ç»ƒå‰è§£å†³è¿™äº›é—®é¢˜")

    # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†
    print("ğŸ“ åˆ›å»ºå¾®å¤šæ™®å‹’æ•°æ®é›†ç±»...")
    create_custom_dataset()

    # åˆ›å»ºé…ç½®æ–‡ä»¶
    print("ğŸ“ åˆ›å»ºå®˜æ–¹3é˜¶æ®µé…ç½®...")
    config_dir = save_configs()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("vavae_finetuned")
    output_dir.mkdir(exist_ok=True)

    print("âš™ï¸ å®˜æ–¹3é˜¶æ®µå¾®è°ƒç­–ç•¥ (ä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´):")
    print("   é˜¶æ®µ1 (50 epochs): DINOv2å¯¹é½, vf_weight=0.5, disc_start=5001, margin=0")
    print("   é˜¶æ®µ2 (15 epochs): é‡å»ºä¼˜åŒ–, vf_weight=0.1, disc_start=1, margin=0")
    print("   é˜¶æ®µ3 (15 epochs): è¾¹è·ä¼˜åŒ–, vf_weight=0.1, margin=0.25/0.5")
    print("   æ€»è®¡: 80 epochs (åŸé¡¹ç›®130epochsçš„ä¼˜åŒ–ç‰ˆ)")
    print("   åŸºäº: åŸé¡¹ç›®f16d32_vfdinov2_long.yaml + LPIPSWithDiscriminator")
    print("   ğŸ”§ åŒGPUé…ç½®: devices=2, batch_size=8x2=16, ddp_find_unused_parameters_true")

    print(f"\nğŸ“ é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    print("\nğŸ”§ æ‰‹åŠ¨è¿è¡Œè®­ç»ƒå‘½ä»¤:")
    print("   cd LightningDiT/vavae")
    print("   export CONFIG_PATH=../../vavae_finetune_configs/stage1_alignment.yaml")
    print("   python main.py --base $CONFIG_PATH --train")
    print("   # é˜¶æ®µ1å®Œæˆåï¼Œè¿è¡Œé˜¶æ®µ2å’Œé˜¶æ®µ3")

    print("\nğŸ’¡ é‡è¦è¯´æ˜:")
    print("   1. âœ… 3é˜¶æ®µç­–ç•¥ä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´")
    print("   2. âœ… ä½¿ç”¨LPIPSWithDiscriminatoræŸå¤±å‡½æ•°")
    print("   3. âœ… åŒ…å«DINOv2è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½")
    print("   4. âœ… è‡ªé€‚åº”æƒé‡æœºåˆ¶ (adaptive_vf=True)")
    print("   5. âœ… å®Œæ•´çš„VAEæ¶æ„ (f16d32)")
    print("   6. ğŸ”§ å·²åˆ›å»ºMicroDopplerDatasetå¤„ç†æ—¶é¢‘å›¾åƒ")
    print("   7. ğŸ“ éœ€è¦æ‰‹åŠ¨ä¾æ¬¡è¿è¡Œ3ä¸ªé˜¶æ®µæˆ–ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬")

    return True

def create_training_script():
    """åˆ›å»ºè‡ªåŠ¨åŒ–è®­ç»ƒè„šæœ¬"""
    script_content = '''#!/bin/bash
# VA-VAE 3é˜¶æ®µè‡ªåŠ¨åŒ–è®­ç»ƒè„šæœ¬

echo "ğŸš€ å¼€å§‹VA-VAE 3é˜¶æ®µå¾®è°ƒ"
echo "================================"

# è®¾ç½®ç¯å¢ƒå˜é‡ - åŒGPUé…ç½®
export CUDA_VISIBLE_DEVICES=0,1
export PYTHONPATH="${PYTHONPATH}:$(pwd)/LightningDiT/vavae"

cd LightningDiT/vavae

# é˜¶æ®µ1: å¯¹é½è®­ç»ƒ (50 epochs)
echo "ğŸ“ é˜¶æ®µ1: DINOv2å¯¹é½è®­ç»ƒ (50 epochs)"
echo "   vf_weight=0.5, disc_start=5001, margin=0"
export CONFIG_PATH=../../vavae_finetune_configs/stage1_alignment.yaml
python main.py --base $CONFIG_PATH --train

if [ $? -ne 0 ]; then
    echo "âŒ é˜¶æ®µ1è®­ç»ƒå¤±è´¥"
    exit 1
fi

echo "âœ… é˜¶æ®µ1å®Œæˆ"

# é˜¶æ®µ2: é‡å»ºä¼˜åŒ– (15 epochs)
echo "ğŸ“ é˜¶æ®µ2: é‡å»ºä¼˜åŒ–è®­ç»ƒ (15 epochs)"
echo "   vf_weight=0.1, disc_start=1, margin=0"
export CONFIG_PATH=../../vavae_finetune_configs/stage2_reconstruction.yaml
python main.py --base $CONFIG_PATH --train

if [ $? -ne 0 ]; then
    echo "âŒ é˜¶æ®µ2è®­ç»ƒå¤±è´¥"
    exit 1
fi

echo "âœ… é˜¶æ®µ2å®Œæˆ"

# é˜¶æ®µ3: è¾¹è·ä¼˜åŒ– (15 epochs)
echo "ğŸ“ é˜¶æ®µ3: è¾¹è·ä¼˜åŒ–è®­ç»ƒ (15 epochs)"
echo "   vf_weight=0.1, disc_start=1, margin=0.25/0.5"
export CONFIG_PATH=../../vavae_finetune_configs/stage3_margin.yaml
python main.py --base $CONFIG_PATH --train

if [ $? -ne 0 ]; then
    echo "âŒ é˜¶æ®µ3è®­ç»ƒå¤±è´¥"
    exit 1
fi

echo "âœ… é˜¶æ®µ3å®Œæˆ"
echo "ğŸ‰ VA-VAE 3é˜¶æ®µå¾®è°ƒå…¨éƒ¨å®Œæˆï¼"
echo "ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: ../../vavae_finetuned/"
'''

    script_path = Path("run_vavae_3stage_training.sh")
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)

    # è®¾ç½®æ‰§è¡Œæƒé™
    import stat
    script_path.chmod(script_path.stat().st_mode | stat.S_IEXEC)

    print(f"âœ… åˆ›å»ºè®­ç»ƒè„šæœ¬: {script_path}")
    return script_path

def validate_config_consistency():
    """éªŒè¯é…ç½®ä¸åŸé¡¹ç›®çš„ä¸€è‡´æ€§"""
    print("ğŸ” éªŒè¯é…ç½®ä¸€è‡´æ€§...")

    # æ£€æŸ¥å…³é”®é…ç½®å‚æ•°
    stage1, stage2, stage3 = create_stage_configs()

    # éªŒè¯é˜¶æ®µ1é…ç½®
    stage1_loss = stage1['model']['params']['lossconfig']['params']
    assert stage1_loss['vf_weight'] == 0.5, "é˜¶æ®µ1 vf_weightåº”ä¸º0.5"
    assert stage1_loss['disc_start'] == 5001, "é˜¶æ®µ1 disc_startåº”ä¸º5001"
    assert stage1_loss['distmat_margin'] == 0, "é˜¶æ®µ1 distmat_marginåº”ä¸º0"
    assert stage1_loss['cos_margin'] == 0, "é˜¶æ®µ1 cos_marginåº”ä¸º0"

    # éªŒè¯é˜¶æ®µ2é…ç½®
    stage2_loss = stage2['model']['params']['lossconfig']['params']
    assert stage2_loss['vf_weight'] == 0.1, "é˜¶æ®µ2 vf_weightåº”ä¸º0.1"
    assert stage2_loss['disc_start'] == 1, "é˜¶æ®µ2 disc_startåº”ä¸º1"

    # éªŒè¯é˜¶æ®µ3é…ç½®
    stage3_loss = stage3['model']['params']['lossconfig']['params']
    assert stage3_loss['distmat_margin'] == 0.25, "é˜¶æ®µ3 distmat_marginåº”ä¸º0.25"
    assert stage3_loss['cos_margin'] == 0.5, "é˜¶æ®µ3 cos_marginåº”ä¸º0.5"

    print("âœ… é…ç½®éªŒè¯é€šè¿‡ - ä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´")
    return True

def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ– - Kaggleä¼˜åŒ–ç‰ˆ"""
    import subprocess
    import sys
    
    print("ğŸ”§ æ£€æŸ¥å’Œå®‰è£…ä¾èµ–...")
    
    # åœ¨Kaggleä¸­ï¼Œå¿…é¡»åœ¨ä¸»è¿›ç¨‹ä¸­å®‰è£…å¹¶é‡å¯
    taming_installed = False
    try:
        import taming
        print("âœ… taming-transformers å·²å®‰è£…")
        taming_installed = True
    except ImportError:
        print("ğŸ”§ åœ¨ä¸»è¿›ç¨‹ä¸­å®‰è£… taming-transformers...")
        try:
            # åœ¨Kaggleä¸­å¼ºåˆ¶å®‰è£…
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                "git+https://github.com/CompVis/taming-transformers.git",
                "--force-reinstall", "--no-cache-dir"
            ])
            print("âœ… taming-transformers å®‰è£…æˆåŠŸ")
            
            # åœ¨Kaggleä¸­ï¼Œéœ€è¦é‡æ–°å¯¼å…¥sys.path
            import importlib
            import site
            importlib.reload(site)
            
            # å†æ¬¡å°è¯•å¯¼å…¥
            try:
                import taming
                print("âœ… taming-transformers éªŒè¯æˆåŠŸ")
                taming_installed = True
            except ImportError:
                print("âš ï¸ taming-transformers å®‰è£…åä»æ— æ³•å¯¼å…¥")
                print("ğŸ’¡ è¿™æ˜¯Kaggleç¯å¢ƒçš„å¸¸è§é—®é¢˜ï¼Œå°†åœ¨è®­ç»ƒä¸­å¤„ç†")
                
        except Exception as e:
            print(f"âŒ taming-transformers å®‰è£…å¤±è´¥: {str(e)}")
            print("ğŸ’¡ è¯·æ‰‹åŠ¨åœ¨Kaggleä¸­æ‰§è¡Œ:")
            print("   !pip install git+https://github.com/CompVis/taming-transformers.git")
            print("   ç„¶åé‡å¯å†…æ ¸")
            return False
    
    # æ£€æŸ¥å…¶ä»–ä¾èµ–
    dependencies = {
        'pytorch_lightning': 'pytorch-lightning',
        'omegaconf': 'omegaconf',
        'einops': 'einops',
        'transformers': 'transformers'
    }
    
    missing_deps = []
    for module, package in dependencies.items():
        try:
            __import__(module)
            print(f"âœ… {module} å·²å®‰è£…")
        except ImportError:
            missing_deps.append(package)
    
    if missing_deps:
        print(f"ğŸ”§ å®‰è£…ç¼ºå°‘çš„ä¾èµ–: {', '.join(missing_deps)}")
        try:
            subprocess.check_call([
                sys.executable, "-m", "pip", "install"
            ] + missing_deps)
            print("âœ… ä¾èµ–å®‰è£…æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ä¾èµ–å®‰è£…å¤±è´¥: {str(e)}")
            return False
    
    return True

def fix_taming_compatibility():
    """ä¿®å¤ taming-transformers å…¼å®¹æ€§é—®é¢˜"""
    import os
    from pathlib import Path
    
    print("ğŸ”§ ä¿®å¤ taming-transformers å…¼å®¹æ€§...")
    
    # æŸ¥æ‰¾ taming å®‰è£…è·¯å¾„
    try:
        import taming
        taming_path = Path(taming.__file__).parent
        utils_file = taming_path / "data" / "utils.py"
        
        if utils_file.exists():
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(utils_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä¿®å¤ torch._six å¯¼å…¥é—®é¢˜
            if "from torch._six import string_classes" in content:
                content = content.replace(
                    "from torch._six import string_classes",
                    "from six import string_types as string_classes"
                )
                
                with open(utils_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print("âœ… ä¿®å¤ taming-transformers å…¼å®¹æ€§æˆåŠŸ")
            else:
                print("âœ… taming-transformers å·²æ˜¯å…¼å®¹ç‰ˆæœ¬")
        else:
            print("âš ï¸ æœªæ‰¾åˆ° taming utils.py æ–‡ä»¶")
            
    except Exception as e:
        print(f"âš ï¸ taming å…¼å®¹æ€§ä¿®å¤å¤±è´¥: {str(e)}")
        print("ğŸ’¡ å¯èƒ½éœ€è¦æ‰‹åŠ¨ä¿®å¤ï¼Œä½†ä¸å½±å“è®­ç»ƒ")

def auto_execute_training():
    """è‡ªåŠ¨æ‰§è¡Œ3é˜¶æ®µè®­ç»ƒ - ä¸€é”®è¿è¡Œ"""
    import subprocess
    import time
    
    print("\nğŸ¤– å¼€å§‹è‡ªåŠ¨æ‰§è¡Œ3é˜¶æ®µè®­ç»ƒ...")
    print("="*60)
    
    # å®‰è£…ä¾èµ–
    if not install_dependencies():
        return False
    
    # ä¿®å¤å…¼å®¹æ€§
    fix_taming_compatibility()
    
    # æ£€æŸ¥Pythonç¯å¢ƒå’Œä¾èµ–
    try:
        import pytorch_lightning as pl
        print(f"âœ… PyTorch Lightning: {pl.__version__}")
    except ImportError:
        print("âŒ ç¼ºå°‘ pytorch_lightningï¼Œè¯·å…ˆå®‰è£…")
        return False
    
    # åˆ‡æ¢åˆ°è®­ç»ƒç›®å½•
    vavae_dir = Path("LightningDiT/vavae")
    if not vavae_dir.exists():
        print(f"âŒ è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {vavae_dir}")
        return False
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ - ä¿®å¤Kaggleå­è¿›ç¨‹åŒ…æ‰¾ä¸åˆ°é—®é¢˜
    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = '0,1'
    
    # è·å–å½“å‰ Python çš„ site-packages è·¯å¾„
    import site
    import sys
    
    # æ„å»ºå®Œæ•´çš„ PYTHONPATH
    python_paths = [
        os.path.abspath('LightningDiT/vavae'),  # VA-VAE è·¯å¾„
    ]
    
    # æ·»åŠ å½“å‰ Python çš„æ‰€æœ‰è·¯å¾„
    python_paths.extend(sys.path)
    
    # æ·»åŠ  site-packages è·¯å¾„
    python_paths.extend(site.getsitepackages())
    
    # æ·»åŠ ç”¨æˆ·ç«™ç‚¹åŒ…è·¯å¾„
    if hasattr(site, 'getusersitepackages'):
        python_paths.append(site.getusersitepackages())
    
    # è®¾ç½® PYTHONPATH
    env['PYTHONPATH'] = ':'.join(filter(None, python_paths))
    
    print(f"ğŸ”§ è®¾ç½® PYTHONPATH: {len(python_paths)} ä¸ªè·¯å¾„")
    
    # éªŒè¯ taming æ˜¯å¦å¯ç”¨
    try:
        import taming
        taming_path = os.path.dirname(taming.__file__)
        print(f"âœ… taming è·¯å¾„: {taming_path}")
        # ç¡®ä¿ tamingè·¯å¾„åœ¨PYTHONPATHä¸­
        if taming_path not in env['PYTHONPATH']:
            env['PYTHONPATH'] = f"{taming_path}:{env['PYTHONPATH']}"
    except ImportError:
        print("âš ï¸ taming ä»ç„¶ä¸å¯ç”¨ï¼Œå°†åœ¨å­è¿›ç¨‹ä¸­é‡è¯•")
    
    stages = [
        ("stage1_alignment.yaml", "é˜¶æ®µ1: DINOv2å¯¹é½", "50 epochs, vf_weight=0.5"),
        ("stage2_reconstruction.yaml", "é˜¶æ®µ2: é‡å»ºä¼˜åŒ–", "15 epochs, vf_weight=0.1"),
        ("stage3_margin.yaml", "é˜¶æ®µ3: è¾¹è·ä¼˜åŒ–", "15 epochs, margin=0.25/0.5")
    ]
    
    total_start_time = time.time()
    
    for i, (config_file, stage_name, description) in enumerate(stages, 1):
        print(f"\nğŸ“ {stage_name} ({description})")
        print("-" * 50)
        
        config_path = Path(f"vavae_finetune_configs/{config_file}")
        if not config_path.exists():
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
        
        # åœ¨å­è¿›ç¨‹ä¸­éªŒè¯å’Œå®‰è£…ä¾èµ– - å¢å¼ºç‰ˆæœ¬
        pre_script = """
import sys
import subprocess
import os
import time

print("[INFO] Starting stage training subprocess...")

# Force install taming-transformers
print("[INFO] Force installing taming-transformers...")
try:
    subprocess.check_call([
        sys.executable, "-m", "pip", "install", 
        "git+https://github.com/CompVis/taming-transformers.git",
        "--force-reinstall", "--no-cache-dir", "--quiet"
    ])
    print("[OK] taming-transformers force installation completed")
except Exception as e:
    print(f"[ERROR] Failed to install taming-transformers: {{e}}")
    sys.exit(1)

# Verify installation
try:
    import taming
    print(f"[OK] taming-transformers verified: {{taming.__file__}}")
except ImportError as e:
    print(f"[ERROR] taming-transformers still not available: {{e}}")
    sys.exit(1)

# Execute main training script with proper error handling
print("[INFO] Starting main training script...")
start_time = time.time()

result = os.system('python main.py --base ../../{} --train')

end_time = time.time()
print(f"[INFO] Training completed in {{end_time - start_time:.1f}} seconds")
print(f"[INFO] Exit code: {{result}}")

if result != 0:
    print("[ERROR] Training failed with non-zero exit code")
    sys.exit(1)
else:
    print("[OK] Training completed successfully")
""".format(config_path)
        
        # åˆ›å»ºä¸´æ—¶è„šæœ¬æ–‡ä»¶
        temp_script = vavae_dir / f"temp_stage{i}_train.py"
        with open(temp_script, 'w', encoding='utf-8') as f:
            f.write(pre_script)  # pre_scriptå·²ç»æ ¼å¼åŒ–è¿‡äº†
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤ - ä½¿ç”¨ä¸´æ—¶è„šæœ¬
        cmd = [sys.executable, str(temp_script.name)]
        
        print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"ğŸ“ å·¥ä½œç›®å½•: {vavae_dir.absolute()}")
        
        stage_start_time = time.time()
        
        try:
            # æ‰§è¡Œè®­ç»ƒ
            process = subprocess.Popen(
                cmd,
                cwd=vavae_dir,
                env=env,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            # å®æ—¶æ˜¾ç¤ºè¾“å‡ºå¹¶æ£€æµ‹è®­ç»ƒè¿›åº¦
            epoch_count = 0
            error_messages = []
            training_started = False
            
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    output_lower = output.lower()
                    # æ£€æµ‹è®­ç»ƒå¼€å§‹
                    if 'epoch' in output_lower and ('/' in output or 'training' in output_lower):
                        training_started = True
                        epoch_count += 1
                        print(f"  ğŸ“Š {output.strip()}")
                    # æ£€æµ‹é”™è¯¯ä¿¡æ¯
                    elif any(keyword in output_lower for keyword in 
                          ['error', 'exception', 'traceback', 'failed', 'modulenotfounderror']):
                        error_messages.append(output.strip())
                        print(f"  âŒ {output.strip()}")
                    # æ˜¾ç¤ºå…¶ä»–é‡è¦ä¿¡æ¯
                    elif any(keyword in output_lower for keyword in 
                          ['loss', 'val/', 'finished', 'completed', 'saving']):
                        print(f"  ğŸ“Š {output.strip()}")
            
            return_code = process.poll()
            stage_time = time.time() - stage_start_time
            
            # æ£€æŸ¥è®­ç»ƒæ˜¯å¦çœŸæ­£å®Œæˆ
            if return_code != 0 or not training_started or stage_time < 60:  # å°‘äº1åˆ†é’Ÿå¯èƒ½æ˜¯å¤±è´¥
                print(f"âŒ {stage_name}è®­ç»ƒå¤±è´¥:")
                print(f"   - è¿”å›ç : {return_code}")
                print(f"   - è®­ç»ƒå¼€å§‹: {'Yes' if training_started else 'No'}")
                print(f"   - ç”¨æ—¶: {stage_time:.1f}ç§’")
                print(f"   - Epochæ•°: {epoch_count}")
                if error_messages:
                    print(f"   - é”™è¯¯ä¿¡æ¯: {error_messages[-1]}")
                return False
            
            print(f"âœ… {stage_name}å®Œæˆ (ç”¨æ—¶: {stage_time/60:.1f}åˆ†é’Ÿ, {epoch_count} epochs)")
            
        except Exception as e:
            print(f"âŒ {stage_name}æ‰§è¡Œå‡ºé”™: {str(e)}")
            return False
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if temp_script.exists():
                    temp_script.unlink()
            except:
                pass
    
    total_time = time.time() - total_start_time
    print(f"\nğŸ‰ VA-VAE 3é˜¶æ®µå¾®è°ƒå…¨éƒ¨å®Œæˆï¼")
    print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time/3600:.1f}å°æ—¶")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: vavae_finetuned/")
    
    return True

def main():
    """ä¸»å‡½æ•° - Kaggleä¸€é”®è®­ç»ƒ"""
    print("ğŸ¯ VA-VAEå®˜æ–¹å¾®è°ƒå·¥å…· - Kaggleç‰ˆ")
    print("="*50)

    print("ğŸ“š åŸºäºåŸé¡¹ç›®çš„å®Œæ•´3é˜¶æ®µè®­ç»ƒç­–ç•¥:")
    print("   - ä½¿ç”¨åŸé¡¹ç›®çš„LDMè®­ç»ƒæ¡†æ¶")
    print("   - å®Œæ•´çš„æŸå¤±å‡½æ•° (LPIPS + åˆ¤åˆ«å™¨ + DINOv2)")
    print("   - å®˜æ–¹çš„3é˜¶æ®µå‚æ•°è®¾ç½®")
    print("   - ğŸ¤– Kaggleè‡ªåŠ¨åŒ–æ‰§è¡Œ3ä¸ªé˜¶æ®µ")

    # éªŒè¯é…ç½®ä¸€è‡´æ€§
    validate_config_consistency()

    # å‡†å¤‡é…ç½®æ–‡ä»¶
    success = run_official_finetune()
    if not success:
        print("âŒ é…ç½®å‡†å¤‡å¤±è´¥")
        return False

    print("\nğŸ”§ åˆ›å»ºè‡ªåŠ¨åŒ–è®­ç»ƒè„šæœ¬...")
    script_path = create_training_script()
    
    # Kaggleç¯å¢ƒç›´æ¥æ‰§è¡Œä¸€é”®è®­ç»ƒ
    print("\nğŸ¤– Kaggleç¯å¢ƒæ£€æµ‹ - ç›´æ¥å¯åŠ¨æ™ºèƒ½ä¸€é”®è®­ç»ƒ...")
    print("â±ï¸ é¢„è®¡è®­ç»ƒæ—¶é—´: 6-10å°æ—¶")
    print("ğŸ“Š é¢„æœŸæ”¹å–„: FIDä»16é™åˆ°1-3")
    print("ğŸ“ å¤‡ç”¨æ‰‹åŠ¨è„šæœ¬: {}".format(script_path))
    
    success = auto_execute_training()
    if success:
        print("\nğŸŠ VA-VAEå¾®è°ƒå®Œæˆï¼")
        print("ğŸ“Š å¯ä»¥ä½¿ç”¨evaluate_finetuned_vae.pyè¯„ä¼°æ•ˆæœ")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: vavae_finetuned/")
    else:
        print("\nâŒ è‡ªåŠ¨è®­ç»ƒå¤±è´¥")
        print(f"ğŸ“ å¯å°è¯•æ‰‹åŠ¨æ‰§è¡Œ: bash {script_path}")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

#!/usr/bin/env python3
"""
æ‰‹åŠ¨ç”Ÿæˆæ¡ä»¶DiTæ¨¡å‹çš„å¯è§†åŒ–å›¾åƒ
ç”¨äºä»è®­ç»ƒå¥½çš„æ£€æŸ¥ç‚¹ç”Ÿæˆç”¨æˆ·æ¡ä»¶æ ·æœ¬
"""

import torch
import torch.nn.functional as F
import yaml
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import argparse
from datetime import datetime

# å¯¼å…¥å¿…è¦çš„æ¨¡å‹ç±»
import sys
sys.path.append('LightningDiT')

from models.lightningdit import LightningDiT_models
from tokenizer.vavae import VA_VAE


class UserConditionEncoder(torch.nn.Module):
    """ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨"""
    def __init__(self, num_users, embed_dim=1152):
        super().__init__()
        self.user_embedding = torch.nn.Embedding(num_users, embed_dim)
        self.projection = torch.nn.Linear(embed_dim, embed_dim)
        
    def forward(self, user_ids):
        embeddings = self.user_embedding(user_ids)
        return self.projection(embeddings)


class ConditionalDiT(torch.nn.Module):
    """æ¡ä»¶DiTæ¨¡å‹"""
    def __init__(self, 
                 model_name="LightningDiT-XL/1",
                 num_users=31,
                 condition_dim=1152,
                 pretrained_path=None):
        super().__init__()
        
        # è·å–DiTæ¨¡å‹
        self.dit = LightningDiT_models[model_name](
            input_size=16,
            patch_size=1,
            in_channels=32,
            num_classes=num_users,
            use_qknorm=False,
            use_swiglu=False,
            use_rope=False,
            use_rmsnorm=False,
            wo_shift=False
        )
        
        # ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨
        self.user_encoder = UserConditionEncoder(num_users, condition_dim)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡(å¦‚æœæä¾›)
        if pretrained_path and Path(pretrained_path).exists():
            print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
            checkpoint = torch.load(pretrained_path, map_location='cpu')
            if 'model' in checkpoint:
                # åªåŠ è½½DiTç›¸å…³çš„æƒé‡ï¼Œå¿½ç•¥æ–°å¢çš„æ¡ä»¶ç¼–ç å™¨
                dit_state_dict = {}
                for k, v in checkpoint['model'].items():
                    if k.startswith('dit.'):
                        dit_state_dict[k[4:]] = v  # å»æ‰'dit.'å‰ç¼€
                self.dit.load_state_dict(dit_state_dict, strict=False)
                print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½å®Œæˆ")
        
        self.num_users = num_users
    
    def forward(self, x, t, user_ids):
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç ç”¨æˆ·æ¡ä»¶
        user_condition = self.user_encoder(user_ids)
        
        # DiTå‰å‘ä¼ æ’­
        return self.dit(x, t, y=user_condition)


def load_trained_model(checkpoint_path, config_path, device='cuda'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    # åŠ è½½é…ç½®
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: epoch {checkpoint['epoch']}")
    
    # åˆ›å»ºæ¨¡å‹
    model_config = config.get('model', {}).get('params', {})
    model = ConditionalDiT(
        model_name=model_config.get('model', "LightningDiT-XL/1"),
        num_users=model_config.get('num_users', 31),
        condition_dim=model_config.get('condition_dim', 1152)
    )
    
    # åŠ è½½è®­ç»ƒçš„æƒé‡
    if 'model_state_dict' in checkpoint:
        # å¤„ç†DataParallelä¿å­˜çš„æƒé‡
        state_dict = checkpoint['model_state_dict']
        if any(k.startswith('module.') for k in state_dict.keys()):
            # ç§»é™¤DataParallelçš„'module.'å‰ç¼€
            state_dict = {k[7:]: v for k, v in state_dict.items() if k.startswith('module.')}
        
        model.load_state_dict(state_dict, strict=False)
        print("âœ… è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
    
    model.to(device)
    model.eval()
    return model, config


def load_vae_model(device='cuda'):
    """åŠ è½½VA-VAEæ¨¡å‹"""
    vae_config_path = "LightningDiT/tokenizer/configs/vae_config.yaml"
    
    if not Path(vae_config_path).exists():
        raise FileNotFoundError(f"VA-VAEé…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {vae_config_path}")
    
    vae = VA_VAE(vae_config_path)
    vae.to(device)
    vae.eval()
    print("âœ… VA-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
    return vae


def generate_samples(model, vae, device='cuda', num_samples_per_user=4, selected_users=None):
    """ç”Ÿæˆç”¨æˆ·æ¡ä»¶æ ·æœ¬"""
    model.eval()
    vae.eval()
    
    # å¦‚æœæœªæŒ‡å®šç”¨æˆ·ï¼Œåˆ™é€‰æ‹©ä»£è¡¨æ€§ç”¨æˆ·
    if selected_users is None:
        # é€‰æ‹©8ä¸ªä»£è¡¨æ€§ç”¨æˆ· (0-basedç´¢å¼•ï¼Œå¯¹åº”ID_1åˆ°ID_31)
        selected_users = [0, 4, 9, 14, 19, 24, 29, 30]
    
    num_users_to_sample = len(selected_users)
    total_samples = num_users_to_sample * num_samples_per_user
    
    # å‡†å¤‡ç”¨æˆ·æ ‡ç­¾
    user_ids = []
    for user_idx in selected_users:
        user_ids.extend([user_idx] * num_samples_per_user)
    user_ids = torch.tensor(user_ids, device=device)
    
    print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆæ ·æœ¬...")
    print(f"   - ç”¨æˆ·æ•°: {num_users_to_sample}")
    print(f"   - æ¯ç”¨æˆ·æ ·æœ¬æ•°: {num_samples_per_user}")
    print(f"   - æ€»æ ·æœ¬æ•°: {total_samples}")
    
    with torch.no_grad():
        # ç”Ÿæˆéšæœºå™ªå£° (VA-VAEæ½œå‘é‡å½¢çŠ¶: B x 32 x 16 x 16)
        z_shape = (total_samples, 32, 16, 16)
        z = torch.randn(z_shape, device=device)
        
        # åˆ›å»ºæ—¶é—´æ­¥ (ä½¿ç”¨éšæœºæ—¶é—´æ­¥è¿›è¡Œå¤šæ ·æ€§)
        t = torch.randint(0, 1000, (total_samples,), device=device)
        
        # æ¡ä»¶ç”Ÿæˆ
        print("ğŸ”„ DiTç”Ÿæˆä¸­...")
        generated_z = model(z, t, user_ids)
        
        # è§£ç ä¸ºå›¾åƒ
        print("ğŸ”„ VA-VAEè§£ç ä¸­...")
        generated_images = vae.decode_to_images(generated_z)
        
        print("âœ… ç”Ÿæˆå®Œæˆ!")
        
        return generated_images, selected_users, num_samples_per_user


def visualize_and_save(images, selected_users, num_samples_per_user, output_path=None):
    """å¯è§†åŒ–å¹¶ä¿å­˜ç»“æœ"""
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"manual_visualization_{timestamp}.png"
    
    num_users = len(selected_users)
    
    # åˆ›å»ºå›¾åƒç½‘æ ¼
    fig, axes = plt.subplots(num_users, num_samples_per_user, 
                            figsize=(num_samples_per_user * 3, num_users * 3))
    
    if num_users == 1:
        axes = axes.reshape(1, -1)
    if num_samples_per_user == 1:
        axes = axes.reshape(-1, 1)
    
    fig.suptitle('User-Conditional Micro-Doppler Generation', fontsize=16, fontweight='bold')
    
    for user_idx in range(num_users):
        for sample_idx in range(num_samples_per_user):
            img_idx = user_idx * num_samples_per_user + sample_idx
            img = images[img_idx]
            
            # å¤„ç†å›¾åƒæ ¼å¼
            if isinstance(img, torch.Tensor):
                img = img.detach().cpu().numpy()
            
            if img.ndim == 3 and img.shape[0] in [1, 3]:  # (C, H, W) -> (H, W, C)
                img = img.transpose(1, 2, 0)
            
            if img.shape[-1] == 1:  # ç°åº¦å›¾
                img = img.squeeze(-1)
            
            # æ˜¾ç¤ºå›¾åƒ
            axes[user_idx, sample_idx].imshow(img, cmap='viridis')
            
            # è®¾ç½®æ ‡é¢˜ (æ˜¾ç¤ºå®é™…ç”¨æˆ·ID)
            actual_user_id = selected_users[user_idx] + 1  # 0-based -> 1-based
            if sample_idx == 0:  # åªåœ¨ç¬¬ä¸€åˆ—æ˜¾ç¤ºç”¨æˆ·ID
                axes[user_idx, sample_idx].set_ylabel(f'User ID_{actual_user_id}', 
                                                    fontweight='bold', fontsize=12)
            
            axes[user_idx, sample_idx].set_xticks([])
            axes[user_idx, sample_idx].set_yticks([])
    
    # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ“¸ å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_path}")
    return output_path


def main():
    parser = argparse.ArgumentParser(description='æ‰‹åŠ¨ç”Ÿæˆæ¡ä»¶DiTå¯è§†åŒ–')
    parser.add_argument('--checkpoint', type=str, required=True, 
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (best.ckpt æˆ– last.ckpt)')
    parser.add_argument('--config', type=str, required=True,
                        help='é…ç½®æ–‡ä»¶è·¯å¾„ (config.yaml)')
    parser.add_argument('--output', type=str, default=None,
                        help='è¾“å‡ºå›¾åƒè·¯å¾„ (é»˜è®¤: manual_visualization_timestamp.png)')
    parser.add_argument('--users', type=int, nargs='+', default=None,
                        help='è¦ç”Ÿæˆçš„ç”¨æˆ·IDåˆ—è¡¨ (0-based, ä¾‹å¦‚: --users 0 5 10 15)')
    parser.add_argument('--samples-per-user', type=int, default=4,
                        help='æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•° (é»˜è®¤: 4)')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®¡ç®—è®¾å¤‡ (é»˜è®¤: cuda)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.checkpoint).exists():
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
    if not Path(args.config).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
    
    print("ğŸš€ å¼€å§‹æ‰‹åŠ¨ç”Ÿæˆå¯è§†åŒ–...")
    print(f"   - æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"   - é…ç½®: {args.config}")
    print(f"   - è®¾å¤‡: {args.device}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # åŠ è½½æ¨¡å‹
        print("\nğŸ“¥ åŠ è½½æ¨¡å‹...")
        model, config = load_trained_model(args.checkpoint, args.config, device)
        vae = load_vae_model(device)
        
        # ç”Ÿæˆæ ·æœ¬
        print("\nğŸ¨ ç”Ÿæˆæ ·æœ¬...")
        images, selected_users, num_samples_per_user = generate_samples(
            model, vae, device, 
            num_samples_per_user=args.samples_per_user,
            selected_users=args.users
        )
        
        # å¯è§†åŒ–å¹¶ä¿å­˜
        print("\nğŸ“¸ ä¿å­˜å¯è§†åŒ–...")
        output_path = visualize_and_save(
            images, selected_users, num_samples_per_user, args.output
        )
        
        print(f"\nğŸ‰ å®Œæˆ! å¯è§†åŒ–å›¾åƒå·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()

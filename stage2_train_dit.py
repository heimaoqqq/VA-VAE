#!/usr/bin/env python3
"""
é˜¶æ®µ2: ç”¨æˆ·æ¡ä»¶åŒ–DiTè®­ç»ƒ
åœ¨æ½œåœ¨ç©ºé—´è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œéµå¾ªLightningDiTçš„train.pyå®ç°
"""

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
import argparse
import os
import sys
from pathlib import Path
import numpy as np
from safetensors import safe_open
import math

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from models.lightningdit import LightningDiT
from transport import create_transport

class LatentDataset(Dataset):
    """
    æ½œåœ¨ç‰¹å¾æ•°æ®é›†
    éµå¾ªLightningDiTçš„ImgLatentDatasetå®ç°
    """
    
    def __init__(self, latent_file, latent_norm=True, latent_multiplier=1.0):
        print(f"ğŸ“Š åŠ è½½æ½œåœ¨ç‰¹å¾: {latent_file}")
        
        # ä½¿ç”¨safetensorsåŠ è½½æ•°æ®
        with safe_open(latent_file, framework="pt", device="cpu") as f:
            self.latents = f.get_tensor('latents')  # (N, 32, 16, 16)
            self.user_ids = f.get_tensor('user_ids')  # (N,)
            
            # è¯»å–å…ƒæ•°æ®
            self.num_samples = f.get_tensor('num_samples').item()
            self.num_users = f.get_tensor('num_users').item()
        
        self.latent_norm = latent_norm
        self.latent_multiplier = latent_multiplier
        
        print(f"  æ ·æœ¬æ•°é‡: {len(self.latents)}")
        print(f"  ç‰¹å¾å½¢çŠ¶: {self.latents.shape}")
        print(f"  ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"  ç”¨æˆ·IDèŒƒå›´: [{self.user_ids.min()}, {self.user_ids.max()}]")
        
        # è®¡ç®—æ½œåœ¨ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        if self.latent_norm:
            self._compute_latent_stats()
    
    def _compute_latent_stats(self):
        """åŠ è½½æˆ–è®¡ç®—æ½œåœ¨ç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®"""
        print("ğŸ“ˆ åŠ è½½æ½œåœ¨ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯...")

        # é¦–å…ˆå°è¯•åŠ è½½é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯
        stats_file = Path(self.latent_file).parent / "latents_stats.pt"

        if stats_file.exists():
            print(f"ğŸ“Š åŠ è½½ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
            stats = torch.load(stats_file)
            self.latent_mean = stats['mean']  # (1, 32, 1, 1)
            self.latent_std = stats['std']    # (1, 32, 1, 1)

            print(f"  ä½¿ç”¨é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯")
            print(f"  å‡å€¼å½¢çŠ¶: {self.latent_mean.shape}")
            print(f"  æ ‡å‡†å·®å½¢çŠ¶: {self.latent_std.shape}")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨å…¨å±€ç»Ÿè®¡")
            # å›é€€åˆ°å…¨å±€ç»Ÿè®¡ä¿¡æ¯
            self.latent_mean = self.latents.mean()
            self.latent_std = self.latents.std()

            print(f"  å…¨å±€å‡å€¼: {self.latent_mean:.4f}")
            print(f"  å…¨å±€æ ‡å‡†å·®: {self.latent_std:.4f}")
    
    def __len__(self):
        return len(self.latents)
    
    def __getitem__(self, idx):
        latent = self.latents[idx].clone()  # (32, 16, 16)
        user_id = self.user_ids[idx].item()
        
        # åº”ç”¨å½’ä¸€åŒ–
        if self.latent_norm:
            latent = (latent - self.latent_mean) / self.latent_std
        
        # åº”ç”¨ç¼©æ”¾å› å­
        latent = latent * self.latent_multiplier
        
        return {
            'latent': latent,
            'user_id': user_id,
            'y': user_id - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•ï¼Œç”¨äºDiTçš„ç±»åˆ«æ¡ä»¶
        }

class UserConditionedDiT(pl.LightningModule):
    """
    ç”¨æˆ·æ¡ä»¶åŒ–çš„DiTæ¨¡å‹
    åŸºäºLightningDiTå®ç°ï¼Œæ·»åŠ ç”¨æˆ·æ¡ä»¶
    """
    
    def __init__(
        self,
        num_users,
        input_size=16,  # 16x16 latent
        patch_size=2,
        in_channels=32,  # VA-VAEçš„æ½œåœ¨ç»´åº¦
        hidden_size=1152,  # DiT-XLé…ç½®
        depth=28,
        num_heads=16,
        mlp_ratio=4.0,
        learn_sigma=True,
        lr=1e-4,
        **kwargs
    ):
        super().__init__()
        self.save_hyperparameters()
        
        self.num_users = num_users
        self.lr = lr
        
        # åˆ›å»ºDiTæ¨¡å‹
        self.dit = LightningDiT(
            input_size=input_size,
            patch_size=patch_size,
            in_channels=in_channels,
            hidden_size=hidden_size,
            depth=depth,
            num_heads=num_heads,
            mlp_ratio=mlp_ratio,
            class_dropout_prob=0.1,  # ç”¨äºclassifier-free guidance
            num_classes=num_users,   # ç”¨æˆ·ä½œä¸ºç±»åˆ«æ¡ä»¶
            learn_sigma=learn_sigma,
        )
        
        # åˆ›å»ºæ‰©æ•£ä¼ è¾“
        self.transport = create_transport(
            path_type="Linear",
            prediction="velocity",
            loss_weight=None,
            train_eps=1e-5,
            sample_eps=1e-4,
        )
        
        print(f"ğŸ¤– åˆ›å»ºç”¨æˆ·æ¡ä»¶åŒ–DiTæ¨¡å‹:")
        print(f"  ç”¨æˆ·æ•°é‡: {num_users}")
        print(f"  è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
        print(f"  æ½œåœ¨ç»´åº¦: {in_channels}")
        print(f"  éšè—ç»´åº¦: {hidden_size}")
        print(f"  å±‚æ•°: {depth}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    
    def forward(self, x, t, y):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: (B, 32, 16, 16) å™ªå£°æ½œåœ¨ç‰¹å¾
            t: (B,) æ—¶é—´æ­¥
            y: (B,) ç”¨æˆ·ID (0-based)
        """
        return self.dit(x, t, y)
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        latents = batch['latent']  # (B, 32, 16, 16)
        user_ids = batch['y']      # (B,) 0-basedç”¨æˆ·ID
        
        # æ‰©æ•£è®­ç»ƒ
        model_kwargs = dict(y=user_ids)
        loss_dict = self.transport.training_losses(self.dit, latents, model_kwargs)
        loss = loss_dict["loss"].mean()
        
        # è®°å½•æŸå¤±
        self.log('train/loss', loss, prog_bar=True, logger=True, sync_dist=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        latents = batch['latent']
        user_ids = batch['y']
        
        model_kwargs = dict(y=user_ids)
        loss_dict = self.transport.training_losses(self.dit, latents, model_kwargs)
        loss = loss_dict["loss"].mean()
        
        self.log('val/loss', loss, prog_bar=True, logger=True, sync_dist=True)
        
        return loss
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.lr,
            weight_decay=0.0,
            betas=(0.9, 0.95),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.trainer.max_epochs,
            eta_min=self.lr * 0.1
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "epoch",
            }
        }

class MicroDopplerDataModule(pl.LightningDataModule):
    """
    å¾®å¤šæ™®å‹’æ•°æ®æ¨¡å—
    """
    
    def __init__(
        self,
        train_latent_file,
        val_latent_file,
        batch_size=32,
        num_workers=4,
        latent_norm=True,
        latent_multiplier=1.0
    ):
        super().__init__()
        self.train_latent_file = train_latent_file
        self.val_latent_file = val_latent_file
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.latent_norm = latent_norm
        self.latent_multiplier = latent_multiplier
    
    def setup(self, stage=None):
        """è®¾ç½®æ•°æ®é›†"""
        if stage == "fit" or stage is None:
            self.train_dataset = LatentDataset(
                self.train_latent_file,
                latent_norm=self.latent_norm,
                latent_multiplier=self.latent_multiplier
            )
            self.val_dataset = LatentDataset(
                self.val_latent_file,
                latent_norm=self.latent_norm,
                latent_multiplier=self.latent_multiplier
            )
            
            # ç¡®ä¿ç”¨æˆ·æ•°é‡ä¸€è‡´
            assert self.train_dataset.num_users == self.val_dataset.num_users
            self.num_users = self.train_dataset.num_users
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=True if self.num_workers > 0 else False
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=True if self.num_workers > 0 else False
        )

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒç”¨æˆ·æ¡ä»¶åŒ–DiTæ¨¡å‹')
    parser.add_argument('--latent_dir', type=str, required=True,
                       help='æ½œåœ¨ç‰¹å¾ç›®å½• (åŒ…å«train.safetensorså’Œval.safetensors)')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_epochs', type=int, default=100,
                       help='æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--devices', type=int, default=1,
                       help='GPUæ•°é‡')
    parser.add_argument('--precision', type=str, default='16-mixed',
                       help='è®­ç»ƒç²¾åº¦')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(args.seed)
    
    print("ğŸ¯ ç”¨æˆ·æ¡ä»¶åŒ–DiTè®­ç»ƒ - é˜¶æ®µ2")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    data_module = MicroDopplerDataModule(
        train_latent_file=os.path.join(args.latent_dir, 'train.safetensors'),
        val_latent_file=os.path.join(args.latent_dir, 'val.safetensors'),
        batch_size=args.batch_size,
        num_workers=4
    )
    
    # è®¾ç½®æ•°æ®æ¨¡å—ä»¥è·å–ç”¨æˆ·æ•°é‡
    data_module.setup()
    
    # åˆ›å»ºæ¨¡å‹
    model = UserConditionedDiT(
        num_users=data_module.num_users,
        lr=args.lr
    )
    
    # è®¾ç½®å›è°ƒ
    callbacks = [
        ModelCheckpoint(
            dirpath=os.path.join(args.output_dir, 'checkpoints'),
            filename='dit-{epoch:02d}-{val/loss:.4f}',
            monitor='val/loss',
            mode='min',
            save_top_k=3,
            save_last=True
        ),
        LearningRateMonitor(logging_interval='epoch')
    ]
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        devices=args.devices,
        accelerator='gpu',
        strategy='ddp' if args.devices > 1 else 'auto',
        precision=args.precision,
        callbacks=callbacks,
        log_every_n_steps=50,
        val_check_interval=1.0,
        enable_progress_bar=True,
        enable_model_summary=True,
        default_root_dir=args.output_dir
    )
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.fit(model, data_module)
    
    print("âœ… è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main()

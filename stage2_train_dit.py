#!/usr/bin/env python3
"""
é˜¶æ®µ2: ç”¨æˆ·æ¡ä»¶åŒ–DiTè®­ç»ƒ
åœ¨æ½œåœ¨ç©ºé—´è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œéµå¾ªLightningDiTçš„train.pyå®ç°
"""

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
import argparse
import os
import sys
from pathlib import Path
import numpy as np
from safetensors import safe_open
import math

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from models.lightningdit import LightningDiT
from transport import create_transport

class LatentDataset(Dataset):
    """
    æ½œåœ¨ç‰¹å¾æ•°æ®é›†
    éµå¾ªLightningDiTçš„ImgLatentDatasetå®ç°
    """
    
    def __init__(self, latent_file, latent_norm=True, latent_multiplier=1.0):
        print(f"ğŸ“Š åŠ è½½æ½œåœ¨ç‰¹å¾: {latent_file}")

        # ä¿å­˜æ–‡ä»¶è·¯å¾„
        self.latent_file = latent_file

        # ä½¿ç”¨safetensorsåŠ è½½æ•°æ®
        with safe_open(latent_file, framework="pt", device="cpu") as f:
            self.latents = f.get_tensor('latents')  # (N, 32, 16, 16)
            self.user_ids = f.get_tensor('user_ids')  # (N,)

            # è¯»å–å…ƒæ•°æ®
            self.num_samples = f.get_tensor('num_samples').item()
            self.num_users = f.get_tensor('num_users').item()

        self.latent_norm = latent_norm
        self.latent_multiplier = latent_multiplier
        
        print(f"  æ ·æœ¬æ•°é‡: {len(self.latents)}")
        print(f"  ç‰¹å¾å½¢çŠ¶: {self.latents.shape}")
        print(f"  ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"  ç”¨æˆ·IDèŒƒå›´: [{self.user_ids.min()}, {self.user_ids.max()}]")
        
        # è®¡ç®—æ½œåœ¨ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        if self.latent_norm:
            self._compute_latent_stats()
    
    def _compute_latent_stats(self):
        """åŠ è½½æˆ–è®¡ç®—æ½œåœ¨ç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®"""
        print("ğŸ“ˆ åŠ è½½æ½œåœ¨ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯...")

        latent_dir = Path(self.latent_file).parent

        # æ£€æŸ¥æ¨èä½¿ç”¨å“ªä¸ªç»Ÿè®¡ä¿¡æ¯
        recommendation_file = latent_dir / "stats_recommendation.txt"
        use_imagenet = False

        if recommendation_file.exists():
            with open(recommendation_file, 'r') as f:
                content = f.read()
                if "imagenet" in content.lower():
                    use_imagenet = True
                    print("ğŸ“‹ æ¨èä½¿ç”¨ImageNetç»Ÿè®¡ä¿¡æ¯")

        # é€‰æ‹©ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶
        if use_imagenet:
            stats_file = latent_dir / "latents_stats_imagenet.pt"
            stats_type = "ImageNet"
        else:
            stats_file = latent_dir / "latents_stats.pt"
            stats_type = "å¾®å¤šæ™®å‹’"

        if stats_file.exists():
            print(f"ğŸ“Š åŠ è½½{stats_type}ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
            stats = torch.load(stats_file)
            self.latent_mean = stats['mean']  # (1, 32, 1, 1)
            self.latent_std = stats['std']    # (1, 32, 1, 1)

            print(f"  ä½¿ç”¨{stats_type}ç»Ÿè®¡ä¿¡æ¯")
            print(f"  å‡å€¼å½¢çŠ¶: {self.latent_mean.shape}")
            print(f"  æ ‡å‡†å·®å½¢çŠ¶: {self.latent_std.shape}")
            print(f"  å…¨å±€å‡å€¼: {self.latent_mean.mean():.4f}")
            print(f"  å…¨å±€æ ‡å‡†å·®: {self.latent_std.mean():.4f}")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨å…¨å±€ç»Ÿè®¡")
            # å›é€€åˆ°å…¨å±€ç»Ÿè®¡ä¿¡æ¯
            self.latent_mean = self.latents.mean()
            self.latent_std = self.latents.std()

            print(f"  å…¨å±€å‡å€¼: {self.latent_mean:.4f}")
            print(f"  å…¨å±€æ ‡å‡†å·®: {self.latent_std:.4f}")
    
    def __len__(self):
        return len(self.latents)
    
    def __getitem__(self, idx):
        latent = self.latents[idx].clone()  # (32, 16, 16)
        user_id = self.user_ids[idx].item()

        # åº”ç”¨å½’ä¸€åŒ–
        if self.latent_norm:
            # ç¡®ä¿ç»Ÿè®¡ä¿¡æ¯çš„å½¢çŠ¶åŒ¹é…
            # latent: (32, 16, 16), mean/std: (1, 32, 1, 1)
            # éœ€è¦squeezeæ‰ç¬¬ä¸€ä¸ªç»´åº¦
            mean = self.latent_mean.squeeze(0)  # (32, 1, 1)
            std = self.latent_std.squeeze(0)    # (32, 1, 1)
            latent = (latent - mean) / std

        # åº”ç”¨ç¼©æ”¾å› å­
        latent = latent * self.latent_multiplier

        # ç¡®ä¿è¿”å›çš„latentæ˜¯3ç»´çš„ (C, H, W)
        if len(latent.shape) != 3:
            print(f"âš ï¸  è­¦å‘Š: latentç»´åº¦å¼‚å¸¸ {latent.shape}, å°è¯•ä¿®å¤")
            latent = latent.squeeze()  # ç§»é™¤æ‰€æœ‰å¤§å°ä¸º1çš„ç»´åº¦
            if len(latent.shape) != 3:
                raise ValueError(f"æ— æ³•ä¿®å¤latentç»´åº¦: {latent.shape}")

        return {
            'latent': latent,
            'user_id': user_id,
            'y': user_id - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•ï¼Œç”¨äºDiTçš„ç±»åˆ«æ¡ä»¶
        }

class UserConditionedDiT(pl.LightningModule):
    """
    ç”¨æˆ·æ¡ä»¶åŒ–çš„DiTæ¨¡å‹
    åŸºäºLightningDiTå®ç°ï¼Œæ·»åŠ ç”¨æˆ·æ¡ä»¶
    """
    
    def __init__(
        self,
        num_users,
        input_size=16,  # 16x16 latent
        patch_size=2,
        in_channels=32,  # VA-VAEçš„æ½œåœ¨ç»´åº¦
        hidden_size=1152,  # DiT-XLé…ç½®
        depth=28,
        num_heads=16,
        mlp_ratio=4.0,
        learn_sigma=True,
        lr=1e-4,
        **kwargs
    ):
        super().__init__()
        self.save_hyperparameters()
        
        self.num_users = num_users
        self.lr = lr
        
        # åˆ›å»ºDiTæ¨¡å‹
        self.dit = LightningDiT(
            input_size=input_size,
            patch_size=patch_size,
            in_channels=in_channels,
            hidden_size=hidden_size,
            depth=depth,
            num_heads=num_heads,
            mlp_ratio=mlp_ratio,
            class_dropout_prob=0.1,  # ç”¨äºclassifier-free guidance
            num_classes=num_users,   # ç”¨æˆ·ä½œä¸ºç±»åˆ«æ¡ä»¶
            learn_sigma=learn_sigma,
        )
        
        # åˆ›å»ºæ‰©æ•£ä¼ è¾“
        self.transport = create_transport(
            path_type="Linear",
            prediction="velocity",
            loss_weight=None,
            train_eps=1e-5,
            sample_eps=1e-4,
        )
        
        print(f"ğŸ¤– åˆ›å»ºç”¨æˆ·æ¡ä»¶åŒ–DiTæ¨¡å‹:")
        print(f"  ç”¨æˆ·æ•°é‡: {num_users}")
        print(f"  è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
        print(f"  æ½œåœ¨ç»´åº¦: {in_channels}")
        print(f"  éšè—ç»´åº¦: {hidden_size}")
        print(f"  å±‚æ•°: {depth}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    
    def forward(self, x, t, y):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: (B, 32, 16, 16) å™ªå£°æ½œåœ¨ç‰¹å¾
            t: (B,) æ—¶é—´æ­¥
            y: (B,) ç”¨æˆ·ID (0-based)
        """
        return self.dit(x, t, y)
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        latents = batch['latent']  # (B, 32, 16, 16)
        user_ids = batch['y']      # (B,) 0-basedç”¨æˆ·ID

        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ•°æ®ç»´åº¦å’ŒGPUä½¿ç”¨
        if batch_idx == 0:
            print(f"ğŸ” è®­ç»ƒè°ƒè¯•ä¿¡æ¯:")
            print(f"  latentså½¢çŠ¶: {latents.shape}")
            print(f"  user_idså½¢çŠ¶: {user_ids.shape}")
            print(f"  æœŸæœ›latentså½¢çŠ¶: (B, 32, 16, 16)")

            # æ˜¾ç¤ºGPUä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                current_device = latents.device
                print(f"  å½“å‰è®¾å¤‡: {current_device}")
                print(f"  GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(current_device) / 1e9:.2f}GB")

                # æ£€æŸ¥å¤šGPUæƒ…å†µ
                if torch.cuda.device_count() > 1:
                    print(f"  å¤šGPUçŠ¶æ€:")
                    for i in range(torch.cuda.device_count()):
                        memory_used = torch.cuda.memory_allocated(i) / 1e9
                        memory_total = torch.cuda.get_device_properties(i).total_memory / 1e9
                        print(f"    GPU {i}: {memory_used:.2f}GB / {memory_total:.1f}GB")

        # ä¿®å¤latentsç»´åº¦é—®é¢˜
        if len(latents.shape) == 5:
            print(f"ğŸ”§ ä¿®å¤5ç»´å¼ é‡: {latents.shape} -> ", end="")
            # å¦‚æœæ˜¯5ç»´ï¼Œå°è¯•squeezeæ‰å¤§å°ä¸º1çš„ç»´åº¦
            latents = latents.squeeze()
            print(f"{latents.shape}")

        # ç¡®ä¿latentsæ˜¯4ç»´çš„ (B, C, H, W)
        if len(latents.shape) != 4:
            print(f"âŒ é”™è¯¯çš„latentsç»´åº¦: {latents.shape}")
            raise ValueError(f"æœŸæœ›4ç»´å¼ é‡ (B, C, H, W)ï¼Œå¾—åˆ° {latents.shape}")

        # æ‰©æ•£è®­ç»ƒ
        model_kwargs = dict(y=user_ids)
        loss_dict = self.transport.training_losses(self.dit, latents, model_kwargs)
        loss = loss_dict["loss"].mean()

        # è®°å½•æŸå¤±
        self.log('train/loss', loss, prog_bar=True, logger=True, sync_dist=True)

        return loss
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        latents = batch['latent']
        user_ids = batch['y']

        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ•°æ®ç»´åº¦
        if batch_idx == 0:
            print(f"ğŸ” éªŒè¯è°ƒè¯•ä¿¡æ¯:")
            print(f"  latentså½¢çŠ¶: {latents.shape}")
            print(f"  user_idså½¢çŠ¶: {user_ids.shape}")
            print(f"  æœŸæœ›latentså½¢çŠ¶: (B, 32, 16, 16)")

        # ä¿®å¤latentsç»´åº¦é—®é¢˜
        if len(latents.shape) == 5:
            print(f"ğŸ”§ ä¿®å¤5ç»´å¼ é‡: {latents.shape} -> ", end="")
            # å¦‚æœæ˜¯5ç»´ï¼Œå°è¯•squeezeæ‰å¤§å°ä¸º1çš„ç»´åº¦
            latents = latents.squeeze()
            print(f"{latents.shape}")

        # ç¡®ä¿latentsæ˜¯4ç»´çš„ (B, C, H, W)
        if len(latents.shape) != 4:
            print(f"âŒ é”™è¯¯çš„latentsç»´åº¦: {latents.shape}")
            raise ValueError(f"æœŸæœ›4ç»´å¼ é‡ (B, C, H, W)ï¼Œå¾—åˆ° {latents.shape}")

        model_kwargs = dict(y=user_ids)
        loss_dict = self.transport.training_losses(self.dit, latents, model_kwargs)
        loss = loss_dict["loss"].mean()

        self.log('val/loss', loss, prog_bar=True, logger=True, sync_dist=True)

        return loss
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.lr,
            weight_decay=0.0,
            betas=(0.9, 0.95),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.trainer.max_epochs,
            eta_min=self.lr * 0.1
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "epoch",
            }
        }

class MicroDopplerDataModule(pl.LightningDataModule):
    """
    å¾®å¤šæ™®å‹’æ•°æ®æ¨¡å—
    """
    
    def __init__(
        self,
        train_latent_file,
        val_latent_file,
        batch_size=32,
        num_workers=4,
        latent_norm=True,
        latent_multiplier=1.0
    ):
        super().__init__()
        self.train_latent_file = train_latent_file
        self.val_latent_file = val_latent_file
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.latent_norm = latent_norm
        self.latent_multiplier = latent_multiplier
    
    def setup(self, stage=None):
        """è®¾ç½®æ•°æ®é›†"""
        if stage == "fit" or stage is None:
            self.train_dataset = LatentDataset(
                self.train_latent_file,
                latent_norm=self.latent_norm,
                latent_multiplier=self.latent_multiplier
            )
            self.val_dataset = LatentDataset(
                self.val_latent_file,
                latent_norm=self.latent_norm,
                latent_multiplier=self.latent_multiplier
            )
            
            # ç¡®ä¿ç”¨æˆ·æ•°é‡ä¸€è‡´
            assert self.train_dataset.num_users == self.val_dataset.num_users
            self.num_users = self.train_dataset.num_users
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=True if self.num_workers > 0 else False
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=True if self.num_workers > 0 else False
        )

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒç”¨æˆ·æ¡ä»¶åŒ–DiTæ¨¡å‹')
    parser.add_argument('--latent_dir', type=str, required=True,
                       help='æ½œåœ¨ç‰¹å¾ç›®å½• (åŒ…å«train.safetensorså’Œval.safetensors)')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_epochs', type=int, default=100,
                       help='æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--devices', type=int, default=1,
                       help='GPUæ•°é‡')
    parser.add_argument('--precision', type=str, default='16-mixed',
                       help='è®­ç»ƒç²¾åº¦')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(args.seed)
    
    print("ğŸ¯ ç”¨æˆ·æ¡ä»¶åŒ–DiTè®­ç»ƒ - é˜¶æ®µ2")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    data_module = MicroDopplerDataModule(
        train_latent_file=os.path.join(args.latent_dir, 'train.safetensors'),
        val_latent_file=os.path.join(args.latent_dir, 'val.safetensors'),
        batch_size=args.batch_size,
        num_workers=4
    )
    
    # è®¾ç½®æ•°æ®æ¨¡å—ä»¥è·å–ç”¨æˆ·æ•°é‡
    data_module.setup()
    
    # åˆ›å»ºæ¨¡å‹
    model = UserConditionedDiT(
        num_users=data_module.num_users,
        lr=args.lr
    )
    
    # è®¾ç½®å›è°ƒ
    callbacks = [
        ModelCheckpoint(
            dirpath=os.path.join(args.output_dir, 'checkpoints'),
            filename='dit-{epoch:02d}-{val/loss:.4f}',
            monitor='val/loss',
            mode='min',
            save_top_k=3,
            save_last=True
        ),
        LearningRateMonitor(logging_interval='epoch')
    ]
    
    # æ£€æŸ¥å®é™…å¯ç”¨çš„GPUæ•°é‡
    import torch
    available_gpus = torch.cuda.device_count()
    actual_devices = min(args.devices, available_gpus) if available_gpus > 0 else 1

    print(f"ğŸ”§ GPUé…ç½®:")
    print(f"  è¯·æ±‚GPUæ•°é‡: {args.devices}")
    print(f"  å¯ç”¨GPUæ•°é‡: {available_gpus}")
    print(f"  å®é™…ä½¿ç”¨GPUæ•°é‡: {actual_devices}")

    # æ˜¾ç¤ºGPUè¯¦ç»†ä¿¡æ¯
    if available_gpus > 0:
        for i in range(available_gpus):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
            print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")

    # é€‰æ‹©åˆé€‚çš„ç­–ç•¥
    if actual_devices > 1:
        # å¤šGPUä½¿ç”¨DDPç­–ç•¥ï¼Œé…ç½®DDPå‚æ•°
        from pytorch_lightning.strategies import DDPStrategy
        strategy = DDPStrategy(
            find_unused_parameters=False,  # æé«˜DDPæ€§èƒ½
            static_graph=True,             # é™æ€å›¾ä¼˜åŒ–
        )
        print(f"ğŸš€ ä½¿ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP) - {actual_devices} GPUs")
    else:
        # å•GPUä½¿ç”¨autoç­–ç•¥
        strategy = 'auto'
        print(f"ğŸš€ ä½¿ç”¨å•GPUè®­ç»ƒ")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        devices=actual_devices if available_gpus > 0 else 'auto',
        accelerator='gpu' if available_gpus > 0 else 'cpu',
        strategy=strategy,
        precision=args.precision if available_gpus > 0 else 32,
        callbacks=callbacks,
        log_every_n_steps=50,
        val_check_interval=1.0,
        enable_progress_bar=True,
        enable_model_summary=True,
        default_root_dir=args.output_dir,
        # å…¶ä»–é…ç½®
        sync_batchnorm=True if actual_devices > 1 else False,
    )
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.fit(model, data_module)
    
    print("âœ… è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main()

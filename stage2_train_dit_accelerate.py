#!/usr/bin/env python3
"""
åŸºäºAccelerateçš„DiTè®­ç»ƒè„šæœ¬
å‚è€ƒLightningDiTåŸé¡¹ç›®çš„å¤šGPUè®­ç»ƒæ–¹å¼
"""

import os
import torch
import torch.nn as nn
import argparse
from pathlib import Path
from tqdm import tqdm
import math

from accelerate import Accelerator
from accelerate.utils import set_seed
from torch.utils.data import DataLoader
from safetensors import safe_open

# å¯¼å…¥LightningDiTç»„ä»¶
from LightningDiT.models.lightningdit import LightningDiT_models
from LightningDiT.transport import create_transport

class LatentDataset(torch.utils.data.Dataset):
    """æ½œåœ¨ç‰¹å¾æ•°æ®é›†"""
    
    def __init__(self, latent_file, latent_norm=True, latent_multiplier=1.0):
        print(f"ğŸ“Š åŠ è½½æ½œåœ¨ç‰¹å¾: {latent_file}")
        
        # ä¿å­˜æ–‡ä»¶è·¯å¾„
        self.latent_file = latent_file
        
        # ä½¿ç”¨safetensorsåŠ è½½æ•°æ®
        with safe_open(latent_file, framework="pt", device="cpu") as f:
            self.latents = f.get_tensor('latents')  # (N, 32, 16, 16)
            self.user_ids = f.get_tensor('user_ids')  # (N,)
            
            # è¯»å–å…ƒæ•°æ®
            self.num_samples = f.get_tensor('num_samples').item()
            self.num_users = f.get_tensor('num_users').item()
        
        self.latent_norm = latent_norm
        self.latent_multiplier = latent_multiplier
        
        print(f"  æ ·æœ¬æ•°é‡: {self.num_samples}")
        print(f"  ç‰¹å¾å½¢çŠ¶: {self.latents.shape}")
        print(f"  ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"  ç”¨æˆ·IDèŒƒå›´: [{self.user_ids.min().item()}, {self.user_ids.max().item()}]")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self._compute_latent_stats()
    
    def _compute_latent_stats(self):
        """åŠ è½½æˆ–è®¡ç®—æ½œåœ¨ç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®"""
        print("ğŸ“ˆ åŠ è½½æ½œåœ¨ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯...")
        
        latent_dir = Path(self.latent_file).parent
        
        # æ£€æŸ¥æ¨èä½¿ç”¨å“ªä¸ªç»Ÿè®¡ä¿¡æ¯
        recommendation_file = latent_dir / "stats_recommendation.txt"
        use_imagenet = False
        
        if recommendation_file.exists():
            with open(recommendation_file, 'r') as f:
                content = f.read()
                if "imagenet" in content.lower():
                    use_imagenet = True
                    print("ğŸ“‹ æ¨èä½¿ç”¨ImageNetç»Ÿè®¡ä¿¡æ¯")
        
        # é€‰æ‹©ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶
        if use_imagenet:
            stats_file = latent_dir / "latents_stats_imagenet.pt"
            stats_type = "ImageNet"
        else:
            stats_file = latent_dir / "latents_stats.pt"
            stats_type = "å¾®å¤šæ™®å‹’"
        
        if stats_file.exists():
            print(f"ğŸ“Š åŠ è½½{stats_type}ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
            stats = torch.load(stats_file)
            self.latent_mean = stats['mean']  # (1, 32, 1, 1)
            self.latent_std = stats['std']    # (1, 32, 1, 1)
            
            print(f"  ä½¿ç”¨{stats_type}ç»Ÿè®¡ä¿¡æ¯")
            print(f"  å‡å€¼å½¢çŠ¶: {self.latent_mean.shape}")
            print(f"  æ ‡å‡†å·®å½¢çŠ¶: {self.latent_std.shape}")
            print(f"  å…¨å±€å‡å€¼: {self.latent_mean.mean():.4f}")
            print(f"  å…¨å±€æ ‡å‡†å·®: {self.latent_std.mean():.4f}")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨å…¨å±€ç»Ÿè®¡")
            # å›é€€åˆ°å…¨å±€ç»Ÿè®¡ä¿¡æ¯
            self.latent_mean = self.latents.mean()
            self.latent_std = self.latents.std()
            
            print(f"  å…¨å±€å‡å€¼: {self.latent_mean:.4f}")
            print(f"  å…¨å±€æ ‡å‡†å·®: {self.latent_std:.4f}")
    
    def __len__(self):
        return len(self.latents)
    
    def __getitem__(self, idx):
        latent = self.latents[idx].clone()  # (32, 16, 16)
        user_id = self.user_ids[idx].item()
        
        # åº”ç”¨å½’ä¸€åŒ–
        if self.latent_norm:
            # ç¡®ä¿ç»Ÿè®¡ä¿¡æ¯çš„å½¢çŠ¶åŒ¹é…
            # latent: (32, 16, 16), mean/std: (1, 32, 1, 1)
            # éœ€è¦squeezeæ‰ç¬¬ä¸€ä¸ªç»´åº¦
            mean = self.latent_mean.squeeze(0)  # (32, 1, 1)
            std = self.latent_std.squeeze(0)    # (32, 1, 1)
            latent = (latent - mean) / std
        
        # åº”ç”¨ç¼©æ”¾å› å­
        latent = latent * self.latent_multiplier
        
        # ç¡®ä¿è¿”å›çš„latentæ˜¯3ç»´çš„ (C, H, W)
        if len(latent.shape) != 3:
            print(f"âš ï¸  è­¦å‘Š: latentç»´åº¦å¼‚å¸¸ {latent.shape}, å°è¯•ä¿®å¤")
            latent = latent.squeeze()  # ç§»é™¤æ‰€æœ‰å¤§å°ä¸º1çš„ç»´åº¦
            if len(latent.shape) != 3:
                raise ValueError(f"æ— æ³•ä¿®å¤latentç»´åº¦: {latent.shape}")
        
        return {
            'latent': latent,
            'user_id': user_id,
            'y': user_id - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•ï¼Œç”¨äºDiTçš„ç±»åˆ«æ¡ä»¶
        }

def main():
    parser = argparse.ArgumentParser(description='åŸºäºAccelerateçš„DiTè®­ç»ƒ')
    parser.add_argument('--latent_dir', type=str, required=True, help='æ½œåœ¨ç‰¹å¾ç›®å½•')
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_epochs', type=int, default=100, help='æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–Accelerator
    accelerator = Accelerator(
        mixed_precision='fp16',  # ä½¿ç”¨æ··åˆç²¾åº¦
        gradient_accumulation_steps=1,
        log_with="tensorboard",
        project_dir=args.output_dir
    )
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    print("ğŸ¯ åŸºäºAccelerateçš„ç”¨æˆ·æ¡ä»¶åŒ–DiTè®­ç»ƒ")
    print("=" * 60)
    print(f"ğŸ”§ Acceleratoré…ç½®:")
    print(f"  è¿›ç¨‹æ•°: {accelerator.num_processes}")
    print(f"  å½“å‰è¿›ç¨‹: {accelerator.process_index}")
    print(f"  è®¾å¤‡: {accelerator.device}")
    print(f"  æ··åˆç²¾åº¦: {accelerator.mixed_precision}")
    print(f"  åˆ†å¸ƒå¼: {accelerator.distributed_type}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = LatentDataset(
        latent_file=os.path.join(args.latent_dir, 'train.safetensors'),
        latent_norm=True,
        latent_multiplier=1.0
    )
    
    val_dataset = LatentDataset(
        latent_file=os.path.join(args.latent_dir, 'val.safetensors'),
        latent_norm=True,
        latent_multiplier=1.0
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = LightningDiT_models['LightningDiT-XL/1'](
        input_size=16,  # 16x16 latent
        num_classes=train_dataset.num_users,  # ç”¨æˆ·æ•°é‡ä½œä¸ºç±»åˆ«æ•°
        in_channels=32,  # 32é€šé“
        use_qknorm=False,
        use_swiglu=True,
        use_rope=True,
        use_rmsnorm=True,
        wo_shift=False
    )
    
    # åˆ›å»ºtransport (æ‰©æ•£è¿‡ç¨‹)
    transport = create_transport(
        path_type="Linear",
        prediction="velocity",
        loss_weight=None,
        train_eps=None,
        sample_eps=None
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        betas=(0.9, 0.95),
        weight_decay=0.0
    )
    
    # ä½¿ç”¨Acceleratorå‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨
    model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, val_dataloader
    )
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.max_epochs):
        model.train()
        total_loss = 0
        
        progress_bar = tqdm(
            train_dataloader, 
            desc=f"Epoch {epoch+1}/{args.max_epochs}",
            disable=not accelerator.is_local_main_process
        )
        
        for step, batch in enumerate(progress_bar):
            with accelerator.accumulate(model):
                latents = batch['latent']  # (B, 32, 16, 16)
                user_ids = batch['y']      # (B,) 0-basedç”¨æˆ·ID
                
                # æ‰©æ•£è®­ç»ƒ
                model_kwargs = dict(y=user_ids)
                loss_dict = transport.training_losses(model, latents, model_kwargs)
                loss = loss_dict["loss"].mean()
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
                
                total_loss += loss.item()
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{total_loss/(step+1):.4f}'
                })
        
        # éªŒè¯
        if accelerator.is_main_process:
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch in val_dataloader:
                    latents = batch['latent']
                    user_ids = batch['y']
                    
                    model_kwargs = dict(y=user_ids)
                    loss_dict = transport.training_losses(model, latents, model_kwargs)
                    val_loss += loss_dict["loss"].mean().item()
            
            val_loss /= len(val_dataloader)
            print(f"Epoch {epoch+1}: Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {val_loss:.4f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                accelerator.save_state(os.path.join(args.output_dir, f"checkpoint_epoch_{epoch+1}"))
    
    print("âœ… è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main()

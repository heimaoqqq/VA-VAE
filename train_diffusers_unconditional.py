"""
ä½¿ç”¨HuggingFace diffusersè®­ç»ƒæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹
é€‚é…32é€šé“VA-VAE latentç©ºé—´
"""

import os
import argparse
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from diffusers import UNet2DModel, DDPMScheduler, DDIMScheduler
from diffusers.optimization import get_cosine_schedule_with_warmup
import numpy as np
from tqdm import tqdm
from PIL import Image
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

from simplified_vavae import SimplifiedVAVAE
from microdoppler_data_loader import MicrodopplerDataset


class DiffusersTrainer:
    """åŸºäºdiffusersçš„ç¨³å®šè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–VAE
        print("ğŸ”§ åŠ è½½VA-VAE...")
        self.vae = SimplifiedVAVAE(args.vae_checkpoint)
        self.vae.eval()
        
        # åˆå§‹åŒ–UNet - ä½¿ç”¨diffusersçš„æ ‡å‡†UNet
        print("ğŸ”§ åˆå§‹åŒ–UNet2Dæ¨¡å‹...")
        self.unet = UNet2DModel(
            sample_size=16,  # latentç©ºé—´å¤§å° 16x16
            in_channels=32,  # VA-VAE latenté€šé“æ•°
            out_channels=32,
            layers_per_block=2,
            block_out_channels=(128, 256, 512, 512),
            down_block_types=(
                "DownBlock2D",
                "DownBlock2D", 
                "DownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "UpBlock2D",
                "UpBlock2D", 
                "UpBlock2D",
            ),
        ).to(self.device)
        
        # å™ªå£°è°ƒåº¦å™¨
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule="linear",
            prediction_type="epsilon"  # é¢„æµ‹å™ªå£°
        )
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.unet.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
        
        # æ•°æ®åŠ è½½å™¨
        print("ğŸ“Š å‡†å¤‡æ•°æ®...")
        self.train_loader = self.prepare_dataloader()
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.lr_scheduler = get_cosine_schedule_with_warmup(
            optimizer=self.optimizer,
            num_warmup_steps=args.warmup_steps,
            num_training_steps=len(self.train_loader) * args.num_epochs
        )
        
        # åˆ†å¸ƒå¯¹é½å‚æ•°
        self.use_distribution_alignment = False
        self.latent_mean = None
        self.latent_std = None
        
    def prepare_dataloader(self):
        """å‡†å¤‡æ•°æ®åŠ è½½å™¨"""
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # å½’ä¸€åŒ–åˆ°[-1, 1]
        ])
        
        dataset = MicrodopplerDataset(
            root_dir=self.args.image_dir,
            split_file=self.args.split_file,
            split='train',
            transform=transform,
            return_user_id=False  # æ— æ¡ä»¶ç”Ÿæˆä¸éœ€è¦ç”¨æˆ·ID
        )
        
        return DataLoader(
            dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
    
    def detect_distribution_alignment(self, latents):
        """æ£€æµ‹å¹¶é…ç½®åˆ†å¸ƒå¯¹é½"""
        with torch.no_grad():
            mean = latents.mean().item()
            std = latents.std().item()
            
            print(f"\nğŸ“Š Latentåˆ†å¸ƒåˆ†æ:")
            print(f"   Mean: {mean:.6f}")
            print(f"   Std: {std:.6f}")
            print(f"   Range: [{latents.min().item():.2f}, {latents.max().item():.2f}]")
            
            # å¦‚æœstdåç¦»1.0è¾ƒå¤šï¼Œå¯ç”¨åˆ†å¸ƒå¯¹é½
            if abs(std - 1.0) > 0.3:
                print(f"âœ… å¯ç”¨åˆ†å¸ƒå¯¹é½ (std={std:.3f} åç¦»1.0)")
                self.use_distribution_alignment = True
                self.latent_mean = mean
                self.latent_std = std
                
                # éªŒè¯å¯¹é½æ•ˆæœ
                aligned = (latents - mean) / std
                print(f"   å¯¹é½å: Mean={aligned.mean().item():.6f}, Std={aligned.std().item():.6f}")
            else:
                print(f"ğŸ“Š åˆ†å¸ƒæ­£å¸¸ï¼Œæ— éœ€å¯¹é½ (std={std:.3f})")
                
    def normalize_latents(self, latents):
        """å½’ä¸€åŒ–latents"""
        if not self.use_distribution_alignment:
            return latents
        return (latents - self.latent_mean) / self.latent_std
    
    def denormalize_latents(self, latents):
        """åå½’ä¸€åŒ–latents"""
        if not self.use_distribution_alignment:
            return latents
        return latents * self.latent_std + self.latent_mean
    
    def train_step(self, batch):
        """å•ä¸ªè®­ç»ƒæ­¥éª¤"""
        images = batch[0].to(self.device)  # åªå–å›¾åƒï¼Œå¿½ç•¥ç”¨æˆ·ID
        
        # VAEç¼–ç 
        with torch.no_grad():
            latents = self.vae.encode(images)
            
            # å½’ä¸€åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.use_distribution_alignment:
                latents = self.normalize_latents(latents)
        
        # é‡‡æ ·å™ªå£°å’Œæ—¶é—´æ­¥
        noise = torch.randn_like(latents)
        timesteps = torch.randint(
            0, self.noise_scheduler.config.num_train_timesteps,
            (latents.shape[0],), device=self.device
        )
        
        # æ·»åŠ å™ªå£°
        noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)
        
        # é¢„æµ‹å™ªå£°
        noise_pred = self.unet(noisy_latents, timesteps).sample
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(noise_pred, noise)
        
        return loss
    
    @torch.no_grad()
    def generate_samples(self, num_samples=4, num_inference_steps=50):
        """ç”Ÿæˆæ ·æœ¬"""
        self.unet.eval()
        
        # åˆå§‹å™ªå£°
        latents = torch.randn(
            num_samples, 32, 16, 16,  # 32é€šé“ï¼Œ16x16
            device=self.device
        )
        
        # DDIMæ¨ç†
        ddim_scheduler = DDIMScheduler.from_config(self.noise_scheduler.config)
        ddim_scheduler.set_timesteps(num_inference_steps)
        
        for timestep in tqdm(ddim_scheduler.timesteps, desc="ç”Ÿæˆä¸­"):
            timestep_batch = timestep.repeat(num_samples).to(self.device)
            
            # é¢„æµ‹å™ªå£°
            noise_pred = self.unet(latents, timestep_batch).sample
            
            # å»å™ª
            latents = ddim_scheduler.step(noise_pred, timestep, latents).prev_sample
        
        # åå½’ä¸€åŒ–
        if self.use_distribution_alignment:
            latents = self.denormalize_latents(latents)
        
        # VAEè§£ç 
        images = self.vae.decode(latents)
        
        return images
    
    def save_samples(self, images, epoch, save_dir):
        """ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬ - å‚è€ƒVA-VAEè®­ç»ƒè„šæœ¬çš„å¯è§†åŒ–æ–¹å¼"""
        import matplotlib.pyplot as plt
        
        os.makedirs(save_dir, exist_ok=True)
        
        # åˆ›å»ºç½‘æ ¼å¯è§†åŒ– - å‚è€ƒstep4_train_vavae.pyçš„é£æ ¼
        num_samples = min(8, len(images))
        fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))
        fig.suptitle(f'Epoch {epoch} - æ‰©æ•£ç”Ÿæˆæ ·æœ¬')
        
        if num_samples == 1:
            axes = [axes]
        
        for i in range(num_samples):
            # åå½’ä¸€åŒ–åˆ°[0,1]
            img_tensor = (images[i] + 1) / 2
            img_tensor = torch.clamp(img_tensor, 0, 1)
            
            # è½¬æ¢ä¸ºnumpyæ˜¾ç¤ºæ ¼å¼
            img_array = img_tensor.cpu().numpy()
            if img_array.shape[0] == 3:  # RGB
                img_array = np.transpose(img_array, (1, 2, 0))
            elif img_array.shape[0] == 1:  # ç°åº¦
                img_array = img_array.squeeze(0)
                
            axes[i].imshow(img_array, cmap='gray' if len(img_array.shape) == 2 else None)
            axes[i].set_title(f'æ ·æœ¬ {i+1}')
            axes[i].axis('off')
        
        # ä¿å­˜å¯è§†åŒ–å›¾åƒ
        plt.tight_layout()
        plt.savefig(f"{save_dir}/epoch_{epoch:03d}_samples.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        # åŒæ—¶ä¿å­˜å•ä¸ªå›¾åƒæ–‡ä»¶
        for i, img_tensor in enumerate(images):
            # åå½’ä¸€åŒ–åˆ°[0,1]
            img_tensor = (img_tensor + 1) / 2
            img_tensor = torch.clamp(img_tensor, 0, 1)
            
            # è½¬æ¢ä¸ºPIL
            img_array = img_tensor.cpu().numpy().transpose(1, 2, 0)
            if img_array.shape[2] == 1:
                img_array = img_array.squeeze(2)
                img = Image.fromarray((img_array * 255).astype(np.uint8), 'L')
            else:
                img = Image.fromarray((img_array * 255).astype(np.uint8))
            
            img.save(f"{save_dir}/epoch_{epoch:03d}_sample_{i:02d}.png")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {self.device}")
        
        # é¦–æ¬¡æ£€æµ‹åˆ†å¸ƒå¯¹é½
        first_batch = next(iter(self.train_loader))
        with torch.no_grad():
            sample_latents = self.vae.encode(first_batch[0][:4].to(self.device))
            self.detect_distribution_alignment(sample_latents)
        
        # è®­ç»ƒå¾ªç¯
        global_step = 0
        
        for epoch in range(self.args.num_epochs):
            self.unet.train()
            epoch_losses = []
            
            pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.args.num_epochs}")
            
            for batch in pbar:
                # è®­ç»ƒæ­¥éª¤
                loss = self.train_step(batch)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.unet.parameters(), 1.0)
                
                self.optimizer.step()
                self.lr_scheduler.step()
                
                # è®°å½•
                epoch_losses.append(loss.item())
                pbar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'lr': f"{self.lr_scheduler.get_last_lr()[0]:.6f}"
                })
                
                global_step += 1
            
            avg_loss = np.mean(epoch_losses)
            print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.args.save_freq == 0:
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': self.unet.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.lr_scheduler.state_dict(),
                    'use_distribution_alignment': self.use_distribution_alignment,
                    'latent_mean': self.latent_mean,
                    'latent_std': self.latent_std,
                }
                
                save_path = f"checkpoints/diffusion_epoch_{epoch+1:03d}.pt"
                os.makedirs("checkpoints", exist_ok=True)
                torch.save(checkpoint, save_path)
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {save_path}")
            
            # æ¯è½®ç”Ÿæˆæ ·æœ¬ - å‚è€ƒstep4_train_vavae.pyçš„åšæ³•
            print("ğŸ¨ ç”Ÿæˆæ ·æœ¬...")
            sample_images = self.generate_samples(num_samples=8)
            self.save_samples(sample_images, epoch+1, "samples")
            print(f"âœ… Epoch {epoch+1} æ ·æœ¬å·²ä¿å­˜åˆ° samples/ ç›®å½•")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--image_dir', type=str, required=True,
                       help='å›¾åƒæ•°æ®ç›®å½•')
    parser.add_argument('--vae_checkpoint', type=str, required=True,
                       help='VAEæ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--split_file', type=str, required=True,
                       help='æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-6,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--warmup_steps', type=int, default=1000,
                       help='é¢„çƒ­æ­¥æ•°')
    parser.add_argument('--save_freq', type=int, default=10,
                       help='ä¿å­˜é¢‘ç‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = DiffusersTrainer(args)
    trainer.train()
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()

"""
ä½¿ç”¨HuggingFace diffusersè®­ç»ƒæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹
é€‚é…32é€šé“VA-VAE latentç©ºé—´
"""

import os
import argparse
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from diffusers import UNet2DModel, DDPMScheduler, DDIMScheduler
from diffusers.optimization import get_cosine_schedule_with_warmup
import numpy as np
from tqdm import tqdm
from PIL import Image
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from pathlib import Path

from simplified_vavae import SimplifiedVAVAE
from microdoppler_dataset_diffusion import MicrodopplerDataset
from latent_processing import MixedLatentDataset, PreEncodedLatentDataset


class DiffusersTrainer:
    """åŸºäºdiffusersçš„ç¨³å®šè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–VAE
        print("ğŸ”§ åŠ è½½VA-VAE...")
        self.vae = SimplifiedVAVAE(args.vae_checkpoint)
        self.vae.to(self.device)  # ç§»åŠ¨åˆ°CUDAè®¾å¤‡
        self.vae.eval()
        
        # åˆå§‹åŒ–UNet - å¢å¼ºæ¶æ„æå‡ç‰¹å¾å­¦ä¹ èƒ½åŠ›
        print("ğŸ”§ åˆå§‹åŒ–UNet2Dæ¨¡å‹...")
        self.unet = UNet2DModel(
            sample_size=16,  # latentç©ºé—´å¤§å° 16x16
            in_channels=32,  # VA-VAE latenté€šé“æ•°
            out_channels=32,
            layers_per_block=3,  # å¢åŠ å±‚æ•°
            block_out_channels=(128, 256, 512, 768),  # å¢å¼ºå®¹é‡
            down_block_types=(
                "DownBlock2D",
                "DownBlock2D", 
                "DownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "UpBlock2D",
                "UpBlock2D", 
                "UpBlock2D",
            ),
        ).to(self.device)
        
        # å™ªå£°è°ƒåº¦å™¨
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule="linear",
            prediction_type="epsilon"  # é¢„æµ‹å™ªå£°
        )
        
        # ä¼˜åŒ–å™¨ - é™ä½å­¦ä¹ ç‡æå‡ç»†èŠ‚å­¦ä¹ 
        self.optimizer = torch.optim.AdamW(
            self.unet.parameters(),
            lr=args.learning_rate * 0.5,  # é™ä½å­¦ä¹ ç‡
            weight_decay=args.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # æ•°æ®åŠ è½½å™¨
        print("ğŸ“Š å‡†å¤‡æ•°æ®...")
        self.train_loader = self.prepare_dataloader('train')
        self.val_loader = self.prepare_dataloader('val')
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.lr_scheduler = get_cosine_schedule_with_warmup(
            optimizer=self.optimizer,
            num_warmup_steps=args.warmup_steps,
            num_training_steps=len(self.train_loader) * args.num_epochs
        )
        
        # åˆ†å¸ƒå¯¹é½å‚æ•°
        self.use_distribution_alignment = False
        self.latent_mean = None
        self.latent_std = None
        
    def prepare_dataloader(self, split='train'):
        """å‡†å¤‡æ•°æ®åŠ è½½å™¨ - æ”¯æŒé¢„ç¼–ç latentå’Œå›¾åƒ"""
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„ç¼–ç latentæ–‡ä»¶
        latent_file = None
        if hasattr(self.args, 'latent_dir') and self.args.latent_dir:
            latent_file = Path(self.args.latent_dir) / f"{split}_latents.pt"
            
        # åˆ›å»ºfallbackå›¾åƒæ•°æ®é›†
        image_dataset = MicrodopplerDataset(
            root_dir=self.args.image_dir,
            split_file=self.args.split_file,
            split=split,
            transform=None,
            return_user_id=False,
            image_size=256
        )
        
        # ä½¿ç”¨æ··åˆæ•°æ®é›†ï¼ˆä¼˜å…ˆlatentï¼Œfallbackå›¾åƒï¼‰
        dataset = MixedLatentDataset(
            latent_file=latent_file,
            image_dataset=image_dataset,
            return_user_id=False
        )
        
        # æ£€æµ‹æ•°æ®é›†ç±»å‹ï¼ˆåªåœ¨è®­ç»ƒé›†æ—¶è®¾ç½®ï¼‰
        if split == 'train':
            self.use_preencoded_latents = hasattr(dataset, 'use_preencoded') and dataset.use_preencoded
            if self.use_preencoded_latents:
                print("ğŸš€ ä½¿ç”¨é¢„ç¼–ç latentè®­ç»ƒ - æ˜¾è‘—åŠ é€Ÿï¼")
            else:
                print("ğŸ“Š ä½¿ç”¨å›¾åƒè®­ç»ƒ - å®æ—¶ç¼–ç ")
        
        return DataLoader(
            dataset,
            batch_size=self.args.batch_size,
            shuffle=(split == 'train'),  # åªæœ‰è®­ç»ƒé›†æ‰“ä¹±
            num_workers=4 if not getattr(self, 'use_preencoded_latents', False) else 0,
            pin_memory=True
        )
    
    def detect_distribution_alignment(self, latents):
        """æ£€æµ‹å¹¶é…ç½®åˆ†å¸ƒå¯¹é½ - ä½¿ç”¨channel-wiseå½’ä¸€åŒ–"""
        with torch.no_grad():
            # Channel-wiseç»Ÿè®¡ï¼ˆLightningDiTå®˜æ–¹æ–¹æ³•ï¼‰
            # latents shape: [B, C=32, H=16, W=16]
            # è®¡ç®—æ¯ä¸ªé€šé“çš„meanå’Œstd
            self.latent_mean = latents.mean(dim=[0, 2, 3], keepdim=True)  # [1, 32, 1, 1]
            self.latent_std = latents.std(dim=[0, 2, 3], keepdim=True)    # [1, 32, 1, 1]
            
            # å…¨å±€ç»Ÿè®¡ç”¨äºæ˜¾ç¤º
            global_mean = latents.mean().item()
            global_std = latents.std().item()
            
            print(f"\nğŸ“Š Latentåˆ†å¸ƒåˆ†æ:")
            print(f"   å…¨å±€ Mean: {global_mean:.6f}")
            print(f"   å…¨å±€ Std: {global_std:.6f}")
            print(f"   Range: [{latents.min().item():.2f}, {latents.max().item():.2f}]")
            print(f"ğŸ”§ ä½¿ç”¨Channel-wiseå½’ä¸€åŒ–ï¼ˆLightningDiTå®˜æ–¹æ–¹æ³•ï¼‰")
            print(f"   ç­–ç•¥: æ¯ä¸ªé€šé“ç‹¬ç«‹å½’ä¸€åŒ–ï¼Œä¿æŒé€šé“é—´ç›¸å¯¹å…³ç³»")
            
            # éªŒè¯å½’ä¸€åŒ–æ•ˆæœ
            normalized = (latents - self.latent_mean) / self.latent_std
            print(f"   å½’ä¸€åŒ–å: mean={normalized.mean().item():.6f}, std={normalized.std().item():.6f}")
                
    def normalize_latents(self, latents):
        """å½’ä¸€åŒ–latents"""
        if not self.use_distribution_alignment:
            return latents
        return (latents - self.latent_mean) / self.latent_std
    
    def denormalize_latents(self, latents):
        """åå½’ä¸€åŒ–latents"""
        if not self.use_distribution_alignment:
            return latents
        return latents * self.latent_std + self.latent_mean
    
    def train_step(self, batch):
        """å•ä¸ªè®­ç»ƒæ­¥éª¤ - æ”¯æŒé¢„ç¼–ç latentå’Œå›¾åƒ"""
        data = batch[0].to(self.device)  # å¯èƒ½æ˜¯å›¾åƒæˆ–latent
        
        if self.use_preencoded_latents:
            # ç›´æ¥ä½¿ç”¨é¢„ç¼–ç latent
            latents = data  # å·²ç»æ˜¯latentæ ¼å¼ [B, 32, 16, 16]
        else:
            # VAEç¼–ç å›¾åƒ
            with torch.no_grad():
                # æ ¼å¼è½¬æ¢ï¼šBHWC -> BCHW
                if data.dim() == 4 and data.shape[-1] == 3:  # BHWCæ ¼å¼
                    data = data.permute(0, 3, 1, 2)  # è½¬ä¸ºBCHW
                
                latents = self.vae.encode(data)
        
        # Channel-wiseå½’ä¸€åŒ–ï¼ˆä¿æŒè¯­ä¹‰ç©ºé—´ï¼‰
        # ä½¿ç”¨.to(self.device)ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        latents = (latents - self.latent_mean.to(self.device)) / self.latent_std.to(self.device)
        
        # é‡‡æ ·å™ªå£°å’Œæ—¶é—´æ­¥
        noise = torch.randn_like(latents)
        timesteps = torch.randint(
            0, self.noise_scheduler.config.num_train_timesteps,
            (latents.shape[0],), device=self.device
        )
        
        # æ·»åŠ å™ªå£°
        noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)
        
        # é¢„æµ‹å™ªå£°
        noise_pred = self.unet(noisy_latents, timesteps).sample
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(noise_pred, noise)
        
        return loss
    
    def validate(self):
        """éªŒè¯é›†è¯„ä¼°"""
        self.unet.eval()
        val_losses = []
        
        with torch.no_grad():
            for batch in self.val_loader:
                loss = self.train_step(batch)
                val_losses.append(loss.item())  # è½¬æ¢ä¸ºPythonæ ‡é‡
        
        return np.mean(val_losses)
    
    def generate_samples(self, num_samples=2, num_inference_steps=100):
        """ç”Ÿæˆæ ·æœ¬"""
        self.unet.eval()
        
        # åˆå§‹å™ªå£° - å§‹ç»ˆä»æ ‡å‡†æ­£æ€å¼€å§‹ï¼Œè®­ç»ƒæ—¶å½’ä¸€åŒ–ç¡®ä¿ä¸€è‡´æ€§
        print(f"ğŸ“Š ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒå¼€å§‹å»å™ª: N(0, 1)")
        latents = torch.randn(
            num_samples, 32, 16, 16, device=self.device
        )
        
        # DDIMæ¨ç†
        ddim_scheduler = DDIMScheduler.from_config(self.noise_scheduler.config)
        ddim_scheduler.set_timesteps(num_inference_steps)
        
        for timestep in tqdm(ddim_scheduler.timesteps, desc="ç”Ÿæˆä¸­"):
            timestep_batch = timestep.repeat(num_samples).to(self.device)
            
            # é¢„æµ‹å™ªå£°
            noise_pred = self.unet(latents, timestep_batch).sample
            
            # å»å™ª
            latents = ddim_scheduler.step(noise_pred, timestep, latents).prev_sample
        
        # Channel-wiseåå½’ä¸€åŒ–åˆ°åŸå§‹latentç©ºé—´
        latents = latents * self.latent_std.to(self.device) + self.latent_mean.to(self.device)
        
        # VAEè§£ç  - åŒ¹é…VA-VAEçš„è°ƒç”¨æ–¹å¼
        images = self.vae.decode(latents)
        
        # ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®
        if images.dim() == 4 and images.shape[1] == 3:  # BCHWæ ¼å¼
            # ä¿æŒBCHWæ ¼å¼ç”¨äºåç»­å¤„ç†
            pass
        
        return images
    
    def save_samples(self, images, epoch, save_dir):
        """ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬ - åªç”Ÿæˆä¸€å¼ 8æ ·æœ¬ç½‘æ ¼å›¾"""
        import matplotlib.pyplot as plt
        
        os.makedirs(save_dir, exist_ok=True)
        
        # åˆ›å»ºç½‘æ ¼å¯è§†åŒ– - å‡å°‘åˆ°2ä¸ªæ ·æœ¬èŠ‚çœå†…å­˜
        num_samples = min(2, len(images))
        fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))
        fig.suptitle(f'Epoch {epoch} - æ‰©æ•£ç”Ÿæˆæ ·æœ¬')
        
        if num_samples == 1:
            axes = [axes]
        
        for i in range(num_samples):
            # VA-VAE decodeå·²ç»è¿”å›[0,1]èŒƒå›´ï¼Œæ— éœ€å†å½’ä¸€åŒ–
            img_tensor = torch.clamp(images[i], 0, 1)
            
            # è½¬æ¢ä¸ºnumpyæ˜¾ç¤ºæ ¼å¼
            img_array = img_tensor.cpu().numpy()
            if img_array.shape[0] == 3:  # RGB - BCHWæ ¼å¼
                img_array = np.transpose(img_array, (1, 2, 0))  # CHW -> HWC
            elif img_array.shape[0] == 1:  # ç°åº¦
                img_array = img_array.squeeze(0)
                
            axes[i].imshow(img_array, cmap='gray' if len(img_array.shape) == 2 else None)
            axes[i].set_title(f'æ ·æœ¬ {i+1}')
            axes[i].axis('off')
        
        # åªä¿å­˜ç½‘æ ¼å¯è§†åŒ–å›¾åƒ
        plt.tight_layout()
        plt.savefig(f"{save_dir}/epoch_{epoch:03d}_samples.png", dpi=150, bbox_inches='tight')
        plt.close()
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {self.device}")
        
        # é¦–æ¬¡æ£€æµ‹åˆ†å¸ƒå¯¹é½
        first_batch = next(iter(self.train_loader))
        with torch.no_grad():
            sample_data = first_batch[0][:4].to(self.device)
            
            if self.use_preencoded_latents:
                # ç›´æ¥ä½¿ç”¨é¢„ç¼–ç latent
                sample_latents = sample_data
            else:
                # VAEç¼–ç å›¾åƒ
                # æ ¼å¼è½¬æ¢ï¼šBHWC -> BCHW
                if sample_data.dim() == 4 and sample_data.shape[-1] == 3:  # BHWCæ ¼å¼
                    sample_data = sample_data.permute(0, 3, 1, 2)  # è½¬ä¸ºBCHW
                
                sample_latents = self.vae.encode(sample_data)
            
            self.detect_distribution_alignment(sample_latents)
        
        # è®­ç»ƒå¾ªç¯
        global_step = 0
        
        for epoch in range(self.args.num_epochs):
            self.unet.train()
            epoch_losses = []
            
            pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.args.num_epochs}")
            
            for batch in pbar:
                # è®­ç»ƒæ­¥éª¤
                loss = self.train_step(batch)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.unet.parameters(), 1.0)
                
                self.optimizer.step()
                self.lr_scheduler.step()
                
                # è®°å½•
                epoch_losses.append(loss.item())
                pbar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'lr': f"{self.lr_scheduler.get_last_lr()[0]:.6f}"
                })
                
                global_step += 1
            
            avg_loss = np.mean(epoch_losses)
            print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.args.save_freq == 0:
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': self.unet.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.lr_scheduler.state_dict(),
                    'use_distribution_alignment': self.use_distribution_alignment,
                    'latent_mean': self.latent_mean,
                    'latent_std': self.latent_std,
                }
                
                save_path = f"checkpoints/diffusion_epoch_{epoch+1:03d}.pt"
                os.makedirs("checkpoints", exist_ok=True)
                torch.save(checkpoint, save_path)
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {save_path}")
            
            # éªŒè¯é›†è¯„ä¼°
            val_loss = self.validate()
            print(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.4f}")
            
            # æ¯è½®ç”Ÿæˆæ ·æœ¬ - è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨
            print("ğŸ¨ ç”Ÿæˆæ ·æœ¬...")
            sample_images = self.generate_samples(num_samples=2)
            self.save_samples(sample_images, epoch+1, "samples")
            print(f"âœ… Epoch {epoch+1} å®Œæˆ - è®­ç»ƒæŸå¤±: {avg_loss:.4f}, éªŒè¯æŸå¤±: {val_loss:.4f}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--image_dir', type=str, required=True,
                       help='å›¾åƒæ•°æ®ç›®å½•')
    parser.add_argument('--vae_checkpoint', type=str, required=True,
                       help='VAEæ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--split_file', type=str, required=True,
                       help='æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶')
    parser.add_argument('--batch_size', type=int, default=4,
                       help='æ‰¹æ¬¡å¤§å° - åŒ¹é…VA-VAEé»˜è®¤å€¼')
    parser.add_argument('--num_epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡ - åŒ¹é…VA-VAE Stage1')
    parser.add_argument('--weight_decay', type=float, default=1e-6,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--warmup_steps', type=int, default=1000,
                       help='é¢„çƒ­æ­¥æ•°')
    parser.add_argument('--save_freq', type=int, default=10,
                       help='ä¿å­˜é¢‘ç‡')
    parser.add_argument('--latent_dir', type=str, default=None,
                       help='é¢„ç¼–ç latentç›®å½•ï¼ˆå¯é€‰ï¼Œä½¿ç”¨åæ˜¾è‘—åŠ é€Ÿè®­ç»ƒï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = DiffusersTrainer(args)
    trainer.train()
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()

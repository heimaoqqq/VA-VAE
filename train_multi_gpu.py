#!/usr/bin/env python3
"""
å¤šGPUè®­ç»ƒè„šæœ¬
ä¸“é—¨ç”¨äºç¡®ä¿æ­£ç¡®çš„å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
"""

import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import pytorch_lightning as pl
from pytorch_lightning.strategies import DDPStrategy

def setup_multi_gpu_training():
    """è®¾ç½®å¤šGPUè®­ç»ƒç¯å¢ƒ"""
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå¤šGPUè®­ç»ƒ")
        return False
    
    gpu_count = torch.cuda.device_count()
    if gpu_count < 2:
        print(f"âŒ åªæœ‰{gpu_count}ä¸ªGPUï¼Œæ— æ³•è¿›è¡Œå¤šGPUè®­ç»ƒ")
        return False
    
    print(f"âœ… æ£€æµ‹åˆ°{gpu_count}ä¸ªGPUï¼Œå¯ä»¥è¿›è¡Œå¤šGPUè®­ç»ƒ")
    
    # æ˜¾ç¤ºGPUä¿¡æ¯
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    return True

def create_ddp_trainer(args, callbacks):
    """åˆ›å»ºDDPè®­ç»ƒå™¨"""
    
    # è®¾ç½®DDPç­–ç•¥
    ddp_strategy = DDPStrategy(
        process_group_backend="nccl",  # ä½¿ç”¨NCCLåç«¯
        find_unused_parameters=False,   # æé«˜æ€§èƒ½
        static_graph=True,             # é™æ€å›¾ä¼˜åŒ–
        ddp_comm_hook=None,            # å¯ä»¥æ·»åŠ é€šä¿¡é’©å­
    )
    
    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        devices=2,  # æ˜ç¡®æŒ‡å®šä½¿ç”¨2ä¸ªGPU
        accelerator='gpu',
        strategy=ddp_strategy,
        precision=args.precision,
        callbacks=callbacks,
        log_every_n_steps=50,
        val_check_interval=1.0,
        enable_progress_bar=True,
        enable_model_summary=True,
        default_root_dir=args.output_dir,
        sync_batchnorm=True,  # åŒæ­¥BatchNorm
        # æ·»åŠ DDPç›¸å…³é…ç½®
        replace_sampler_ddp=True,  # è‡ªåŠ¨æ›¿æ¢é‡‡æ ·å™¨
    )
    
    return trainer

def monitor_gpu_usage():
    """ç›‘æ§GPUä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        return
    
    print("\nğŸ“Š GPUä½¿ç”¨æƒ…å†µ:")
    for i in range(torch.cuda.device_count()):
        memory_used = torch.cuda.memory_allocated(i) / 1e9
        memory_cached = torch.cuda.memory_reserved(i) / 1e9
        memory_total = torch.cuda.get_device_properties(i).total_memory / 1e9
        utilization = (memory_used / memory_total) * 100
        
        print(f"  GPU {i}: {memory_used:.2f}GB / {memory_total:.1f}GB ({utilization:.1f}%)")
        print(f"    ç¼“å­˜: {memory_cached:.2f}GB")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¤šGPU DiTè®­ç»ƒ')
    parser.add_argument('--latent_dir', type=str, required=True, help='æ½œåœ¨ç‰¹å¾ç›®å½•')
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å° (æ¯ä¸ªGPU)')
    parser.add_argument('--max_epochs', type=int, default=100, help='æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--precision', default='16-mixed', help='ç²¾åº¦')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    print("ğŸ¯ å¤šGPU DiTè®­ç»ƒ")
    print("=" * 50)
    
    # æ£€æŸ¥å¤šGPUç¯å¢ƒ
    if not setup_multi_gpu_training():
        print("âŒ å¤šGPUç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(args.seed)
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    from stage2_train_dit import MicroDopplerDataModule, UserConditionedDiT
    from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    data_module = MicroDopplerDataModule(
        train_latent_file=os.path.join(args.latent_dir, 'train.safetensors'),
        val_latent_file=os.path.join(args.latent_dir, 'val.safetensors'),
        batch_size=args.batch_size,  # æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°
        num_workers=4
    )
    
    # è®¾ç½®æ•°æ®æ¨¡å—
    data_module.setup()
    
    # åˆ›å»ºæ¨¡å‹
    model = UserConditionedDiT(
        num_users=data_module.num_users,
        lr=args.lr
    )
    
    # è®¾ç½®å›è°ƒ
    callbacks = [
        ModelCheckpoint(
            dirpath=os.path.join(args.output_dir, 'checkpoints'),
            filename='dit-multi-gpu-{epoch:02d}-{val/loss:.4f}',
            monitor='val/loss',
            mode='min',
            save_top_k=3,
            save_last=True
        ),
        LearningRateMonitor(logging_interval='epoch')
    ]
    
    # åˆ›å»ºDDPè®­ç»ƒå™¨
    trainer = create_ddp_trainer(args, callbacks)
    
    print("ğŸš€ å¼€å§‹å¤šGPUè®­ç»ƒ...")
    print(f"  æ€»æ‰¹æ¬¡å¤§å°: {args.batch_size * 2} (æ¯GPU: {args.batch_size})")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.fit(model, data_module)
    
    # è®­ç»ƒå®Œæˆåç›‘æ§GPUä½¿ç”¨
    monitor_gpu_usage()
    
    print("âœ… å¤šGPUè®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main()

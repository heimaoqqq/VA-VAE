"""
è°ƒè¯•generation_filtering.pyçš„æ¯ä¸ªç­›é€‰æ­¥éª¤
æ‰¾å‡º0.2%é€šè¿‡ç‡çš„çœŸæ­£åŸå› 
"""

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
from collections import defaultdict

def load_classifier(model_path, device):
    """åŠ è½½åˆ†ç±»å™¨ï¼ˆå¤åˆ¶è‡ªgeneration_filtering.pyï¼‰"""
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    
    if 'feature_projector.0.weight' in checkpoint['model_state_dict']:
        from train_calibrated_classifier import DomainAdaptiveClassifier
        model = DomainAdaptiveClassifier(
            num_classes=checkpoint['num_classes'],
            dropout_rate=0.3,
            feature_dim=512
        )
    else:
        from improved_classifier_training import ImprovedClassifier
        model = ImprovedClassifier(
            num_classes=checkpoint['num_classes'],
            backbone='resnet18',
            dropout_rate=0.5,
            freeze_layers='minimal'
        )
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    return model

def simple_quality_check(images):
    """ç®€åŒ–çš„å›¾åƒè´¨é‡æ£€æŸ¥ï¼ˆå¤åˆ¶è‡ªgeneration_filtering.pyï¼‰"""
    quality_scores = []
    
    for img in images:
        img_array = np.array(img)
        
        # åªæ£€æµ‹åŸºæœ¬åƒç´ å€¼å¼‚å¸¸
        pixel_mean = np.mean(img_array)
        pixel_std = np.std(img_array)
        
        # ç®€å•çš„è´¨é‡åˆ†æ•°ï¼šåªæ£€æŸ¥æ˜¯å¦å…¨é»‘/å…¨ç™½æˆ–æ— å˜åŒ–
        is_valid = (
            10 < pixel_mean < 245 and  # ä¸æ˜¯å…¨é»‘æˆ–å…¨ç™½
            pixel_std > 5              # æœ‰ä¸€å®šå˜åŒ–
        )
        
        quality_score = {
            'pixel_mean': pixel_mean,
            'pixel_std': pixel_std,
            'is_valid': is_valid,
            'overall': 1.0 if is_valid else 0.0
        }
        
        quality_scores.append(quality_score)
    
    return quality_scores

def compute_user_specific_metrics_debug(images, classifier, user_id, device):
    """è°ƒè¯•ç‰ˆæœ¬çš„ç”¨æˆ·æŒ‡æ ‡è®¡ç®—"""
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    user_metrics_list = []
    
    for img in images:
        img_tensor = transform(img).unsqueeze(0).to(device)
        
        with torch.no_grad():
            # è·å–åˆ†ç±»å™¨è¾“å‡ºå’Œç‰¹å¾
            outputs = classifier(img_tensor)
            
            # å¤„ç†åˆ†ç±»å™¨è¾“å‡ºæ ¼å¼ï¼ˆå¯èƒ½æ˜¯tupleï¼‰
            if isinstance(outputs, tuple):
                logits = outputs[0]  # é€šå¸¸ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯logits
            else:
                logits = outputs
            
            probs = F.softmax(logits, dim=1)
            features = classifier.backbone(img_tensor)
            
            # 1. åŸºæœ¬æŒ‡æ ‡
            confidence, pred = torch.max(probs, dim=1)
            sorted_probs, _ = torch.sort(probs, dim=1, descending=True)
            margin = (sorted_probs[0, 0] - sorted_probs[0, 1]).item()
            
            # 2. ç”¨æˆ·ç‰¹å¼‚æ€§åˆ†æ•°ï¼ˆä¸å…¶ä»–ç”¨æˆ·çš„åŒºåˆ†åº¦ï¼‰
            user_prob = probs[0, user_id].item()
            other_probs = torch.cat([probs[0, :user_id], probs[0, user_id+1:]])
            max_other_prob = torch.max(other_probs).item()
            # ä½¿ç”¨æ¯”ä¾‹è€Œéå·®å€¼
            user_specificity = user_prob / (user_prob + max_other_prob) if (user_prob + max_other_prob) > 0 else 0.0
            
            metrics = {
                'predicted': pred.item(),
                'confidence': confidence.item(),
                'margin': margin,
                'user_specificity': user_specificity,
                'correct': pred.item() == user_id,
                'features': features.cpu().numpy().flatten()
            }
            
            user_metrics_list.append(metrics)
    
    return user_metrics_list

def debug_filtering_steps(samples_dir, classifier_path, device='cuda:0', user_id=0, num_samples=100):
    """é€æ­¥è°ƒè¯•ç­›é€‰è¿‡ç¨‹"""
    
    device = torch.device(device if torch.cuda.is_available() else 'cpu')
    
    print(f"ğŸ” è°ƒè¯•ç”¨æˆ·{user_id}çš„ç­›é€‰è¿‡ç¨‹")
    print(f"ğŸ“‚ æ ·æœ¬ç›®å½•: {samples_dir}")
    
    # åŠ è½½åˆ†ç±»å™¨
    classifier = load_classifier(classifier_path, device)
    
    # è·å–æ ·æœ¬
    samples_path = Path(samples_dir) / f"user_{user_id:02d}"
    if not samples_path.exists():
        print(f"âŒ ç”¨æˆ·ç›®å½•ä¸å­˜åœ¨: {samples_path}")
        return
    
    image_files = list(samples_path.glob('*.png'))[:num_samples]
    print(f"ğŸ“· åŠ è½½{len(image_files)}å¼ å›¾åƒè¿›è¡Œè°ƒè¯•")
    
    # åŠ è½½å›¾åƒ
    images = []
    for img_path in image_files:
        try:
            img = Image.open(img_path).convert('RGB')
            images.append(img)
        except:
            continue
    
    if len(images) == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾åƒ")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½{len(images)}å¼ å›¾åƒ")
    
    # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—æŒ‡æ ‡
    print("\nğŸ“Š æ­¥éª¤1: è®¡ç®—å„é¡¹æŒ‡æ ‡")
    metrics_list = compute_user_specific_metrics_debug(images, classifier, user_id, device)
    visual_quality_scores = simple_quality_check(images)
    
    # ç»Ÿè®¡å„é¡¹æŒ‡æ ‡
    stats = defaultdict(list)
    for i, (metrics, visual) in enumerate(zip(metrics_list, visual_quality_scores)):
        stats['correct'].append(metrics['correct'])
        stats['confidence'].append(metrics['confidence'])
        stats['user_specificity'].append(metrics['user_specificity'])
        stats['margin'].append(metrics['margin'])
        stats['visual_valid'].append(visual['is_valid'])
    
    print("æŒ‡æ ‡ç»Ÿè®¡:")
    for key, values in stats.items():
        if key != 'features':
            pass_rate = np.mean(values) * 100
            mean_val = np.mean(values) if key != 'correct' and key != 'visual_valid' else pass_rate/100
            print(f"   {key}: å‡å€¼={mean_val:.3f}, é€šè¿‡ç‡={pass_rate:.1f}%")
    
    # ç¬¬äºŒæ­¥ï¼šåº”ç”¨å„é¡¹é˜ˆå€¼
    print(f"\nğŸ¯ æ­¥éª¤2: åº”ç”¨ç­›é€‰é˜ˆå€¼")
    
    thresholds = {
        'confidence': 0.9,
        'user_specificity': 0.7,  # ä½¿ç”¨æ–°çš„æ¯”ä¾‹æ¨¡å¼
        'margin': 0.8,
        'diversity': 0.1
    }
    
    # ç¬¬ä¸€è½®ç­›é€‰ï¼ˆä¸åŒ…æ‹¬å¤šæ ·æ€§ï¼‰
    first_stage_candidates = []
    filter_stats = defaultdict(int)
    
    for i, (metrics, visual) in enumerate(zip(metrics_list, visual_quality_scores)):
        filter_stats['total'] += 1
        
        # æ£€æŸ¥æ¯ä¸ªæ¡ä»¶
        conditions = {
            'correct': metrics['correct'],
            'confidence': metrics['confidence'] > thresholds['confidence'],
            'user_specificity': metrics['user_specificity'] > thresholds['user_specificity'],
            'margin': metrics['margin'] > thresholds['margin'],
            'visual_valid': visual['is_valid']
        }
        
        # è®°å½•å„ä¸ªæ¡ä»¶çš„é€šè¿‡æƒ…å†µ
        for cond, passed in conditions.items():
            if passed:
                filter_stats[f'{cond}_pass'] += 1
            else:
                filter_stats[f'{cond}_fail'] += 1
        
        # æ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³æ‰è¿›å…¥å€™é€‰
        if all(conditions.values()):
            first_stage_candidates.append({
                'image': images[i],
                'features': metrics['features'],
                'metrics': metrics,
                'index': i
            })
            filter_stats['first_stage_pass'] += 1
    
    print("ç¬¬ä¸€é˜¶æ®µç­›é€‰ç»“æœ:")
    total = filter_stats['total']
    for key in ['correct', 'confidence', 'user_specificity', 'margin', 'visual_valid']:
        pass_count = filter_stats[f'{key}_pass']
        fail_count = filter_stats[f'{key}_fail']
        pass_rate = pass_count / total * 100
        print(f"   {key}: {pass_count}/{total} ({pass_rate:.1f}%)")
    
    first_pass_rate = filter_stats['first_stage_pass'] / total * 100
    print(f"   ğŸ“Š ç¬¬ä¸€é˜¶æ®µæ€»é€šè¿‡ç‡: {filter_stats['first_stage_pass']}/{total} ({first_pass_rate:.1f}%)")
    
    # ç¬¬ä¸‰æ­¥ï¼šå¤šæ ·æ€§ç­›é€‰
    print(f"\nğŸŒˆ æ­¥éª¤3: å¤šæ ·æ€§ç­›é€‰")
    
    if len(first_stage_candidates) == 0:
        print("âŒ ç¬¬ä¸€é˜¶æ®µæ²¡æœ‰å€™é€‰æ ·æœ¬ï¼Œæ— æ³•è¿›è¡Œå¤šæ ·æ€§ç­›é€‰")
        return
    
    from sklearn.metrics.pairwise import cosine_similarity
    
    final_candidates = []
    collected_features = []
    
    for candidate in first_stage_candidates:
        # æ£€æŸ¥ä¸å·²æ”¶é›†æ ·æœ¬çš„å¤šæ ·æ€§
        diversity_score = 1.0  # é»˜è®¤å¤šæ ·æ€§åˆ†æ•°ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
        if len(collected_features) > 0:
            candidate_features = candidate['features'].reshape(1, -1)
            collected_array = np.array(collected_features)
            
            # è®¡ç®—ä¸ç°æœ‰æ ·æœ¬çš„æœ€å¤§ç›¸ä¼¼åº¦
            similarities = cosine_similarity(candidate_features, collected_array)[0]
            max_similarity = np.max(similarities)
            diversity_score = 1.0 - max_similarity
        
        # åº”ç”¨å¤šæ ·æ€§é˜ˆå€¼
        if diversity_score >= thresholds['diversity']:
            final_candidates.append(candidate)
            collected_features.append(candidate['features'])
    
    final_pass_rate = len(final_candidates) / total * 100
    diversity_pass_rate = len(final_candidates) / len(first_stage_candidates) * 100 if len(first_stage_candidates) > 0 else 0
    
    print(f"å¤šæ ·æ€§ç­›é€‰ç»“æœ:")
    print(f"   è¾“å…¥å€™é€‰: {len(first_stage_candidates)}")
    print(f"   æœ€ç»ˆé€šè¿‡: {len(final_candidates)}")
    print(f"   å¤šæ ·æ€§é€šè¿‡ç‡: {diversity_pass_rate:.1f}%")
    print(f"   ğŸ“Š æœ€ç»ˆæ€»é€šè¿‡ç‡: {len(final_candidates)}/{total} ({final_pass_rate:.1f}%)")
    
    # è¯¦ç»†åˆ†æå¤±è´¥åŸå› 
    print(f"\nâŒ å¤±è´¥åŸå› åˆ†æ:")
    fail_reasons = {}
    for key in ['correct', 'confidence', 'user_specificity', 'margin', 'visual_valid']:
        fail_count = filter_stats[f'{key}_fail']
        fail_reasons[key] = fail_count / total * 100
    
    # æŒ‰å¤±è´¥ç‡æ’åº
    sorted_failures = sorted(fail_reasons.items(), key=lambda x: x[1], reverse=True)
    for reason, rate in sorted_failures:
        print(f"   {reason}: {rate:.1f}%æ ·æœ¬å¤±è´¥")
    
    return {
        'total_samples': total,
        'first_stage_pass': filter_stats['first_stage_pass'],
        'final_pass': len(final_candidates),
        'first_pass_rate': first_pass_rate,
        'final_pass_rate': final_pass_rate,
        'filter_stats': filter_stats,
        'fail_reasons': fail_reasons
    }

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--samples_dir', type=str, default='/kaggle/working/VA-VAE/generated_samples2')
    parser.add_argument('--classifier_path', type=str, default='/kaggle/working/VA-VAE/domain_adaptive_classifier/best_calibrated_model.pth')
    parser.add_argument('--device', type=str, default='cuda:0')
    parser.add_argument('--user_id', type=int, default=0)
    parser.add_argument('--num_samples', type=int, default=100)
    
    args = parser.parse_args()
    
    debug_filtering_steps(
        args.samples_dir, 
        args.classifier_path, 
        args.device, 
        args.user_id,
        args.num_samples
    )

if __name__ == "__main__":
    main()

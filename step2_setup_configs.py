#!/usr/bin/env python3
"""
æ­¥éª¤2: è®¾ç½®é…ç½®æ–‡ä»¶
ä¸¥æ ¼æŒ‰ç…§LightningDiT READMEå’Œtutorialæ­¥éª¤
"""

import os
import yaml
from pathlib import Path

def create_inference_config():
    """åˆ›å»ºæ¨ç†é…ç½®æ–‡ä»¶ - åŸºäºå®˜æ–¹reproductionsé…ç½®"""
    
    config_path = "inference_config.yaml"
    models_dir = Path("./official_models")
    
    # åŸºäºå®˜æ–¹configs/reproductions/lightningdit_xl_vavae_f16d32_800ep_cfg.yaml
    # æ³¨æ„ï¼šæ¨ç†è„šæœ¬åœ¨LightningDiT/ç›®å½•ä¸‹è¿è¡Œï¼Œæ‰€ä»¥è·¯å¾„éœ€è¦ç›¸å¯¹äºLightningDiT/
    config = {
        'ckpt_path': str(Path("..") / models_dir / "lightningdit-xl-imagenet256-800ep.pt"),
        'data': {
            'data_path': str(Path("..") / models_dir),  # æŒ‡å‘ç›®å½•ï¼Œä¸æ˜¯æ–‡ä»¶
            'fid_reference_file': 'path/to/your/VIRTUAL_imagenet256_labeled.npz',
            'image_size': 256,
            'num_classes': 1000,
            'num_workers': 8,
            'latent_norm': True,
            'latent_multiplier': 1.0
        },
        'vae': {
            'model_name': 'vavae_f16d32',
            'downsample_ratio': 16
        },
        'model': {
            'model_type': 'LightningDiT-XL/1',
            'use_qknorm': False,
            'use_swiglu': True,
            'use_rope': True,
            'use_rmsnorm': True,
            'wo_shift': False,
            'in_chans': 32
        },
        'train': {
            'max_steps': 80000,
            'global_batch_size': 1024,
            'global_seed': 0,  # è¿™æ˜¯ç¼ºå¤±çš„å…³é”®å­—æ®µ
            'output_dir': 'output',
            'exp_name': 'lightningdit_xl_vavae_f16d32',
            'ckpt': None,
            'log_every': 100,
            'ckpt_every': 20000
        },
        'optimizer': {
            'lr': 0.0002,
            'beta2': 0.95
        },
        'transport': {
            'path_type': 'Linear',
            'prediction': 'velocity',
            'loss_weight': None,
            'sample_eps': None,
            'train_eps': None,
            'use_cosine_loss': True,
            'use_lognorm': True
        },
        'sample': {
            'mode': 'ODE',
            'sampling_method': 'euler',
            'atol': 0.000001,
            'rtol': 0.001,
            'reverse': False,
            'likelihood': False,
            'num_sampling_steps': 250,
            'cfg_scale': 6.7,  # å®˜æ–¹800epæ¨¡å‹ä½¿ç”¨6.7
            'per_proc_batch_size': 4,
            'fid_num': 50000,  # ç¼ºå¤±çš„å…³é”®å­—æ®µ
            'cfg_interval_start': 0.125,
            'timestep_shift': 0.3
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)
    
    print(f"âœ… æ¨ç†é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
    return config_path

def update_vavae_config():
    """æ›´æ–°VA-VAEé…ç½® - å®˜æ–¹tutorialè¦æ±‚çš„æ­¥éª¤"""

    print("\nğŸ”§ æ›´æ–°VA-VAEé…ç½®...")

    vavae_config_path = "LightningDiT/tokenizer/configs/vavae_f16d32.yaml"
    models_dir = Path("./official_models")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(vavae_config_path):
        print(f"âŒ VA-VAEé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {vavae_config_path}")
        print("ğŸ” æ£€æŸ¥LightningDiTç›®å½•ç»“æ„...")
        if os.path.exists("LightningDiT"):
            print("âœ… LightningDiTç›®å½•å­˜åœ¨")
            if os.path.exists("LightningDiT/tokenizer"):
                print("âœ… tokenizerç›®å½•å­˜åœ¨")
                if os.path.exists("LightningDiT/tokenizer/configs"):
                    print("âœ… configsç›®å½•å­˜åœ¨")
                    print("ğŸ“ configsç›®å½•å†…å®¹:")
                    for f in os.listdir("LightningDiT/tokenizer/configs"):
                        print(f"   - {f}")
                else:
                    print("âŒ configsç›®å½•ä¸å­˜åœ¨")
            else:
                print("âŒ tokenizerç›®å½•ä¸å­˜åœ¨")
        else:
            print("âŒ LightningDiTç›®å½•ä¸å­˜åœ¨")
        return False

    try:
        # è¯»å–ç°æœ‰é…ç½®
        with open(vavae_config_path, 'r') as f:
            config = yaml.safe_load(f)

        # æ›´æ–°æ£€æŸ¥ç‚¹è·¯å¾„ (å®˜æ–¹tutorialæ­¥éª¤)
        # æ³¨æ„ï¼šVA-VAEé…ç½®ä¹Ÿéœ€è¦ç›¸å¯¹äºLightningDiT/ç›®å½•çš„è·¯å¾„
        old_path = config.get('ckpt_path', 'N/A')
        new_path = str(Path("..") / models_dir / "vavae-imagenet256-f16d32-dinov2.pt")
        config['ckpt_path'] = new_path

        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        with open(vavae_config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False, indent=2)

        print(f"âœ… VA-VAEé…ç½®å·²æ›´æ–°:")
        print(f"   æ—§è·¯å¾„: {old_path}")
        print(f"   æ–°è·¯å¾„: {new_path}")

        return True

    except Exception as e:
        print(f"âŒ æ›´æ–°VA-VAEé…ç½®å¤±è´¥: {e}")
        return False

def main():
    """æ­¥éª¤2: è®¾ç½®é…ç½®æ–‡ä»¶"""
    
    print("âš™ï¸ æ­¥éª¤2: è®¾ç½®é…ç½®æ–‡ä»¶")
    print("=" * 50)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    models_dir = Path("./official_models")
    required_files = [
        "vavae-imagenet256-f16d32-dinov2.pt",
        "lightningdit-xl-imagenet256-800ep.pt",
        "latents_stats.pt"
    ]
    
    missing_files = []
    for filename in required_files:
        if not (models_dir / filename).exists():
            missing_files.append(filename)
    
    if missing_files:
        print("âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶:")
        for filename in missing_files:
            print(f"   - {filename}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python step1_download_models.py")
        return
    
    print("âœ… æ‰€æœ‰æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨")
    
    # åˆ›å»ºæ¨ç†é…ç½®æ–‡ä»¶
    print("\nğŸ“ åˆ›å»ºæ¨ç†é…ç½®æ–‡ä»¶...")
    config_path = create_inference_config()
    
    # æ›´æ–°VA-VAEé…ç½®
    vavae_success = update_vavae_config()

    print("\nâœ… æ­¥éª¤2å®Œæˆï¼é…ç½®æ–‡ä»¶å·²è®¾ç½®")
    print(f"ğŸ“„ æ¨ç†é…ç½®: {config_path}")
    if vavae_success:
        print("ğŸ“„ VA-VAEé…ç½®: LightningDiT/tokenizer/configs/vavae_f16d32.yaml")
    else:
        print("âš ï¸ VA-VAEé…ç½®æ›´æ–°å¤±è´¥ï¼Œä½†å¯ä»¥ç»§ç»­å°è¯•æ¨ç†")
    print("\nğŸ¯ ä¸‹ä¸€æ­¥: è¿è¡Œ python step3_run_inference.py")

if __name__ == "__main__":
    main()

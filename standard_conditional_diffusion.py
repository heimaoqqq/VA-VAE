"""
æ ‡å‡†æ¡ä»¶æ‰©æ•£æ¨¡å‹ - åŸºäºDiffusersæ ‡å‡†å®ç°
å‚è€ƒï¼šhuggingface/diffusers/examples/unconditional_image_generation/train_unconditional.py
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers import UNet2DConditionModel, DDPMScheduler, DDIMScheduler
from tqdm import tqdm


class PrototypicalEncoder(nn.Module):
    """åŸå‹ç¼–ç å™¨ - ç”¨äºå­¦ä¹ ç”¨æˆ·ç‰¹å¾"""
    def __init__(self, input_dim, prototype_dim=768):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(input_dim, 2048),
            nn.ReLU(),
            nn.Linear(2048, prototype_dim),
            nn.LayerNorm(prototype_dim)
        )
    
    def forward(self, x):
        return self.encoder(x)


class ContrastiveLearning(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å—"""
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, predicted_latents, target_latents, user_ids):
        # ç®€åŒ–çš„å¯¹æ¯”æŸå¤±
        batch_size = predicted_latents.size(0)
        
        # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µ
        pred_flat = predicted_latents.view(batch_size, -1)
        target_flat = target_latents.view(batch_size, -1)
        
        # L2 normalize
        pred_norm = F.normalize(pred_flat, p=2, dim=1)
        target_norm = F.normalize(target_flat, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = torch.matmul(pred_norm, target_norm.T) / self.temperature
        
        # InfoNCEæŸå¤±
        labels = torch.arange(batch_size, device=similarity.device)
        loss = F.cross_entropy(similarity, labels)
        
        return loss


class StandardConditionalDiffusion(nn.Module):
    """æ ‡å‡†æ¡ä»¶æ‰©æ•£æ¨¡å‹ - åŸºäºDiffusers"""
    
    def __init__(self, vae, num_users=31, prototype_dim=768):
        super().__init__()
        
        # VAEï¼ˆå†»ç»“ï¼‰
        self.vae = vae
        for param in self.vae.parameters():
            param.requires_grad = False
        
        # ğŸ”§ æ¢å¤ç¨³å®šUNetæ¶æ„ - ä¿®å¤ç”Ÿæˆçˆ†ç‚¸
        self.unet = UNet2DConditionModel(
            sample_size=16,  # VAE latent size
            in_channels=32,  # VAE latent channels
            out_channels=32,
            layers_per_block=2,  # æ¢å¤è¶³å¤Ÿæ·±åº¦
            block_out_channels=(128, 256, 512),  # å¢åŠ å®¹é‡é˜²æ­¢ä¸ç¨³å®š
            down_block_types=(
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
            ),
            up_block_types=(
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
            ),
            cross_attention_dim=prototype_dim,
            attention_head_dim=8,
        )
        
        # å™ªå£°è°ƒåº¦å™¨
        self.scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule="linear",
            prediction_type="epsilon"
        )
        
        # ç”¨æˆ·åŸå‹ç³»ç»Ÿ
        self.num_users = num_users
        self.prototype_dim = prototype_dim
        self.user_prototypes = nn.ParameterDict()
        
        # å¯¹æ¯”å­¦ä¹ 
        self.contrastive = ContrastiveLearning()
        
        print("âœ… ä½¿ç”¨æ ‡å‡†æ‰©æ•£æ¨¡å‹ - æ— è‡ªå®šä¹‰æ ‡å‡†åŒ–")
    
    def get_user_condition(self, user_ids):
        """è·å–ç”¨æˆ·æ¡ä»¶ç¼–ç """
        batch_size = len(user_ids)
        device = next(self.parameters()).device
        
        conditions = torch.zeros(batch_size, 1, self.prototype_dim, device=device)
        
        for i, user_id in enumerate(user_ids):
            user_id_int = user_id.item() if torch.is_tensor(user_id) else user_id
            user_key = str(user_id_int)
            
            if user_key not in self.user_prototypes:
                # åˆå§‹åŒ–æ–°ç”¨æˆ·åŸå‹
                self.user_prototypes[user_key] = nn.Parameter(
                    torch.randn(1, self.prototype_dim, device=device) * 0.1
                )
            
            conditions[i] = self.user_prototypes[user_key]
        
        return conditions
    
    def update_user_prototypes(self, user_latents):
        """æ›´æ–°ç”¨æˆ·åŸå‹ - åŸºäºlatentç‰¹å¾"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨latentçš„å…¨å±€æ± åŒ–ä½œä¸ºç‰¹å¾
        for user_id, latents in user_latents.items():
            user_key = str(user_id)
            if user_key in self.user_prototypes:
                # latents shape: [N, 32, 16, 16] -> æå–å…¨å±€ç‰¹å¾ [N, feature_dim]
                # å…¨å±€å¹³å‡æ± åŒ– + æŠ•å½±åˆ°åŸå‹ç©ºé—´
                pooled_features = latents.mean(dim=[2, 3])  # [N, 32]
                
                # ç®€å•çº¿æ€§æŠ•å½±åˆ°åŸå‹ç»´åº¦ (å¦‚æœéœ€è¦)
                if pooled_features.size(-1) != self.prototype_dim:
                    # é‡å¤æˆ–æˆªæ–­åˆ°åŒ¹é…åŸå‹ç»´åº¦
                    if pooled_features.size(-1) < self.prototype_dim:
                        # é‡å¤å¡«å……
                        repeat_times = self.prototype_dim // pooled_features.size(-1)
                        remainder = self.prototype_dim % pooled_features.size(-1)
                        expanded = pooled_features.repeat(1, repeat_times)
                        if remainder > 0:
                            expanded = torch.cat([expanded, pooled_features[:, :remainder]], dim=1)
                        pooled_features = expanded
                    else:
                        # æˆªæ–­
                        pooled_features = pooled_features[:, :self.prototype_dim]
                
                # å¹³å‡æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
                new_feature = pooled_features.mean(dim=0, keepdim=True)  # [1, prototype_dim]
                
                # ä½¿ç”¨ç§»åŠ¨å¹³å‡æ›´æ–°
                current_prototype = self.user_prototypes[user_key]
                self.user_prototypes[user_key].data = (
                    0.9 * current_prototype.data + 0.1 * new_feature
                )
    
    def training_step(self, clean_latents, user_conditions):
        """æ ‡å‡†è®­ç»ƒæ­¥éª¤ - åŸºäºDiffusers"""
        device = clean_latents.device
        batch_size = clean_latents.size(0)
        
        # âœ… ç›´æ¥åœ¨åŸå§‹latentç©ºé—´å·¥ä½œï¼Œæ— æ ‡å‡†åŒ–
        
        # éšæœºæ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.scheduler.config.num_train_timesteps, 
            (batch_size,), device=device
        ).long()
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(clean_latents)
        noisy_latents = self.scheduler.add_noise(clean_latents, noise, timesteps)
        
        # ç¡®ä¿æ¡ä»¶å¼ é‡æ ¼å¼æ­£ç¡®
        if user_conditions.dim() == 2:
            user_conditions = user_conditions.unsqueeze(1)
        
        # UNeté¢„æµ‹å™ªå£°
        noise_pred = self.unet(
            noisy_latents,
            timesteps,
            encoder_hidden_states=user_conditions,
        ).sample
        
        # æ‰©æ•£æŸå¤±ï¼ˆMSEï¼‰
        diffusion_loss = F.mse_loss(noise_pred, noise)
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.contrastive(noise_pred, noise, None) * 0.1
        
        total_loss = diffusion_loss + contrastive_loss
        
        return {
            'total_loss': total_loss,
            'diffusion_loss': diffusion_loss, 
            'contrastive_loss': contrastive_loss
        }
    
    def generate(self, user_ids, num_samples=4, guidance_scale=7.5, num_inference_steps=50):
        """ç”Ÿæˆæ ·æœ¬"""
        self.eval()
        device = next(self.parameters()).device
        
        with torch.no_grad():
            # è·å–ç”¨æˆ·æ¡ä»¶
            base_conditions = self.get_user_condition(user_ids)
            
            # é‡å¤ç”¨æˆ·æ¡ä»¶ä»¥åŒ¹é…æ ·æœ¬æ•°é‡
            samples_per_user = num_samples // len(user_ids)
            user_conditions = base_conditions.repeat_interleave(samples_per_user, dim=0)
            
            # å¦‚æœæ ·æœ¬æ•°ä¸æ•´é™¤ç”¨æˆ·æ•°ï¼Œå¤„ç†å‰©ä½™æ ·æœ¬
            remaining = num_samples - len(user_conditions)
            if remaining > 0:
                extra_conditions = base_conditions[:remaining]
                user_conditions = torch.cat([user_conditions, extra_conditions], dim=0)
            
            print(f"ğŸ“Š ç”¨æˆ·æ¡ä»¶å½¢çŠ¶: {user_conditions.shape}")
            
            # åˆå§‹åŒ–å™ªå£° - âœ… æ ‡å‡†æ–¹å¼ä»N(0,1)å¼€å§‹
            latent_shape = (num_samples, 32, 16, 16)  # VAE latent shape
            latents = torch.randn(latent_shape, device=device)
            
            # ğŸ”§ ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„DDPMè°ƒåº¦å™¨ - ç¡®ä¿å‚æ•°ä¸€è‡´æ€§
            from diffusers import DDPMScheduler
            inference_scheduler = DDPMScheduler(
                num_train_timesteps=1000,
                beta_start=0.0001,
                beta_end=0.02,
                beta_schedule="linear",
                prediction_type="epsilon"
            )
            inference_scheduler.set_timesteps(num_inference_steps)
            
            # æ­£ç¡®ç¼©æ”¾åˆå§‹å™ªå£°
            latents = latents * inference_scheduler.init_noise_sigma
            
            # ğŸ“Š ç›‘æ§ç¼©æ”¾åçš„åˆå§‹latent std
            init_std = latents.std().item()
            print(f"ğŸ“Š ç¼©æ”¾ååˆå§‹latent std: {init_std:.6f} (sigma={inference_scheduler.init_noise_sigma:.3f})")
            
            # CFGçš„æ— æ¡ä»¶è¾“å…¥
            if guidance_scale > 1.0:
                uncond_conditions = torch.zeros_like(user_conditions)
            
            # âœ… æ ‡å‡†å»å™ªå¾ªç¯
            for i, t in enumerate(tqdm(inference_scheduler.timesteps, desc="ç”Ÿæˆä¸­")):
                # æ¡ä»¶é¢„æµ‹
                noise_pred_cond = self.unet(
                    latents, t, encoder_hidden_states=user_conditions
                ).sample
                
                if guidance_scale > 1.0:
                    # æ— æ¡ä»¶é¢„æµ‹
                    noise_pred_uncond = self.unet(
                        latents, t, encoder_hidden_states=uncond_conditions
                    ).sample
                    
                    # CFG
                    noise_pred = noise_pred_uncond + guidance_scale * (
                        noise_pred_cond - noise_pred_uncond
                    )
                else:
                    noise_pred = noise_pred_cond
                
                # è°ƒåº¦å™¨æ­¥éª¤
                latents = inference_scheduler.step(noise_pred, t, latents).prev_sample
                
                # ğŸ”§ é˜²æ­¢æ•°å€¼çˆ†ç‚¸ - è£å‰ªå¼‚å¸¸latentå€¼
                latents = torch.clamp(latents, min=-10.0, max=10.0)
                
                # ğŸ“Š æ¯10æ­¥ç›‘æ§latent std
                if i % 10 == 0:
                    current_std = latents.std().item()
                    print(f"   Step {i:2d}/{num_inference_steps}: latent std = {current_std:.6f}")
            
            # ğŸ“Š ç›‘æ§æœ€ç»ˆlatent std
            final_std = latents.std().item()
            print(f"ğŸ“Š ç”Ÿæˆæœ€ç»ˆlatent std: {final_std:.6f}")
            
            # ğŸ”§ æ¿€è¿›ä¿®å¤ï¼šå¼ºåˆ¶åŒ¹é…è®­ç»ƒåˆ†å¸ƒ
            target_std = 1.54  # è®­ç»ƒæ•°æ®çš„å®é™…std
            if final_std < target_std * 0.8:  # å¦‚æœstdå¤ªä½
                print(f"âš ï¸  æ£€æµ‹åˆ°stdè¿‡ä½ï¼Œè¿›è¡Œåˆ†å¸ƒæ ¡æ­£...")
                
                # ä¿æŒå‡å€¼ï¼Œé‡æ–°ç¼©æ”¾std
                latent_mean = latents.mean()
                latents_centered = latents - latent_mean
                scale_factor = target_std / final_std
                latents = latents_centered * scale_factor + latent_mean
                
                corrected_std = latents.std().item()
                print(f"ğŸ“Š æ ¡æ­£ålatent std: {corrected_std:.6f} (ç¼©æ”¾å› å­: {scale_factor:.3f})")
        
        self.train()
        return latents


# ä½¿ç”¨ç¤ºä¾‹
def create_standard_diffusion_system(vae_checkpoint, num_users=31):
    """åˆ›å»ºæ ‡å‡†æ‰©æ•£ç³»ç»Ÿ"""
    
    # åŠ è½½ç®€åŒ–VA-VAE
    from simplified_vavae import SimplifiedVAVAE
    vae = SimplifiedVAVAE(vae_checkpoint)
    
    # åˆ›å»ºæ ‡å‡†æ‰©æ•£æ¨¡å‹
    diffusion_model = StandardConditionalDiffusion(
        vae=vae,
        num_users=num_users,
        prototype_dim=768
    )
    
    return diffusion_model, vae


if __name__ == "__main__":
    import argparse
    import sys
    print("âŒ é”™è¯¯ï¼šstandard_conditional_diffusion.py ä¸æ˜¯è®­ç»ƒè„šæœ¬ï¼")
    print("âœ… è¯·ä½¿ç”¨ï¼špython train_enhanced_conditional.py")
    print("ğŸ“– è¯´æ˜ï¼šstandard_conditional_diffusion.py åªæ˜¯æ¨¡å‹å®šä¹‰æ–‡ä»¶")
    sys.exit(1)

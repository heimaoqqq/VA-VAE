"""
æ ‡å‡†æ¡ä»¶æ‰©æ•£æ¨¡å‹ - åŸºäºDiffusersæ ‡å‡†å®ç°
å‚è€ƒï¼šhuggingface/diffusers/examples/unconditional_image_generation/train_unconditional.py
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers import UNet2DConditionModel, DDPMScheduler, DDIMScheduler
from tqdm import tqdm


class PrototypicalEncoder(nn.Module):
    """åŸå‹ç¼–ç å™¨ - ç”¨äºå­¦ä¹ ç”¨æˆ·ç‰¹å¾"""
    def __init__(self, input_dim, prototype_dim=768):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(input_dim, 2048),
            nn.ReLU(),
            nn.Linear(2048, prototype_dim),
            nn.LayerNorm(prototype_dim)
        )
    
    def forward(self, x):
        return self.encoder(x)


class ContrastiveLearning(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å—"""
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, predicted_latents, target_latents, user_ids):
        # ç®€åŒ–çš„å¯¹æ¯”æŸå¤±
        return F.mse_loss(predicted_latents, target_latents)


class StandardConditionalDiffusion(nn.Module):
    """æ ‡å‡†æ¡ä»¶æ‰©æ•£æ¨¡å‹ - é‡‡ç”¨Diffusersæ ‡å‡†åšæ³•"""
    
    def __init__(
        self,
        vae,
        num_timesteps=1000,
        noise_schedule="linear",
        num_users=31,
        prototype_dim=768,
    ):
        super().__init__()
        
        # VAEç»„ä»¶
        self.vae = vae
        
        # åŸå‹å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ ç»„ä»¶
        self.prototype_encoder = PrototypicalEncoder(
            input_dim=32*16*16,  # VA-VAE latentç»´åº¦
            prototype_dim=prototype_dim
        )
        self.contrastive_learning = ContrastiveLearning(temperature=0.07)
        
        # æ ‡å‡†UNetæ‰©æ•£æ¨¡å‹
        self.unet = UNet2DConditionModel(
            in_channels=32,  # VA-VAEæœ‰32ä¸ªé€šé“
            out_channels=32,
            down_block_types=("DownBlock2D", "DownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D"),
            up_block_types=("CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "UpBlock2D", "UpBlock2D"),
            block_out_channels=(128, 256, 384, 512),
            layers_per_block=2,
            attention_head_dim=8,
            cross_attention_dim=prototype_dim,
            sample_size=16
        )
        
        self.num_timesteps = num_timesteps
        
        # æ ‡å‡†å™ªå£°è°ƒåº¦å™¨ - ç›´æ¥ä½¿ç”¨é»˜è®¤é…ç½®
        self.scheduler = DDPMScheduler(
            num_train_timesteps=num_timesteps,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule=noise_schedule,
            clip_sample=False,
            prediction_type="epsilon",
        )
        
        self.ddim_scheduler = DDIMScheduler(
            num_train_timesteps=num_timesteps,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule=noise_schedule,
            clip_sample=False,
            prediction_type="epsilon",
        )
        
        # ç”¨æˆ·åŸå‹ç¼“å­˜
        self.user_prototypes = {}
        
        print("âœ… ä½¿ç”¨æ ‡å‡†æ‰©æ•£æ¨¡å‹ - æ— è‡ªå®šä¹‰æ ‡å‡†åŒ–")
    
    def get_user_condition(self, user_ids):
        """è·å–ç”¨æˆ·æ¡ä»¶ç¼–ç """
        batch_size = len(user_ids)
        device = next(self.parameters()).device
        
        # ç¡®ä¿è¿”å›æ­£ç¡®çš„3Då¼ é‡å½¢çŠ¶ [batch_size, sequence_length=1, hidden_dim=768]
        conditions = torch.zeros(batch_size, 1, self.prototype_encoder.encoder[-1].normalized_shape[0], device=device)
        
        for i, user_id in enumerate(user_ids):
            user_id_int = user_id.item() if torch.is_tensor(user_id) else user_id
            if user_id_int in self.user_prototypes:
                prototype = self.user_prototypes[user_id_int]
                if prototype.dim() == 1:
                    prototype = prototype.unsqueeze(0)
                conditions[i] = prototype
            else:
                # éšæœºåˆå§‹åŒ–æœªè§è¿‡çš„ç”¨æˆ·
                conditions[i] = torch.randn(1, self.prototype_encoder.encoder[-1].normalized_shape[0], device=device) * 0.1
        
        # ç¡®ä¿ç»´åº¦æ­£ç¡®
        if conditions.dim() != 3 or conditions.shape[-1] != self.prototype_encoder.encoder[-1].normalized_shape[0]:
            conditions = conditions.view(batch_size, 1, self.prototype_encoder.encoder[-1].normalized_shape[0])
        
        return conditions
    
    def training_step(self, clean_latents, user_conditions):
        """æ ‡å‡†è®­ç»ƒæ­¥éª¤ - å‚è€ƒDiffuserså®ç°"""
        batch_size = clean_latents.shape[0]
        device = clean_latents.device
        
        # âœ… å…³é”®ï¼šç›´æ¥åœ¨VAE latentä¸Šå·¥ä½œï¼Œæ— é¢å¤–æ ‡å‡†åŒ–
        # è¿™å°±æ˜¯Stable Diffusionç­‰æ ‡å‡†å®ç°çš„åšæ³•
        
        # éšæœºæ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.scheduler.config.num_train_timesteps,
            (batch_size,), device=device
        ).long()
        
        # âœ… æ ‡å‡†åšæ³•ï¼šç›´æ¥æ·»åŠ å™ªå£°åˆ°clean_latents
        noise = torch.randn_like(clean_latents)
        noisy_latents = self.scheduler.add_noise(clean_latents, noise, timesteps)
        
        # âœ… é¢„æµ‹å™ªå£°
        # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
        noisy_latents = noisy_latents.to(device)
        timesteps = timesteps.to(device)
        user_conditions = user_conditions.to(device)
        
        # ç¡®ä¿user_conditionsæ˜¯3Då¼ é‡
        if user_conditions.dim() == 2:
            user_conditions = user_conditions.unsqueeze(1)
        elif user_conditions.dim() == 1:
            user_conditions = user_conditions.unsqueeze(0).unsqueeze(0)
        
        # UNeté¢„æµ‹å™ªå£°
        noise_pred = self.unet(
            noisy_latents,
            timesteps,
            encoder_hidden_states=user_conditions,
        ).sample
        
        # æ‰©æ•£æŸå¤±
        diff_loss = F.mse_loss(noise_pred, noise)
        
        # å¯¹æ¯”æŸå¤±ï¼ˆåœ¨åŸå§‹clean latentä¸Šè®¡ç®—ï¼‰
        # è·å–é¢„æµ‹çš„clean latentï¼ˆç”¨äºåŸå‹å­¦ä¹ ï¼‰
        predicted_clean = self.scheduler.step(
            noise_pred, timesteps[0], noisy_latents
        ).pred_original_sample
        
        # åœ¨åŸå§‹ç©ºé—´è®¡ç®—å¯¹æ¯”æŸå¤±
        clean_latents_flat = clean_latents.view(batch_size, -1)
        predicted_clean_flat = predicted_clean.view(batch_size, -1)
        
        # è®¡ç®—åŸå‹
        clean_prototypes = self.prototype_encoder(clean_latents_flat)
        predicted_prototypes = self.prototype_encoder(predicted_clean_flat)
        
        # å¯¹æ¯”æŸå¤±
        contrastive_loss = self.contrastive_learning(
            predicted_prototypes, clean_prototypes, None
        )
        
        # æ€»æŸå¤±
        total_loss = diff_loss + 0.1 * contrastive_loss
        
        return total_loss, diff_loss, contrastive_loss
    
    def update_user_prototypes(self, user_samples_dict):
        """æ›´æ–°ç”¨æˆ·åŸå‹ç¼“å­˜"""
        self.user_prototypes = {}
        
        for user_id, samples in user_samples_dict.items():
            # samples: [N, 32, 16, 16] - VAE latentç©ºé—´
            with torch.no_grad():
                samples_flat = samples.view(samples.shape[0], -1)
                prototypes = self.prototype_encoder(samples_flat)
                user_prototype = torch.mean(prototypes, dim=0, keepdim=True)
                self.user_prototypes[user_id] = user_prototype
    
    @torch.no_grad()
    def generate_samples(
        self,
        user_ids,
        num_samples_per_user=2,
        num_inference_steps=100,
        guidance_scale=1.0,
        use_ddim=True
    ):
        """æ ‡å‡†ç”Ÿæˆè¿‡ç¨‹"""
        self.eval()
        
        if isinstance(user_ids, int):
            user_ids = [user_ids]
        
        batch_size = len(user_ids) * num_samples_per_user
        device = next(self.parameters()).device
        
        # é€‰æ‹©è°ƒåº¦å™¨
        scheduler = self.ddim_scheduler if use_ddim else self.scheduler
        scheduler.set_timesteps(num_inference_steps, device=device)
        
        # âœ… æ ‡å‡†åˆå§‹åŒ–ï¼šä»çº¯å™ªå£°å¼€å§‹
        shape = (batch_size, 32, 16, 16)
        latents = torch.randn(shape, device=device)
        
        print(f"ğŸ“Š ç”Ÿæˆé…ç½®:")
        print(f"   æ ‡å‡†é«˜æ–¯N(0,1)åˆå§‹åŒ–")
        print(f"   åˆå§‹std: {latents.std():.4f}")
        
        # å‡†å¤‡æ¡ä»¶
        user_ids_expanded = []
        for uid in user_ids:
            user_ids_expanded.extend([uid] * num_samples_per_user)
        user_conditions = self.get_user_condition(user_ids_expanded)
        
        # CFGå‡†å¤‡
        if guidance_scale > 1.0:
            uncond_conditions = torch.zeros_like(user_conditions)
        
        # âœ… æ ‡å‡†å»å™ªå¾ªç¯
        for t in tqdm(scheduler.timesteps, desc="ç”Ÿæˆä¸­"):
            # æ¡ä»¶é¢„æµ‹
            noise_pred_cond = self.unet(
                latents, t, encoder_hidden_states=user_conditions
            ).sample
            
            if guidance_scale > 1.0:
                # æ— æ¡ä»¶é¢„æµ‹
                noise_pred_uncond = self.unet(
                    latents, t, encoder_hidden_states=uncond_conditions
                ).sample
                
                # CFG
                noise_pred = noise_pred_uncond + guidance_scale * (
                    noise_pred_cond - noise_pred_uncond
                )
            else:
                noise_pred = noise_pred_cond
            
            # âœ… æ ‡å‡†å»å™ªæ­¥éª¤
            latents = scheduler.step(noise_pred, t, latents).prev_sample
        
        # éªŒè¯åˆ†å¸ƒ
        print(f"ğŸ“Š ç”Ÿæˆlatentåˆ†å¸ƒ:")
        print(f"   æœ€ç»ˆlatent: mean={latents.mean():.4f}, std={latents.std():.4f}")
        print(f"   åº”è¯¥æ¥è¿‘è®­ç»ƒæ•°æ®çš„latentåˆ†å¸ƒ")
        
        return latents


# ä½¿ç”¨ç¤ºä¾‹
def create_standard_diffusion_system(vae_checkpoint, num_users=31):
    """åˆ›å»ºæ ‡å‡†æ‰©æ•£ç³»ç»Ÿ"""
    
    # åŠ è½½ç®€åŒ–VA-VAE
    from simplified_vavae import SimplifiedVAVAE
    vae = SimplifiedVAVAE(vae_checkpoint)
    
    # åˆ›å»ºæ ‡å‡†æ‰©æ•£æ¨¡å‹
    diffusion_model = StandardConditionalDiffusion(
        vae=vae,
        num_users=num_users,
        prototype_dim=768
    )
    
    return diffusion_model, vae


if __name__ == "__main__":
    import argparse
    import sys
    print("âŒ é”™è¯¯ï¼šstandard_conditional_diffusion.py ä¸æ˜¯è®­ç»ƒè„šæœ¬ï¼")
    print("âœ… è¯·ä½¿ç”¨ï¼špython train_enhanced_conditional.py")
    print("ğŸ“– è¯´æ˜ï¼šstandard_conditional_diffusion.py åªæ˜¯æ¨¡å‹å®šä¹‰æ–‡ä»¶")
    sys.exit(1)

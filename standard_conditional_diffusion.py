"""
æ ‡å‡†æ¡ä»¶æ‰©æ•£æ¨¡å‹ - åŸºäºDiffusersæ ‡å‡†å®ç°
å‚è€ƒï¼šhuggingface/diffusers/examples/unconditional_image_generation/train_unconditional.py
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers import UNet2DConditionModel, DDPMScheduler, DDIMScheduler
from tqdm import tqdm


class PrototypicalEncoder(nn.Module):
    """åŸå‹ç¼–ç å™¨ - ç”¨äºå­¦ä¹ ç”¨æˆ·ç‰¹å¾"""
    def __init__(self, input_dim, prototype_dim=768):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(input_dim, 2048),
            nn.ReLU(),
            nn.Linear(2048, prototype_dim),
            nn.LayerNorm(prototype_dim)
        )
    
    def forward(self, x):
        return self.encoder(x)


class ContrastiveLearning(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å—"""
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, predicted_latents, target_latents, user_ids):
        # ç®€åŒ–çš„å¯¹æ¯”æŸå¤±
        batch_size = predicted_latents.size(0)
        
        # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µ
        pred_flat = predicted_latents.view(batch_size, -1)
        target_flat = target_latents.view(batch_size, -1)
        
        # L2 normalize
        pred_norm = F.normalize(pred_flat, p=2, dim=1)
        target_norm = F.normalize(target_flat, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = torch.matmul(pred_norm, target_norm.T) / self.temperature
        
        # InfoNCEæŸå¤±
        labels = torch.arange(batch_size, device=similarity.device)
        loss = F.cross_entropy(similarity, labels)
        
        return loss


class StandardConditionalDiffusion(nn.Module):
    """æ ‡å‡†æ¡ä»¶æ‰©æ•£æ¨¡å‹ - åŸºäºDiffusers"""
    
    def __init__(self, vae, num_users=31, prototype_dim=768):
        super().__init__()
        
        # VAEï¼ˆå†»ç»“ï¼‰
        self.vae = vae
        for param in self.vae.parameters():
            param.requires_grad = False
        
        # æ‰©æ•£ç»„ä»¶
        self.unet = UNet2DConditionModel(
            sample_size=16,  # VAE latent size
            in_channels=32,  # VAE latent channels
            out_channels=32,
            layers_per_block=2,
            block_out_channels=(128, 256, 512, 512),
            down_block_types=(
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
            ),
            cross_attention_dim=prototype_dim,
            attention_head_dim=8,
        )
        
        # å™ªå£°è°ƒåº¦å™¨
        self.scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule="linear",
            prediction_type="epsilon"
        )
        
        # ç”¨æˆ·åŸå‹ç³»ç»Ÿ
        self.num_users = num_users
        self.prototype_dim = prototype_dim
        self.user_prototypes = nn.ParameterDict()
        
        # å¯¹æ¯”å­¦ä¹ 
        self.contrastive = ContrastiveLearning()
        
        print("âœ… ä½¿ç”¨æ ‡å‡†æ‰©æ•£æ¨¡å‹ - æ— è‡ªå®šä¹‰æ ‡å‡†åŒ–")
    
    def get_user_condition(self, user_ids):
        """è·å–ç”¨æˆ·æ¡ä»¶ç¼–ç """
        batch_size = len(user_ids)
        device = next(self.parameters()).device
        
        conditions = torch.zeros(batch_size, 1, self.prototype_dim, device=device)
        
        for i, user_id in enumerate(user_ids):
            user_id_int = user_id.item() if torch.is_tensor(user_id) else user_id
            user_key = str(user_id_int)
            
            if user_key not in self.user_prototypes:
                # åˆå§‹åŒ–æ–°ç”¨æˆ·åŸå‹
                self.user_prototypes[user_key] = nn.Parameter(
                    torch.randn(1, self.prototype_dim, device=device) * 0.1
                )
            
            conditions[i] = self.user_prototypes[user_key]
        
        return conditions
    
    def training_step(self, clean_latents, user_conditions):
        """æ ‡å‡†è®­ç»ƒæ­¥éª¤ - åŸºäºDiffusers"""
        device = clean_latents.device
        batch_size = clean_latents.size(0)
        
        # âœ… ç›´æ¥åœ¨åŸå§‹latentç©ºé—´å·¥ä½œï¼Œæ— æ ‡å‡†åŒ–
        
        # éšæœºæ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.scheduler.config.num_train_timesteps, 
            (batch_size,), device=device
        ).long()
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(clean_latents)
        noisy_latents = self.scheduler.add_noise(clean_latents, noise, timesteps)
        
        # ç¡®ä¿æ¡ä»¶å¼ é‡æ ¼å¼æ­£ç¡®
        if user_conditions.dim() == 2:
            user_conditions = user_conditions.unsqueeze(1)
        
        # UNeté¢„æµ‹å™ªå£°
        noise_pred = self.unet(
            noisy_latents,
            timesteps,
            encoder_hidden_states=user_conditions,
        ).sample
        
        # æ‰©æ•£æŸå¤±ï¼ˆMSEï¼‰
        diffusion_loss = F.mse_loss(noise_pred, noise)
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.contrastive(noise_pred, noise, None) * 0.1
        
        total_loss = diffusion_loss + contrastive_loss
        
        return {
            'total_loss': total_loss,
            'diffusion_loss': diffusion_loss, 
            'contrastive_loss': contrastive_loss
        }
    
    def generate(self, user_ids, num_samples=4, guidance_scale=7.5, num_inference_steps=50):
        """ç”Ÿæˆæ ·æœ¬"""
        self.eval()
        device = next(self.parameters()).device
        
        with torch.no_grad():
            # è·å–ç”¨æˆ·æ¡ä»¶
            user_conditions = self.get_user_condition(user_ids)
            
            # åˆå§‹åŒ–å™ªå£° - âœ… æ ‡å‡†æ–¹å¼ä»N(0,1)å¼€å§‹
            latent_shape = (num_samples, 32, 16, 16)  # VAE latent shape
            latents = torch.randn(latent_shape, device=device)
            
            # ğŸ“Š ç›‘æ§ç”Ÿæˆåˆå§‹latent std
            init_std = latents.std().item()
            print(f"ğŸ“Š ç”Ÿæˆåˆå§‹latent std: {init_std:.6f}")
            
            # è®¾ç½®æ¨ç†è°ƒåº¦å™¨
            inference_scheduler = DDIMScheduler(
                num_train_timesteps=1000,
                beta_start=0.0001,
                beta_end=0.02,
                beta_schedule="linear",
                prediction_type="epsilon"
            )
            inference_scheduler.set_timesteps(num_inference_steps)
            
            # CFGçš„æ— æ¡ä»¶è¾“å…¥
            if guidance_scale > 1.0:
                uncond_conditions = torch.zeros_like(user_conditions)
            
            # âœ… æ ‡å‡†å»å™ªå¾ªç¯
            for i, t in enumerate(tqdm(inference_scheduler.timesteps, desc="ç”Ÿæˆä¸­")):
                # æ¡ä»¶é¢„æµ‹
                noise_pred_cond = self.unet(
                    latents, t, encoder_hidden_states=user_conditions
                ).sample
                
                if guidance_scale > 1.0:
                    # æ— æ¡ä»¶é¢„æµ‹
                    noise_pred_uncond = self.unet(
                        latents, t, encoder_hidden_states=uncond_conditions
                    ).sample
                    
                    # CFG
                    noise_pred = noise_pred_uncond + guidance_scale * (
                        noise_pred_cond - noise_pred_uncond
                    )
                else:
                    noise_pred = noise_pred_cond
                
                # è°ƒåº¦å™¨æ­¥éª¤
                latents = inference_scheduler.step(noise_pred, t, latents).prev_sample
                
                # ğŸ“Š æ¯10æ­¥ç›‘æ§latent std
                if i % 10 == 0:
                    current_std = latents.std().item()
                    print(f"   Step {i:2d}/{num_inference_steps}: latent std = {current_std:.6f}")
            
            # ğŸ“Š ç›‘æ§æœ€ç»ˆlatent std
            final_std = latents.std().item()
            print(f"ğŸ“Š ç”Ÿæˆæœ€ç»ˆlatent std: {final_std:.6f}")
        
        self.train()
        return latents


# ä½¿ç”¨ç¤ºä¾‹
def create_standard_diffusion_system(vae_checkpoint, num_users=31):
    """åˆ›å»ºæ ‡å‡†æ‰©æ•£ç³»ç»Ÿ"""
    
    # åŠ è½½ç®€åŒ–VA-VAE
    from simplified_vavae import SimplifiedVAVAE
    vae = SimplifiedVAVAE(vae_checkpoint)
    
    # åˆ›å»ºæ ‡å‡†æ‰©æ•£æ¨¡å‹
    diffusion_model = StandardConditionalDiffusion(
        vae=vae,
        num_users=num_users,
        prototype_dim=768
    )
    
    return diffusion_model, vae


if __name__ == "__main__":
    import argparse
    import sys
    print("âŒ é”™è¯¯ï¼šstandard_conditional_diffusion.py ä¸æ˜¯è®­ç»ƒè„šæœ¬ï¼")
    print("âœ… è¯·ä½¿ç”¨ï¼špython train_enhanced_conditional.py")
    print("ğŸ“– è¯´æ˜ï¼šstandard_conditional_diffusion.py åªæ˜¯æ¨¡å‹å®šä¹‰æ–‡ä»¶")
    sys.exit(1)

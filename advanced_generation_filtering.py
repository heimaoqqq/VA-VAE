"""
é«˜çº§ç”Ÿæˆå’Œç­›é€‰ç­–ç•¥
å®ç°å¤šæŒ‡æ ‡ç­›é€‰ã€æ¸è¿›å¼ç”Ÿæˆã€æ¡¥æ¥æ ·æœ¬é€‰æ‹©
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import sys
import os
import yaml
import numpy as np
from PIL import Image
from tqdm import tqdm
import argparse
from pathlib import Path
import torchvision.transforms as transforms
from scipy.spatial.distance import mahalanobis
from sklearn.metrics.pairwise import cosine_similarity
import json
from collections import defaultdict

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from models.lightningdit import LightningDiT_models
from transport import create_transport, Sampler
from simplified_vavae import SimplifiedVAVAE

# å¯¼å…¥å·²è®­ç»ƒçš„åˆ†ç±»å™¨
sys.path.append('./')
from train_calibrated_classifier import DomainAdaptiveClassifier


def setup_ddp():
    """DDPåˆå§‹åŒ–"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        print("Not using distributed generation")
        return 0, 1, 0
    
    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend='nccl')
    
    return rank, world_size, local_rank


def cleanup_ddp():
    """DDPæ¸…ç†"""
    if dist.is_initialized():
        dist.destroy_process_group()

class AdvancedSampleFilter:
    """å¤šæŒ‡æ ‡æ ·æœ¬ç­›é€‰å™¨"""
    
    def __init__(self, classifiers, device, feature_bank=None):
        """
        Args:
            classifiers: åˆ†ç±»å™¨åˆ—è¡¨ï¼ˆå¤šä¸ªä¸åŒè®¾ç½®çš„åˆ†ç±»å™¨ï¼‰
            device: è®¡ç®—è®¾å¤‡
            feature_bank: çœŸå®æ•°æ®ç‰¹å¾åº“ï¼ˆç”¨äºè®¡ç®—ç‰¹å¾è·ç¦»ï¼‰
        """
        self.classifiers = classifiers if isinstance(classifiers, list) else [classifiers]
        self.device = device
        self.feature_bank = feature_bank
        
    def extract_features(self, model, images):
        """æå–å€’æ•°ç¬¬äºŒå±‚ç‰¹å¾"""
        features = []
        model.eval()
        with torch.no_grad():
            # è·å–backboneç‰¹å¾
            if hasattr(model, 'backbone'):
                features = model.backbone(images)
            else:
                # ä½¿ç”¨hookæå–ç‰¹å¾
                def hook_fn(module, input, output):
                    features.append(output)
                handle = model.classifier[-1].register_forward_hook(hook_fn)
                _ = model(images)
                handle.remove()
        return features[0] if features else None
    
    def compute_consistency_score(self, image, user_id):
        """å¤šæ¨¡å‹ä¸€è‡´æ€§è¯„åˆ†"""
        predictions = []
        confidences = []
        
        for classifier in self.classifiers:
            classifier.eval()
            with torch.no_grad():
                output = classifier(image.unsqueeze(0))
                prob = torch.softmax(output, dim=1)
                conf, pred = torch.max(prob, dim=1)
                predictions.append(pred.item())
                confidences.append(conf.item())
        
        # æ£€æŸ¥é¢„æµ‹ä¸€è‡´æ€§
        is_consistent = all(p == user_id for p in predictions)
        avg_confidence = np.mean(confidences)
        confidence_std = np.std(confidences)  # ç½®ä¿¡åº¦æ–¹å·®ï¼Œè¶Šå°è¶Šå¥½
        
        # ç»¼åˆè¯„åˆ†
        consistency_score = avg_confidence * (1 - confidence_std) if is_consistent else 0
        
        return consistency_score, is_consistent, avg_confidence
    
    def compute_margin_score(self, classifier, image):
        """è®¡ç®—top-1å’Œtop-2ä¹‹é—´çš„margin"""
        classifier.eval()
        with torch.no_grad():
            output = classifier(image.unsqueeze(0))
            logits = output[0]
            top2_values, _ = torch.topk(logits, 2)
            margin = (top2_values[0] - top2_values[1]).item()
        return margin
    
    def compute_feature_distance(self, image, user_id):
        """è®¡ç®—ä¸çœŸå®æ•°æ®åŸå‹çš„ç‰¹å¾è·ç¦»"""
        if self.feature_bank is None:
            return 0.5  # é»˜è®¤ä¸­ç­‰è·ç¦»
        
        # æå–ç‰¹å¾
        features = self.extract_features(self.classifiers[0], image.unsqueeze(0))
        if features is None:
            return 0.5
        
        features = features.cpu().numpy().flatten()
        
        # è·å–è¯¥ç”¨æˆ·çš„çœŸå®æ•°æ®åŸå‹
        if user_id in self.feature_bank:
            user_prototype = self.feature_bank[user_id]['mean']
            user_cov = self.feature_bank[user_id]['cov']
            
            # é©¬æ°è·ç¦»ï¼ˆè€ƒè™‘åæ–¹å·®ï¼‰
            try:
                dist = mahalanobis(features, user_prototype, np.linalg.inv(user_cov))
            except:
                # å¦‚æœåæ–¹å·®çŸ©é˜µå¥‡å¼‚ï¼Œä½¿ç”¨æ¬§æ°è·ç¦»
                dist = np.linalg.norm(features - user_prototype)
            
            # å½’ä¸€åŒ–åˆ°[0, 1]
            normalized_dist = 1 / (1 + np.exp(-0.1 * (dist - 10)))
            return normalized_dist
        
        return 0.5
    
    def compute_augmentation_consistency(self, classifier, image, num_augmentations=5):
        """æµ‹è¯•è½»å¾®å¢å¼ºä¸‹çš„é¢„æµ‹ç¨³å®šæ€§"""
        classifier.eval()
        
        # å®šä¹‰è½»å¾®å¢å¼º
        light_augment = transforms.Compose([
            transforms.RandomAffine(degrees=5, translate=(0.05, 0.05)),
            transforms.RandomHorizontalFlip(p=0.3),
            transforms.ColorJitter(brightness=0.1, contrast=0.1),
        ])
        
        predictions = []
        confidences = []
        
        with torch.no_grad():
            for _ in range(num_augmentations):
                # åº”ç”¨å¢å¼º
                aug_image = image.clone()
                if aug_image.dim() == 3:  # CHWæ ¼å¼
                    # è½¬ä¸ºPILå†è½¬å›
                    pil_img = transforms.ToPILImage()(aug_image)
                    aug_pil = light_augment(pil_img)
                    aug_tensor = transforms.ToTensor()(aug_pil)
                    aug_tensor = transforms.Normalize(
                        mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225]
                    )(aug_tensor)
                    aug_image = aug_tensor.to(self.device)
                
                output = classifier(aug_image.unsqueeze(0))
                prob = torch.softmax(output, dim=1)
                conf, pred = torch.max(prob, dim=1)
                predictions.append(pred.item())
                confidences.append(conf.item())
        
        # è®¡ç®—ç¨³å®šæ€§
        unique_preds = len(set(predictions))
        stability = 1.0 / unique_preds  # é¢„æµ‹è¶Šä¸€è‡´ï¼Œç¨³å®šæ€§è¶Šé«˜
        avg_conf = np.mean(confidences)
        
        return stability * avg_conf, predictions[0]  # è¿”å›ç¨³å®šæ€§åˆ†æ•°å’Œä¸»è¦é¢„æµ‹
    
    def filter_samples(self, images, user_ids, config):
        """
        ç»¼åˆå¤šæŒ‡æ ‡ç­›é€‰æ ·æœ¬
        
        Args:
            images: æ‰¹é‡å›¾åƒ tensor [B, C, H, W]
            user_ids: å¯¹åº”çš„ç”¨æˆ·IDåˆ—è¡¨
            config: ç­›é€‰é…ç½®
                - consistency_threshold: ä¸€è‡´æ€§é˜ˆå€¼
                - margin_threshold: marginé˜ˆå€¼
                - feature_dist_range: ç‰¹å¾è·ç¦»èŒƒå›´ [min, max]
                - augment_stability_threshold: å¢å¼ºç¨³å®šæ€§é˜ˆå€¼
                - weights: å„æŒ‡æ ‡æƒé‡
        
        Returns:
            selected_indices: é€šè¿‡ç­›é€‰çš„æ ·æœ¬ç´¢å¼•
            scores: å„æ ·æœ¬çš„ç»¼åˆè¯„åˆ†
        """
        batch_size = images.shape[0]
        scores = []
        details = []
        
        for i in range(batch_size):
            image = images[i]
            user_id = user_ids[i]
            
            # 1. å¤šæ¨¡å‹ä¸€è‡´æ€§
            consistency_score, is_consistent, avg_conf = self.compute_consistency_score(image, user_id)
            
            # 2. Marginè¯„åˆ†
            margin = self.compute_margin_score(self.classifiers[0], image)
            margin_score = min(margin / 5.0, 1.0)  # å½’ä¸€åŒ–
            
            # 3. ç‰¹å¾è·ç¦»
            feature_dist = self.compute_feature_distance(image, user_id)
            # ç†æƒ³è·ç¦»åœ¨[0.3, 0.7]èŒƒå›´ï¼ˆä¸å¤ªè¿‘ä¹Ÿä¸å¤ªè¿œï¼‰
            if config['feature_dist_range'][0] <= feature_dist <= config['feature_dist_range'][1]:
                feature_score = 1.0
            else:
                feature_score = 0.5
            
            # 4. å¢å¼ºç¨³å®šæ€§
            aug_score, _ = self.compute_augmentation_consistency(self.classifiers[0], image)
            
            # ç»¼åˆè¯„åˆ†
            weights = config.get('weights', {
                'consistency': 0.35,
                'margin': 0.25,
                'feature': 0.20,
                'augmentation': 0.20
            })
            
            total_score = (
                weights['consistency'] * consistency_score +
                weights['margin'] * margin_score +
                weights['feature'] * feature_score +
                weights['augmentation'] * aug_score
            )
            
            scores.append(total_score)
            details.append({
                'user_id': user_id,
                'consistency': consistency_score,
                'margin': margin,
                'feature_dist': feature_dist,
                'aug_stability': aug_score,
                'total': total_score
            })
        
        # ç­›é€‰é«˜åˆ†æ ·æœ¬
        threshold = config.get('total_threshold', 0.7)
        selected_indices = [i for i, s in enumerate(scores) if s >= threshold]
        
        return selected_indices, scores, details


class ProgressiveGenerator:
    """æ¸è¿›å¼ç”Ÿæˆç­–ç•¥ - æ”¯æŒåˆ†å¸ƒå¼ç”Ÿæˆ"""
    
    def __init__(self, model, vae, transport, device, rank=0, world_size=1):
        self.model = model
        self.vae = vae
        self.transport = transport
        self.device = device
        self.rank = rank
        self.world_size = world_size
        
    def generate_progressive(self, user_id, num_rounds=3, samples_per_user=300):
        """
        æ¸è¿›å¼ç”Ÿæˆï¼šé€æ­¥è°ƒæ•´CFGå’Œé‡‡æ ·æ­¥æ•°
        æ”¯æŒåˆ†å¸ƒå¼ç”Ÿæˆï¼Œæ¯ä¸ªGPUç”Ÿæˆä¸€éƒ¨åˆ†æ ·æœ¬
        """
        all_samples = []
        
        # åˆ†å¸ƒå¼ç”Ÿæˆï¼šæ¯ä¸ªGPUç”Ÿæˆä¸€éƒ¨åˆ†
        per_gpu_samples = samples_per_user // self.world_size
        if self.rank < (samples_per_user % self.world_size):
            per_gpu_samples += 1
        
        configs = [
            {'cfg': 6.0, 'steps': 100, 'ratio': 0.5, 'name': 'diverse'},
            {'cfg': 8.0, 'steps': 150, 'ratio': 0.3, 'name': 'balanced'}, 
            {'cfg': 10.0, 'steps': 200, 'ratio': 0.2, 'name': 'quality'}
        ]
        
        # æŒ‰æ¯”ä¾‹åˆ†é…æ¯ä¸€è½®çš„æ ·æœ¬æ•°
        for config in configs:
            config['batch_size'] = max(1, int(per_gpu_samples * config['ratio']))
        
        for round_idx, config in enumerate(configs):
            if self.rank == 0:
                print(f"ğŸ“ ç¬¬{round_idx+1}è½®ç”Ÿæˆ ({config['name']}): CFG={config['cfg']}, Steps={config['steps']}, Samples={config['batch_size']}")
            
            if config['batch_size'] == 0:
                continue
            
            # åˆ›å»ºé‡‡æ ·å™¨
            sampler = Sampler(self.transport)
            sample_fn = sampler.sample_ode(
                sampling_method="dopri5",
                num_steps=config['steps'],
                atol=1e-6,
                rtol=1e-3,
                reverse=False,
                timestep_shift=0.1
            )
            
            # ç”Ÿæˆæ ·æœ¬
            with torch.no_grad():
                # ä¸ºäº†ç¡®ä¿ä¸åŒ GPU ç”Ÿæˆä¸åŒæ ·æœ¬ï¼Œè®¾ç½®ä¸åŒéšæœºç§å­
                torch.manual_seed(42 + self.rank * 1000 + round_idx * 100)
                
                y = torch.tensor([user_id] * config['batch_size'], device=self.device)
                z = torch.randn(config['batch_size'], 32, 16, 16, device=self.device)
                
                # CFGé‡‡æ ·
                if config['cfg'] > 1.0:
                    z_cfg = torch.cat([z, z], 0)
                    y_null = torch.tensor([31] * config['batch_size'], device=self.device)
                    y_cfg = torch.cat([y, y_null], 0)
                    
                    model_kwargs = dict(
                        y=y_cfg, 
                        cfg_scale=config['cfg'], 
                        cfg_interval=True, 
                        cfg_interval_start=0.11
                    )
                    
                    samples = sample_fn(z_cfg, self.model, **model_kwargs)
                    samples = samples[-1]
                    samples, _ = samples.chunk(2, dim=0)
                else:
                    samples = sample_fn(z, self.model, **dict(y=y))
                    samples = samples[-1]
                
                # è§£ç å¹¶æ”¶é›†æ‰€æœ‰GPUçš„ç»“æœ
                if self.vae is not None:
                    images = self.vae.decode_to_images(samples)
                    all_samples.extend(images)
        
        return all_samples
    
    def save_samples_distributed(self, samples, user_id, output_dir):
        """åˆ†å¸ƒå¼ä¿å­˜æ ·æœ¬"""
        output_dir = Path(output_dir)
        user_dir = output_dir / f"ID_{user_id}"
        user_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¯ä¸ªGPUä¿å­˜è‡ªå·±çš„æ ·æœ¬
        for i, img in enumerate(samples):
            filename = f"generated_rank{self.rank}_idx{i:04d}.jpg"
            img.save(user_dir / filename, quality=95)
        
        if self.rank == 0:
            print(f"ğŸ’¾ Rank {self.rank} saved {len(samples)} samples for ID_{user_id}")


def build_feature_bank(real_data_dir, classifier, device, num_classes=31):
    """æ„å»ºçœŸå®æ•°æ®ç‰¹å¾åº“"""
    print("ğŸ“Š æ„å»ºçœŸå®æ•°æ®ç‰¹å¾åº“...")
    feature_bank = {}
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    for user_id in range(num_classes):
        user_dir = Path(real_data_dir) / f"User_{user_id:02d}"
        if not user_dir.exists():
            continue
        
        features_list = []
        image_files = list(user_dir.glob("*.png"))[:50]  # æ¯ä¸ªç”¨æˆ·å–50ä¸ªæ ·æœ¬
        
        for img_file in image_files:
            img = Image.open(img_file).convert('RGB')
            img_tensor = transform(img).unsqueeze(0).to(device)
            
            # æå–ç‰¹å¾
            classifier.eval()
            with torch.no_grad():
                if hasattr(classifier, 'backbone'):
                    feat = classifier.backbone(img_tensor)
                    features_list.append(feat.cpu().numpy().flatten())
        
        if features_list:
            features_array = np.array(features_list)
            feature_bank[user_id] = {
                'mean': np.mean(features_array, axis=0),
                'cov': np.cov(features_array.T),
                'samples': features_array
            }
    
    print(f"âœ… ç‰¹å¾åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(feature_bank)} ä¸ªç”¨æˆ·")
    return feature_bank


def load_trained_classifier(checkpoint_path, num_classes, device):
    """åŠ è½½å·²è®­ç»ƒçš„åˆ†ç±»å™¨"""
    print(f"Loading classifier from: {checkpoint_path}")
    
    # åˆ›å»ºæ¨¡å‹
    model = DomainAdaptiveClassifier(num_classes=num_classes, dropout_rate=0.3)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    model.to(device)
    model.eval()
    
    return model


def main():
    parser = argparse.ArgumentParser(description='Advanced generation and filtering with DDP')
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument('--dit_checkpoint', type=str, 
                       default='./LightningDiT/checkpoints/lightningdit-xl-imagenet256-64ep.pt')
    parser.add_argument('--classifier_checkpoint', type=str, 
                       default='./calibrated_classifier/best_calibrated_model.pth',
                       help='å·²è®­ç»ƒçš„åˆ†ç±»å™¨æ£€æŸ¥ç‚¹')
    parser.add_argument('--vavae_checkpoint', type=str,
                       default='./LightningDiT/checkpoints/vavae_checkpoint.pth')
    parser.add_argument('--config', type=str, 
                       default='configs/dit_s_microdoppler.yaml')
    
    # ç”Ÿæˆé…ç½®  
    parser.add_argument('--output_dir', type=str, 
                       default='/kaggle/working/advanced_filtered_samples')
    parser.add_argument('--samples_per_user', type=int, default=300,
                       help='æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•°')
    parser.add_argument('--target_users', type=str, nargs='+', 
                       default=[f'{i}' for i in range(31)],
                       help='ç›®æ ‡ç”¨æˆ·IDåˆ—è¡¨')
    
    # ç­›é€‰é…ç½®
    parser.add_argument('--confidence_threshold', type=float, default=0.95)
    parser.add_argument('--feature_similarity_threshold', type=float, default=0.8)
    parser.add_argument('--total_threshold', type=float, default=0.8)
    parser.add_argument('--max_samples_per_user', type=int, default=150,
                       help='ç­›é€‰åæ¯ä¸ªç”¨æˆ·ä¿ç•™çš„æœ€å¤§æ ·æœ¬æ•°')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–DDP
    rank, world_size, local_rank = setup_ddp()
    device = torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')
    
    if rank == 0:
        print("ğŸš€ é«˜çº§åˆ†å¸ƒå¼ç”Ÿæˆå’Œç­›é€‰ç³»ç»Ÿå¯åŠ¨")
        print(f"ğŸ“Š ä½¿ç”¨ {world_size} ä¸ª GPU å¹¶è¡Œç”Ÿæˆ")
        print(f"ğŸ¯ ç­›é€‰é˜ˆå€¼: ç½®ä¿¡åº¦>{args.confidence_threshold}")
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    try:
        # åŠ è½½é…ç½®
        with open(args.config, 'r') as f:
            config = yaml.safe_load(f)
        
        # åŠ è½½ VA-VAE
        if rank == 0:
            print("ğŸ“‹ åŠ è½½ VA-VAE...")
        vae = SimplifiedVAVAE.from_pretrained(args.vavae_checkpoint)
        vae.to(device)
        vae.eval()
        
        # åŠ è½½ DiT æ¨¡å‹
        if rank == 0:
            print("ğŸ“‹ åŠ è½½ DiT æ¨¡å‹...")
        dit_model = LightningDiT_models['LightningDiT-XL/2'](
            input_size=16,
            num_classes=32  # 31ä¸ªç”¨æˆ· + 1ä¸ªç©ºç±»
        )
        dit_checkpoint = torch.load(args.dit_checkpoint, map_location='cpu')
        dit_model.load_state_dict(dit_checkpoint['model'])
        dit_model.to(device)
        
        # åŒ…è£…ä¸ºDDP
        if world_size > 1:
            dit_model = DDP(dit_model, device_ids=[local_rank])
        
        # åŠ è½½åˆ†ç±»å™¨
        if rank == 0:
            print("ğŸ“‹ åŠ è½½å·²è®­ç»ƒçš„åˆ†ç±»å™¨...")
        classifier = load_trained_classifier(args.classifier_checkpoint, 31, device)
        
        # åˆ›å»º transport
        transport = create_transport(
            path_type="Linear",
            prediction="velocity", 
            loss_weight=None,
            train_eps=None
        )
        
        # åˆ›å»ºç”Ÿæˆå™¨å’Œç­›é€‰å™¨
        generator = ProgressiveGenerator(dit_model, vae, transport, device, rank, world_size)
        sample_filter = AdvancedSampleFilter([classifier], device)
        
        # ä¸»ç”Ÿæˆå¾ªç¯
        target_users = [int(uid) for uid in args.target_users]
        
        for user_id in target_users:
            if rank == 0:
                print(f"\nğŸ“ å¼€å§‹ç”Ÿæˆç”¨æˆ· ID_{user_id} çš„æ•°æ®...")
            
            # ç”Ÿæˆæ ·æœ¬
            generated_samples = generator.generate_progressive(
                user_id=user_id, 
                samples_per_user=args.samples_per_user
            )
            
            if rank == 0:
                print(f"ğŸ¨ Rank {rank} ç”Ÿæˆäº† {len(generated_samples)} ä¸ªæ ·æœ¬")
            
            # ç­›é€‰é«˜è´¨é‡æ ·æœ¬
            if len(generated_samples) > 0:
                # è½¬æ¢ä¸º tensor
                transform = transforms.Compose([
                    transforms.Resize((256, 256)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
                
                sample_tensors = []
                for img in generated_samples:
                    if isinstance(img, Image.Image):
                        tensor = transform(img)
                        sample_tensors.append(tensor)
                
                if sample_tensors:
                    batch_tensors = torch.stack(sample_tensors).to(device)
                    user_ids = [user_id] * len(sample_tensors)
                    
                    # ç®€åŒ–çš„ç­›é€‰ï¼šåªç”¨ç½®ä¿¡åº¦
                    selected_samples = []
                    classifier.eval()
                    
                    with torch.no_grad():
                        for i, img_tensor in enumerate(batch_tensors):
                            logits, _ = classifier(img_tensor.unsqueeze(0))
                            probs = F.softmax(logits, dim=1)
                            confidence, pred = torch.max(probs, dim=1)
                            
                            if pred.item() == user_id and confidence.item() >= args.confidence_threshold:
                                selected_samples.append(generated_samples[i])
                    
                    # é™åˆ¶æ•°é‡
                    if len(selected_samples) > args.max_samples_per_user:
                        selected_samples = selected_samples[:args.max_samples_per_user]
                    
                    # ä¿å­˜ç­›é€‰åçš„æ ·æœ¬
                    generator.save_samples_distributed(selected_samples, user_id, args.output_dir)
                    
                    if rank == 0:
                        print(f"âœ… ç­›é€‰å®Œæˆï¼š{len(generated_samples)} -> {len(selected_samples)} ä¸ªé«˜è´¨é‡æ ·æœ¬")
        
        if rank == 0:
            print("\nğŸ‰ æ‰€æœ‰ç”¨æˆ·çš„æ•°æ®ç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    
    except Exception as e:
        if rank == 0:
            print(f"âŒ é”™è¯¯: {e}")
        raise
    
    finally:
        cleanup_ddp()
    
if __name__ == "__main__":
    main()

"""
Step 6: è®­ç»ƒæ¡ä»¶LightningDiTæ¨¡å‹
ä½¿ç”¨å¾®è°ƒåçš„VA-VAEæ½œç©ºé—´è¿›è¡Œmicro-Dopplerå›¾åƒç”Ÿæˆ
é’ˆå¯¹Kaggle T4Ã—2 GPUç¯å¢ƒä¼˜åŒ–
"""

import os
import sys
import time
import json
import logging
import types
from pathlib import Path
from PIL import Image
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from omegaconf import OmegaConf
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ LightningDiTè·¯å¾„ï¼ˆå‚è€ƒstep4ï¼‰
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))
sys.path.insert(0, str(project_root))  # æ·»åŠ æ ¹ç›®å½•ä»¥å¯¼å…¥è‡ªå®šä¹‰æ•°æ®é›†

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Kaggle T4å†…å­˜ä¼˜åŒ–è®¾ç½®
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['TORCH_COMPILE_DISABLE'] = '1'
os.environ['TORCHDYNAMO_DISABLE'] = '1'

# ç¦ç”¨torch.compile
import torch._dynamo
torch._dynamo.disable()

# DataParallelè¾…åŠ©å‡½æ•°
def is_main_process():
    """å§‹ç»ˆè¿”å›Trueï¼Œå› ä¸ºä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ"""
    return True


# å¯¼å…¥LightningDiTæ¨¡å—
from transport import create_transport
from models.lightningdit import LightningDiT_models, LightningDiT_B_1

# å¯¼å…¥VA-VAE
from tokenizer.vavae import VA_VAE

# ==================== æ•°æ®é›†å®šä¹‰ ====================
class MicroDopplerLatentDataset(Dataset):
    """å¾®è°ƒåçš„æ½œç©ºé—´æ•°æ®é›†ï¼ŒåŒ…å«ç”¨æˆ·æ¡ä»¶"""
    
    def __init__(self, data_dir, split='train', val_ratio=0.2, latent_norm=True):
        self.data_dir = Path(data_dir)
        self.split = split
        self.latent_norm = latent_norm
        
        # åŠ è½½æ½œç©ºé—´æ•°æ®å’Œæ ‡ç­¾ - ä»å¯å†™ç›®å½•åŠ è½½
        latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'
        stats_file = Path("/kaggle/working") / 'latents_stats.pt'
        
        # åŠ è½½å¾®å¤šæ™®å‹’æ•°æ®é›†è‡ªå·±çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ä½¿ç”¨å®˜æ–¹ImageNetç»Ÿè®¡ï¼‰
        if stats_file.exists():
            stats = torch.load(stats_file)
            self.latent_mean = stats['mean'].float()
            self.latent_std = stats['std'].float()
            logger.info(f"åŠ è½½å¾®å¤šæ™®å‹’æ½œç©ºé—´ç»Ÿè®¡: mean={self.latent_mean.shape}, std={self.latent_std.shape}")
        else:
            self.latent_mean = None
            self.latent_std = None
            
        if latents_file.exists():
            data = np.load(latents_file)
            self.latents = torch.from_numpy(data['latents']).float()
            self.user_ids = torch.from_numpy(data['user_ids']).long()
            
            # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä»æ•°æ®ä¸­è®¡ç®—
            if self.latent_mean is None:
                self.latent_mean = self.latents.mean(dim=[0, 2, 3], keepdim=True)
                self.latent_std = self.latents.std(dim=[0, 2, 3], keepdim=True)
                logger.info("ä»æ•°æ®è®¡ç®—æ½œç©ºé—´ç»Ÿè®¡ä¿¡æ¯")
            
            logger.info(f"Loaded {len(self.latents)} latent samples from {latents_file}")
            
            # è°ƒè¯•ä¿¡æ¯
            if len(self.latents) == 0:
                logger.error("æ½œç©ºé—´æ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºï¼")
                raise ValueError("Empty latents file")
        else:
            # å¦‚æœé¢„è®¡ç®—çš„æ½œç©ºé—´ä¸å­˜åœ¨ï¼Œå®æ—¶ç¼–ç 
            logger.warning(f"Pre-computed latents not found at {latents_file}")
            self.latents = None
            self.load_images()
        
        # åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
        if self.latents is not None:
            n_samples = len(self.latents)
        elif hasattr(self, 'image_paths'):
            n_samples = len(self.image_paths)
        else:
            logger.error("æ²¡æœ‰æ•°æ®å¯ç”¨äºåˆ›å»ºæ•°æ®é›†")
            n_samples = 0
            
        if n_samples == 0:
            logger.error(f"æ•°æ®é›†ä¸ºç©ºï¼latents={self.latents is not None}, "
                        f"has_image_paths={hasattr(self, 'image_paths')}")
            self.indices = np.array([])
        else:
            indices = np.arange(n_samples)
            np.random.seed(42)
            np.random.shuffle(indices)
            
            n_val = int(n_samples * val_ratio)
            if split == 'train':
                self.indices = indices[n_val:]
            else:
                self.indices = indices[:n_val]
        
        logger.info(f"{split} dataset: {len(self.indices)} samples (total: {n_samples})")
    
    def load_images(self):
        """åŠ è½½åŸå§‹å›¾åƒè·¯å¾„"""
        self.image_paths = []
        self.user_ids = []
        
        data_path = self.data_dir / 'processed_microdoppler'
        for user_dir in sorted(data_path.glob('ID_*')):
            user_id = int(user_dir.name.split('_')[1]) - 1
            for img_path in user_dir.glob('*.png'):
                self.image_paths.append(img_path)
                self.user_ids.append(user_id)
        
        self.user_ids = torch.tensor(self.user_ids, dtype=torch.long)
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        real_idx = self.indices[idx]
        
        if self.latents is not None:
            latent = self.latents[real_idx]
            user_id = self.user_ids[real_idx]
            
            # æ½œç©ºé—´å½’ä¸€åŒ–
            if self.latent_norm and self.latent_mean is not None:
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if latent.dim() == 3 and self.latent_mean.dim() == 4:
                    latent = latent.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                latent = (latent - self.latent_mean) / self.latent_std
                if latent.dim() == 4 and latent.shape[0] == 1:
                    latent = latent.squeeze(0)  # ç§»é™¤ä¸´æ—¶batchç»´åº¦
        else:
            # å®æ—¶ç¼–ç ï¼ˆéœ€è¦VAEæ¨¡å‹ï¼‰
            img_path = self.image_paths[real_idx]
            image = Image.open(img_path).convert('RGB')
            image = torch.from_numpy(np.array(image)).float() / 127.5 - 1.0
            image = image.permute(2, 0, 1)
            latent = image  # è¿™é‡Œéœ€è¦VAEç¼–ç ï¼Œæš‚æ—¶è¿”å›åŸå›¾
            user_id = self.user_ids[real_idx]
        
        return latent, user_id


# ==================== è®­ç»ƒå‡½æ•° ====================
def train_dit():
    """ä¸»è®­ç»ƒå‡½æ•° - DataParallelæ¨¡å¼"""
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    logger.info(f"Using device: {device}")
    if torch.cuda.is_available():
        num_gpus = torch.cuda.device_count()
        logger.info(f"Available GPUs: {num_gpus}")
        for i in range(num_gpus):
            logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            logger.info(f"  GPU Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")
    
    # ===== 1. åˆå§‹åŒ–VA-VAEï¼ˆä»…ç”¨äºç¼–ç ï¼‰ =====
    logger.info("=== åˆå§‹åŒ–VA-VAEç¼–ç å™¨ ===")
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å‘æˆ‘ä»¬çš„checkpoint
    vae_checkpoint = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
    
    # åŠ è½½åŸå§‹é…ç½®å¹¶ä¿®æ”¹checkpointè·¯å¾„
    vae_config_path = project_root / 'LightningDiT' / 'tokenizer' / 'configs' / 'vavae_f16d32.yaml'
    vae_config = OmegaConf.load(str(vae_config_path))
    vae_config.ckpt_path = vae_checkpoint  # è®¾ç½®checkpointè·¯å¾„
    
    # ä¿å­˜ä¿®æ”¹åçš„é…ç½®åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_config_path = project_root / 'temp_vavae_config.yaml'
    OmegaConf.save(vae_config, str(temp_config_path))
    
    # ä½¿ç”¨ä¿®æ”¹åçš„é…ç½®åˆå§‹åŒ–VA-VAE
    vae = VA_VAE(
        config=str(temp_config_path),
        img_size=256,
        horizon_flip=0.0,  # è®­ç»ƒæ—¶ä¸éœ€è¦æ°´å¹³ç¿»è½¬ï¼ˆæ•°æ®å¢å¼ºå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•ˆæœå¾ˆå·®ï¼‰
        fp16=True
    )
    
    # VA-VAEå·²ç»é€šè¿‡é…ç½®æ–‡ä»¶åŠ è½½äº†checkpoint
    if os.path.exists(vae_checkpoint):
        logger.info("âœ“ VA-VAE loaded successfully with checkpoint")
    else:
        logger.warning(f"âš ï¸ VA-VAE checkpoint not found at {vae_checkpoint}")
        logger.warning("Using randomly initialized weights")
    
    # VA-VAEçš„modelå·²ç»åœ¨åˆå§‹åŒ–æ—¶è°ƒç”¨äº†.cuda()ï¼Œä¸éœ€è¦.to(device)
    # vae.modelå·²ç»æ˜¯evalæ¨¡å¼
    
    # ===== 2. åˆå§‹åŒ–LightningDiT-Bæ¨¡å‹ =====
    logger.info("=== åˆå§‹åŒ–LightningDiT-B ===")
    latent_size = 16  # 256/16 = 16
    num_users = 31
    
    model = LightningDiT_models["LightningDiT-B/1"](
        input_size=latent_size,
        num_classes=num_users,
        use_qknorm=False,
        use_swiglu=True,
        use_rope=True,
        use_rmsnorm=True,
        wo_shift=False,
        in_channels=32  # VA-VAE f16d32
    )
    
    logger.info(f"Model: LightningDiT-B/1 (768-dim, 12 layers)")
    logger.info(f"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    pretrained_base = "models/lightningdit-b-imagenet256.pt"
    pretrained_xl = "models/lightningdit-xl-imagenet256-64ep.pt"
    
    if os.path.exists(pretrained_base):
        logger.info(f"Loading Base model weights: {pretrained_base}")
        checkpoint = torch.load(pretrained_base, map_location='cpu')
        state_dict = checkpoint.get('model', checkpoint)
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        logger.info(f"Missing: {len(missing)}, Unexpected: {len(unexpected)}")
    elif os.path.exists(pretrained_xl):
        logger.info(f"Base weights not found, trying partial XL loading: {pretrained_xl}")
        checkpoint = torch.load(pretrained_xl, map_location='cpu')
        state_dict = checkpoint.get('model', checkpoint)
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        
        # åªåŠ è½½å…¼å®¹çš„æƒé‡
        compatible = {}
        model_state = model.state_dict()
        for k, v in state_dict.items():
            if k in model_state and v.shape == model_state[k].shape:
                compatible[k] = v
        
        if compatible:
            model.load_state_dict(compatible, strict=False)
            logger.info(f"Loaded {len(compatible)} compatible weights from XL")
    else:
        logger.warning("No pretrained weights found, using random init")
    
    model.to(device)
    
    # å¦‚æœæ˜¯å¤šGPUï¼Œä½¿ç”¨DataParallelåŒ…è£…
    num_gpus = torch.cuda.device_count()
    if num_gpus > 1:
        model = DataParallel(model)
        logger.info(f"Model wrapped with DataParallel using {num_gpus} GPUs")
    
    # åˆ›å»ºEMAæ¨¡å‹
    from copy import deepcopy
    ema_model = deepcopy(model.module if num_gpus > 1 else model).to(device)
    for p in ema_model.parameters():
        p.requires_grad = False
    
    # ===== 3. åˆ›å»ºTransportï¼ˆæ‰©æ•£è¿‡ç¨‹ï¼‰ =====
    transport = create_transport(
        path_type="Linear",
        prediction="velocity",
        loss_weight=None,
        train_eps=None,
        sample_eps=None
    )
    
    # ===== 4. å‡†å¤‡æ•°æ®é›† =====
    if is_main_process():
        logger.info("=== å‡†å¤‡æ•°æ®é›† ===")
    
    # é¦–å…ˆå°è¯•ç”Ÿæˆæ½œç©ºé—´ï¼ˆå¦‚æœéœ€è¦ï¼‰
    # ä¿®æ”¹ä¸ºæ­£ç¡®çš„æ•°æ®é›†è·¯å¾„ - ä¸step3_prepare_dataset.pyä¸€è‡´
    data_dir = Path("/kaggle/input/dataset")
    latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'  # ä¿å­˜åˆ°å¯å†™ç›®å½•
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¸­é¢„è®¡ç®—æ½œç©ºé—´ï¼Œé¿å…å¤šè¿›ç¨‹å†²çª
    if not latents_file.exists() and is_main_process():
        logger.info("é¢„è®¡ç®—æ½œç©ºé—´è¡¨ç¤º...")
        encode_dataset_to_latents(vae, data_dir, device)
        logger.info("æ½œç©ºé—´ç¼–ç å®Œæˆï¼Œç­‰å¾…æ–‡ä»¶å†™å…¥...")
        # ç¡®ä¿æ–‡ä»¶å·²å†™å…¥ç£ç›˜
        import time
        time.sleep(2)
    
    # DataParallelæ¨¡å¼ä¸éœ€è¦åŒæ­¥
    
    # åˆ›å»ºæ•°æ®é‡‡æ ·å™¨ï¼ˆåˆ†å¸ƒå¼ï¼‰
    train_dataset = MicroDopplerLatentDataset(data_dir, split='train')
    val_dataset = MicroDopplerLatentDataset(data_dir, split='val')
    
    # DataParallelæ¨¡å¼ä¸éœ€è¦ç‰¹æ®Šé‡‡æ ·å™¨
    train_sampler = None
    val_sampler = None
    
    # Kaggleç¯å¢ƒæ•°æ®åŠ è½½å™¨ä¼˜åŒ–
    train_loader = DataLoader(
        train_dataset,
        batch_size=4,  # æ¯GPUæ‰¹æ¬¡å¤§å°
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=1,  # Kaggleç¯å¢ƒå‡å°‘workeræ•°é‡
        pin_memory=True,
        persistent_workers=False,  # é¿å…Kaggleç¯å¢ƒä¸­çš„å†…å­˜é—®é¢˜
        prefetch_factor=1
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=4,
        shuffle=False,
        sampler=val_sampler,
        num_workers=1,  # ä¿æŒä¸€è‡´
        pin_memory=True,
        persistent_workers=False,
        prefetch_factor=1
    )
    
    # ===== 5. è®¾ç½®ä¼˜åŒ–å™¨ =====
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=5e-5,
        weight_decay=0.01,
        betas=(0.9, 0.95)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=25,
        eta_min=1e-7
    )
    
    # ===== 6. è®­ç»ƒå¾ªç¯ =====
    logger.info("=== å¼€å§‹è®­ç»ƒ ===")
    
    num_epochs = 25
    best_val_loss = float('inf')
    patience = 5
    patience_counter = 0
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()
    
    for epoch in range(num_epochs):
        # DataParallelæ¨¡å¼ä¸éœ€è¦è®¾ç½®epoch
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_steps = 0
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', disable=not is_main_process())
        for batch in pbar:
            latents = batch[0].to(device)
            user_ids = batch[1].to(device)
            
            # å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
            with torch.cuda.amp.autocast():
                # Transportå†…éƒ¨è‡ªåŠ¨é‡‡æ ·æ—¶é—´
                model_kwargs = {"y": user_ids}
                # DataParallelæ—¶ä½¿ç”¨module
                dit_model = model.module if isinstance(model, DataParallel) else model
                loss_dict = transport.training_losses(dit_model, latents, model_kwargs)
                loss = loss_dict["loss"].mean()
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_norm'])
            
            scaler.step(optimizer)
            scaler.update()
            
            # å®šæœŸæ¸…ç†æ˜¾å­˜ç¼“å­˜
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()
            
            # æ›´æ–°EMAï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
            if is_main_process():
                ema_decay = 0.9999
                model_params = model.module.parameters() if isinstance(model, DataParallel) else model.parameters()
                with torch.no_grad():
                    for ema_p, p in zip(ema_model.parameters(), model_params):
                        ema_p.data.mul_(ema_decay).add_(p.data, alpha=1-ema_decay)
            
            train_loss += loss.item()
            train_steps += 1
            
            if is_main_process():
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_train_loss = train_loss / train_steps
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_steps = 0
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', disable=not is_main_process())
            for batch in val_pbar:
                latents = batch[0].to(device)
                user_ids = batch[1].to(device)
                
                with torch.cuda.amp.autocast():
                    model_kwargs = {"y": user_ids}
                    dit_model = model.module if isinstance(model, DataParallel) else model
                    loss_dict = transport.training_losses(dit_model, latents, model_kwargs)
                    loss = loss_dict["loss"].mean()
                
                val_loss += loss.item()
                val_steps += 1
        
        avg_val_loss = val_loss / val_steps
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±ï¼ˆKaggleä¼˜åŒ–ï¼‰
        if world_size > 1:
            # ä½¿ç”¨æ›´å®‰å…¨çš„æŸå¤±åŒæ­¥æ–¹å¼
            train_loss_tensor = torch.tensor(avg_train_loss, device=device, dtype=torch.float32)
            val_loss_tensor = torch.tensor(avg_val_loss, device=device, dtype=torch.float32)
            
            try:
                dist.all_reduce(train_loss_tensor, op=dist.ReduceOp.AVG)
                dist.all_reduce(val_loss_tensor, op=dist.ReduceOp.AVG)
                avg_train_loss = train_loss_tensor.item()
                avg_val_loss = val_loss_tensor.item()
            except Exception as e:
                if is_main_process():
                    logger.warning(f"Loss synchronization failed: {e}, using local loss")
        
        # æ—¥å¿—è®°å½•ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            logger.info(f"Epoch {epoch+1}/{num_epochs}")
            logger.info(f"  Train Loss: {avg_train_loss:.4f}")
            logger.info(f"  Val Loss: {avg_val_loss:.4f}")
            logger.info(f"  LR: {scheduler.get_last_lr()[0]:.2e}")
        
        # æ—©åœæ£€æŸ¥ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        should_stop = False
        if is_main_process():
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                save_path = f"outputs/dit_best_epoch{epoch+1}_val{avg_val_loss:.4f}.pt"
                os.makedirs("outputs", exist_ok=True)
                model_state = model.module.state_dict() if isinstance(model, DataParallel) else model.state_dict()
                
                try:
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model_state,
                        'ema_state_dict': ema_model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'train_loss': avg_train_loss,
                        'val_loss': avg_val_loss,
                    }, save_path)
                    logger.info(f"  âœ“ Saved best model to {save_path}")
                except Exception as e:
                    logger.warning(f"Failed to save model: {e}")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    logger.info(f"Early stopping triggered at epoch {epoch+1}")
                    should_stop = True
        
        # DataParallelæ¨¡å¼ä¸éœ€è¦åŒæ­¥æ—©åœå†³ç­–
        
        if should_stop:
            break
        
        # å®šæœŸç”Ÿæˆæ ·æœ¬ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if (epoch + 1) % 5 == 0 and is_main_process():
            logger.info("Generating samples...")
            generate_samples(ema_model, vae, transport, device, epoch+1)
        
        # å†…å­˜æ¸…ç†
        torch.cuda.empty_cache()
    
    if is_main_process():
        logger.info("=== è®­ç»ƒå®Œæˆ ===")
        logger.info(f"Best validation loss: {best_val_loss:.4f}")
    
    # DataParallelæ¨¡å¼ä¸éœ€è¦æ¸…ç†


def encode_dataset_to_latents(vae, data_dir, device, stats_path=None):
    """é¢„è®¡ç®—å¹¶ä¿å­˜æ•´ä¸ªæ•°æ®é›†çš„æ½œç©ºé—´è¡¨ç¤º"""
    logger.info("ç¼–ç æ•°æ®é›†åˆ°æ½œç©ºé—´...")
    
    latents_list = []
    user_ids_list = []
    
    # éå†æ‰€æœ‰ç”¨æˆ·æ–‡ä»¶å¤¹
    for user_dir in sorted(data_dir.glob('ID_*')):
        user_id = int(user_dir.name.split('_')[1]) - 1  # ID_1 -> 0
        
        # æ”¶é›†è¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒï¼ˆä¿®æ­£ä¸º.jpgæ ¼å¼ï¼‰
        image_files = sorted(list(user_dir.glob('*.jpg')))
        
        if not image_files:
            logger.warning(f"ç”¨æˆ· {user_dir.name} æ²¡æœ‰æ‰¾åˆ°.jpgå›¾åƒæ–‡ä»¶")
            continue
            
        logger.info(f"ç¼–ç ç”¨æˆ· {user_dir.name}: {len(image_files)} å¼ å›¾åƒ")
        
        for img_path in image_files:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            image = torch.from_numpy(np.array(image)).float() / 127.5 - 1.0
            image = image.permute(2, 0, 1).unsqueeze(0).to(device)
            
            # ç¼–ç åˆ°æ½œç©ºé—´ - ä½¿ç”¨VA_VAEçš„encode_imagesæ–¹æ³•
            with torch.no_grad():
                latent = vae.encode_images(image)
            
            latents_list.append(latent.cpu().numpy())
            user_ids_list.append(user_id)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if not latents_list:
        logger.error("æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ½œç©ºé—´æ•°æ®ï¼")
        raise ValueError("No latent data collected")
    
    # ä¿å­˜æ½œç©ºé—´æ•°æ®
    latents = np.concatenate(latents_list, axis=0)
    user_ids = np.array(user_ids_list)
    
    logger.info(f"å‡†å¤‡ä¿å­˜ {len(latents)} ä¸ªæ½œç©ºé—´æ ·æœ¬")
    logger.info(f"æ½œç©ºé—´å½¢çŠ¶: {latents.shape}")
    logger.info(f"ç”¨æˆ·IDæ•°é‡: {len(user_ids)}, å”¯ä¸€ç”¨æˆ·: {len(np.unique(user_ids))}")
    
    # è®¡ç®—æ½œç©ºé—´ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
    mean = latents.mean(axis=(0, 2, 3), keepdims=True)  # [1, C, 1, 1]
    std = latents.std(axis=(0, 2, 3), keepdims=True)
    
    # ä¿å­˜åˆ°å¯å†™ç›®å½•
    save_path = Path("/kaggle/working") / 'latents_microdoppler.npz'
    stats_save_path = Path("/kaggle/working") / 'latents_stats.pt'
    
    np.savez(save_path, latents=latents, user_ids=user_ids, mean=mean, std=std)
    torch.save({'mean': torch.from_numpy(mean), 'std': torch.from_numpy(std)}, stats_save_path)
    
    logger.info(f"æˆåŠŸä¿å­˜æ½œç©ºé—´æ•°æ®åˆ° {save_path}")
    logger.info(f"æˆåŠŸä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° {stats_save_path}")
    logger.info(f"æ½œç©ºé—´å‡å€¼å½¢çŠ¶: {mean.shape}, æ ‡å‡†å·®å½¢çŠ¶: {std.shape}")
    
    # éªŒè¯ä¿å­˜
    test_data = np.load(save_path)
    logger.info(f"éªŒè¯: åŠ è½½äº† {len(test_data['latents'])} ä¸ªæ ·æœ¬")


def get_training_config():
    """è·å–è®­ç»ƒé…ç½®å‚æ•°"""
    return {
        'batch_size': 8,           # æ¯GPUæ‰¹æ¬¡å¤§å°ï¼ˆä¼˜åŒ–æ˜¾å­˜åˆ©ç”¨ç‡ï¼‰
        'num_epochs': 25,          # è®­ç»ƒè½®æ•°
        'learning_rate': 1e-4,     # å­¦ä¹ ç‡
        'weight_decay': 0.01,      # æƒé‡è¡°å‡
        'num_workers': 1,          # æ•°æ®åŠ è½½å™¨workeræ•°ï¼ˆå‡å°‘CPUå†…å­˜ï¼‰
        'patience': 5,             # æ—©åœè€å¿ƒ
        'gradient_clip_norm': 1.0, # æ¢¯åº¦è£å‰ª
        'warmup_steps': 100,       # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°
        'gradient_checkpointing': True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    }

def print_training_config(model, optimizer, scheduler, config, 
                         train_size, val_size, num_gpus, train_dataset=None):
    """è¾“å‡ºè¯¦ç»†çš„è®­ç»ƒé…ç½®ä¿¡æ¯"""
    
    # è®¡ç®—æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # è·å–æ¨¡å‹é…ç½®
    if hasattr(model, 'module'):  # DDP wrapped
        model_config = model.module
    else:
        model_config = model
    
    # åŠ¨æ€è·å–ç”¨æˆ·ç±»åˆ«æ•°
    num_classes = 31  # é»˜è®¤å€¼
    if train_dataset is not None:
        try:
            # å°è¯•ä»æ•°æ®é›†è·å–ç”¨æˆ·æ•°é‡
            if hasattr(train_dataset, 'user_ids'):
                num_classes = len(torch.unique(train_dataset.user_ids))
            elif hasattr(train_dataset, 'num_classes'):
                num_classes = train_dataset.num_classes
        except:
            pass  # ä½¿ç”¨é»˜è®¤å€¼
    
    print("\n" + "="*80)
    print("ğŸš€ DiTå¾®è°ƒè®­ç»ƒé…ç½®")
    print("="*80)
    
    print(f"ğŸ“Š æ•°æ®é…ç½®:")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {train_size:,}")
    print(f"  éªŒè¯æ ·æœ¬æ•°: {val_size:,}")
    print(f"  æ¯GPUæ‰¹é‡å¤§å°: {config['batch_size']}")
    print(f"  æ€»æ‰¹é‡å¤§å°: {config['batch_size'] * num_gpus}")
    print(f"  ç”¨æˆ·ç±»åˆ«æ•°: {num_classes}")
    
    # åŠ¨æ€è·å–æ¨¡å‹ä¿¡æ¯
    model_type = getattr(model_config, '__class__', type(model_config)).__name__
    input_channels = getattr(model_config, 'in_channels', 'Unknown')
    input_size = getattr(model_config, 'input_size', 'Unknown')
    
    print(f"\nğŸ—ï¸  æ¨¡å‹é…ç½®:")
    print(f"  æ¨¡å‹ç±»å‹: {model_type}")
    if input_size != 'Unknown':
        print(f"  è¾“å…¥å°ºå¯¸: {input_size}Ã—{input_size} (æ½œç©ºé—´)")
    else:
        print(f"  è¾“å…¥å°ºå¯¸: æ¨æ–­ä¸º16Ã—16 (æ½œç©ºé—´)")
    print(f"  è¾“å…¥é€šé“: {input_channels}")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    if hasattr(model_config, 'depth'):
        print(f"  Transformerå±‚æ•°: {model_config.depth}")
    if hasattr(model_config, 'hidden_size'):
        print(f"  éšè—å±‚ç»´åº¦: {model_config.hidden_size}")
    if hasattr(model_config, 'num_heads'):
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {model_config.num_heads}")
    
    print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  ä¼˜åŒ–å™¨: {optimizer.__class__.__name__}")
    print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
    print(f"  æƒé‡è¡°å‡: {optimizer.param_groups[0]['weight_decay']:.2e}")
    print(f"  æ¢¯åº¦è£å‰ª: {config['gradient_clip_norm']}")
    print(f"  æ•°æ®åŠ è½½å™¨workeræ•°: {config['num_workers']}")
    print(f"  è°ƒåº¦å™¨: {scheduler.__class__.__name__}")
    print(f"  æ··åˆç²¾åº¦: å¯ç”¨")
    
    print(f"\nğŸ”§ ç¡¬ä»¶é…ç½®:")
    print(f"  GPUæ•°é‡: {num_gpus}")
    print(f"  å¹¶è¡Œæ–¹å¼: {'DataParallel' if num_gpus > 1 else 'Single GPU'}")
    
    print(f"\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
    print(f"  â€¢ è®­ç»ƒ/éªŒè¯æŸå¤±")
    print(f"  â€¢ æ¢¯åº¦èŒƒæ•°")
    print(f"  â€¢ å­¦ä¹ ç‡å˜åŒ–")
    print(f"  â€¢ æ¯ç§’å¤„ç†æ ·æœ¬æ•°")
    print(f"  â€¢ GPUå†…å­˜ä½¿ç”¨ç‡")
    
    print("="*80 + "\n")


def calculate_metrics(model, loss, optimizer):
    """è®¡ç®—è®­ç»ƒè´¨é‡è¯„ä¼°æŒ‡æ ‡"""
    metrics = {}
    
    # åŸºç¡€æŸå¤±
    metrics['loss'] = loss.item()
    
    # æ¢¯åº¦èŒƒæ•°
    total_norm = 0
    param_count = 0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += 1
    
    if param_count > 0:
        metrics['grad_norm'] = total_norm ** (1. / 2)
    else:
        metrics['grad_norm'] = 0.0
    
    # å­¦ä¹ ç‡
    metrics['lr'] = optimizer.param_groups[0]['lr']
    
    return metrics


def train_dit_kaggle():
    """Kaggleç¯å¢ƒä¸‹çš„DiTå¾®è°ƒè®­ç»ƒ - ä½¿ç”¨DataParallel"""
    logger.info("å¼€å§‹Kaggle DiTå¾®è°ƒè®­ç»ƒ...")
    
    # æ£€æŸ¥GPUçŠ¶æ€
    if not torch.cuda.is_available():
        raise RuntimeError("éœ€è¦GPUè¿›è¡Œè®­ç»ƒ")
    
    # æ£€æµ‹GPUæ•°é‡
    n_gpus = torch.cuda.device_count()
    logger.info(f"æ£€æµ‹åˆ° {n_gpus} ä¸ªGPU")
    
    # è¯¦ç»†GPUä¿¡æ¯
    for i in range(n_gpus):
        props = torch.cuda.get_device_properties(i)
        logger.info(f"GPU {i}: {props.name}, æ˜¾å­˜: {props.total_memory / 1024**3:.1f}GB")
        torch.cuda.set_device(i)
        torch.cuda.empty_cache()
    
    # é¢„å¤„ç†ï¼šç¡®ä¿æ½œç©ºé—´æ•°æ®å‡†å¤‡å¥½
    latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'
    if not latents_file.exists():
        logger.info("é¢„è®¡ç®—æ½œç©ºé—´æ•°æ®...")
        prepare_latents_for_training()
        logger.info("æ½œç©ºé—´é¢„è®¡ç®—å®Œæˆ")
    
    # ç›´æ¥ä½¿ç”¨DataParallelè®­ç»ƒï¼Œé¿å…DDPçš„å¤æ‚æ€§
    train_with_dataparallel(n_gpus)


def train_with_dataparallel(n_gpus):
    """ä½¿ç”¨DataParallelè¿›è¡Œè®­ç»ƒ"""
    # è·å–è®­ç»ƒé…ç½®
    config = get_training_config()
    
    # è®¾ç½®ä¸»è®¾å¤‡
    device = torch.device('cuda:0')
    torch.cuda.set_device(0)
    
    # æ¸…ç†æ˜¾å­˜
    for i in range(n_gpus):
        torch.cuda.set_device(i)
        torch.cuda.empty_cache()
    torch.cuda.set_device(0)
    
    # åŠ è½½é…ç½®
    config_path = Path("../configs/microdoppler_finetune.yaml")
    model_config = OmegaConf.load(config_path).model
    
    # ä»é…ç½®æ–‡ä»¶è·å–æ½œç©ºé—´ä¿¡æ¯
    H_latent = model_config.params.latent_size
    W_latent = model_config.params.latent_size  
    C_latent = model_config.params.in_channels
    
    logger.info(f"æ½œç©ºé—´ç»´åº¦: {H_latent}x{W_latent}x{C_latent}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MicroDopplerLatentDataset(
        data_dir="/kaggle/input/dataset",
        split='train'
    )
    
    val_dataset = MicroDopplerLatentDataset(
        data_dir="/kaggle/input/dataset", 
        split='val'
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'] * n_gpus,  # æ€»batch size
        shuffle=True,
        num_workers=config['num_workers'],
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'] * n_gpus,
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨Bæ¨¡å‹ä»¥é€‚é…T4æ˜¾å­˜
    model = LightningDiT_B_1(
        input_size=H_latent,
        in_channels=C_latent,
        num_classes=31,
        learn_sigma=True
    ).to(device)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    model.use_checkpoint = True
    
    # DataParallelåŒ…è£…
    if n_gpus > 1:
        logger.info(f"ä½¿ç”¨ DataParallel åœ¨ {n_gpus} ä¸ªGPUä¸Šè®­ç»ƒ")
        model = nn.DataParallel(model, device_ids=list(range(n_gpus)))
    
    # åˆ›å»ºtransport
    transport = create_transport(
        'Linear',
        'velocity',
        None,
        None,
        None,
    )
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config['num_epochs'] * len(train_loader),
        eta_min=1e-6
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler(init_scale=65536.0, growth_interval=2000)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print_training_config(model, optimizer, scheduler, config, 
                         len(train_dataset), len(val_dataset), n_gpus, train_dataset)
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    train_metrics_history = []
    val_metrics_history = []
    
    for epoch in range(config['num_epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_steps = 0
        train_grad_norm = 0
        train_samples = 0
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config["num_epochs"]} [Train]')
        
        for batch_idx, batch in enumerate(pbar):
            batch_start_time = time.time()
            latents = batch[0].to(device)
            user_ids = batch[1].to(device)
            
            # å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast():
                model_kwargs = {"y": user_ids}
                loss_dict = transport.training_losses(model, latents, model_kwargs)
                loss = loss_dict["loss"].mean()
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            
            # æ¢¯åº¦è£å‰ª
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_norm'])
            
            scaler.step(optimizer)
            scaler.update()
            
            # å®šæœŸæ¸…ç†æ˜¾å­˜
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()
            
            # ç»Ÿè®¡
            train_loss += loss.item()
            train_grad_norm += grad_norm.item() if hasattr(grad_norm, 'item') else grad_norm
            train_steps += 1
            train_samples += latents.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            current_lr = optimizer.param_groups[0]['lr']
            samples_per_sec = latents.size(0) / (time.time() - batch_start_time)
            current_memory = torch.cuda.max_memory_allocated(0) / 1024**3
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{current_lr:.2e}',
                'grad': f'{grad_norm:.2f}',
                'sps': f'{samples_per_sec:.1f}',
                'mem': f'{current_memory:.1f}GB'
            })
            
            scheduler.step()
        
        # è®­ç»ƒé˜¶æ®µç»Ÿè®¡
        avg_train_loss = train_loss / train_steps
        avg_train_grad_norm = train_grad_norm / train_steps
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_steps = 0
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{config["num_epochs"]} [Val]')
            for batch in pbar:
                latents = batch[0].to(device)
                user_ids = batch[1].to(device)
                
                with torch.cuda.amp.autocast():
                    model_kwargs = {"y": user_ids}
                    loss_dict = transport.training_losses(model, latents, model_kwargs)
                    loss = loss_dict["loss"].mean()
                
                val_loss += loss.item()
                val_steps += 1
                
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_val_loss = val_loss / val_steps
        
        # è®°å½•æŒ‡æ ‡
        train_metrics_history.append({
            'epoch': epoch + 1,
            'loss': avg_train_loss,
            'grad_norm': avg_train_grad_norm,
            'lr': current_lr
        })
        
        val_metrics_history.append({
            'epoch': epoch + 1,
            'loss': avg_val_loss
        })
        
        # æ‰“å°epochæ€»ç»“
        logger.info(f"\nEpoch {epoch+1}/{config['num_epochs']} å®Œæˆ:")
        logger.info(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
        logger.info(f"  éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
        logger.info(f"  æ¢¯åº¦èŒƒæ•°: {avg_train_grad_norm:.4f}")
        logger.info(f"  å­¦ä¹ ç‡: {current_lr:.2e}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_path = Path("/kaggle/working") / f"best_dit_epoch_{epoch+1}.pt"
            model_state = model.module.state_dict() if n_gpus > 1 else model.state_dict()
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model_state,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_loss': avg_val_loss,
                'config': config
            }, best_model_path)
            logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {best_model_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = Path("/kaggle/working") / "final_dit_model.pt"
    model_state = model.module.state_dict() if n_gpus > 1 else model.state_dict()
    torch.save({
        'model_state_dict': model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'config': config,
        'training_history': {
            'train_metrics': train_metrics_history,
            'val_metrics': val_metrics_history
        }
    }, final_model_path)
    
    logger.info(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    logger.info(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {final_model_path}")


def prepare_latents_for_training():
    """é¢„å¤„ç†ï¼šåªåœ¨ä¸»è¿›ç¨‹ä¸­å‡†å¤‡æ½œç©ºé—´æ•°æ®"""
    # æ£€æŸ¥GPUçŠ¶æ€å’Œæ˜¾å­˜
    device = torch.device('cuda:0')
    torch.cuda.set_device(0)
    torch.cuda.empty_cache()
    
    logger.info("=== å‡†å¤‡æ½œç©ºé—´æ•°æ® ===")
    initial_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"åˆå§‹æ˜¾å­˜ä½¿ç”¨: {initial_memory:.2f}GB")
    
    # ä¿®æ­£é…ç½®ä¸­çš„checkpointè·¯å¾„ä¸ºå¾®è°ƒæ¨¡å‹
    logger.info("å‡†å¤‡VA-VAEé…ç½®...")
    vae_config_path = Path("/kaggle/working/VA-VAE/LightningDiT/tokenizer/configs/vavae_f16d32.yaml")
    
    with open(vae_config_path, 'r') as f:
        config_content = f.read()
    
    # ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹  
    finetuned_checkpoint = '/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt'
    if finetuned_checkpoint not in config_content:
        config_content = config_content.replace(
            'ckpt_path: /path/to/checkpoint.pt',
            f'ckpt_path: {finetuned_checkpoint}'
        )
        with open(vae_config_path, 'w') as f:
            f.write(config_content)
        logger.info(f"å·²æ›´æ–°VA-VAE checkpointè·¯å¾„: {finetuned_checkpoint}")
    
    # åˆå§‹åŒ–VA-VAEè¿›è¡Œæ½œç©ºé—´ç¼–ç 
    from tokenizer.vavae import VA_VAE
    logger.info("åŠ è½½VA-VAEæ¨¡å‹...")
    vae = VA_VAE(str(vae_config_path), img_size=256, horizon_flip=False, fp16=True)
    vae_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"VA-VAEåŠ è½½å®Œæˆï¼Œæ˜¾å­˜: {vae_memory:.2f}GB")
    
    # é¢„è®¡ç®—æ½œç©ºé—´
    data_dir = Path("/kaggle/input/dataset")
    latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'
    if not latents_file.exists():
        logger.info("å¼€å§‹ç¼–ç æ•°æ®é›†åˆ°æ½œç©ºé—´...")
        encode_dataset_to_latents(vae, data_dir, device)
        logger.info("æ½œç©ºé—´ç¼–ç å®Œæˆ")
    else:
        logger.info("æ½œç©ºé—´æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç¼–ç ")
    
    # é‡Šæ”¾VA-VAEæ˜¾å­˜
    del vae
    torch.cuda.empty_cache()
    final_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"VA-VAEé‡Šæ”¾åæ˜¾å­˜: {final_memory:.2f}GB")


# DDPè®­ç»ƒå‡½æ•°å·²åˆ é™¤ï¼Œæ”¹ç”¨DataParallel


def generate_samples(model, vae, transport, device, epoch):
    """ç”Ÿæˆæ ·æœ¬ç”¨äºå¯è§†åŒ–"""
    model.eval()
    vae.eval()
    
    with torch.no_grad():
        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆä¸€ä¸ªæ ·æœ¬
        num_samples = min(8, 31)  # ç”Ÿæˆ8ä¸ªç”¨æˆ·çš„æ ·æœ¬
        user_ids = torch.arange(num_samples, device=device)
        
        # é‡‡æ ·æ½œç©ºé—´
        z = torch.randn(num_samples, 32, 16, 16, device=device)
        model_kwargs = {"y": user_ids}
        
        # ä½¿ç”¨DDIMé‡‡æ ·
        samples = transport.sample_with_model_kwargs(
            model, z, model_kwargs, 
            steps=50,  # é‡‡æ ·æ­¥æ•°
            eta=0.0,   # DDIMç¡®å®šæ€§é‡‡æ ·
            guidance_scale=2.0  # CFGå¼ºåº¦
        )
        
        # è§£ç åˆ°å›¾åƒç©ºé—´
        images = vae.decode(samples)
        images = (images + 1) / 2  # [-1,1] -> [0,1]
        images = images.clamp(0, 1)
        
        # ä¿å­˜å›¾åƒ
        save_dir = Path("outputs/samples")
        save_dir.mkdir(parents=True, exist_ok=True)
        
        for i, img in enumerate(images):
            img_pil = Image.fromarray((img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))
            img_pil.save(save_dir / f"epoch{epoch}_user{user_ids[i].item()}.png")
        
        logger.info(f"Saved {num_samples} samples to {save_dir}")


def main():
    """ä¸»å‡½æ•° - Kaggle T4x2ä¼˜åŒ–ç‰ˆæœ¬"""
    # Kaggleç¯å¢ƒGPUæ£€æŸ¥
    if not torch.cuda.is_available():
        logger.error("CUDA not available! Please enable GPU accelerator in Kaggle.")
        return
    
    num_gpus = torch.cuda.device_count()
    logger.info(f"Detected {num_gpus} GPU(s)")
    
    # Kaggle T4x2ä½¿ç”¨DataParallelï¼Œä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    train_dit_kaggle()

if __name__ == "__main__":
    main()

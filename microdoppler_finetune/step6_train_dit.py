"""
Step 6: è®­ç»ƒæ¡ä»¶LightningDiTæ¨¡å‹
ä½¿ç”¨å¾®è°ƒåçš„VA-VAEæ½œç©ºé—´è¿›è¡Œmicro-Dopplerå›¾åƒç”Ÿæˆ
é’ˆå¯¹Kaggle T4Ã—2 GPUç¯å¢ƒä¼˜åŒ–
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, DistributedSampler
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import numpy as np
from pathlib import Path
import json
import logging
from tqdm import tqdm
from datetime import datetime, timedelta
import time
import matplotlib.pyplot as plt
from PIL import Image
from omegaconf import OmegaConf
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ LightningDiTè·¯å¾„ï¼ˆå‚è€ƒstep4ï¼‰
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))
sys.path.insert(0, str(project_root))  # æ·»åŠ æ ¹ç›®å½•ä»¥å¯¼å…¥è‡ªå®šä¹‰æ•°æ®é›†

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Kaggle T4å†…å­˜ä¼˜åŒ–è®¾ç½®
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['TORCH_COMPILE_DISABLE'] = '1'
os.environ['TORCHDYNAMO_DISABLE'] = '1'

# ç¦ç”¨torch.compile
import torch._dynamo
torch._dynamo.disable()

# åˆ†å¸ƒå¼è®­ç»ƒç›¸å…³å‡½æ•°
def setup(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ - Kaggleä¼˜åŒ–ç‰ˆæœ¬"""
    # Kaggleç¯å¢ƒé…ç½®
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29500'  # ä½¿ç”¨æ ‡å‡†ç«¯å£
    os.environ['WORLD_SIZE'] = str(world_size)
    os.environ['RANK'] = str(rank)
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
    dist.init_process_group(
        backend="nccl", 
        rank=rank, 
        world_size=world_size,
        timeout=timedelta(seconds=60)
    )
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU
    torch.cuda.set_device(rank)
    
    # æ‰“å°åˆ†å¸ƒå¼ä¿¡æ¯
    if rank == 0:
        logger.info(f"Initialized DDP with {world_size} GPUs")
        for i in range(world_size):
            logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")

def cleanup():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.destroy_process_group()

def is_main_process():
    """æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return not dist.is_initialized() or dist.get_rank() == 0

# å¯¼å…¥LightningDiTæ¨¡å—
from transport import create_transport
from models.lightningdit import LightningDiT_models

# å¯¼å…¥VA-VAE
from tokenizer.vavae import VA_VAE

# ==================== æ•°æ®é›†å®šä¹‰ ====================
class MicroDopplerLatentDataset(Dataset):
    """å¾®è°ƒåçš„æ½œç©ºé—´æ•°æ®é›†ï¼ŒåŒ…å«ç”¨æˆ·æ¡ä»¶"""
    
    def __init__(self, data_dir, split='train', val_ratio=0.2, latent_norm=True):
        self.data_dir = Path(data_dir)
        self.split = split
        self.latent_norm = latent_norm
        
        # åŠ è½½æ½œç©ºé—´æ•°æ®å’Œæ ‡ç­¾ - ä»å¯å†™ç›®å½•åŠ è½½
        latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'
        stats_file = Path("/kaggle/working") / 'latents_stats.pt'
        
        # åŠ è½½å¾®å¤šæ™®å‹’æ•°æ®é›†è‡ªå·±çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ä½¿ç”¨å®˜æ–¹ImageNetç»Ÿè®¡ï¼‰
        if stats_file.exists():
            stats = torch.load(stats_file)
            self.latent_mean = stats['mean'].float()
            self.latent_std = stats['std'].float()
            logger.info(f"åŠ è½½å¾®å¤šæ™®å‹’æ½œç©ºé—´ç»Ÿè®¡: mean={self.latent_mean.shape}, std={self.latent_std.shape}")
        else:
            self.latent_mean = None
            self.latent_std = None
            
        if latents_file.exists():
            data = np.load(latents_file)
            self.latents = torch.from_numpy(data['latents']).float()
            self.user_ids = torch.from_numpy(data['user_ids']).long()
            
            # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä»æ•°æ®ä¸­è®¡ç®—
            if self.latent_mean is None:
                self.latent_mean = self.latents.mean(dim=[0, 2, 3], keepdim=True)
                self.latent_std = self.latents.std(dim=[0, 2, 3], keepdim=True)
                logger.info("ä»æ•°æ®è®¡ç®—æ½œç©ºé—´ç»Ÿè®¡ä¿¡æ¯")
            
            logger.info(f"Loaded {len(self.latents)} latent samples from {latents_file}")
            
            # è°ƒè¯•ä¿¡æ¯
            if len(self.latents) == 0:
                logger.error("æ½œç©ºé—´æ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºï¼")
                raise ValueError("Empty latents file")
        else:
            # å¦‚æœé¢„è®¡ç®—çš„æ½œç©ºé—´ä¸å­˜åœ¨ï¼Œå®æ—¶ç¼–ç 
            logger.warning(f"Pre-computed latents not found at {latents_file}")
            self.latents = None
            self.load_images()
        
        # åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
        if self.latents is not None:
            n_samples = len(self.latents)
        elif hasattr(self, 'image_paths'):
            n_samples = len(self.image_paths)
        else:
            logger.error("æ²¡æœ‰æ•°æ®å¯ç”¨äºåˆ›å»ºæ•°æ®é›†")
            n_samples = 0
            
        if n_samples == 0:
            logger.error(f"æ•°æ®é›†ä¸ºç©ºï¼latents={self.latents is not None}, "
                        f"has_image_paths={hasattr(self, 'image_paths')}")
            self.indices = np.array([])
        else:
            indices = np.arange(n_samples)
            np.random.seed(42)
            np.random.shuffle(indices)
            
            n_val = int(n_samples * val_ratio)
            if split == 'train':
                self.indices = indices[n_val:]
            else:
                self.indices = indices[:n_val]
        
        logger.info(f"{split} dataset: {len(self.indices)} samples (total: {n_samples})")
    
    def load_images(self):
        """åŠ è½½åŸå§‹å›¾åƒè·¯å¾„"""
        self.image_paths = []
        self.user_ids = []
        
        data_path = self.data_dir / 'processed_microdoppler'
        for user_dir in sorted(data_path.glob('ID_*')):
            user_id = int(user_dir.name.split('_')[1]) - 1
            for img_path in user_dir.glob('*.png'):
                self.image_paths.append(img_path)
                self.user_ids.append(user_id)
        
        self.user_ids = torch.tensor(self.user_ids, dtype=torch.long)
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        real_idx = self.indices[idx]
        
        if self.latents is not None:
            latent = self.latents[real_idx]
            user_id = self.user_ids[real_idx]
            
            # æ½œç©ºé—´å½’ä¸€åŒ–
            if self.latent_norm and self.latent_mean is not None:
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if latent.dim() == 3 and self.latent_mean.dim() == 4:
                    latent = latent.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                latent = (latent - self.latent_mean) / self.latent_std
                if latent.dim() == 4 and latent.shape[0] == 1:
                    latent = latent.squeeze(0)  # ç§»é™¤ä¸´æ—¶batchç»´åº¦
        else:
            # å®æ—¶ç¼–ç ï¼ˆéœ€è¦VAEæ¨¡å‹ï¼‰
            img_path = self.image_paths[real_idx]
            image = Image.open(img_path).convert('RGB')
            image = torch.from_numpy(np.array(image)).float() / 127.5 - 1.0
            image = image.permute(2, 0, 1)
            latent = image  # è¿™é‡Œéœ€è¦VAEç¼–ç ï¼Œæš‚æ—¶è¿”å›åŸå›¾
            user_id = self.user_ids[real_idx]
        
        return latent, user_id


# ==================== è®­ç»ƒå‡½æ•° ====================
def train_dit(rank=0, world_size=1):
    """ä¸»è®­ç»ƒå‡½æ•° - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ"""
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    if world_size > 1:
        setup(rank, world_size)
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')
    
    if is_main_process():
        logger.info(f"World size: {world_size}, Rank: {rank}")
        logger.info(f"Using device: {device}")
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(rank)}")
            logger.info(f"GPU Memory: {torch.cuda.get_device_properties(rank).total_memory / 1e9:.1f} GB")
    
    # ===== 1. åˆå§‹åŒ–VA-VAEï¼ˆä»…ç”¨äºç¼–ç ï¼‰ =====
    logger.info("=== åˆå§‹åŒ–VA-VAEç¼–ç å™¨ ===")
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å‘æˆ‘ä»¬çš„checkpoint
    vae_checkpoint = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
    
    # åŠ è½½åŸå§‹é…ç½®å¹¶ä¿®æ”¹checkpointè·¯å¾„
    vae_config_path = project_root / 'LightningDiT' / 'tokenizer' / 'configs' / 'vavae_f16d32.yaml'
    vae_config = OmegaConf.load(str(vae_config_path))
    vae_config.ckpt_path = vae_checkpoint  # è®¾ç½®checkpointè·¯å¾„
    
    # ä¿å­˜ä¿®æ”¹åçš„é…ç½®åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_config_path = project_root / 'temp_vavae_config.yaml'
    OmegaConf.save(vae_config, str(temp_config_path))
    
    # ä½¿ç”¨ä¿®æ”¹åçš„é…ç½®åˆå§‹åŒ–VA-VAE
    vae = VA_VAE(
        config=str(temp_config_path),
        img_size=256,
        horizon_flip=0.0,  # è®­ç»ƒæ—¶ä¸éœ€è¦æ°´å¹³ç¿»è½¬ï¼ˆæ•°æ®å¢å¼ºå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•ˆæœå¾ˆå·®ï¼‰
        fp16=True
    )
    
    # VA-VAEå·²ç»é€šè¿‡é…ç½®æ–‡ä»¶åŠ è½½äº†checkpoint
    if os.path.exists(vae_checkpoint):
        logger.info("âœ“ VA-VAE loaded successfully with checkpoint")
    else:
        logger.warning(f"âš ï¸ VA-VAE checkpoint not found at {vae_checkpoint}")
        logger.warning("Using randomly initialized weights")
    
    # VA-VAEçš„modelå·²ç»åœ¨åˆå§‹åŒ–æ—¶è°ƒç”¨äº†.cuda()ï¼Œä¸éœ€è¦.to(device)
    # vae.modelå·²ç»æ˜¯evalæ¨¡å¼
    
    # ===== 2. åˆå§‹åŒ–LightningDiT-Bæ¨¡å‹ =====
    logger.info("=== åˆå§‹åŒ–LightningDiT-B ===")
    latent_size = 16  # 256/16 = 16
    num_users = 31
    
    model = LightningDiT_models["LightningDiT-B/1"](
        input_size=latent_size,
        num_classes=num_users,
        use_qknorm=False,
        use_swiglu=True,
        use_rope=True,
        use_rmsnorm=True,
        wo_shift=False,
        in_channels=32  # VA-VAE f16d32
    )
    
    logger.info(f"Model: LightningDiT-B/1 (768-dim, 12 layers)")
    logger.info(f"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    pretrained_base = "models/lightningdit-b-imagenet256.pt"
    pretrained_xl = "models/lightningdit-xl-imagenet256-64ep.pt"
    
    if os.path.exists(pretrained_base):
        logger.info(f"Loading Base model weights: {pretrained_base}")
        checkpoint = torch.load(pretrained_base, map_location='cpu')
        state_dict = checkpoint.get('model', checkpoint)
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        logger.info(f"Missing: {len(missing)}, Unexpected: {len(unexpected)}")
    elif os.path.exists(pretrained_xl):
        logger.info(f"Base weights not found, trying partial XL loading: {pretrained_xl}")
        checkpoint = torch.load(pretrained_xl, map_location='cpu')
        state_dict = checkpoint.get('model', checkpoint)
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        
        # åªåŠ è½½å…¼å®¹çš„æƒé‡
        compatible = {}
        model_state = model.state_dict()
        for k, v in state_dict.items():
            if k in model_state and v.shape == model_state[k].shape:
                compatible[k] = v
        
        if compatible:
            model.load_state_dict(compatible, strict=False)
            logger.info(f"Loaded {len(compatible)} compatible weights from XL")
    else:
        logger.warning("No pretrained weights found, using random init")
    
    model.to(device)
    
    # åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ¨¡å‹ï¼ˆKaggleä¼˜åŒ–ï¼‰
    if world_size > 1:
        model = DDP(
            model, 
            device_ids=[rank], 
            output_device=rank, 
            find_unused_parameters=False,  # è®¾ä¸ºFalseä»¥æé«˜æ€§èƒ½
            broadcast_buffers=True,
            gradient_as_bucket_view=True  # å†…å­˜ä¼˜åŒ–
        )
        if is_main_process():
            logger.info(f"Model wrapped with DDP on rank {rank}")
    
    # åˆ›å»ºEMAæ¨¡å‹
    from copy import deepcopy
    ema_model = deepcopy(model.module if world_size > 1 else model).to(device)
    for p in ema_model.parameters():
        p.requires_grad = False
    
    # ===== 3. åˆ›å»ºTransportï¼ˆæ‰©æ•£è¿‡ç¨‹ï¼‰ =====
    transport = create_transport(
        path_type="Linear",
        prediction="velocity",
        loss_weight=None,
        train_eps=None,
        sample_eps=None
    )
    
    # ===== 4. å‡†å¤‡æ•°æ®é›† =====
    if is_main_process():
        logger.info("=== å‡†å¤‡æ•°æ®é›† ===")
    
    # é¦–å…ˆå°è¯•ç”Ÿæˆæ½œç©ºé—´ï¼ˆå¦‚æœéœ€è¦ï¼‰
    # ä¿®æ”¹ä¸ºæ­£ç¡®çš„æ•°æ®é›†è·¯å¾„ - ä¸step3_prepare_dataset.pyä¸€è‡´
    data_dir = Path("/kaggle/input/dataset")
    latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'  # ä¿å­˜åˆ°å¯å†™ç›®å½•
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¸­é¢„è®¡ç®—æ½œç©ºé—´ï¼Œé¿å…å¤šè¿›ç¨‹å†²çª
    if not latents_file.exists() and is_main_process():
        logger.info("é¢„è®¡ç®—æ½œç©ºé—´è¡¨ç¤º...")
        encode_dataset_to_latents(vae, data_dir, device)
        logger.info("æ½œç©ºé—´ç¼–ç å®Œæˆï¼Œç­‰å¾…æ–‡ä»¶å†™å…¥...")
        # ç¡®ä¿æ–‡ä»¶å·²å†™å…¥ç£ç›˜
        import time
        time.sleep(2)
    
    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿æ½œç©ºé—´æ•°æ®å·²ç”Ÿæˆ
    if world_size > 1:
        dist.barrier()
        logger.info(f"Rank {rank} åŒæ­¥å®Œæˆ")
    
    # åˆ›å»ºæ•°æ®é‡‡æ ·å™¨ï¼ˆåˆ†å¸ƒå¼ï¼‰
    train_dataset = MicroDopplerLatentDataset(data_dir, split='train')
    val_dataset = MicroDopplerLatentDataset(data_dir, split='val')
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼ˆKaggleä¼˜åŒ–ï¼‰
    if world_size > 1:
        train_sampler = DistributedSampler(
            train_dataset, 
            num_replicas=world_size, 
            rank=rank,
            shuffle=True,
            drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
        )
        val_sampler = DistributedSampler(
            val_dataset, 
            num_replicas=world_size, 
            rank=rank, 
            shuffle=False,
            drop_last=False
        )
    else:
        train_sampler = None
        val_sampler = None
    
    # Kaggleç¯å¢ƒæ•°æ®åŠ è½½å™¨ä¼˜åŒ–
    train_loader = DataLoader(
        train_dataset,
        batch_size=4,  # æ¯GPUæ‰¹æ¬¡å¤§å°
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=1,  # Kaggleç¯å¢ƒå‡å°‘workeræ•°é‡
        pin_memory=True,
        persistent_workers=False,  # é¿å…Kaggleç¯å¢ƒä¸­çš„å†…å­˜é—®é¢˜
        prefetch_factor=1
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=4,
        shuffle=False,
        sampler=val_sampler,
        num_workers=1,  # ä¿æŒä¸€è‡´
        pin_memory=True,
        persistent_workers=False,
        prefetch_factor=1
    )
    
    # ===== 5. è®¾ç½®ä¼˜åŒ–å™¨ =====
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=5e-5,
        weight_decay=0.01,
        betas=(0.9, 0.95)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=25,
        eta_min=1e-7
    )
    
    # ===== 6. è®­ç»ƒå¾ªç¯ =====
    logger.info("=== å¼€å§‹è®­ç»ƒ ===")
    
    num_epochs = 25
    best_val_loss = float('inf')
    patience = 5
    patience_counter = 0
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()
    
    for epoch in range(num_epochs):
        # è®¾ç½®epoch for distributed sampler
        if train_sampler is not None:
            train_sampler.set_epoch(epoch)
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_steps = 0
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', disable=not is_main_process())
        for batch in pbar:
            latents = batch[0].to(device)
            user_ids = batch[1].to(device)
            
            # å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
            with torch.cuda.amp.autocast():
                # Transportå†…éƒ¨è‡ªåŠ¨é‡‡æ ·æ—¶é—´
                model_kwargs = {"y": user_ids}
                # åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä½¿ç”¨module
                dit_model = model.module if world_size > 1 else model
                loss_dict = transport.training_losses(dit_model, latents, model_kwargs)
                loss = loss_dict["loss"].mean()
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_norm'])
            
            scaler.step(optimizer)
            scaler.update()
            
            # å®šæœŸæ¸…ç†æ˜¾å­˜ç¼“å­˜
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()
            
            # æ›´æ–°EMAï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
            if is_main_process():
                ema_decay = 0.9999
                model_params = model.module.parameters() if world_size > 1 else model.parameters()
                with torch.no_grad():
                    for ema_p, p in zip(ema_model.parameters(), model_params):
                        ema_p.data.mul_(ema_decay).add_(p.data, alpha=1-ema_decay)
            
            train_loss += loss.item()
            train_steps += 1
            
            if is_main_process():
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_train_loss = train_loss / train_steps
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_steps = 0
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', disable=not is_main_process())
            for batch in val_pbar:
                latents = batch[0].to(device)
                user_ids = batch[1].to(device)
                
                with torch.cuda.amp.autocast():
                    model_kwargs = {"y": user_ids}
                    dit_model = model.module if world_size > 1 else model
                    loss_dict = transport.training_losses(dit_model, latents, model_kwargs)
                    loss = loss_dict["loss"].mean()
                
                val_loss += loss.item()
                val_steps += 1
        
        avg_val_loss = val_loss / val_steps
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±ï¼ˆKaggleä¼˜åŒ–ï¼‰
        if world_size > 1:
            # ä½¿ç”¨æ›´å®‰å…¨çš„æŸå¤±åŒæ­¥æ–¹å¼
            train_loss_tensor = torch.tensor(avg_train_loss, device=device, dtype=torch.float32)
            val_loss_tensor = torch.tensor(avg_val_loss, device=device, dtype=torch.float32)
            
            try:
                dist.all_reduce(train_loss_tensor, op=dist.ReduceOp.AVG)
                dist.all_reduce(val_loss_tensor, op=dist.ReduceOp.AVG)
                avg_train_loss = train_loss_tensor.item()
                avg_val_loss = val_loss_tensor.item()
            except Exception as e:
                if is_main_process():
                    logger.warning(f"Loss synchronization failed: {e}, using local loss")
        
        # æ—¥å¿—è®°å½•ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            logger.info(f"Epoch {epoch+1}/{num_epochs}")
            logger.info(f"  Train Loss: {avg_train_loss:.4f}")
            logger.info(f"  Val Loss: {avg_val_loss:.4f}")
            logger.info(f"  LR: {scheduler.get_last_lr()[0]:.2e}")
        
        # æ—©åœæ£€æŸ¥ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        should_stop = False
        if is_main_process():
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                save_path = f"outputs/dit_best_epoch{epoch+1}_val{avg_val_loss:.4f}.pt"
                os.makedirs("outputs", exist_ok=True)
                model_state = model.module.state_dict() if world_size > 1 else model.state_dict()
                
                try:
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model_state,
                        'ema_state_dict': ema_model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'train_loss': avg_train_loss,
                        'val_loss': avg_val_loss,
                    }, save_path)
                    logger.info(f"  âœ“ Saved best model to {save_path}")
                except Exception as e:
                    logger.warning(f"Failed to save model: {e}")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    logger.info(f"Early stopping triggered at epoch {epoch+1}")
                    should_stop = True
        
        # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åŒæ­¥æ—©åœå†³ç­–
        if world_size > 1:
            stop_tensor = torch.tensor(1 if should_stop else 0, device=device, dtype=torch.int)
            dist.broadcast(stop_tensor, src=0)
            should_stop = stop_tensor.item() == 1
        
        if should_stop:
            break
        
        # å®šæœŸç”Ÿæˆæ ·æœ¬ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if (epoch + 1) % 5 == 0 and is_main_process():
            logger.info("Generating samples...")
            generate_samples(ema_model, vae, transport, device, epoch+1)
        
        # å†…å­˜æ¸…ç†
        torch.cuda.empty_cache()
    
    if is_main_process():
        logger.info("=== è®­ç»ƒå®Œæˆ ===")
        logger.info(f"Best validation loss: {best_val_loss:.4f}")
    
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    if world_size > 1:
        cleanup()


def encode_dataset_to_latents(vae, data_dir, device, stats_path=None):
    """é¢„è®¡ç®—å¹¶ä¿å­˜æ•´ä¸ªæ•°æ®é›†çš„æ½œç©ºé—´è¡¨ç¤º"""
    logger.info("ç¼–ç æ•°æ®é›†åˆ°æ½œç©ºé—´...")
    
    latents_list = []
    user_ids_list = []
    
    # éå†æ‰€æœ‰ç”¨æˆ·æ–‡ä»¶å¤¹
    for user_dir in sorted(data_dir.glob('ID_*')):
        user_id = int(user_dir.name.split('_')[1]) - 1  # ID_1 -> 0
        
        # æ”¶é›†è¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒï¼ˆä¿®æ­£ä¸º.jpgæ ¼å¼ï¼‰
        image_files = sorted(list(user_dir.glob('*.jpg')))
        
        if not image_files:
            logger.warning(f"ç”¨æˆ· {user_dir.name} æ²¡æœ‰æ‰¾åˆ°.jpgå›¾åƒæ–‡ä»¶")
            continue
            
        logger.info(f"ç¼–ç ç”¨æˆ· {user_dir.name}: {len(image_files)} å¼ å›¾åƒ")
        
        for img_path in image_files:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            image = torch.from_numpy(np.array(image)).float() / 127.5 - 1.0
            image = image.permute(2, 0, 1).unsqueeze(0).to(device)
            
            # ç¼–ç åˆ°æ½œç©ºé—´ - ä½¿ç”¨VA_VAEçš„encode_imagesæ–¹æ³•
            with torch.no_grad():
                latent = vae.encode_images(image)
            
            latents_list.append(latent.cpu().numpy())
            user_ids_list.append(user_id)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if not latents_list:
        logger.error("æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ½œç©ºé—´æ•°æ®ï¼")
        raise ValueError("No latent data collected")
    
    # ä¿å­˜æ½œç©ºé—´æ•°æ®
    latents = np.concatenate(latents_list, axis=0)
    user_ids = np.array(user_ids_list)
    
    logger.info(f"å‡†å¤‡ä¿å­˜ {len(latents)} ä¸ªæ½œç©ºé—´æ ·æœ¬")
    logger.info(f"æ½œç©ºé—´å½¢çŠ¶: {latents.shape}")
    logger.info(f"ç”¨æˆ·IDæ•°é‡: {len(user_ids)}, å”¯ä¸€ç”¨æˆ·: {len(np.unique(user_ids))}")
    
    # è®¡ç®—æ½œç©ºé—´ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
    mean = latents.mean(axis=(0, 2, 3), keepdims=True)  # [1, C, 1, 1]
    std = latents.std(axis=(0, 2, 3), keepdims=True)
    
    # ä¿å­˜åˆ°å¯å†™ç›®å½•
    save_path = Path("/kaggle/working") / 'latents_microdoppler.npz'
    stats_save_path = Path("/kaggle/working") / 'latents_stats.pt'
    
    np.savez(save_path, latents=latents, user_ids=user_ids, mean=mean, std=std)
    torch.save({'mean': torch.from_numpy(mean), 'std': torch.from_numpy(std)}, stats_save_path)
    
    logger.info(f"æˆåŠŸä¿å­˜æ½œç©ºé—´æ•°æ®åˆ° {save_path}")
    logger.info(f"æˆåŠŸä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° {stats_save_path}")
    logger.info(f"æ½œç©ºé—´å‡å€¼å½¢çŠ¶: {mean.shape}, æ ‡å‡†å·®å½¢çŠ¶: {std.shape}")
    
    # éªŒè¯ä¿å­˜
    test_data = np.load(save_path)
    logger.info(f"éªŒè¯: åŠ è½½äº† {len(test_data['latents'])} ä¸ªæ ·æœ¬")


def get_training_config():
    """è·å–è®­ç»ƒé…ç½®å‚æ•°"""
    return {
        'batch_size': 2,           # æ¯GPUæ‰¹æ¬¡å¤§å°ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼‰
        'num_epochs': 25,          # è®­ç»ƒè½®æ•°
        'learning_rate': 1e-4,     # å­¦ä¹ ç‡
        'weight_decay': 0.01,      # æƒé‡è¡°å‡
        'num_workers': 1,          # æ•°æ®åŠ è½½å™¨workeræ•°ï¼ˆå‡å°‘CPUå†…å­˜ï¼‰
        'patience': 5,             # æ—©åœè€å¿ƒ
        'gradient_clip_norm': 1.0, # æ¢¯åº¦è£å‰ª
        'warmup_steps': 100,       # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°
        'gradient_checkpointing': True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    }

def print_training_config(model, optimizer, scheduler, config, 
                         train_size, val_size, world_size, train_dataset=None):
    """è¾“å‡ºè¯¦ç»†çš„è®­ç»ƒé…ç½®ä¿¡æ¯"""
    
    # è®¡ç®—æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # è·å–æ¨¡å‹é…ç½®
    if hasattr(model, 'module'):  # DDP wrapped
        model_config = model.module
    else:
        model_config = model
    
    # åŠ¨æ€è·å–ç”¨æˆ·ç±»åˆ«æ•°
    num_classes = 31  # é»˜è®¤å€¼
    if train_dataset is not None:
        try:
            # å°è¯•ä»æ•°æ®é›†è·å–ç”¨æˆ·æ•°é‡
            if hasattr(train_dataset, 'user_ids'):
                num_classes = len(torch.unique(train_dataset.user_ids))
            elif hasattr(train_dataset, 'num_classes'):
                num_classes = train_dataset.num_classes
        except:
            pass  # ä½¿ç”¨é»˜è®¤å€¼
    
    print("\n" + "="*80)
    print("ğŸš€ DiTå¾®è°ƒè®­ç»ƒé…ç½®")
    print("="*80)
    
    print(f"ğŸ“Š æ•°æ®é…ç½®:")
    print(f"  è®­ç»ƒæ ·æœ¬æ•°: {train_size:,}")
    print(f"  éªŒè¯æ ·æœ¬æ•°: {val_size:,}")
    print(f"  æ¯GPUæ‰¹é‡å¤§å°: {config['batch_size']}")
    print(f"  æ€»æ‰¹é‡å¤§å°: {config['batch_size'] * world_size}")
    print(f"  ç”¨æˆ·ç±»åˆ«æ•°: {num_classes}")
    
    # åŠ¨æ€è·å–æ¨¡å‹ä¿¡æ¯
    model_type = getattr(model_config, '__class__', type(model_config)).__name__
    input_channels = getattr(model_config, 'in_channels', 'Unknown')
    input_size = getattr(model_config, 'input_size', 'Unknown')
    
    print(f"\nğŸ—ï¸  æ¨¡å‹é…ç½®:")
    print(f"  æ¨¡å‹ç±»å‹: {model_type}")
    if input_size != 'Unknown':
        print(f"  è¾“å…¥å°ºå¯¸: {input_size}Ã—{input_size} (æ½œç©ºé—´)")
    else:
        print(f"  è¾“å…¥å°ºå¯¸: æ¨æ–­ä¸º16Ã—16 (æ½œç©ºé—´)")
    print(f"  è¾“å…¥é€šé“: {input_channels}")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    if hasattr(model_config, 'depth'):
        print(f"  Transformerå±‚æ•°: {model_config.depth}")
    if hasattr(model_config, 'hidden_size'):
        print(f"  éšè—å±‚ç»´åº¦: {model_config.hidden_size}")
    if hasattr(model_config, 'num_heads'):
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {model_config.num_heads}")
    
    print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  ä¼˜åŒ–å™¨: {optimizer.__class__.__name__}")
    print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
    print(f"  æƒé‡è¡°å‡: {optimizer.param_groups[0]['weight_decay']:.2e}")
    print(f"  æ¢¯åº¦è£å‰ª: {config['gradient_clip_norm']}")
    print(f"  æ•°æ®åŠ è½½å™¨workeræ•°: {config['num_workers']}")
    print(f"  è°ƒåº¦å™¨: {scheduler.__class__.__name__}")
    print(f"  æ··åˆç²¾åº¦: å¯ç”¨")
    
    print(f"\nğŸ”§ ç¡¬ä»¶é…ç½®:")
    print(f"  GPUæ•°é‡: {world_size}")
    print(f"  å¹¶è¡Œæ–¹å¼: {'DistributedDataParallel' if world_size > 1 else 'Single GPU'}")
    if world_size > 1:
        print(f"  é€šä¿¡åç«¯: NCCL")
    
    print(f"\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
    print(f"  â€¢ è®­ç»ƒ/éªŒè¯æŸå¤±")
    print(f"  â€¢ æ¢¯åº¦èŒƒæ•°")
    print(f"  â€¢ å­¦ä¹ ç‡å˜åŒ–")
    print(f"  â€¢ æ¯ç§’å¤„ç†æ ·æœ¬æ•°")
    print(f"  â€¢ GPUå†…å­˜ä½¿ç”¨ç‡")
    
    print("="*80 + "\n")


def calculate_metrics(model, loss, optimizer):
    """è®¡ç®—è®­ç»ƒè´¨é‡è¯„ä¼°æŒ‡æ ‡"""
    metrics = {}
    
    # åŸºç¡€æŸå¤±
    metrics['loss'] = loss.item()
    
    # æ¢¯åº¦èŒƒæ•°
    total_norm = 0
    param_count = 0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += 1
    
    if param_count > 0:
        metrics['grad_norm'] = total_norm ** (1. / 2)
    else:
        metrics['grad_norm'] = 0.0
    
    # å­¦ä¹ ç‡
    metrics['lr'] = optimizer.param_groups[0]['lr']
    
    return metrics


def train_dit_kaggle():
    """Kaggleç¯å¢ƒä¸‹çš„DiTå¾®è°ƒè®­ç»ƒ"""
    logger.info("å¼€å§‹Kaggle DiTå¾®è°ƒè®­ç»ƒ...")
    
    # æ£€æŸ¥GPUçŠ¶æ€
    if not torch.cuda.is_available():
        raise RuntimeError("éœ€è¦GPUè¿›è¡Œè®­ç»ƒ")
    
    world_size = torch.cuda.device_count()
    logger.info(f"æ£€æµ‹åˆ° {world_size} ä¸ªGPU")
    
    # å¼ºåˆ¶æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰2ä¸ªGPU
    if world_size == 1:
        # å°è¯•æ‰‹åŠ¨æ£€æµ‹ç¬¬äºŒä¸ªGPU
        try:
            torch.cuda.get_device_name(1)
            logger.warning("å‘ç°ç¬¬äºŒä¸ªGPUä½†torch.cuda.device_count()åªè¿”å›1ï¼Œå¼ºåˆ¶è®¾ç½®world_size=2")
            world_size = 2
        except:
            logger.info("ç¡®è®¤åªæœ‰1ä¸ªGPUå¯ç”¨")
    
    # æ˜¾ç¤ºGPUä¿¡æ¯
    for i in range(world_size):
        try:
            logger.info(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            logger.info(f"æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f}GB")
            # æµ‹è¯•GPUå¯ç”¨æ€§
            torch.cuda.set_device(i)
            test_tensor = torch.randn(10, device=f'cuda:{i}')
            logger.info(f"GPU {i} å¯ç”¨æ€§æµ‹è¯•: é€šè¿‡")
            del test_tensor
        except Exception as e:
            logger.error(f"GPU {i} ä¸å¯ç”¨: {e}")
            if i == 1:  # å¦‚æœGPU 1ä¸å¯ç”¨ï¼Œå›é€€åˆ°å•GPU
                world_size = 1
                break
    
    # é¢„å¤„ç†ï¼šç¡®ä¿æ½œç©ºé—´æ•°æ®å‡†å¤‡å¥½
    latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'
    if not latents_file.exists():
        logger.info("é¢„è®¡ç®—æ½œç©ºé—´æ•°æ®...")
        prepare_latents_for_training()
        logger.info("æ½œç©ºé—´é¢„è®¡ç®—å®Œæˆ")
    
    # é…ç½®å¤šè¿›ç¨‹ç¯å¢ƒå˜é‡
    os.environ['MASTER_ADDR'] = '127.0.0.1'  # ä½¿ç”¨IPè€Œélocalhost
    os.environ['MASTER_PORT'] = '12457'      # æ¢ä¸€ä¸ªç«¯å£é¿å…å†²çª
    # Kaggleç¯å¢ƒä¼˜åŒ–
    os.environ['NCCL_DEBUG'] = 'INFO'        # æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
    os.environ['NCCL_IB_DISABLE'] = '1'
    os.environ['NCCL_P2P_DISABLE'] = '1'
    os.environ['NCCL_SOCKET_IFNAME'] = 'lo'
    os.environ['NCCL_TIMEOUT'] = '1800'      # å¢åŠ è¶…æ—¶æ—¶é—´
    os.environ['NCCL_BLOCKING_WAIT'] = '1'   # é˜»å¡ç­‰å¾…
    
    logger.info(f"æœ€ç»ˆworld_size: {world_size}")
    
    if world_size > 1:
        logger.info(f"å¯åŠ¨ {world_size} è¿›ç¨‹åˆ†å¸ƒå¼è®­ç»ƒ...")
        # å¤šGPU DDPè®­ç»ƒ
        try:
            # æ˜¾å¼æ£€æŸ¥ä¸¤ä¸ªGPUæ˜¯å¦çœŸçš„å¯ç”¨
            for i in range(world_size):
                torch.cuda.set_device(i)
                torch.cuda.empty_cache()
                memory_free = torch.cuda.get_device_properties(i).total_memory / 1024**3
                logger.info(f"GPU {i}: {torch.cuda.get_device_name(i)}, æ€»æ˜¾å­˜: {memory_free:.1f}GB")
            
            # æµ‹è¯•è¿›ç¨‹é—´é€šä¿¡
            import socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            try:
                sock.bind(('127.0.0.1', int(os.environ['MASTER_PORT'])))
                sock.close()
                logger.info(f"ç«¯å£ {os.environ['MASTER_PORT']} å¯ç”¨")
            except Exception as port_e:
                logger.warning(f"ç«¯å£æµ‹è¯•å¤±è´¥: {port_e}")
                # å°è¯•å…¶ä»–ç«¯å£
                for test_port in range(12400, 12500):
                    try:
                        sock.bind(('127.0.0.1', test_port))
                        sock.close()
                        os.environ['MASTER_PORT'] = str(test_port)
                        logger.info(f"ä½¿ç”¨ç«¯å£ {test_port}")
                        break
                    except:
                        continue
                else:
                    raise Exception("æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£")
            
            # å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒ
            logger.info(f"å¯åŠ¨å¤šè¿›ç¨‹ï¼Œç«¯å£: {os.environ['MASTER_PORT']}")
            torch.multiprocessing.spawn(train_worker, args=(world_size,), nprocs=world_size, join=True)
        except Exception as e:
            logger.error(f"å¤šGPUè®­ç»ƒå¤±è´¥: {e}")
            logger.info("å›é€€åˆ°å•GPUè®­ç»ƒ...")
            world_size = 1
            train_worker(0, 1)
    else:
        logger.info("ä½¿ç”¨å•GPUè®­ç»ƒ...")
        train_worker(0, 1)


def prepare_latents_for_training():
    """é¢„å¤„ç†ï¼šåªåœ¨ä¸»è¿›ç¨‹ä¸­å‡†å¤‡æ½œç©ºé—´æ•°æ®"""
    # æ£€æŸ¥GPUçŠ¶æ€å’Œæ˜¾å­˜
    device = torch.device('cuda:0')
    torch.cuda.set_device(0)
    torch.cuda.empty_cache()
    
    logger.info("=== å‡†å¤‡æ½œç©ºé—´æ•°æ® ===")
    initial_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"åˆå§‹æ˜¾å­˜ä½¿ç”¨: {initial_memory:.2f}GB")
    
    # ä¿®æ­£é…ç½®ä¸­çš„checkpointè·¯å¾„ä¸ºå¾®è°ƒæ¨¡å‹
    logger.info("å‡†å¤‡VA-VAEé…ç½®...")
    vae_config_path = Path("/kaggle/working/VA-VAE/LightningDiT/tokenizer/configs/vavae_f16d32.yaml")
    
    with open(vae_config_path, 'r') as f:
        config_content = f.read()
    
    # ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹  
    finetuned_checkpoint = '/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt'
    if finetuned_checkpoint not in config_content:
        config_content = config_content.replace(
            'ckpt_path: /path/to/checkpoint.pt',
            f'ckpt_path: {finetuned_checkpoint}'
        )
        with open(vae_config_path, 'w') as f:
            f.write(config_content)
        logger.info(f"å·²æ›´æ–°VA-VAE checkpointè·¯å¾„: {finetuned_checkpoint}")
    
    # åˆå§‹åŒ–VA-VAEè¿›è¡Œæ½œç©ºé—´ç¼–ç 
    from tokenizer.vavae import VA_VAE
    logger.info("åŠ è½½VA-VAEæ¨¡å‹...")
    vae = VA_VAE(str(vae_config_path), img_size=256, horizon_flip=False, fp16=True)
    vae_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"VA-VAEåŠ è½½å®Œæˆï¼Œæ˜¾å­˜: {vae_memory:.2f}GB")
    
    # é¢„è®¡ç®—æ½œç©ºé—´
    data_dir = Path("/kaggle/input/dataset")
    latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'
    if not latents_file.exists():
        logger.info("å¼€å§‹ç¼–ç æ•°æ®é›†åˆ°æ½œç©ºé—´...")
        encode_dataset_to_latents(vae, data_dir, device)
        logger.info("æ½œç©ºé—´ç¼–ç å®Œæˆ")
    else:
        logger.info("æ½œç©ºé—´æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç¼–ç ")
    
    # é‡Šæ”¾VA-VAEæ˜¾å­˜
    del vae
    torch.cuda.empty_cache()
    final_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"VA-VAEé‡Šæ”¾åæ˜¾å­˜: {final_memory:.2f}GB")


def train_worker(rank, world_size):
    """DDPè®­ç»ƒå·¥ä½œè¿›ç¨‹ - ä¸åŠ è½½VA-VAEï¼Œåªå¤„ç†DiTè®­ç»ƒ"""
    
    # è·å–è®­ç»ƒé…ç½®
    config = get_training_config()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    if world_size > 1:
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        master_addr = os.environ.get('MASTER_ADDR', '127.0.0.1')
        master_port = os.environ.get('MASTER_PORT', '12457')
        
        logger.info(f"è¿›ç¨‹{rank}: è¿æ¥åˆ° {master_addr}:{master_port}")
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼ŒæŒ‡å®šè®¾å¤‡
        torch.distributed.init_process_group(
            backend="nccl", 
            rank=rank, 
            world_size=world_size,
            init_method=f'tcp://{master_addr}:{master_port}'
        )
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device(f'cuda:{rank}')
    torch.cuda.set_device(rank)
    
    # æ¸…ç†CUDAç¼“å­˜
    torch.cuda.empty_cache()
    
    # æ˜¾å­˜ä½¿ç”¨ä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    logger.info(f"è¿›ç¨‹ {rank}/{world_size}: ä½¿ç”¨è®¾å¤‡ {device}")
    
    # æ£€æŸ¥åˆå§‹æ˜¾å­˜çŠ¶æ€ï¼ˆæ‰€æœ‰è¿›ç¨‹éƒ½æŠ¥å‘Šï¼‰
    initial_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"è¿›ç¨‹{rank}åˆå§‹æ˜¾å­˜ä½¿ç”¨: {initial_memory:.2f}GB")
    
    # éªŒè¯è¿›ç¨‹åˆ†é…
    if world_size > 1:
        logger.info(f"è¿›ç¨‹{rank}: ç­‰å¾…æ‰€æœ‰è¿›ç¨‹åŒæ­¥...")
        torch.distributed.barrier()
        logger.info(f"è¿›ç¨‹{rank}: åŒæ­¥å®Œæˆ")
    
    # åˆ›å»ºæ•°æ®é›† - åªä½¿ç”¨é¢„è®¡ç®—çš„æ½œç©ºé—´
    data_dir = Path("/kaggle/input/dataset")
    train_dataset = MicroDopplerLatentDataset(data_dir, split='train')
    val_dataset = MicroDopplerLatentDataset(data_dir, split='val')
    
    # DDPæ•°æ®é‡‡æ ·å™¨
    batch_size = config['batch_size']  # æ¯ä¸ªGPUçš„batch size
    
    if world_size > 1:
        logger.info(f"è¿›ç¨‹{rank}: é…ç½®DistributedSamplerï¼Œæ€»è¿›ç¨‹={world_size}")
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            train_dataset, num_replicas=world_size, rank=rank, shuffle=True
        )
        val_sampler = torch.utils.data.distributed.DistributedSampler(
            val_dataset, num_replicas=world_size, rank=rank, shuffle=False
        )
        shuffle = False
        
        # æŠ¥å‘Šæ•°æ®åˆ†ç‰‡æƒ…å†µ
        logger.info(f"è¿›ç¨‹{rank}: è®­ç»ƒæ•°æ®åˆ†ç‰‡ {len(train_sampler)} samples")
        logger.info(f"è¿›ç¨‹{rank}: éªŒè¯æ•°æ®åˆ†ç‰‡ {len(val_sampler)} samples") 
    else:
        logger.info("å•GPUæ¨¡å¼: ä½¿ç”¨æ ‡å‡†DataLoader")
        train_sampler = None
        val_sampler = None
        shuffle = True
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        sampler=train_sampler,
        shuffle=shuffle,
        num_workers=config['num_workers'],
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        sampler=val_sampler,
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True
    )
    
    if rank == 0:
        logger.info(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    if rank == 0:
        logger.info("åˆå§‹åŒ–DiTæ¨¡å‹...")
    
    from transport import create_transport
    from models.lightningdit import LightningDiT
    
    model = LightningDiT(
        input_size=16,
        patch_size=1,
        in_channels=32,
        hidden_size=1152,
        depth=28,
        num_heads=16,
        mlp_ratio=4.0,
        class_dropout_prob=0.1,
        num_classes=31,
        learn_sigma=False
    ).to(device)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    if config.get('gradient_checkpointing', False):
        if hasattr(model, 'enable_input_require_grads'):
            model.enable_input_require_grads()
        # ä¸ºtransformerå±‚å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if hasattr(model, 'blocks'):
            for block in model.blocks:
                if hasattr(block, 'checkpoint'):
                    block.checkpoint = True
    
    # DDPåŒ…è£…
    if world_size > 1:
        model = torch.nn.parallel.DistributedDataParallel(
            model, device_ids=[rank], output_device=rank
        )
        if rank == 0:
            logger.info(f"ä½¿ç”¨DistributedDataParallelï¼Œ{world_size}ä¸ªGPU")
    else:
        if rank == 0:
            logger.info("ä½¿ç”¨å•GPUè®­ç»ƒ")
    
    # Transporté…ç½®
    transport = create_transport(
        path_type='Linear',
        prediction='velocity',
        loss_weight=None,
        use_cosine_loss=True,
        use_lognorm=True
    )
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨é…ç½®
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config['learning_rate'], 
        weight_decay=config['weight_decay']
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=config['num_epochs']
    )
    
    # è¾“å‡ºè¯¦ç»†è®­ç»ƒé…ç½®
    if rank == 0:
        print_training_config(model, optimizer, scheduler, config, 
                             len(train_dataset), len(val_dataset), world_size, train_dataset)
    
    # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä½¿ç”¨æ›´ç§¯æçš„ç¼©æ”¾ç­–ç•¥ï¼‰
    scaler = torch.cuda.amp.GradScaler(
        init_scale=1024.0,  # é™ä½åˆå§‹ç¼©æ”¾
        growth_factor=1.1,   # å‡å°‘å¢é•¿å› å­
        backoff_factor=0.8   # å¢åŠ å›é€€å› å­
    )
    
    # æ˜¾å­˜ç›‘æ§
    if rank == 0:
        model_memory = torch.cuda.memory_allocated(device) / 1024**3
        logger.info(f"æ¨¡å‹åŠ è½½åæ˜¾å­˜: {model_memory:.2f}GB")
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    train_metrics_history = []
    val_metrics_history = []
    
    for epoch in range(config['num_epochs']):
        epoch_start_time = time.time()
        
        # DDP sampleréœ€è¦è®¾ç½®epoch
        if train_sampler is not None:
            train_sampler.set_epoch(epoch)
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_steps = 0
        train_grad_norm = 0
        train_samples = 0
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        if rank == 0:
            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config["num_epochs"]} [Train]')
        else:
            pbar = train_loader
            
        for batch_idx, batch in enumerate(pbar):
            try:
                batch_start_time = time.time()
                
                # è°ƒè¯•ï¼šç¬¬ä¸€ä¸ªbatchçš„è¯¦ç»†ä¿¡æ¯
                if epoch == 0 and batch_idx == 0:
                    logger.info(f"è¿›ç¨‹{rank}: å¼€å§‹å¤„ç†ç¬¬ä¸€ä¸ªbatch")
                
                latents = batch[0].to(device)
                user_ids = batch[1].to(device)
                
                # è°ƒè¯•ï¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªbatchçš„shapeï¼ˆä»…rank 0ï¼‰
                if epoch == 0 and batch_idx == 0 and rank == 0:
                    logger.info(f"\nğŸ“‹ æ•°æ®æ ¼å¼æ£€æŸ¥:")
                    logger.info(f"  æ½œç©ºé—´shape: {latents.shape}")
                    logger.info(f"  ç”¨æˆ·ID shape: {user_ids.shape}")
                    logger.info(f"  æ½œç©ºé—´æ•°æ®ç±»å‹: {latents.dtype}")
                    logger.info(f"  æ½œç©ºé—´æ•°å€¼èŒƒå›´: [{latents.min():.3f}, {latents.max():.3f}]")
                    print()
                
                # å‰å‘ä¼ æ’­
                if epoch == 0 and batch_idx == 0:
                    logger.info(f"è¿›ç¨‹{rank}: å¼€å§‹å‰å‘ä¼ æ’­")
                
                with torch.cuda.amp.autocast():
                    model_kwargs = {"y": user_ids}
                    loss_dict = transport.training_losses(model, latents, model_kwargs)
                    loss = loss_dict["loss"].mean()
                
                if epoch == 0 and batch_idx == 0:
                    logger.info(f"è¿›ç¨‹{rank}: å‰å‘ä¼ æ’­å®Œæˆï¼Œloss={loss.item():.4f}")
            
            except Exception as e:
                logger.error(f"è¿›ç¨‹{rank}: batch {batch_idx} å‡ºé”™: {e}")
                raise
            
            # åå‘ä¼ æ’­
            if epoch == 0 and batch_idx == 0:
                logger.info(f"è¿›ç¨‹{rank}: å¼€å§‹åå‘ä¼ æ’­")
            
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            
            if epoch == 0 and batch_idx == 0:
                logger.info(f"è¿›ç¨‹{rank}: backwardå®Œæˆï¼Œå¼€å§‹æ¢¯åº¦è£å‰ª")
            
            scaler.unscale_(optimizer)
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_norm'])
            
            if epoch == 0 and batch_idx == 0:
                logger.info(f"è¿›ç¨‹{rank}: æ¢¯åº¦è£å‰ªå®Œæˆï¼Œgrad_norm={grad_norm:.4f}")
            
            scaler.step(optimizer)
            scaler.update()
            
            if epoch == 0 and batch_idx == 0:
                logger.info(f"è¿›ç¨‹{rank}: ä¼˜åŒ–å™¨stepå®Œæˆ")
            
            # å®šæœŸæ¸…ç†æ˜¾å­˜ç¼“å­˜
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()
            
            # ç»Ÿè®¡
            train_loss += loss.item()
            train_grad_norm += grad_norm.item() if hasattr(grad_norm, 'item') else grad_norm
            train_steps += 1
            train_samples += latents.size(0)
            
            if rank == 0:
                # è®¡ç®—å½“å‰æŒ‡æ ‡
                current_lr = optimizer.param_groups[0]['lr']
                samples_per_sec = latents.size(0) / (time.time() - batch_start_time)
                
                # æ˜¾å­˜ç›‘æ§
                current_memory = torch.cuda.memory_allocated(device) / 1024**3
                
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'lr': f'{current_lr:.2e}',
                    'sps': f'{samples_per_sec:.1f}',
                    'mem': f'{current_memory:.1f}GB'
                })
                
                # æ˜¾å­˜è­¦å‘Š
                if current_memory > 13.0:  # T4æ€»æ˜¾å­˜çº¦14.7GBï¼Œè­¦å‘Šé˜ˆå€¼13GB
                    logger.warning(f"æ˜¾å­˜ä½¿ç”¨è¿‡é«˜: {current_memory:.2f}GB")
                    
            # ç¬¬ä¸€ä¸ªbatchå®Œæˆåçš„åŒæ­¥
            if epoch == 0 and batch_idx == 0 and world_size > 1:
                logger.info(f"è¿›ç¨‹{rank}: ç¬¬ä¸€ä¸ªbatchå®Œæˆï¼Œç­‰å¾…åŒæ­¥...")
                torch.distributed.barrier()
                logger.info(f"è¿›ç¨‹{rank}: åŒæ­¥å®Œæˆ")
        
        # è®­ç»ƒé˜¶æ®µç»Ÿè®¡
        avg_train_loss = train_loss / train_steps
        avg_train_grad_norm = train_grad_norm / train_steps
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_steps = 0
        val_samples = 0
        
        with torch.no_grad():
            if rank == 0:
                val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{config["num_epochs"]} [Val]')
            else:
                val_pbar = val_loader
                
            for batch in val_pbar:
                latents = batch[0].to(device)
                user_ids = batch[1].to(device)
                
                with torch.cuda.amp.autocast():
                    model_kwargs = {"y": user_ids}
                    loss_dict = transport.training_losses(model, latents, model_kwargs)
                    loss = loss_dict["loss"].mean()
                
                val_loss += loss.item()
                val_steps += 1
                val_samples += latents.size(0)
                
                if rank == 0:
                    val_pbar.set_postfix({'val_loss': f'{loss.item():.4f}'})
        
        avg_val_loss = val_loss / val_steps
        scheduler.step()
        
        # è®¡ç®—epochç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        train_samples_per_sec = train_samples / epoch_time * world_size  # æ€»ååé‡
        current_lr = optimizer.param_groups[0]['lr']
        
        # GPUå†…å­˜ç»Ÿè®¡ï¼ˆä»…rank 0ï¼‰
        if rank == 0:
            gpu_memory = torch.cuda.max_memory_allocated(device) / 1024**3  # GB
            
            # è¾“å‡ºè¯¦ç»†çš„epochç»Ÿè®¡
            print(f"\nğŸ“Š Epoch {epoch+1}/{config['num_epochs']} è®­ç»ƒç»Ÿè®¡")
            print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            print(f"ğŸ¯ æŸå¤±æŒ‡æ ‡:")
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
            print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
            print(f"  æŸå¤±å˜åŒ–: {avg_val_loss - avg_train_loss:+.6f}")
            
            print(f"\nâš¡ è®­ç»ƒåŠ¨æ€:")
            print(f"  æ¢¯åº¦èŒƒæ•°: {avg_train_grad_norm:.4f}")
            print(f"  å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"  è®­ç»ƒæ—¶é—´: {epoch_time:.1f}s")
            print(f"  è®­ç»ƒåå: {train_samples_per_sec:.1f} samples/sec")
            
            print(f"\nğŸ’¾ èµ„æºä½¿ç”¨:")
            print(f"  GPUæ˜¾å­˜å³°å€¼: {gpu_memory:.2f}GB")
            print(f"  è®­ç»ƒæ ·æœ¬æ•°: {train_samples}")
            print(f"  éªŒè¯æ ·æœ¬æ•°: {val_samples}")
            
            # è®­ç»ƒè´¨é‡è¯„ä¼°
            is_improving = avg_val_loss < best_val_loss
            improvement_rate = (best_val_loss - avg_val_loss) / best_val_loss * 100 if best_val_loss != float('inf') else 0
            
            print(f"\nğŸ“ˆ è´¨é‡è¯„ä¼°:")
            print(f"  æ¨¡å‹æ”¹è¿›: {'âœ… æ˜¯' if is_improving else 'âŒ å¦'}")
            if is_improving:
                print(f"  æ”¹è¿›å¹…åº¦: {improvement_rate:.2f}%")
                print(f"  æœ€ä½³æŸå¤±: {avg_val_loss:.6f}")
            else:
                print(f"  æœ€ä½³æŸå¤±: {best_val_loss:.6f}")
            
            # è¿‡æ‹Ÿåˆæ£€æµ‹
            overfitting_gap = avg_train_loss - avg_val_loss
            if overfitting_gap < -0.01:
                print(f"  âš ï¸  å¯èƒ½è¿‡æ‹Ÿåˆ (gap: {overfitting_gap:.4f})")
            elif overfitting_gap > 0.05:
                print(f"  âš ï¸  å¯èƒ½æ¬ æ‹Ÿåˆ (gap: {overfitting_gap:.4f})")
            else:
                print(f"  âœ… æ‹Ÿåˆè‰¯å¥½ (gap: {overfitting_gap:.4f})")
            
            print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
            
            # æœ€ä½³æ¨¡å‹ä¿å­˜
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                if rank == 0:
                    best_model_path = Path("/kaggle/working") / f"best_dit_epoch_{epoch+1}.pt"
                    torch.save({
                        'epoch': epoch + 1,
                        'model_state_dict': model.module.state_dict() if world_size > 1 else model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'val_loss': avg_val_loss,
                        'config': config
                    }, best_model_path)
                    logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {best_model_path}")
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
    
    # è®­ç»ƒå®Œæˆ
    if rank == 0:
        logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = Path("/kaggle/working") / "final_dit_model.pt"
        torch.save({
            'model_state_dict': model.module.state_dict() if world_size > 1 else model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'config': config,
            'training_history': {
                'train_metrics': train_metrics_history,
                'val_metrics': val_metrics_history
            }
        }, final_model_path)
        logger.info(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {final_model_path}")
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    if world_size > 1:
        logger.info(f"è¿›ç¨‹{rank}: æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ...")
        torch.distributed.destroy_process_group()
        logger.info(f"è¿›ç¨‹{rank}: åˆ†å¸ƒå¼ç¯å¢ƒæ¸…ç†å®Œæˆ")
        logger.info("è®­ç»ƒå®Œæˆï¼")


def generate_samples(model, vae, transport, device, epoch):
    """ç”Ÿæˆæ ·æœ¬ç”¨äºå¯è§†åŒ–"""
    model.eval()
    vae.eval()
    
    with torch.no_grad():
        # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆä¸€ä¸ªæ ·æœ¬
        num_samples = min(8, 31)  # ç”Ÿæˆ8ä¸ªç”¨æˆ·çš„æ ·æœ¬
        user_ids = torch.arange(num_samples, device=device)
        
        # é‡‡æ ·æ½œç©ºé—´
        z = torch.randn(num_samples, 32, 16, 16, device=device)
        model_kwargs = {"y": user_ids}
        
        # ä½¿ç”¨DDIMé‡‡æ ·
        samples = transport.sample_with_model_kwargs(
            model, z, model_kwargs, 
            steps=50,  # é‡‡æ ·æ­¥æ•°
            eta=0.0,   # DDIMç¡®å®šæ€§é‡‡æ ·
            guidance_scale=2.0  # CFGå¼ºåº¦
        )
        
        # è§£ç åˆ°å›¾åƒç©ºé—´
        images = vae.decode(samples)
        images = (images + 1) / 2  # [-1,1] -> [0,1]
        images = images.clamp(0, 1)
        
        # ä¿å­˜å›¾åƒ
        save_dir = Path("outputs/samples")
        save_dir.mkdir(parents=True, exist_ok=True)
        
        for i, img in enumerate(images):
            img_pil = Image.fromarray((img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))
            img_pil.save(save_dir / f"epoch{epoch}_user{user_ids[i].item()}.png")
        
        logger.info(f"Saved {num_samples} samples to {save_dir}")


def main():
    """ä¸»å‡½æ•° - Kaggle T4x2ä¼˜åŒ–ç‰ˆæœ¬"""
    # Kaggleç¯å¢ƒGPUæ£€æŸ¥
    if not torch.cuda.is_available():
        logger.error("CUDA not available! Please enable GPU accelerator in Kaggle.")
        return
    
    world_size = torch.cuda.device_count()
    logger.info(f"Detected {world_size} GPU(s)")
    
    # Kaggle T4x2ä½¿ç”¨DataParallelï¼Œä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    train_dit_kaggle()

if __name__ == "__main__":
    main()

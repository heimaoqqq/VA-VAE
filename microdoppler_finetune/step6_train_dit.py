"""
Step 6: è®­ç»ƒæ¡ä»¶LightningDiTæ¨¡å‹
ä½¿ç”¨å¾®è°ƒåçš„VA-VAEæ½œç©ºé—´è¿›è¡Œmicro-Dopplerå›¾åƒç”Ÿæˆ
é’ˆå¯¹Kaggle T4Ã—2 GPUç¯å¢ƒä¼˜åŒ–
"""

import os
import sys
import time
import json
import logging
import types
from pathlib import Path
from PIL import Image
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from omegaconf import OmegaConf
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ LightningDiTè·¯å¾„ï¼ˆå‚è€ƒstep4ï¼‰
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))
sys.path.insert(0, str(project_root))  # æ·»åŠ æ ¹ç›®å½•ä»¥å¯¼å…¥è‡ªå®šä¹‰æ•°æ®é›†

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Kaggle T4å†…å­˜ä¼˜åŒ–è®¾ç½®
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['TORCH_COMPILE_DISABLE'] = '1'
os.environ['TORCHDYNAMO_DISABLE'] = '1'

# ç¦ç”¨torch.compile
import torch._dynamo
torch._dynamo.disable()

# DataParallelè¾…åŠ©å‡½æ•°
def is_main_process():
    """å§‹ç»ˆè¿”å›Trueï¼Œå› ä¸ºä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ"""
    return True


# å¯¼å…¥LightningDiTæ¨¡å—
from transport import create_transport, Sampler
from models.lightningdit import LightningDiT_models, LightningDiT_L_2

# å¯¼å…¥VA-VAE
from tokenizer.vavae import VA_VAE

# ==================== æ•°æ®é›†å®šä¹‰ ====================
class MicroDopplerLatentDataset(Dataset):
    """å¾®è°ƒåçš„æ½œç©ºé—´æ•°æ®é›†ï¼ŒåŒ…å«ç”¨æˆ·æ¡ä»¶"""
    
    def __init__(self, data_dir, split='train', val_ratio=0.2, latent_norm=True):
        self.data_dir = Path(data_dir)
        self.split = split
        self.latent_norm = latent_norm
        
        # åŠ è½½æ•°æ®é›†åˆ’åˆ†ä¿¡æ¯
        split_file = Path("/kaggle/working/data_split/dataset_split.json")
        if not split_file.exists():
            # å¦‚æœæ²¡æœ‰åˆ’åˆ†æ–‡ä»¶ï¼Œå°è¯•å…¶ä»–ä½ç½®
            alt_split_file = Path("/kaggle/input/data_split/dataset_split.json")
            if alt_split_file.exists():
                split_file = alt_split_file
            else:
                logger.warning(f"æœªæ‰¾åˆ°æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶: {split_file}")
                logger.warning("è¯·å…ˆè¿è¡Œ step3_prepare_dataset.py åˆ›å»ºæ•°æ®åˆ’åˆ†")
                split_info = None
        else:
            import json
            with open(split_file, 'r') as f:
                split_info = json.load(f)
            logger.info(f"åŠ è½½æ•°æ®åˆ’åˆ†: {split_file}")
        
        # åŠ è½½æ½œç©ºé—´æ•°æ®å’Œæ ‡ç­¾ - ä»å¯å†™ç›®å½•åŠ è½½
        latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'
        stats_file = Path("/kaggle/working") / 'latents_stats.pt'
        
        # åŠ è½½å¾®å¤šæ™®å‹’æ•°æ®é›†è‡ªå·±çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ä½¿ç”¨å®˜æ–¹ImageNetç»Ÿè®¡ï¼‰
        if stats_file.exists():
            stats = torch.load(stats_file)
            self.latent_mean = stats['mean'].float()
            self.latent_std = stats['std'].float()
            logger.info(f"åŠ è½½å¾®å¤šæ™®å‹’æ½œç©ºé—´ç»Ÿè®¡: mean={self.latent_mean.shape}, std={self.latent_std.shape}")
        else:
            self.latent_mean = None
            self.latent_std = None
            
        if latents_file.exists():
            data = np.load(latents_file)
            self.latents = torch.from_numpy(data['latents']).float()
            self.user_ids = torch.from_numpy(data['user_ids']).long()
            
            # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä»æ•°æ®ä¸­è®¡ç®—
            if self.latent_mean is None:
                self.latent_mean = self.latents.mean(dim=[0, 2, 3], keepdim=True)
                self.latent_std = self.latents.std(dim=[0, 2, 3], keepdim=True)
                logger.info("ä»æ•°æ®è®¡ç®—æ½œç©ºé—´ç»Ÿè®¡ä¿¡æ¯")
            
            logger.info(f"Loaded {len(self.latents)} latent samples from {latents_file}")
            
            # è°ƒè¯•ä¿¡æ¯
            if len(self.latents) == 0:
                logger.error("æ½œç©ºé—´æ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºï¼")
                raise ValueError("Empty latents file")
        else:
            # å¦‚æœé¢„è®¡ç®—çš„æ½œç©ºé—´ä¸å­˜åœ¨ï¼Œå®æ—¶ç¼–ç 
            logger.warning(f"Pre-computed latents not found at {latents_file}")
            self.latents = None
            self.load_images()
        
        # ä½¿ç”¨step3åˆ›å»ºçš„æ•°æ®åˆ’åˆ†
        if split_info is not None and split in split_info:
            # ä½¿ç”¨é¢„å®šä¹‰çš„åˆ’åˆ†
            self.file_list = []
            self.user_labels = []
            
            for user_key, image_paths in split_info[split].items():
                user_id = int(user_key.split('_')[1]) - 1  # ID_1 -> 0
                for img_path in image_paths:
                    self.file_list.append(img_path)
                    self.user_labels.append(user_id)
            
            logger.info(f"ä½¿ç”¨step3åˆ’åˆ†: {split} é›†åŒ…å« {len(self.file_list)} å¼ å›¾åƒ")
            self.indices = np.arange(len(self.file_list))
            n_samples = len(self.file_list)  # è®¾ç½®n_samplesç”¨äºåç»­æ—¥å¿—
        else:
            # å¦‚æœæ²¡æœ‰åˆ’åˆ†ä¿¡æ¯ï¼Œä½¿ç”¨åŸæ¥çš„éšæœºåˆ’åˆ†
            if self.latents is not None:
                n_samples = len(self.latents)
            elif hasattr(self, 'image_paths'):
                n_samples = len(self.image_paths)
            else:
                logger.error("æ²¡æœ‰æ•°æ®å¯ç”¨äºåˆ›å»ºæ•°æ®é›†")
                n_samples = 0
                
            if n_samples == 0:
                logger.error(f"æ•°æ®é›†ä¸ºç©ºï¼latents={self.latents is not None}, "
                            f"has_image_paths={hasattr(self, 'image_paths')}")
                self.indices = np.array([])
            else:
                indices = np.arange(n_samples)
                np.random.seed(42)
                np.random.shuffle(indices)
                
                n_val = int(n_samples * val_ratio)
                if split == 'train':
                    self.indices = indices[n_val:]
                else:
                    self.indices = indices[:n_val]
        
        logger.info(f"{split} dataset: {len(self.indices)} samples (total: {n_samples})")
    
    def load_images(self):
        """åŠ è½½åŸå§‹å›¾åƒè·¯å¾„"""
        self.image_paths = []
        self.user_ids = []
        
        # æ£€æŸ¥ä¸åŒçš„æ•°æ®è·¯å¾„æ ¼å¼
        # åŸå§‹æ•°æ®é›†: /kaggle/input/dataset/ID_*
        # å¤„ç†å: processed_microdoppler/ID_*
        data_path = self.data_dir
        if not (data_path / 'ID_1').exists():
            # å°è¯•processed_microdopplerå­ç›®å½•
            alt_path = data_path / 'processed_microdoppler'
            if alt_path.exists():
                data_path = alt_path
                logger.info(f"ä½¿ç”¨å¤„ç†åçš„æ•°æ®è·¯å¾„: {data_path}")
        
        for user_dir in sorted(data_path.glob('ID_*')):
            user_id = int(user_dir.name.split('_')[1]) - 1  # ID_1 -> 0
            
            # æ”¶é›†è¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒï¼ˆä¿®æ­£ä¸º.jpgæ ¼å¼ï¼‰
            image_files = sorted(list(user_dir.glob('*.jpg')))
            
            if not image_files:
                logger.warning(f"ç”¨æˆ· {user_dir.name} æ²¡æœ‰æ‰¾åˆ°.jpgå›¾åƒæ–‡ä»¶")
                continue
                
            logger.info(f"ç¼–ç ç”¨æˆ· {user_dir.name}: {len(image_files)} å¼ å›¾åƒ")
            
            for img_path in image_files:
                self.image_paths.append(str(img_path))
                self.user_ids.append(user_id)
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        real_idx = self.indices[idx]
        
        if self.latents is not None:
            latent = self.latents[real_idx]
            user_id = self.user_ids[real_idx]
            
            # ç¡®ä¿æ˜¯tensoræ ¼å¼
            if isinstance(latent, np.ndarray):
                latent = torch.from_numpy(latent).float()
            else:
                latent = latent.float()
                
            if isinstance(user_id, np.ndarray):
                user_id = torch.from_numpy(user_id).long()
            else:
                user_id = user_id.long()
            
            # æ½œç©ºé—´å½’ä¸€åŒ–
            if self.latent_norm and self.latent_mean is not None:
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if latent.dim() == 3 and self.latent_mean.dim() == 4:
                    latent = latent.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                latent = (latent - self.latent_mean) / self.latent_std
                if latent.dim() == 4 and latent.shape[0] == 1:
                    latent = latent.squeeze(0)  # ç§»é™¤ä¸´æ—¶batchç»´åº¦
        
        return latent, user_id


# ==================== è®­ç»ƒå‡½æ•° ====================
def train_dit():
    """ä¸»è®­ç»ƒå‡½æ•° - DataParallelæ¨¡å¼"""
    
    # ===== è®­ç»ƒé…ç½®ï¼ˆLæ¨¡å‹ + æ™ºèƒ½æƒé‡åˆå§‹åŒ–ï¼‰=====
    config = {
        'num_epochs': 80,  # æ›´å¤šè½®æ•°è¡¥å¿éƒ¨åˆ†éšæœºåˆå§‹åŒ–
        'batch_size': 2,  # ä¿å®ˆè®¾ç½®ï¼Œé¿å…DataParallelçš„GPU0 OOM
        'gradient_accumulation_steps': 3,  # æœ‰æ•ˆbatch_size=6
        'learning_rate': 5e-6,  # æ›´å°å­¦ä¹ ç‡é˜²æ­¢è¿‡æ‹Ÿåˆ
        'weight_decay': 0.1,   # æ›´å¼ºæ­£åˆ™åŒ–
        'gradient_clip_norm': 1.0,
        'warmup_steps': 200,  # è¾ƒçŸ­çš„é¢„çƒ­æœŸ
        'ema_decay': 0.9999,
        
        # é‡‡æ ·é…ç½®ï¼ˆé’ˆå¯¹æ—¶é¢‘å›¾ä¼˜åŒ–ï¼‰
        'sampling_method': 'dopri5',  # é«˜ç²¾åº¦ODEæ±‚è§£å™¨ï¼Œé€‚åˆæ—¶é¢‘å›¾
        'num_steps': 150,  # å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
        'cfg_scale': 7.0,  # é€‚åº¦CFGï¼Œä¿ç•™ç»†èŠ‚
        'cfg_interval_start': 0.11,  # CFGå¼€å§‹æ—¶é—´
        'timestep_shift': 0.1,  # å‡å°åç§»ä¿ç•™æ›´å¤šç»†èŠ‚
        
        # æ•°æ®é…ç½®
        'num_workers': 2,  # Kaggleç¯å¢ƒ
        'pin_memory': True,
        'persistent_workers': True,
    }
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš€ LightningDiT-L è®­ç»ƒé…ç½®ï¼ˆT4Ã—2ä¼˜åŒ–ï¼‰")
    logger.info("="*60)
    for key, value in config.items():
        logger.info(f"  {key}: {value}")
    logger.info("="*60 + "\n")
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    logger.info(f"Using device: {device}")
    if torch.cuda.is_available():
        num_gpus = torch.cuda.device_count()
        logger.info(f"Available GPUs: {num_gpus}")
        for i in range(num_gpus):
            logger.info(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            logger.info(f"  GPU Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")
    
    # ===== 1. åˆå§‹åŒ–VA-VAEï¼ˆä»…ç”¨äºç¼–ç ï¼‰ =====
    logger.info("=== åˆå§‹åŒ–VA-VAEç¼–ç å™¨ ===")
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹VA-VAE checkpoint
    # ä½¿ç”¨æŒ‡å®šçš„VA-VAEæ¨¡å‹è·¯å¾„
    vae_checkpoint = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
    
    if Path(vae_checkpoint).exists():
        logger.info(f"âœ… æ‰¾åˆ°VA-VAEæ¨¡å‹: {vae_checkpoint}")
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size_mb = Path(vae_checkpoint).stat().st_size / (1024 * 1024)
        logger.info(f"   æ¨¡å‹å¤§å°: {size_mb:.2f} MB")
    else:
        logger.error("âŒ æœªæ‰¾åˆ°VA-VAEæ¨¡å‹ï¼")
        logger.error(f"   è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨: {vae_checkpoint}")
        raise FileNotFoundError(f"VA-VAE checkpoint not found at {vae_checkpoint}")
    
    # åŠ è½½åŸå§‹é…ç½®å¹¶ä¿®æ”¹checkpointè·¯å¾„
    vae_config_path = project_root / 'LightningDiT' / 'tokenizer' / 'configs' / 'vavae_f16d32.yaml'
    vae_config = OmegaConf.load(str(vae_config_path))
    vae_config.ckpt_path = vae_checkpoint  # è®¾ç½®checkpointè·¯å¾„
    
    # ä¿å­˜ä¿®æ”¹åçš„é…ç½®åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_config_path = project_root / 'temp_vavae_config.yaml'
    OmegaConf.save(vae_config, str(temp_config_path))
    
    # ä½¿ç”¨ä¿®æ”¹åçš„é…ç½®åˆå§‹åŒ–VA-VAE
    vae = VA_VAE(
        config=str(temp_config_path),
        img_size=256,
        horizon_flip=0.0,  # è®­ç»ƒæ—¶ä¸éœ€è¦æ°´å¹³ç¿»è½¬ï¼ˆæ•°æ®å¢å¼ºå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•ˆæœå¾ˆå·®ï¼‰
        fp16=True
    )
    
    # VA-VAEå·²ç»é€šè¿‡é…ç½®æ–‡ä»¶åŠ è½½äº†checkpoint
    if os.path.exists(vae_checkpoint):
        logger.info("âœ“ VA-VAE loaded successfully with checkpoint")
    else:
        logger.warning(f"âš ï¸ VA-VAE checkpoint not found at {vae_checkpoint}")
    
    # VA-VAEçš„modelå·²ç»åœ¨åˆå§‹åŒ–æ—¶è°ƒç”¨äº†.cuda()ï¼Œä¸éœ€è¦.to(device)
    # vae.modelå·²ç»æ˜¯evalæ¨¡å¼
    
    # ==================== æ¨¡å‹åˆå§‹åŒ– ====================
    print("\n" + "="*60)
    print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    print("="*60)
    
    # 1. åˆå§‹åŒ–LightningDiT-Læ¨¡å‹ - å¹³è¡¡æ€§èƒ½å’Œæ˜¾å­˜
    print("\nğŸ“Š åˆå§‹åŒ–LightningDiT-L...")
    dit_model = LightningDiT_L_2(
        in_channels=32,       # VA-VAEæ½œç©ºé—´é€šé“æ•°
        num_classes=31,       # 31ä¸ªç”¨æˆ·æ¡ä»¶
        use_swiglu=True,
        use_rope=True,
        use_rmsnorm=True
    ).to(device)
    
    logger.info(f"Model: LightningDiT-L (1024-dim, 24 layers)")
    logger.info(f"Parameters: {sum(p.numel() for p in dit_model.parameters()) / 1e6:.2f}M")
    
    # ä½¿ç”¨XLæ¨¡å‹æƒé‡è¿›è¡Œæ™ºèƒ½åˆå§‹åŒ–
    pretrained_xl = "/kaggle/working/VA-VAE/models/lightningdit-xl-imagenet256-64ep.pt"
    
    if os.path.exists(pretrained_xl):
        logger.info(f"âœ… æ‰¾åˆ°LightningDiT-XLæƒé‡ç”¨äºåˆå§‹åŒ–Læ¨¡å‹: {pretrained_xl}")
        size_gb = os.path.getsize(pretrained_xl) / (1024**3)
        logger.info(f"   XLæƒé‡å¤§å°: {size_gb:.2f} GB")
        logger.info("   å°†ä»XLæƒé‡ä¸­æå–å…¼å®¹å±‚åˆå§‹åŒ–Læ¨¡å‹")
    else:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°XLæ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ step2_download_models.py")
        logger.warning(f"   é¢„æœŸè·¯å¾„: {pretrained_xl}")
        logger.warning("   Læ¨¡å‹å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆä¸æ¨èï¼‰")
    
    # æ™ºèƒ½åŠ è½½XLæƒé‡åˆ°Læ¨¡å‹
    logger.info("\n" + "="*60)
    logger.info("ğŸ¯ æ™ºèƒ½æƒé‡åˆå§‹åŒ–ç­–ç•¥")
    logger.info("="*60)
    
    if pretrained_xl and os.path.exists(pretrained_xl):
        try:
            checkpoint = torch.load(pretrained_xl, map_location='cpu')
            xl_state_dict = checkpoint.get('model', checkpoint)
            xl_state_dict = {k.replace('module.', ''): v for k, v in xl_state_dict.items()}
            
            # Læ¨¡å‹å’ŒXLæ¨¡å‹çš„æ˜ å°„ç­–ç•¥
            l_state_dict = dit_model.state_dict()
            loaded_keys = []
            skipped_keys = []
            
            for k, v in l_state_dict.items():
                if k in xl_state_dict:
                    # å¯¹äºtransformer blocksï¼ŒLæœ‰24å±‚ï¼ŒXLæœ‰28å±‚
                    if 'blocks.' in k:
                        block_idx = int(k.split('.')[1])
                        if block_idx < 24:  # Læ¨¡å‹åªæœ‰24å±‚
                            xl_key = k
                            if xl_key in xl_state_dict and xl_state_dict[xl_key].shape == v.shape:
                                l_state_dict[k] = xl_state_dict[xl_key]
                                loaded_keys.append(k)
                            else:
                                skipped_keys.append(k)
                    # éblockå±‚çš„æƒé‡
                    elif xl_state_dict[k].shape == v.shape:
                        l_state_dict[k] = xl_state_dict[k]
                        loaded_keys.append(k)
                    else:
                        skipped_keys.append(k)
            
            dit_model.load_state_dict(l_state_dict, strict=False)
            
            print(f"âœ… æ™ºèƒ½åˆå§‹åŒ–å®Œæˆ:")
            print(f"   ä»XLåŠ è½½: {len(loaded_keys)} ä¸ªæƒé‡")
            print(f"   éšæœºåˆå§‹åŒ–: {len(skipped_keys)} ä¸ªæƒé‡")
            print(f"   æ€»å‚æ•°é‡: {sum(p.numel() for p in dit_model.parameters()) / 1e6:.1f}M")
            
            # ğŸ¯ å…¨å‚æ•°å¾®è°ƒç­–ç•¥ - å……åˆ†é€‚åº”å¾®å¤šæ™®å‹’åŸŸ
            print("   è®­ç»ƒç­–ç•¥: å…¨å‚æ•°å¾®è°ƒ (è·¨åŸŸä»»åŠ¡æœ€ä¼˜)")
            print("   ç†ç”±1: æ—¶é¢‘å›¾ä¸è‡ªç„¶å›¾åƒåŸŸå·®å¼‚æå¤§ï¼Œéœ€è¦æ·±å±‚ç‰¹å¾é‡å­¦ä¹ ")
            print("   ç†ç”±2: ç”¨æˆ·é—´å¾®å¼±å·®å¼‚éœ€è¦ç²¾ç»†ç‰¹å¾ï¼Œå†»ç»“ä¼šé™åˆ¶åˆ¤åˆ«èƒ½åŠ›")
            print("   ç†ç”±3: XLâ†’Læƒé‡æ˜ å°„ä¸å®Œç¾ï¼Œéœ€è¦å¾®è°ƒä¿®æ­£")
            
            trainable_params = sum(p.numel() for p in dit_model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in dit_model.parameters())
            print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e6:.1f}M (100%)")
            print("   è¿‡æ‹Ÿåˆé˜²æŠ¤: å°å­¦ä¹ ç‡ + å¼ºæ­£åˆ™åŒ– + æ¢¯åº¦ç´¯ç§¯")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
    else:
        logger.error("\n" + "âŒ"*30)
        logger.error("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼")
        logger.error("âŒ æ¨¡å‹å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼Œè¿™ä¼šå¯¼è‡´ç”Ÿæˆçº¯å™ªå£°å›¾åƒã€‚")
        logger.error("âŒ"*30)
        logger.error("\nè¯·ç¡®ä¿ï¼š")
        logger.error(f"  1. XLæƒé‡æ–‡ä»¶å­˜åœ¨äº: {pretrained_xl}")
        logger.error("  2. è¿è¡Œ step2_download_models.py ä¸‹è½½æ¨¡å‹")
        logger.error("\nè®­ç»ƒå°†ç«‹å³åœæ­¢ä»¥é¿å…æ—¶é—´æµªè´¹ï¼\n")
        raise ValueError("å¿…é¡»åŠ è½½é¢„è®­ç»ƒæƒé‡æ‰èƒ½æ­£å¸¸è®­ç»ƒï¼")
    
    dit_model.to(device)
    
    # å¦‚æœæ˜¯å¤šGPUï¼Œä½¿ç”¨DataParallelåŒ…è£…
    num_gpus = torch.cuda.device_count()
    if num_gpus > 1:
        # ä½¿ç”¨å¹³è¡¡çš„GPUåˆ†é…ç­–ç•¥
        dit_model = DataParallel(dit_model)
        logger.info(f"âœ… ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡ŒDataParallelè®­ç»ƒ")
        logger.info("âš ï¸ æ³¨æ„: DataParallelåœ¨GPU0ä¸Šä¼šæœ‰é¢å¤–æ˜¾å­˜å¼€é”€")
        logger.info("   å»ºè®®: å¦‚æœOOMï¼Œå¯å°è¯•å‡å°batch_sizeæˆ–ä½¿ç”¨å•GPU")
    
    logger.info(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
    logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in dit_model.parameters()) / 1e6:.2f}M")
    
    # ===== 3. åˆå§‹åŒ–è®­ç»ƒç»„ä»¶ =====
    logger.info("\n=== åˆå§‹åŒ–è®­ç»ƒç»„ä»¶ ===")
    
    # åˆ›å»ºEMAæ¨¡å‹ - ç”¨äºç¨³å®šç”Ÿæˆè´¨é‡
    from copy import deepcopy
    ema_model = deepcopy(dit_model).eval()
    for param in ema_model.parameters():
        param.requires_grad = False
    logger.info("EMAæ¨¡å‹å·²åˆ›å»ºï¼ˆè¡°å‡ç‡=0.9999ï¼‰")
    
    # åˆ›å»ºtransport
    transport = create_transport(
        'Linear',
        'velocity',
        None,
        None,
        None,
    )
    
    # ä¼˜åŒ–å™¨ - åªä¼˜åŒ–æœªå†»ç»“çš„å‚æ•°
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, dit_model.parameters()),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay'],
        betas=(0.9, 0.95),  # å®˜æ–¹é…ç½®ï¼šbeta2=0.95
        eps=1e-8
    )
    
    # ä½¿ç”¨ä½™å¼¦é€€ç«+é¢„çƒ­ï¼Œé¿å…æ—©æœŸå­¦ä¹ ç‡è¿‡é«˜
    from torch.optim.lr_scheduler import LambdaLR
    
    def lr_lambda(current_step):
        warmup_steps = config.get('warmup_steps', 500)  # é€‚åº¦é¢„çƒ­æœŸ
        if current_step < warmup_steps:
            # çº¿æ€§é¢„çƒ­
            return float(current_step) / float(max(1, warmup_steps))
        # ä½™å¼¦é€€ç«ï¼Œæœ€ä½ä¿ç•™å­¦ä¹ ç‡
        total_steps = config['num_epochs'] * len(train_loader)
        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return max(0.01, 0.5 * (1.0 + torch.cos(torch.tensor(progress * 3.14159))))  # æœ€ä½1%
    
    scheduler = LambdaLR(optimizer, lr_lambda)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler(init_scale=65536.0, growth_interval=2000)
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†è·¯å¾„
    possible_data_paths = [
        "/kaggle/input/dataset",
        "/kaggle/input/micro-doppler-data",
        "./dataset",
    ]
    
    data_dir = None
    for path in possible_data_paths:
        if Path(path).exists():
            data_dir = Path(path)
            logger.info(f"æ‰¾åˆ°æ•°æ®é›†: {path}")
            break
    
    if data_dir is None:
        logger.error("æœªæ‰¾åˆ°æ•°æ®é›†ï¼è¯·æ£€æŸ¥è·¯å¾„")
        raise FileNotFoundError("Dataset not found")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MicroDopplerLatentDataset(data_dir, split='train')
    val_dataset = MicroDopplerLatentDataset(data_dir, split='val')
    
    logger.info(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    logger.info(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        pin_memory=config['pin_memory'],
        persistent_workers=config['persistent_workers']
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=config['pin_memory'],
        persistent_workers=config['persistent_workers']
    )
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š è®­ç»ƒä¿¡æ¯æ€»è§ˆ")
    logger.info("="*60)
    logger.info(f"æ¨¡å‹: LightningDiT-L (1024ç»´, 24å±‚, æ™ºèƒ½åˆå§‹åŒ–)")
    logger.info(f"å‚æ•°é‡: {sum(p.numel() for p in dit_model.parameters()) / 1e6:.2f}M")
    logger.info(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    logger.info(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    logger.info(f"æ¢¯åº¦ç´¯ç§¯: {config['gradient_accumulation_steps']} æ­¥")
    logger.info(f"æœ‰æ•ˆæ‰¹æ¬¡: {config['batch_size'] * config['gradient_accumulation_steps']}")
    logger.info(f"è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    logger.info(f"GPUæ•°é‡: {num_gpus}")
    logger.info("="*60)
    
    # ===== 5. å¼€å§‹è®­ç»ƒ =====
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    train_metrics_history = []
    val_metrics_history = []
    
    for epoch in range(config['num_epochs']):
        epoch_start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        dit_model.train()
        train_loss = 0
        train_steps = 0
        train_grad_norm = 0
        train_samples = 0
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config["num_epochs"]} [Train]')
        
        for batch_idx, batch in enumerate(pbar):
            batch_start_time = time.time()
            latents = batch[0].to(device)
            user_ids = batch[1].to(device)
            
            # å‰å‘ä¼ æ’­ - æ·»åŠ CFGè®­ç»ƒ(10%æ— æ¡ä»¶)
            with torch.cuda.amp.autocast():
                # CFGè®­ç»ƒï¼š15%æ¦‚ç‡ä¸¢å¼ƒæ¡ä»¶ï¼ˆå¢å¼ºæ— æ¡ä»¶å­¦ä¹ ï¼‰
                # æ³¨æ„ï¼šä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®null tokenï¼ŒLabelEmbedderä¼šè‡ªåŠ¨å¤„ç†
                # å½“dropoutæ—¶ï¼Œæ¨¡å‹å†…éƒ¨ä¼šä½¿ç”¨num_classes(31)ä½œä¸ºnull token
                model_kwargs = {"y": user_ids}
                
                # ä½¿ç”¨é‡è¦æ€§é‡‡æ ·
                t = torch.rand(latents.shape[0], device=device)
                
                loss_dict = transport.training_losses(dit_model, latents, model_kwargs)
                loss = loss_dict["loss"].mean()
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            
            # æ¢¯åº¦è£å‰ª - åªå¯¹å¯è®­ç»ƒå‚æ•°
            grad_norm = torch.nn.utils.clip_grad_norm_(
                filter(lambda p: p.requires_grad, dit_model.parameters()), 
                config['gradient_clip_norm']
            )
            
            scaler.step(optimizer)
            scaler.update()
            
            # å®šæœŸæ¸…ç†æ˜¾å­˜
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()
            
            # ç»Ÿè®¡
            train_loss += loss.item()
            train_grad_norm += grad_norm.item() if hasattr(grad_norm, 'item') else grad_norm
            train_steps += 1
            train_samples += latents.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            current_lr = optimizer.param_groups[0]['lr']
            samples_per_sec = latents.size(0) / (time.time() - batch_start_time)
            current_memory = torch.cuda.max_memory_allocated(0) / 1024**3
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{current_lr:.2e}',
                'grad': f'{grad_norm:.2f}',
                'sps': f'{samples_per_sec:.1f}',
                'mem': f'{current_memory:.1f}GB'
            })
            
            scheduler.step()
        
        # è®­ç»ƒé˜¶æ®µç»Ÿè®¡
        avg_train_loss = train_loss / train_steps
        avg_train_grad_norm = train_grad_norm / train_steps
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_steps = 0
        
        with tqdm(val_loader, desc=f"Epoch {epoch+1}/{config['num_epochs']} [Val]") as pbar:
            for batch in pbar:
                latents = batch[0].to(device)
                user_ids = batch[1].to(device)
                
                with torch.no_grad():
                    # ä¼ é€’æ¡ä»¶ä¿¡æ¯
                    model_kwargs = {"y": user_ids}
                    loss_dict = transport.training_losses(dit_model, latents, model_kwargs)
                    loss = loss_dict["loss"].mean()
                
                val_loss += loss.item()
                val_steps += 1
                
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_val_loss = val_loss / val_steps
        
        # ç¡®ä¿è¿›åº¦æ¡å®Œå…¨ç»“æŸåå†è¾“å‡ºæ—¥å¿—
        print()  # æ·»åŠ ç©ºè¡Œç¡®ä¿tqdmè¿›åº¦æ¡ç»“æŸ
        
        # è®°å½•æŒ‡æ ‡
        train_metrics_history.append({
            'epoch': epoch + 1,
            'loss': avg_train_loss,
            'grad_norm': avg_train_grad_norm,
            'lr': current_lr
        })
        
        val_metrics_history.append({
            'epoch': epoch + 1,
            'loss': avg_val_loss
        })
        
        # è¯¦ç»†è®­ç»ƒæŠ¥å‘Š - ä½¿ç”¨printç¡®ä¿è¾“å‡º
        print("\n" + "="*80)
        print(f"ğŸ“Š Epoch {epoch+1}/{config['num_epochs']} è®­ç»ƒæŠ¥å‘Š")
        print("="*80)
        
        # è®­ç»ƒç»Ÿè®¡
        print("\nğŸ“ˆ è®­ç»ƒé˜¶æ®µç»Ÿè®¡:")
        print(f"  â€¢ å¹³å‡æŸå¤±: {avg_train_loss:.6f}")
        print(f"  â€¢ æ¢¯åº¦èŒƒæ•°: {avg_train_grad_norm:.4f}")
        print(f"  â€¢ å­¦ä¹ ç‡: {current_lr:.2e}")
        print(f"  â€¢ è®­ç»ƒæ ·æœ¬æ•°: {train_samples}")
        print(f"  â€¢ æ¯ç§’æ ·æœ¬æ•°: {train_samples / (time.time() - epoch_start_time):.1f}")
        
        # éªŒè¯ç»Ÿè®¡
        print("\nğŸ“‰ éªŒè¯é˜¶æ®µç»Ÿè®¡:")
        print(f"  â€¢ å¹³å‡æŸå¤±: {avg_val_loss:.6f}")
        print(f"  â€¢ ç›¸å¯¹æ”¹å–„: {((best_val_loss - avg_val_loss) / best_val_loss * 100):.2f}%" if best_val_loss != float('inf') else "N/A")
        
        # GPUä½¿ç”¨æƒ…å†µ
        print("\nğŸ–¥ï¸ GPUèµ„æºä½¿ç”¨:")
        for i in range(num_gpus):
            mem_allocated = torch.cuda.memory_allocated(i) / 1024**3
            mem_reserved = torch.cuda.memory_reserved(i) / 1024**3
            print(f"  GPU {i}:")
            print(f"    â€¢ å·²åˆ†é…å†…å­˜: {mem_allocated:.2f}GB")
            print(f"    â€¢ å·²é¢„ç•™å†…å­˜: {mem_reserved:.2f}GB")
            print(f"    â€¢ åˆ©ç”¨ç‡: {(mem_allocated/mem_reserved*100):.1f}%" if mem_reserved > 0 else "N/A")
        
        # æ¡ä»¶ä¿¡æ¯éªŒè¯
        print("\nâœ… æ¡ä»¶æ³¨å…¥éªŒè¯:")
        print(f"  â€¢ ç”¨æˆ·ç±»åˆ«æ•°: {31}")
        # LabelEmbedderä½¿ç”¨embedding_tableè€Œéembedding_dim
        actual_model = model.module if hasattr(model, 'module') else model
        if hasattr(actual_model, 'y_embedder') and hasattr(actual_model.y_embedder, 'embedding_table'):
            embed_dim = actual_model.y_embedder.embedding_table.embedding_dim
            num_classes = actual_model.y_embedder.embedding_table.num_embeddings
            print(f"  â€¢ æ¡ä»¶åµŒå…¥ç»´åº¦: {embed_dim}")
            print(f"  â€¢ æ”¯æŒçš„ç±»åˆ«æ•°: {num_classes}")
        print(f"  â€¢ æ¡ä»¶ä¿¡æ¯å·²æ­£ç¡®ä¼ é€’åˆ°æ¨¡å‹")
        
        print("="*80)
        
        # æ¯ä¸ªepochè¾“å‡ºè¯¦ç»†è®­ç»ƒè´¨é‡æŠ¥å‘Š
        print("\n" + "="*80)
        print("ğŸ” è®­ç»ƒè´¨é‡åˆ†æ")
        print("="*80)
        
        # æŸå¤±è¶‹åŠ¿åˆ†æ
        if epoch > 0:
            train_loss_change = avg_train_loss - train_metrics_history[-2]['loss'] if len(train_metrics_history) > 1 else 0
            val_loss_change = avg_val_loss - val_metrics_history[-2]['loss'] if len(val_metrics_history) > 1 else 0
            
            print("\nğŸ“‰ æŸå¤±è¶‹åŠ¿:")
            if len(train_metrics_history) > 1:
                print(f"  â€¢ è®­ç»ƒæŸå¤±å˜åŒ–: {train_loss_change:+.6f} ({(train_loss_change/train_metrics_history[-2]['loss']*100):+.2f}%)")
            else:
                print("  â€¢ è®­ç»ƒæŸå¤±å˜åŒ–: N/A")
            if len(val_metrics_history) > 1:
                print(f"  â€¢ éªŒè¯æŸå¤±å˜åŒ–: {val_loss_change:+.6f} ({(val_loss_change/val_metrics_history[-2]['loss']*100):+.2f}%)")
            else:
                print("  â€¢ éªŒè¯æŸå¤±å˜åŒ–: N/A")
            
            # è¿‡æ‹Ÿåˆæ£€æµ‹
            overfit_gap = avg_val_loss - avg_train_loss
            print("\nâš ï¸ è¿‡æ‹Ÿåˆæ£€æµ‹:")
            print(f"  â€¢ éªŒè¯-è®­ç»ƒæŸå¤±å·®: {overfit_gap:.6f}")
            if overfit_gap > 0.1:
                print("  â€¢ è­¦å‘Š: å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼ŒéªŒè¯æŸå¤±æ˜¾è‘—é«˜äºè®­ç»ƒæŸå¤±")
            elif overfit_gap < -0.05:
                print("  â€¢ æ³¨æ„: éªŒè¯æŸå¤±ä½äºè®­ç»ƒæŸå¤±ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²æˆ–è¯„ä¼°é—®é¢˜")
            else:
                print("  â€¢ çŠ¶æ€: æ­£å¸¸ï¼Œæ— æ˜æ˜¾è¿‡æ‹Ÿåˆ")
        
        # è®­ç»ƒç¨³å®šæ€§åˆ†æ
        print("\nâš¡ è®­ç»ƒç¨³å®šæ€§:")
        if avg_train_grad_norm > 10:
            print(f"  â€¢ è­¦å‘Š: æ¢¯åº¦èŒƒæ•°è¿‡å¤§ ({avg_train_grad_norm:.2f})ï¼Œå¯èƒ½éœ€è¦é™ä½å­¦ä¹ ç‡")
        elif avg_train_grad_norm < 0.01:
            print(f"  â€¢ è­¦å‘Š: æ¢¯åº¦èŒƒæ•°è¿‡å° ({avg_train_grad_norm:.4f})ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜")
        else:
            print(f"  â€¢ æ¢¯åº¦èŒƒæ•°æ­£å¸¸: {avg_train_grad_norm:.4f}")
        
        # æ”¶æ•›çŠ¶æ€åˆ¤æ–­
        print("\nğŸ¯ æ”¶æ•›çŠ¶æ€:")
        if epoch >= 4:  # è‡³å°‘5ä¸ªepochååˆ¤æ–­
            recent_val_losses = [m['loss'] for m in val_metrics_history[-5:]]
            val_std = np.std(recent_val_losses)
            print(f"  â€¢ æœ€è¿‘5è½®éªŒè¯æŸå¤±æ ‡å‡†å·®: {val_std:.6f}")
            if val_std < 0.001:
                print("  â€¢ æ¨¡å‹å¯èƒ½å·²æ”¶æ•›")
            elif val_std < 0.01:
                print("  â€¢ æ¨¡å‹æ¥è¿‘æ”¶æ•›")
            else:
                print("  â€¢ æ¨¡å‹ä»åœ¨ä¼˜åŒ–ä¸­")
        
        if avg_val_loss < 0.4:
            print("  ğŸ“ˆ æ¨¡å‹è¡¨ç°ï¼šä¼˜ç§€ï¼ˆæŸå¤± < 0.4ï¼‰")
        elif avg_val_loss < 0.6:
            print("  ğŸ“Š æ¨¡å‹è¡¨ç°ï¼šè‰¯å¥½ï¼ˆæŸå¤± < 0.6ï¼‰")
        elif avg_val_loss < 0.8:
            print("  âš ï¸ æ¨¡å‹è¡¨ç°ï¼šä¸€èˆ¬ï¼ˆæŸå¤± < 0.8ï¼‰")
        else:
            print("  âŒ æ¨¡å‹è¡¨ç°ï¼šè¾ƒå·®ï¼ˆæŸå¤± >= 0.8ï¼Œéœ€è¦ç»§ç»­è®­ç»ƒï¼‰")
        
        print("="*80)
        
        # æ¯ä¸ªepochç”Ÿæˆæ¡ä»¶æ‰©æ•£æ ·æœ¬
        if True:  # æ¯ä¸ªepochéƒ½ç”Ÿæˆå¯è§†åŒ–å›¾åƒ
            print("\n" + "="*80)
            print(f"ğŸ¨ Epoch {epoch + 1}: ç”Ÿæˆæ¡ä»¶æ‰©æ•£æ ·æœ¬ï¼ˆä½¿ç”¨å¾®è°ƒåçš„VA-VAEï¼‰...")
            print("="*80)
            
            try:
                # å»¶è¿Ÿåˆå§‹åŒ–VAEä»¥èŠ‚çœå†…å­˜
                if vae is None:
                    print("  â€¢ åˆå§‹åŒ–VA-VAEç”¨äºæ ·æœ¬è§£ç ...")
                    from tokenizer.vavae import VA_VAE
                    # ä½¿ç”¨å¾®è°ƒå¥½çš„VA-VAEæ¨¡å‹
                    vae_config_path = Path("/kaggle/working/VA-VAE/LightningDiT/tokenizer/configs/vavae_f16d32.yaml")
                    vae_config = OmegaConf.load(str(vae_config_path))
                    # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹åˆ°çš„VA-VAEæ¨¡å‹
                    vae_config.ckpt_path = vae_checkpoint
                    temp_config_path = Path("/kaggle/working/temp_vae_config.yaml")
                    OmegaConf.save(vae_config, str(temp_config_path))
                    vae = VA_VAE(str(temp_config_path), img_size=256, horizon_flip=False, fp16=True)
                    print("  â€¢ VA-VAEåˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨Stage 3å¾®è°ƒæ¨¡å‹ï¼‰")
                
                # ä½¿ç”¨EMAæ¨¡å‹ç”Ÿæˆæ ·æœ¬ï¼ˆè´¨é‡æ›´å¥½ï¼‰
                generate_conditional_samples(ema_model, vae, transport, device, epoch+1, num_gpus, config)
                print("  â€¢ æ¡ä»¶æ ·æœ¬ç”Ÿæˆå®Œæˆ\n")
            except Exception as e:
                print(f"  âš ï¸ æ¡ä»¶æ ·æœ¬ç”Ÿæˆå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            
            # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹ä»¥èŠ‚çº¦ç©ºé—´
            old_best_models = list(Path("/kaggle/working").glob("best_dit_epoch_*.pt"))
            for old_model in old_best_models:
                try:
                    old_model.unlink()
                    logger.info(f"  â€¢ åˆ é™¤æ—§æ¨¡å‹: {old_model.name}")
                except Exception as e:
                    logger.warning(f"  â€¢ æ— æ³•åˆ é™¤æ—§æ¨¡å‹ {old_model.name}: {e}")
            
            # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
            best_model_path = Path("/kaggle/working") / f"best_dit_epoch_{epoch+1}.pt"
            model_state = model.module.state_dict() if num_gpus > 1 else model.state_dict()
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model_state,
                'ema_state_dict': ema_model.state_dict() if 'ema_model' in locals() else None,
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_loss': avg_val_loss,
                'config': config
            }, best_model_path)
            logger.info(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {best_model_path} (val_loss: {avg_val_loss:.6f})")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = Path("/kaggle/working") / "final_dit_model.pt"
    model_state = model.module.state_dict() if num_gpus > 1 else model.state_dict()
    torch.save({
        'model_state_dict': model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'config': config,
        'training_history': {
            'train_metrics': train_metrics_history,
            'val_metrics': val_metrics_history
        }
    }, final_model_path)
    
    logger.info(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    logger.info(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {final_model_path}")


def prepare_latents_for_training():
    """é¢„å¤„ç†ï¼šåªåœ¨ä¸»è¿›ç¨‹ä¸­å‡†å¤‡æ½œç©ºé—´æ•°æ®"""
    # æ£€æŸ¥GPUçŠ¶æ€å’Œæ˜¾å­˜
    device = torch.device('cuda:0')
    torch.cuda.set_device(0)
    torch.cuda.empty_cache()
    
    logger.info("=== å‡†å¤‡æ½œç©ºé—´æ•°æ® ===")
    initial_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"åˆå§‹æ˜¾å­˜ä½¿ç”¨: {initial_memory:.2f}GB")
    
    # ä½¿ç”¨æŒ‡å®šçš„VA-VAEæ¨¡å‹è·¯å¾„
    vae_checkpoint = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
    
    if Path(vae_checkpoint).exists():
        logger.info(f"âœ… æ‰¾åˆ°VA-VAEæ¨¡å‹: {vae_checkpoint}")
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size_mb = Path(vae_checkpoint).stat().st_size / (1024 * 1024)
        logger.info(f"   æ¨¡å‹å¤§å°: {size_mb:.2f} MB")
    else:
        logger.error("âŒ æœªæ‰¾åˆ°VA-VAEæ¨¡å‹ï¼")
        logger.error(f"   è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨: {vae_checkpoint}")
        raise FileNotFoundError(f"VA-VAE checkpoint not found at {vae_checkpoint}")
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†è·¯å¾„
    possible_data_paths = [
        "/kaggle/input/dataset",  # æ ‡å‡†Kaggleè·¯å¾„
        "/kaggle/input/micro-doppler-data",  # æ›¿ä»£è·¯å¾„
        "./dataset",  # æœ¬åœ°è·¯å¾„
        "G:/micro-doppler-dataset"  # æœ¬åœ°æµ‹è¯•è·¯å¾„
    ]
    
    data_dir = None
    for path in possible_data_paths:
        if Path(path).exists():
            data_dir = Path(path)
            logger.info(f"æ‰¾åˆ°æ•°æ®é›†: {path}")
            break
    
    if data_dir is None:
        logger.error("æœªæ‰¾åˆ°æ•°æ®é›†ï¼è¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„:")
        for path in possible_data_paths:
            logger.error(f"  - {path}")
        raise FileNotFoundError("Dataset not found")
    
    # ä¿®æ­£é…ç½®ä¸­çš„checkpointè·¯å¾„ä¸ºå¾®è°ƒæ¨¡å‹
    logger.info("å‡†å¤‡VA-VAEé…ç½®...")
    vae_config_path = Path("/kaggle/working/VA-VAE/LightningDiT/tokenizer/configs/vavae_f16d32.yaml")
    
    with open(vae_config_path, 'r') as f:
        config_content = f.read()
    
    # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹åˆ°çš„å¾®è°ƒæ¨¡å‹  
    finetuned_checkpoint = vae_checkpoint
    if finetuned_checkpoint not in config_content:
        config_content = config_content.replace(
            'ckpt_path: /path/to/checkpoint.pt',
            f'ckpt_path: {finetuned_checkpoint}'
        )
        with open(vae_config_path, 'w') as f:
            f.write(config_content)
        logger.info(f"å·²æ›´æ–°VA-VAE checkpointè·¯å¾„: {finetuned_checkpoint}")
    
    # åˆå§‹åŒ–VA-VAEè¿›è¡Œæ½œç©ºé—´ç¼–ç 
    from tokenizer.vavae import VA_VAE
    logger.info("åŠ è½½VA-VAEæ¨¡å‹...")
    vae = VA_VAE(str(vae_config_path), img_size=256, horizon_flip=False, fp16=True)
    vae_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"VA-VAEåŠ è½½å®Œæˆï¼Œæ˜¾å­˜: {vae_memory:.2f}GB")
    
    # é¢„è®¡ç®—æ½œç©ºé—´ï¼ˆä½¿ç”¨å·²æ£€æµ‹çš„æ•°æ®è·¯å¾„ï¼‰
    latents_file = Path("/kaggle/working") / 'latents_microdoppler.npz'
    if not latents_file.exists():
        logger.info("å¼€å§‹ç¼–ç æ•°æ®é›†åˆ°æ½œç©ºé—´...")
        encode_dataset_to_latents(vae, data_dir, device)
        logger.info("æ½œç©ºé—´ç¼–ç å®Œæˆ")
    else:
        logger.info("æ½œç©ºé—´æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç¼–ç ")
    
    # é‡Šæ”¾VA-VAEæ˜¾å­˜
    del vae
    torch.cuda.empty_cache()
    final_memory = torch.cuda.memory_allocated(device) / 1024**3
    logger.info(f"VA-VAEé‡Šæ”¾åæ˜¾å­˜: {final_memory:.2f}GB")


# DDPè®­ç»ƒå‡½æ•°å·²åˆ é™¤ï¼Œæ”¹ç”¨DataParallel


def generate_conditional_samples(model, vae, transport, device, epoch, num_gpus, config=None):
    """ç”Ÿæˆæ¡ä»¶æ‰©æ•£æ ·æœ¬ç”¨äºéªŒè¯æ¡ä»¶æ§åˆ¶èƒ½åŠ›
    
    é‡‡æ ·æŠ€æœ¯è¯´æ˜ï¼š
    - dopri5: 5é˜¶è‡ªé€‚åº”Runge-Kuttaæ–¹æ³•ï¼Œç²¾åº¦æœ€é«˜
    - 150æ­¥: å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ï¼ˆå®˜æ–¹æ¨è250æ­¥ï¼‰
    - CFG: æœªå¯ç”¨ï¼ˆéœ€è¦ä¿®æ”¹æ¨¡å‹forwardæ¥å£ï¼‰
    """
    model.eval()
    
    with torch.no_grad():
        print(f"\n  ğŸ“Š å¼€å§‹ç”Ÿæˆæ¡ä»¶æ ·æœ¬ (Epoch {epoch}, euleræ±‚è§£å™¨, 250æ­¥)...")
        
        # ç”Ÿæˆå¤šä¸ªç”¨æˆ·çš„æ ·æœ¬
        num_users_to_sample = min(8, 31)  # é‡‡æ ·8ä¸ªä¸åŒç”¨æˆ·
        samples_per_user = 2  # æ¯ä¸ªç”¨æˆ·ç”Ÿæˆ2ä¸ªæ ·æœ¬
        
        # é€‰æ‹©è¦é‡‡æ ·çš„ç”¨æˆ·
        selected_users = torch.linspace(0, 30, num_users_to_sample, dtype=torch.long, device=device)
        print(f"  â€¢ é€‰æ‹©çš„ç”¨æˆ·ID: {selected_users.tolist()}")
        print(f"  â€¢ æ¯ä¸ªç”¨æˆ·ç”Ÿæˆ {samples_per_user} ä¸ªæ ·æœ¬")
        
        all_samples = []
        all_user_ids = []
        
        for idx, user_id in enumerate(selected_users):
            # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆå¤šä¸ªæ ·æœ¬
            user_batch = user_id.repeat(samples_per_user)
            
            # éšæœºåˆå§‹åŒ–å™ªå£°
            z = torch.randn(samples_per_user, 32, 16, 16, device=device)
            
            # ç”Ÿæˆæ ·æœ¬ - ä½¿ç”¨CFGå¢å¼ºæ¡ä»¶æ§åˆ¶
            actual_model = model.module if num_gpus > 1 else model
            
            # CFGé‡‡æ ·ï¼šåŒæ—¶è®¡ç®—æ¡ä»¶å’Œæ— æ¡ä»¶
            cfg_scale = config.get('cfg_scale', 10.0)  # å®˜æ–¹æ¨èCFG=10.0
            
            # å‡†å¤‡æ¡ä»¶å’Œæ— æ¡ä»¶è¾“å…¥
            y_null = torch.full_like(user_batch, 31)  # null token (ç¬¬32ä¸ªç±»åˆ«)
            y_combined = torch.cat([user_batch, y_null], dim=0)
            
            # å®šä¹‰CFGåŒ…è£…å‡½æ•°ï¼ˆæ”¯æŒcfg_intervalï¼‰
            def model_fn(x, t):
                # åº”ç”¨cfg_intervalï¼šä»…åœ¨ä½å™ªå£°æ—¶ä½¿ç”¨CFG
                cfg_interval_start = config.get('cfg_interval_start', 0.11)
                # æ³¨æ„ï¼štæ˜¯å½’ä¸€åŒ–çš„æ—¶é—´æ­¥ï¼ˆ0åˆ°1ï¼‰ï¼Œt=0æ˜¯çº¯å™ªå£°ï¼Œt=1æ˜¯å¹²å‡€æ•°æ®
                # åªæœ‰å½“t > cfg_interval_startæ—¶æ‰ä½¿ç”¨CFG
                if t.min() > cfg_interval_start:
                    # xå·²ç»æ˜¯å•æ‰¹æ¬¡ï¼Œéœ€è¦å¤åˆ¶ä¸ºæ¡ä»¶å’Œæ— æ¡ä»¶
                    x_combined = torch.cat([x, x], dim=0)
                    # æ—¶é—´æ­¥tä¹Ÿéœ€è¦å¤åˆ¶ä»¥åŒ¹é…æ‰¹é‡å¤§å°
                    t_combined = torch.cat([t, t], dim=0)
                    out = actual_model(x_combined, t_combined, y=y_combined)
                    out_cond, out_uncond = out.chunk(2, dim=0)
                    # CFG: æ— æ¡ä»¶è¾“å‡º + scale * (æ¡ä»¶ - æ— æ¡ä»¶)
                    return out_uncond + cfg_scale * (out_cond - out_uncond)
                else:
                    # é«˜å™ªå£°é˜¶æ®µä¸ä½¿ç”¨CFGï¼Œåªä½¿ç”¨æ¡ä»¶ç”Ÿæˆ
                    return actual_model(x, t, y=user_batch)
            
            # ä½¿ç”¨Samplerè¿›è¡Œé‡‡æ · - å®˜æ–¹æ¨èé…ç½®
            sampler = Sampler(transport)
            sample_fn = sampler.sample_ode(
                sampling_method=config['sampling_method'],
                num_steps=config['num_steps'],
                atol=1e-6,
                rtol=1e-3,
                timestep_shift=config.get('timestep_shift', 0.3)
            )
            samples = sample_fn(z, model_fn)[-1]
            
            all_samples.append(samples)
            all_user_ids.append(user_batch)
            
            if (idx + 1) % 4 == 0:
                print(f"    â€¢ å·²ç”Ÿæˆ {idx + 1}/{num_users_to_sample} ä¸ªç”¨æˆ·çš„æ ·æœ¬")
        
        # åˆå¹¶æ‰€æœ‰æ ·æœ¬
        all_samples = torch.cat(all_samples, dim=0)
        all_user_ids = torch.cat(all_user_ids, dim=0)
        
        print(f"  â€¢ æˆåŠŸç”Ÿæˆ {len(all_samples)} ä¸ªæ¡ä»¶æ ·æœ¬")
        print(f"  â€¢ æ ·æœ¬å½¢çŠ¶: {all_samples.shape}")
        
        # ä½¿ç”¨VA-VAEè§£ç 
        print("\n  ğŸ¨ å¼€å§‹è§£ç æ½œç©ºé—´æ ·æœ¬åˆ°å›¾åƒ...")
        try:
            # å°†æ½œç©ºé—´æ ·æœ¬ç§»åˆ°cuda:0ï¼ˆVA-VAEæ‰€åœ¨çš„è®¾å¤‡ï¼‰
            samples_cuda0 = all_samples.to('cuda:0')
            print(f"    â€¢ æ ·æœ¬å·²ç§»åˆ° cuda:0")
            
            # åæ ‡å‡†åŒ–ï¼ˆå¦‚æœæœ‰ç»Ÿè®¡ä¿¡æ¯ï¼‰
            if hasattr(vae, 'latent_mean') and vae.latent_mean is not None:
                samples_cuda0 = samples_cuda0 * vae.latent_std + vae.latent_mean
                print(f"    â€¢ å·²åº”ç”¨åæ ‡å‡†åŒ–")
            
            # è§£ç 
            print(f"    â€¢ å¼€å§‹VA-VAEè§£ç ...")
            # VA_VAEä½¿ç”¨decode_to_imagesæ–¹æ³•ï¼Œè¿”å›[0,255]çš„uint8å›¾åƒ
            images_np = vae.decode_to_images(samples_cuda0)
            # è½¬æ¢ä¸ºtorch tensorå¹¶æ­£ç¡®å½’ä¸€åŒ–åˆ°[-1, 1]
            images = torch.from_numpy(images_np).float() / 255.0 * 2.0 - 1.0  # [0,255] -> [-1,1]
            images = images.permute(0, 3, 1, 2)  # NHWC -> NCHW
            print(f"    â€¢ è§£ç å®Œæˆï¼Œå›¾åƒå½¢çŠ¶: {images.shape}")
            
            # ä¿å­˜å›¾åƒ
            save_dir = Path("/kaggle/working") / f"samples_epoch_{epoch}"
            save_dir.mkdir(exist_ok=True, parents=True)
            
            from torchvision.utils import save_image
            
            # åˆ›å»ºç½‘æ ¼å›¾
            grid_path = save_dir / "grid.png"
            num_show = min(16, len(images))
            save_image(images[:num_show], grid_path, nrow=4, normalize=True, value_range=(-1, 1))
            print(f"  âœ… ç½‘æ ¼å›¾å·²ä¿å­˜: {grid_path}")
            
            # ä¿å­˜å‰å‡ å¼ å•ç‹¬çš„å›¾åƒ
            num_save = min(8, len(images))
            for i in range(num_save):
                uid = all_user_ids[i].item()
                img_path = save_dir / f"user_{uid}_sample_{i}.png"
                save_image(images[i], img_path, normalize=True, value_range=(-1, 1))
            print(f"  âœ… ä¿å­˜äº† {num_save} å¼ å•ç‹¬å›¾åƒåˆ°: {save_dir}")
            
            # åˆ›å»ºç”¨æˆ·åˆ†ç»„çš„ç½‘æ ¼å›¾
            users_path = save_dir / "users_grid.png"
            save_image(images, users_path, nrow=samples_per_user, normalize=True, value_range=(-1, 1))
            print(f"  âœ… ç”¨æˆ·åˆ†ç»„ç½‘æ ¼å›¾: {users_path}")
            
        except Exception as e:
            print(f"\n  âš ï¸ è§£ç å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            print("  â€¢ æ­£åœ¨ä¿å­˜æ½œç©ºé—´æ ·æœ¬è€Œéå›¾åƒ...")
            
            # ä¿å­˜æ½œç©ºé—´è¡¨ç¤º
            latent_path = Path("/kaggle/working") / f"latents_epoch_{epoch}.pt"
            torch.save({
                'latents': all_samples.cpu(),
                'user_ids': all_user_ids.cpu(),
                'epoch': epoch
            }, latent_path)
            print(f"  âœ… æ½œç©ºé—´æ ·æœ¬å·²ä¿å­˜åˆ°: {latent_path}")


def main():
    """ä¸»å‡½æ•° - Kaggle T4x2ä¼˜åŒ–ç‰ˆæœ¬"""
    # Kaggleç¯å¢ƒGPUæ£€æŸ¥
    if not torch.cuda.is_available():
        logger.error("CUDA not available! Please enable GPU accelerator in Kaggle.")
        return
    
    num_gpus = torch.cuda.device_count()
    logger.info(f"Detected {num_gpus} GPU(s)")
    
    # Kaggle T4x2ä½¿ç”¨DataParallelï¼Œä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    train_dit()

if __name__ == "__main__":
    main()

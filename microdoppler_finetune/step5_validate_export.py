#!/usr/bin/env python3
"""
Step 5: VA-VAEå®Œæ•´éªŒè¯è„šæœ¬ (é›†æˆç‰ˆ)
åŒ…å«åŸºç¡€éªŒè¯ã€VA-VAEç‰¹æœ‰åŠŸèƒ½éªŒè¯å’Œæ¨¡å‹å¯¼å‡º
"""

import os
import sys
import argparse
from pathlib import Path
import json
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score

# æ·»åŠ LightningDiTè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))
sys.path.insert(0, str(project_root))  # æ·»åŠ æ ¹ç›®å½•

# å…³é”®ï¼šåœ¨å¯¼å…¥ldmä¹‹å‰è®¾ç½®tamingè·¯å¾„ï¼
def setup_taming_path():
    """è®¾ç½®tamingè·¯å¾„ï¼Œå¿…é¡»åœ¨å¯¼å…¥ldmä¹‹å‰è°ƒç”¨"""
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥tamingä½ç½®
    taming_locations = [
        Path('/kaggle/working/taming-transformers'),  # Kaggleæ ‡å‡†ä½ç½®
        Path('/kaggle/working/.taming_path'),  # è·¯å¾„æ–‡ä»¶
        Path.cwd().parent / 'taming-transformers',  # é¡¹ç›®æ ¹ç›®å½•
        Path.cwd() / '.taming_path'  # å½“å‰ç›®å½•è·¯å¾„æ–‡ä»¶
    ]
    
    for location in taming_locations:
        if location.name == '.taming_path' and location.exists():
            # è¯»å–è·¯å¾„æ–‡ä»¶
            try:
                with open(location, 'r') as f:
                    taming_path = f.read().strip()
                if Path(taming_path).exists() and taming_path not in sys.path:
                    sys.path.insert(0, taming_path)
                    print(f"ğŸ“‚ å·²åŠ è½½tamingè·¯å¾„: {taming_path}")
                    return True
            except Exception as e:
                continue
        elif location.name == 'taming-transformers' and location.exists():
            # ç›´æ¥è·¯å¾„
            taming_path = str(location.absolute())
            if taming_path not in sys.path:
                sys.path.insert(0, taming_path)
                print(f"ğŸ“‚ å‘ç°å¹¶åŠ è½½taming: {taming_path}")
                return True
    
    # é™é»˜å¤±è´¥ï¼Œå› ä¸ºå¯èƒ½å·²ç»é€šè¿‡å…¶ä»–æ–¹å¼åŠ è½½
    return False

# è®¾ç½®tamingè·¯å¾„ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ldmä¹‹å‰ï¼‰
setup_taming_path()

from omegaconf import OmegaConf
from ldm.util import instantiate_from_config


def load_model(checkpoint_path, config_path=None):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    
    # åŠ è½½é…ç½®
    if config_path and Path(config_path).exists():
        config = OmegaConf.load(config_path)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = OmegaConf.create({
            'model': {
                'target': 'ldm.models.autoencoder.AutoencoderKL',
                'params': {
                    'embed_dim': 32,
                    'use_vf': 'dinov2',
                    'reverse_proj': True,
                    'ddconfig': {
                        'double_z': True,
                        'z_channels': 32,
                        'resolution': 256,
                        'in_channels': 3,
                        'out_ch': 3,
                        'ch': 128,
                        'ch_mult': [1, 1, 2, 2, 4],
                        'num_res_blocks': 2,
                        'attn_resolutions': [16],
                        'dropout': 0.0
                    },
                    'lossconfig': {
                        'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                        'params': {
                            'disc_start': 1,
                            'vf_weight': 0.1,
                            'distmat_weight': 1.0,
                            'cos_weight': 1.0
                        }
                    }
                }
            }
        })
    
    # å®ä¾‹åŒ–æ¨¡å‹
    model = instantiate_from_config(config if isinstance(config, dict) else config.model)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    if 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    model.eval()
    return model


def validate_reconstruction(model, data_root, split_file, num_samples=16, device='cuda'):
    """éªŒè¯é‡å»ºè´¨é‡"""
    
    print("\nğŸ” éªŒè¯é‡å»ºè´¨é‡...")
    
    # åŠ è½½æ•°æ®
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„æ ¼å¼
    if isinstance(split_data['val'], list):
        val_data = split_data['val'][:num_samples]
    else:
        # å¦‚æœvalæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        val_data = list(split_data['val'].values())[:num_samples]
    
    # æ£€æŸ¥æ•°æ®é¡¹æ ¼å¼å¹¶è°ƒè¯•
    if val_data and len(val_data) > 0:
        print(f"ğŸ“Š æ•°æ®é¡¹ç¤ºä¾‹: {val_data[0]}")
        print(f"ğŸ“Š æ•°æ®é¡¹ç±»å‹: {type(val_data[0])}")
    
    # å‡†å¤‡å›¾åƒ
    images = []
    reconstructions = []
    
    model = model.to(device)
    
    with torch.no_grad():
        for item in tqdm(val_data, desc="å¤„ç†å›¾åƒ"):
            # åŠ è½½å›¾åƒ
            img_path = Path(data_root) / item['path']
            if not img_path.exists():
                continue
                
            img = Image.open(img_path).convert('RGB')
            img = img.resize((256, 256), Image.LANCZOS)
            
            # è½¬æ¢ä¸ºtensor
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            # é‡å»º
            reconstructed, _, _, _ = model(img_tensor)
            
            images.append(img_tensor.cpu())
            reconstructions.append(reconstructed.cpu())
    
    # è®¡ç®—æŒ‡æ ‡
    images_cat = torch.cat(images, dim=0)
    recons_cat = torch.cat(reconstructions, dim=0)
    
    # MSE
    mse = torch.mean((images_cat - recons_cat) ** 2).item()
    
    # PSNR
    psnr = 20 * np.log10(2.0) - 10 * np.log10(mse)
    
    print(f"âœ… é‡å»ºæŒ‡æ ‡:")
    print(f"   MSE: {mse:.6f}")
    print(f"   PSNR: {psnr:.2f} dB")
    
    # ä¿å­˜å¯è§†åŒ–
    save_reconstruction_grid(images_cat, recons_cat, 'reconstruction_results.png')
    
    return mse, psnr


def save_reconstruction_grid(images, reconstructions, save_path, num_show=8):
    """ä¿å­˜é‡å»ºå¯¹æ¯”å›¾"""
    
    num_show = min(num_show, len(images))
    
    fig, axes = plt.subplots(2, num_show, figsize=(num_show * 2, 4))
    
    for i in range(num_show):
        # åŸå›¾
        img = images[i].permute(1, 2, 0).numpy()
        img = (img + 1) / 2  # [-1,1] -> [0,1]
        axes[0, i].imshow(np.clip(img, 0, 1))
        axes[0, i].axis('off')
        if i == 0:
            axes[0, i].set_title('Original')
        
        # é‡å»º
        rec = reconstructions[i].permute(1, 2, 0).numpy()
        rec = (rec + 1) / 2
        axes[1, i].imshow(np.clip(rec, 0, 1))
        axes[1, i].axis('off')
        if i == 0:
            axes[1, i].set_title('Reconstructed')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š ä¿å­˜é‡å»ºå¯¹æ¯”å›¾: {save_path}")


def test_vf_alignment(model, data_root, split_file, device='cuda'):
    """æµ‹è¯•Vision Foundationå¯¹é½èƒ½åŠ› - VA-VAEæ ¸å¿ƒåˆ›æ–°"""
    
    print("\nğŸ¨ Vision Foundationå¯¹é½èƒ½åŠ›éªŒè¯...")
    
    model = model.to(device)
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„æ ¼å¼
    if isinstance(split_data['val'], list):
        test_samples = split_data['val'][:20]
    else:
        test_samples = list(split_data['val'].values())[:20]
    vf_similarities, reconstruction_errors = [], []
    
    with torch.no_grad():
        for item in tqdm(test_samples, desc="VFå¯¹é½æµ‹è¯•"):
            img_path = Path(data_root) / item['path']
            if not img_path.exists():
                continue
                
            img = Image.open(img_path).convert('RGB').resize((256, 256))
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            reconstructed, posterior, aux_feature, z = model(img_tensor)
            
            if aux_feature is not None and hasattr(model, 'foundation_model'):
                with torch.no_grad():
                    recon_vf = model.foundation_model(reconstructed)
                    
                similarity = torch.cosine_similarity(
                    aux_feature.flatten(), recon_vf.flatten(), dim=0).item()
                
                vf_similarities.append(similarity)
                reconstruction_errors.append(torch.mean((img_tensor - reconstructed) ** 2).item())
    
    avg_vf_similarity = np.mean(vf_similarities)
    
    print(f"âœ… Vision Foundationå¯¹é½ç»“æœ:")
    print(f"   å¹³å‡VFè¯­ä¹‰ç›¸ä¼¼åº¦: {avg_vf_similarity:.4f}")
    print(f"   VFç›¸ä¼¼åº¦æ ‡å‡†å·®: {np.std(vf_similarities):.4f}")
    
    if avg_vf_similarity > 0.95:
        print(f"   ğŸ† VFå¯¹é½è´¨é‡: ä¼˜ç§€ (>0.95)")
    elif avg_vf_similarity > 0.85:
        print(f"   âœ… VFå¯¹é½è´¨é‡: è‰¯å¥½ (>0.85)")
    else:
        print(f"   âš ï¸ VFå¯¹é½è´¨é‡: éœ€è¦æ”¹è¿› (<0.85)")
    
    return avg_vf_similarity


def test_user_discrimination(model, data_root, split_file, device='cuda'):
    """æµ‹è¯•ç”¨æˆ·åŒºåˆ†èƒ½åŠ› - VA-VAE Stage3åˆ›æ–°"""
    
    print("\nğŸ‘¥ ç”¨æˆ·åŒºåˆ†èƒ½åŠ›éªŒè¯...")
    
    model = model.to(device)
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„æ ¼å¼
    if isinstance(split_data['train'], list):
        train_samples = split_data['train'][:300]
    else:
        train_samples = list(split_data['train'].values())[:300]
    
    user_features, user_labels, all_features = {}, [], []
    
    with torch.no_grad():
        for item in tqdm(train_samples, desc="æå–ç”¨æˆ·ç‰¹å¾"):
            img_path = Path(data_root) / item['path']
            if not img_path.exists():
                continue
                
            user_id = item['user_id']
            
            img = Image.open(img_path).convert('RGB').resize((256, 256))
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            posterior = model.encode(img_tensor)
            z = posterior.sample().cpu().numpy().flatten()
            
            if user_id not in user_features:
                user_features[user_id] = []
            user_features[user_id].append(z)
            all_features.append(z)
            user_labels.append(user_id)
    
    all_features = np.array(all_features)
    
    # è®¡ç®—åˆ†ç¦»åº¦
    silhouette = 0
    if len(set(user_labels)) > 1:
        silhouette = silhouette_score(all_features, user_labels)
        print(f"âœ… ç”¨æˆ·åˆ†ç¦»åº¦ (Silhouette): {silhouette:.4f}")
    
    # ç±»å†…å¤–è·ç¦»æ¯”
    intra_distances, inter_distances = [], []
    user_means = {}
    
    for user_id, features in user_features.items():
        if len(features) > 1:
            features_array = np.array(features)
            user_mean = np.mean(features_array, axis=0)
            user_means[user_id] = user_mean
            
            for feat in features_array:
                intra_distances.append(np.linalg.norm(feat - user_mean))
    
    user_ids = list(user_means.keys())
    for i in range(len(user_ids)):
        for j in range(i+1, len(user_ids)):
            distance = np.linalg.norm(user_means[user_ids[i]] - user_means[user_ids[j]])
            inter_distances.append(distance)
    
    avg_intra = np.mean(intra_distances) if intra_distances else 0
    avg_inter = np.mean(inter_distances) if inter_distances else 0
    separation_ratio = avg_inter / avg_intra if avg_intra > 0 else 0
    
    print(f"âœ… ç±»é—´/ç±»å†…è·ç¦»æ¯”: {separation_ratio:.4f}")
    
    # ç”Ÿæˆt-SNEå¯è§†åŒ–
    if len(all_features) > 50:
        visualize_user_distribution(user_features)
    
    return silhouette, separation_ratio, user_features


def visualize_user_distribution(user_features, save_path='user_distribution.png'):
    """å¯è§†åŒ–ç”¨æˆ·ç‰¹å¾åˆ†å¸ƒ"""
    
    print("\nğŸ“Š ç”Ÿæˆç”¨æˆ·åˆ†å¸ƒå¯è§†åŒ–...")
    
    all_features, all_labels = [], []
    for user_id, features in user_features.items():
        for feat in features:
            all_features.append(feat)
            all_labels.append(user_id)
    
    all_features = np.array(all_features)
    
    if len(all_features) > 50:
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_features)//4))
        features_2d = tsne.fit_transform(all_features)
        
        plt.figure(figsize=(12, 8))
        unique_users = list(set(all_labels))
        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_users)))
        
        for i, user_id in enumerate(unique_users):
            mask = np.array(all_labels) == user_id
            plt.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                       c=[colors[i]], label=f'User {user_id}', alpha=0.7)
        
        plt.title('VA-VAEæ½œåœ¨ç©ºé—´ä¸­çš„ç”¨æˆ·åˆ†å¸ƒ (t-SNE)')
        plt.xlabel('t-SNEç»´åº¦ 1')
        plt.ylabel('t-SNEç»´åº¦ 2')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ç”¨æˆ·åˆ†å¸ƒå›¾ä¿å­˜è‡³: {save_path}")


def extract_latent_statistics(model, data_root, split_file, device='cuda'):
    """æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯"""
    
    print("\nğŸ“ˆ æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡...")
    
    # åŠ è½½æ•°æ®
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„æ ¼å¼
    if isinstance(split_data['train'], list):
        train_data = split_data['train']
    else:
        train_data = list(split_data['train'].values())
    
    model = model.to(device)
    
    all_latents = []
    
    with torch.no_grad():
        for item in tqdm(train_data, desc="ç¼–ç å›¾åƒ"):
            img_path = Path(data_root) / item['path']
            if not img_path.exists():
                continue
            
            img = Image.open(img_path).convert('RGB').resize((256, 256), Image.LANCZOS)
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            # ç¼–ç 
            posterior = model.encode(img_tensor)
            z = posterior.sample()
            
            all_latents.append(z.cpu())
            
            # æŒ‰ç”¨æˆ·åˆ†ç»„
            user_id = item['user_id']
            if user_id not in user_latents:
                user_latents[user_id] = []
            user_latents[user_id].append(z.cpu())
    
    # è®¡ç®—å…¨å±€ç»Ÿè®¡
    all_latents = torch.cat(all_latents, dim=0)
    mean = all_latents.mean(dim=[0, 2, 3])  # [C]
    std = all_latents.std(dim=[0, 2, 3])    # [C]
    
    # è®¡ç®—ç”¨æˆ·é—´å·®å¼‚
    user_means = {}
    for user_id, latents in user_latents.items():
        user_latents_cat = torch.cat(latents, dim=0)
        user_means[user_id] = user_latents_cat.mean(dim=[0, 2, 3])
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'global_mean': mean.numpy().tolist(),
        'global_std': std.numpy().tolist(),
        'num_samples': len(all_latents),
        'latent_dim': all_latents.shape[1],
        'spatial_size': [all_latents.shape[2], all_latents.shape[3]]
    }
    
    stats_path = 'latent_statistics.json'
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"âœ… æ½œåœ¨ç©ºé—´ç»Ÿè®¡:")
    print(f"   ç»´åº¦: {stats['latent_dim']} x {stats['spatial_size'][0]} x {stats['spatial_size'][1]}")
    print(f"   æ ·æœ¬æ•°: {stats['num_samples']}")
    print(f"   å‡å€¼èŒƒå›´: [{min(mean):.3f}, {max(mean):.3f}]")
    print(f"   æ ‡å‡†å·®èŒƒå›´: [{min(std):.3f}, {max(std):.3f}]")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ä¿å­˜è‡³: {stats_path}")
    
    return stats


def export_for_dit(checkpoint_path, output_path):
    """å¯¼å‡ºæ¨¡å‹ç”¨äºDiTè®­ç»ƒ"""
    
    print(f"\nğŸ“¦ å¯¼å‡ºæ¨¡å‹ç”¨äºDiTè®­ç»ƒ...")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # æå–å¿…è¦çš„ç»„ä»¶
    state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint
    
    # åªä¿ç•™ç¼–ç å™¨å’Œé‡åŒ–å±‚ï¼ˆDiTåªéœ€è¦ç¼–ç å™¨ï¼‰
    encoder_keys = [k for k in state_dict.keys() if 
                   k.startswith('encoder.') or 
                   k.startswith('quant_conv.') or
                   k.startswith('linear_proj.')]
    
    encoder_state = {k: state_dict[k] for k in encoder_keys}
    
    # ä¿å­˜DiTç‰ˆæœ¬
    dit_checkpoint = {
        'encoder_state_dict': encoder_state,
        'full_state_dict': state_dict,  # ä¹Ÿä¿ç•™å®Œæ•´ç‰ˆæœ¬
        'config': {
            'embed_dim': 32,
            'z_channels': 32,
            'use_vf': 'dinov2',
            'reverse_proj': True,
            'resolution': 256
        },
        'type': 'vavae_encoder_for_dit'
    }
    
    torch.save(dit_checkpoint, output_path)
    print(f"âœ… DiTç¼–ç å™¨å¯¼å‡ºè‡³: {output_path}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = Path(output_path).stat().st_size / (1024 * 1024)
    print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser()
    
    # è·¯å¾„å‚æ•°
    parser.add_argument('--checkpoint', type=str, 
                       default='checkpoints/stage3/last.ckpt',
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--config', type=str,
                       default='checkpoints/stage3/config.yaml',
                       help='æ¨¡å‹é…ç½®æ–‡ä»¶')
    parser.add_argument('--data_root', type=str,
                       default='/kaggle/input/dataset',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split_file', type=str,
                       default='/kaggle/working/data_split/dataset_split.json',
                       help='æ•°æ®åˆ’åˆ†æ–‡ä»¶')
    
    # åŠŸèƒ½é€‰æ‹©
    parser.add_argument('--validate', action='store_true',
                       help='éªŒè¯é‡å»ºè´¨é‡')
    parser.add_argument('--vf_test', action='store_true',
                       help='æµ‹è¯•VFå¯¹é½èƒ½åŠ›')
    parser.add_argument('--user_test', action='store_true',
                       help='æµ‹è¯•ç”¨æˆ·åŒºåˆ†èƒ½åŠ›')
    parser.add_argument('--extract_stats', action='store_true',
                       help='æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡')
    parser.add_argument('--export_dit', action='store_true',
                       help='å¯¼å‡ºDiTç¼–ç å™¨')
    parser.add_argument('--comprehensive', action='store_true',
                       help='æ‰§è¡Œç»¼åˆVA-VAEéªŒè¯ (æ¨è)')
    parser.add_argument('--all', action='store_true',
                       help='æ‰§è¡Œæ‰€æœ‰åŠŸèƒ½')
    
    # Kaggleæ ‡å¿—
    parser.add_argument('--kaggle', action='store_true',
                       help='Kaggleç¯å¢ƒæ ‡å¿—')
    
    args = parser.parse_args()
    
    # Kaggleç¯å¢ƒæ£€æµ‹
    if args.kaggle:
        kaggle_input = Path('/kaggle/input')
        kaggle_working = Path('/kaggle/working')
        if kaggle_input.exists():
            print("âœ… æ£€æµ‹åˆ°Kaggleç¯å¢ƒ")
            # æŸ¥æ‰¾checkpoint
            if (kaggle_working / 'checkpoints').exists():
                ckpt_dir = kaggle_working / 'checkpoints'
                # æŸ¥æ‰¾æœ€æ–°é˜¶æ®µ
                for stage in [3, 2, 1]:
                    stage_dir = ckpt_dir / f'stage{stage}'
                    if stage_dir.exists() and (stage_dir / 'last.ckpt').exists():
                        args.checkpoint = str(stage_dir / 'last.ckpt')
                        args.config = str(stage_dir / 'config.yaml')
                        print(f"ä½¿ç”¨ç¬¬{stage}é˜¶æ®µcheckpoint")
                        break
    
    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, args.config)
    
    # æ‰§è¡ŒåŠŸèƒ½
    if args.comprehensive or (not any([args.validate, args.vf_test, args.user_test, 
                                      args.extract_stats, args.export_dit, args.all])):
        # é»˜è®¤æ‰§è¡Œç»¼åˆéªŒè¯
        print("ğŸš€ VA-VAEç»¼åˆéªŒè¯æµ‹è¯•")
        print("="*60)
        
        results = {}
        
        # åŸºç¡€é‡å»ºéªŒè¯
        mse, psnr = validate_reconstruction(model, args.data_root, args.split_file, device=device)
        results['mse'] = mse
        results['psnr'] = psnr
        
        # VA-VAEç‰¹æœ‰åŠŸèƒ½éªŒè¯
        vf_score = test_vf_alignment(model, args.data_root, args.split_file, device)
        results['vf_alignment'] = vf_score
        
        silhouette, separation_ratio, user_features = test_user_discrimination(
            model, args.data_root, args.split_file, device)
        results['user_discrimination'] = silhouette
        results['feature_separation'] = separation_ratio
        
        # æ½œåœ¨ç©ºé—´ç»Ÿè®¡
        stats = extract_latent_statistics(model, args.data_root, args.split_file, device)
        results['latent_stats'] = stats
        
        # å¯¼å‡ºDiTç¼–ç å™¨
        export_for_dit(args.checkpoint, 'vavae_encoder_for_dit.pt')
        
        # ç»¼åˆè¯„ä¼°
        print(f"\nğŸ† VA-VAEç»¼åˆè¯„ä¼°æŠ¥å‘Š:")
        print(f"="*60)
        
        # è¯„åˆ†ç³»ç»Ÿ
        mse_grade = "A+" if mse < 0.005 else "A" if mse < 0.01 else "B+" if mse < 0.02 else "B"
        vf_grade = "A+" if vf_score > 0.95 else "A" if vf_score > 0.90 else "B+" if vf_score > 0.85 else "B"
        user_grade = "A+" if silhouette > 0.3 else "A" if silhouette > 0.2 else "B+" if silhouette > 0.1 else "B"
        sep_grade = "A+" if separation_ratio > 2.0 else "A" if separation_ratio > 1.5 else "B+" if separation_ratio > 1.2 else "B"
        
        print(f"ğŸ“Š é‡å»ºè´¨é‡ (MSE): {mse:.6f} (ç­‰çº§: {mse_grade})")
        print(f"ğŸ“Š é‡å»ºè´¨é‡ (PSNR): {psnr:.2f} dB")
        print(f"ğŸ¨ Vision Foundationå¯¹é½: {vf_score:.4f} (ç­‰çº§: {vf_grade})")
        print(f"ğŸ‘¥ ç”¨æˆ·åŒºåˆ†èƒ½åŠ›: {silhouette:.4f} (ç­‰çº§: {user_grade})")
        print(f"ğŸ¯ ç‰¹å¾åˆ†ç¦»åº¦: {separation_ratio:.4f} (ç­‰çº§: {sep_grade})")
        
        # æ•´ä½“è¯„ä»·
        grades = [mse_grade, vf_grade, user_grade, sep_grade]
        grade_scores = {"A+": 4, "A": 3, "B+": 2, "B": 1, "C": 0}
        avg_score = np.mean([grade_scores[g] for g in grades])
        
        if avg_score >= 3.5:
            overall = "ä¼˜ç§€ - å®Œå…¨èƒœä»»å¾®å¤šæ™®å‹’ç”¨æˆ·åŒºåˆ†ä»»åŠ¡"
        elif avg_score >= 2.5:
            overall = "è‰¯å¥½ - åŸºæœ¬èƒœä»»ï¼Œæœ‰æ”¹è¿›ç©ºé—´"
        else:
            overall = "ä¸€èˆ¬ - éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–"
        
        print(f"\nğŸ–ï¸ æ•´ä½“è¯„ä»·: {overall}")
        print(f"="*60)
        
    else:
        # åˆ†åˆ«æ‰§è¡ŒæŒ‡å®šåŠŸèƒ½
        if args.all:
            args.validate = args.vf_test = args.user_test = True
            args.extract_stats = args.export_dit = True
        
        if args.validate:
            validate_reconstruction(model, args.data_root, args.split_file, device=device)
        
        if args.vf_test:
            test_vf_alignment(model, args.data_root, args.split_file, device)
        
        if args.user_test:
            test_user_discrimination(model, args.data_root, args.split_file, device)
        
        if args.extract_stats:
            extract_latent_statistics(model, args.data_root, args.split_file, device=device)
        
        if args.export_dit:
            output_path = 'vavae_encoder_for_dit.pt'
            export_for_dit(args.checkpoint, output_path)
    
    print("\nâœ… æ‰€æœ‰éªŒè¯ä»»åŠ¡å®Œæˆ!")


if __name__ == '__main__':
    main()

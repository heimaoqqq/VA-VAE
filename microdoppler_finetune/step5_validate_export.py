#!/usr/bin/env python
"""VA-VAEæ¨¡å‹éªŒè¯ä¸å¯¼å‡ºè„šæœ¬ - ä¿®å¤ç‰ˆ"""

import os
import sys
import json
import torch
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import argparse
from omegaconf import OmegaConf

def setup_taming_path():
    """è®¾ç½®taming-transformersè·¯å¾„"""
    taming_locations = [
        Path('/kaggle/working/taming-transformers'),
        Path('/kaggle/working/.taming_path'),
        Path.cwd().parent / 'taming-transformers',
        Path.cwd() / '.taming_path'
    ]
    
    for location in taming_locations:
        if location.name == '.taming_path' and location.exists():
            try:
                with open(location, 'r') as f:
                    taming_path = f.read().strip()
                if Path(taming_path).exists() and taming_path not in sys.path:
                    sys.path.insert(0, taming_path)
                    print(f"âœ… Tamingè·¯å¾„å·²æ·»åŠ : {taming_path}")
                    return True
            except Exception:
                continue
        elif location.name == 'taming-transformers' and location.exists():
            taming_path = str(location.absolute())
            if taming_path not in sys.path:
                sys.path.insert(0, taming_path)
                print(f"âœ… Tamingè·¯å¾„å·²æ·»åŠ : {taming_path}")
                return True
    
    print("âš ï¸ æœªæ‰¾åˆ°taming-transformersè·¯å¾„ï¼Œå°è¯•ç›´æ¥å¯¼å…¥...")
    return False

# è®¾ç½®tamingè·¯å¾„
setup_taming_path()

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from ldm.util import instantiate_from_config
    from ldm.models.autoencoder import AutoencoderKL
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿taming-transformerså’Œlatent-diffusionå·²æ­£ç¡®å®‰è£…")
    sys.exit(1)


def load_model(checkpoint_path, device='cuda'):
    """åŠ è½½VA-VAEæ¨¡å‹"""
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # è·å–é…ç½®
    if 'config' in checkpoint:
        config = checkpoint['config']
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config = OmegaConf.create({
            'model': {
                'target': 'ldm.models.autoencoder.AutoencoderKL',
                'params': {
                    'embed_dim': 32,
                    'monitor': 'val/rec_loss',
                    'use_vf': 'dinov2',
                    'reverse_proj': True,
                    'ddconfig': {
                        'double_z': True,
                        'z_channels': 32,
                        'resolution': 256,
                        'in_channels': 3,
                        'out_ch': 3,
                        'ch': 128,
                        'ch_mult': [1, 1, 2, 2, 4],
                        'num_res_blocks': 2,
                        'attn_resolutions': [16],
                        'dropout': 0.0
                    },
                    'lossconfig': {
                        'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                        'params': {
                            'disc_start': 1,
                            'kl_weight': 1e-6,
                            'disc_weight': 0.5
                        }
                    }
                }
            }
        })
    
    # å®ä¾‹åŒ–æ¨¡å‹
    model = instantiate_from_config(config.model)
    
    # åŠ è½½æƒé‡
    if 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    model = model.to(device)
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return model


def validate_reconstruction(model, dataset_root, split_file, num_samples=16, device='cuda'):
    """éªŒè¯é‡å»ºè´¨é‡"""
    print("\nğŸ” é‡å»ºè´¨é‡éªŒè¯...")
    
    # åŠ è½½æ•°æ®åˆ’åˆ†
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„æ ¼å¼
    if isinstance(split_data['val'], list):
        val_data = split_data['val']
    else:
        val_data = list(split_data['val'].values())
    
    # æ£€æŸ¥æ•°æ®æ ¼å¼
    if val_data and len(val_data) > 0:
        print(f"ğŸ“Š éªŒè¯æ•°æ®ç”¨æˆ·æ•°: {len(val_data)}")
        print(f"ğŸ“Š ç¬¬ä¸€ä¸ªç”¨æˆ·çš„å›¾ç‰‡æ•°: {len(val_data[0]) if isinstance(val_data[0], list) else 1}")
    
    images = []
    reconstructions = []
    
    model = model.to(device)
    
    with torch.no_grad():
        processed_count = 0
        
        for user_idx, user_paths in enumerate(tqdm(val_data, desc="å¤„ç†ç”¨æˆ·")):
            if processed_count >= num_samples:
                break
                
            # æ¯ä¸ªuser_pathsæ˜¯ä¸€ä¸ªç”¨æˆ·çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            if isinstance(user_paths, list):
                # è®¡ç®—è¯¥ç”¨æˆ·è¦é€‰æ‹©çš„å›¾ç‰‡æ•°
                remaining = num_samples - processed_count
                num_to_select = min(2, len(user_paths), remaining)
                selected_paths = user_paths[:num_to_select]
            else:
                selected_paths = [user_paths]
                
            for img_path_str in selected_paths:
                if processed_count >= num_samples:
                    break
                    
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                    
                # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                img = Image.open(img_path).convert('RGB')
                img = img.resize((256, 256), Image.LANCZOS)
                
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                # ç¼–ç å’Œè§£ç 
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                reconstructed = model.decode(z)
                
                images.append(img_tensor.cpu())
                reconstructions.append(reconstructed.cpu())
                processed_count += 1
    
    # è®¡ç®—æŒ‡æ ‡
    if images:
        images_tensor = torch.cat(images, dim=0)
        recons_tensor = torch.cat(reconstructions, dim=0)
        
        mse = torch.mean((images_tensor - recons_tensor) ** 2).item()
        psnr = 20 * np.log10(2.0) - 10 * np.log10(mse)
        
        print(f"âœ… é‡å»ºæŒ‡æ ‡:")
        print(f"   MSE: {mse:.6f}")
        print(f"   PSNR: {psnr:.2f} dB")
        
        # ä¿å­˜å¯¹æ¯”å›¾
        save_reconstruction_comparison(images_tensor, recons_tensor)
        
        return mse, psnr
    
    return None, None


def test_vf_alignment(model, dataset_root, split_file, device='cuda'):
    """æµ‹è¯•Vision Foundationå¯¹é½"""
    print("\nğŸ¨ Vision Foundationå¯¹é½èƒ½åŠ›éªŒè¯...")
    
    model = model.to(device)
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # å¤„ç†æ•°æ®æ ¼å¼
    if isinstance(split_data['val'], list):
        test_samples = split_data['val'][:20]
    else:
        test_samples = list(split_data['val'].values())[:20]
    
    vf_similarities = []
    
    with torch.no_grad():
        processed_count = 0
        
        for user_idx, user_paths in enumerate(tqdm(test_samples, desc="VFå¯¹é½æµ‹è¯•")):
            if processed_count >= 20:
                break
                
            if isinstance(user_paths, list):
                remaining = 20 - processed_count
                num_to_select = min(2, len(user_paths), remaining)
                selected_paths = user_paths[:num_to_select]
            else:
                selected_paths = [user_paths]
                
            for img_path_str in selected_paths:
                if processed_count >= 20:
                    break
                    
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                    
                # åŠ è½½å›¾åƒ
                img = Image.open(img_path).convert('RGB').resize((256, 256))
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                # é€šè¿‡æ¨¡å‹
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                reconstructed = model.decode(z)
                
                # å¦‚æœæ¨¡å‹æœ‰VFåŠŸèƒ½ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
                if hasattr(model, 'foundation_model') and model.foundation_model is not None:
                    # æ¯”è¾ƒåŸå§‹å›¾åƒå’Œé‡å»ºå›¾åƒçš„VFç‰¹å¾
                    orig_vf = model.foundation_model(img_tensor)
                    recon_vf = model.foundation_model(reconstructed)
                    
                    similarity = torch.cosine_similarity(
                        orig_vf.flatten(), 
                        recon_vf.flatten(), 
                        dim=0
                    ).item()
                    
                    vf_similarities.append(similarity)
                    processed_count += 1
    
    # è¾“å‡ºç»“æœ
    if vf_similarities:
        avg_similarity = np.mean(vf_similarities)
        std_similarity = np.std(vf_similarities)
        
        print(f"âœ… Vision Foundationå¯¹é½ç»“æœ:")
        print(f"   å¹³å‡VFè¯­ä¹‰ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
        print(f"   VFç›¸ä¼¼åº¦æ ‡å‡†å·®: {std_similarity:.4f}")
        
        if avg_similarity > 0.95:
            print(f"   ğŸ† VFå¯¹é½è´¨é‡: ä¼˜ç§€ (>0.95)")
        elif avg_similarity > 0.85:
            print(f"   âœ… VFå¯¹é½è´¨é‡: è‰¯å¥½ (>0.85)")
        else:
            print(f"   âš ï¸ VFå¯¹é½è´¨é‡: éœ€è¦æ”¹è¿› (<0.85)")
        
        return avg_similarity
    else:
        print("âš ï¸ æ¨¡å‹ä¸æ”¯æŒVFåŠŸèƒ½æˆ–æœªæ‰¾åˆ°VFæ¨¡å—")
        return None


def test_user_discrimination(model, dataset_root, split_file, device='cuda'):
    """æµ‹è¯•ç”¨æˆ·åŒºåˆ†èƒ½åŠ›"""
    print("\nğŸ‘¥ ç”¨æˆ·åŒºåˆ†èƒ½åŠ›éªŒè¯...")
    
    model = model.to(device)
    model.eval()
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # ä½¿ç”¨éªŒè¯é›†æ•°æ®
    if isinstance(split_data['val'], dict):
        # å­—å…¸æ ¼å¼ï¼šé”®æ˜¯ç”¨æˆ·ID
        val_data = split_data['val']
        user_items = [(uid, paths) for uid, paths in val_data.items()]
    else:
        # åˆ—è¡¨æ ¼å¼ï¼šç´¢å¼•ä½œä¸ºç”¨æˆ·ID
        val_data = split_data['val']
        user_items = [(f"user_{idx}", paths) for idx, paths in enumerate(val_data)]
    
    user_features = {}
    
    with torch.no_grad():
        # ä¸ºæ¯ä¸ªç”¨æˆ·æå–ç‰¹å¾
        for user_id, user_paths in tqdm(user_items[:31], desc="æå–ç”¨æˆ·ç‰¹å¾"):
            features = []
            
            if isinstance(user_paths, list):
                selected_paths = user_paths[:min(10, len(user_paths))]  # æ¯ç”¨æˆ·æœ€å¤š10å¼ 
            else:
                selected_paths = [user_paths]
            
            for img_path_str in selected_paths[:10]:  # é™åˆ¶æ¯ç”¨æˆ·æ ·æœ¬æ•°
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                
                # åŠ è½½å›¾åƒ
                img = Image.open(img_path).convert('RGB').resize((256, 256))
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                # ç¼–ç è·å–ç‰¹å¾
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                
                # ä½¿ç”¨å¹³å‡æ± åŒ–è·å–å…¨å±€ç‰¹å¾
                z_pooled = torch.mean(z, dim=[2, 3])  # [B, C]
                features.append(z_pooled.flatten().cpu().numpy())
            
            if features:
                user_features[user_id] = features
    
    # è®¡ç®—åˆ†ç¦»åº¦æŒ‡æ ‡
    if len(user_features) > 1:
        # å‡†å¤‡æ•°æ®ç”¨äºèšç±»åˆ†æ
        all_features = []
        all_labels = []
        
        for idx, (user_id, feats) in enumerate(user_features.items()):
            all_features.extend(feats)
            all_labels.extend([idx] * len(feats))
        
        all_features = np.array(all_features)
        all_labels = np.array(all_labels)
        
        # è®¡ç®—Silhouette Score
        if len(np.unique(all_labels)) > 1:
            from sklearn.metrics import silhouette_score
            silhouette = silhouette_score(all_features, all_labels)
            print(f"âœ… ç”¨æˆ·åˆ†ç¦»åº¦ (Silhouette): {silhouette:.4f}")
        else:
            silhouette = 0
            print("âš ï¸ ç”¨æˆ·æ•°ä¸è¶³ï¼Œæ— æ³•è®¡ç®—Silhouetteåˆ†æ•°")
        
        # è®¡ç®—ç±»é—´/ç±»å†…è·ç¦»æ¯”
        inter_distances = []
        intra_distances = []
        
        user_keys = list(user_features.keys())
        for i, user_i in enumerate(user_keys):
            user_i_feats = np.array(user_features[user_i])
            
            # ç±»å†…è·ç¦»
            if len(user_i_feats) > 1:
                for j in range(len(user_i_feats)):
                    for k in range(j+1, len(user_i_feats)):
                        dist = np.linalg.norm(user_i_feats[j] - user_i_feats[k])
                        intra_distances.append(dist)
            
            # ç±»é—´è·ç¦»ï¼ˆåªè®¡ç®—éƒ¨åˆ†é¿å…è®¡ç®—é‡è¿‡å¤§ï¼‰
            for j in range(i+1, min(i+5, len(user_keys))):
                user_j = user_keys[j]
                user_j_feats = np.array(user_features[user_j])
                
                # é‡‡æ ·è®¡ç®—
                for feat_i in user_i_feats[:3]:
                    for feat_j in user_j_feats[:3]:
                        dist = np.linalg.norm(feat_i - feat_j)
                        inter_distances.append(dist)
        
        # è®¡ç®—å¹³å‡è·ç¦»
        avg_intra = np.mean(intra_distances) if intra_distances else 0
        avg_inter = np.mean(inter_distances) if inter_distances else 0
        separation_ratio = avg_inter / avg_intra if avg_intra > 0 else 0
        
        print(f"âœ… ç±»é—´/ç±»å†…è·ç¦»æ¯”: {separation_ratio:.4f}")
        
        # å¯è§†åŒ–
        visualize_user_distribution(user_features)
        
        return silhouette, separation_ratio
    
    return None, None


def visualize_user_distribution(user_features, save_path='user_distribution.png'):
    """å¯è§†åŒ–ç”¨æˆ·ç‰¹å¾åˆ†å¸ƒ"""
    from sklearn.manifold import TSNE
    
    all_features = []
    all_labels = []
    
    for idx, (user_id, feats) in enumerate(user_features.items()):
        all_features.extend(feats)
        all_labels.extend([idx] * len(feats))
    
    all_features = np.array(all_features)
    
    if len(all_features) > 50:
        # ä½¿ç”¨t-SNEé™ç»´
        tsne = TSNE(n_components=2, random_state=42)
        features_2d = tsne.fit_transform(all_features)
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], 
                            c=all_labels, cmap='tab20', alpha=0.6)
        plt.colorbar(scatter)
        plt.title('ç”¨æˆ·ç‰¹å¾åˆ†å¸ƒ (t-SNE)')
        plt.xlabel('Dimension 1')
        plt.ylabel('Dimension 2')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ç”¨æˆ·åˆ†å¸ƒå›¾ä¿å­˜è‡³: {save_path}")


def save_reconstruction_comparison(originals, reconstructions, save_path='reconstruction_results.png'):
    """ä¿å­˜é‡å»ºå¯¹æ¯”å›¾"""
    n_samples = min(8, len(originals))
    
    fig, axes = plt.subplots(2, n_samples, figsize=(n_samples*2, 4))
    
    for i in range(n_samples):
        # åŸå›¾
        img = originals[i].permute(1, 2, 0).numpy()
        img = (img + 1) / 2  # [-1,1] -> [0,1]
        axes[0, i].imshow(img)
        axes[0, i].axis('off')
        if i == 0:
            axes[0, i].set_title('Original')
        
        # é‡å»ºå›¾
        rec = reconstructions[i].permute(1, 2, 0).numpy()
        rec = (rec + 1) / 2
        axes[1, i].imshow(rec)
        axes[1, i].axis('off')
        if i == 0:
            axes[1, i].set_title('Reconstructed')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“Š ä¿å­˜é‡å»ºå¯¹æ¯”å›¾: {save_path}")


def extract_latent_statistics(model, dataset_root, split_file, device='cuda'):
    """æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“ˆ æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡...")
    
    model = model.to(device)
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # ä½¿ç”¨è®­ç»ƒé›†æ•°æ®
    if isinstance(split_data['train'], list):
        train_data = split_data['train']
    else:
        train_data = list(split_data['train'].values())
    
    all_latents = []
    
    with torch.no_grad():
        processed_count = 0
        
        for user_paths in tqdm(train_data, desc="ç¼–ç å›¾åƒ"):
            if processed_count >= 500:
                break
                
            if isinstance(user_paths, list):
                selected_paths = user_paths[:min(5, len(user_paths))]
            else:
                selected_paths = [user_paths]
            
            for img_path_str in selected_paths:
                if processed_count >= 500:
                    break
                    
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                
                img = Image.open(img_path).convert('RGB').resize((256, 256))
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                
                all_latents.append(z.cpu())
                processed_count += 1
    
    # è®¡ç®—ç»Ÿè®¡
    all_latents = torch.cat(all_latents, dim=0)
    mean = all_latents.mean(dim=[0, 2, 3])
    std = all_latents.std(dim=[0, 2, 3])
    
    stats = {
        'global_mean': mean.numpy().tolist(),
        'global_std': std.numpy().tolist(),
        'num_samples': len(all_latents),
        'latent_dim': all_latents.shape[1],
        'spatial_size': [all_latents.shape[2], all_latents.shape[3]]
    }
    
    stats_path = 'latent_statistics.json'
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"âœ… æ½œåœ¨ç©ºé—´ç»Ÿè®¡å·²ä¿å­˜è‡³: {stats_path}")
    print(f"   æ½œåœ¨ç»´åº¦: {stats['latent_dim']}")
    print(f"   ç©ºé—´å°ºå¯¸: {stats['spatial_size']}")
    print(f"   æ ·æœ¬æ•°é‡: {stats['num_samples']}")
    
    return stats


def export_for_dit(model, save_path='vae_encoder_for_dit.pt'):
    """å¯¼å‡ºç¼–ç å™¨ç”¨äºDiTè®­ç»ƒ"""
    print("\nğŸ’¾ å¯¼å‡ºç¼–ç å™¨ç”¨äºDiTè®­ç»ƒ...")
    
    # åªä¿å­˜ç¼–ç å™¨å’Œè§£ç å™¨çš„æƒé‡
    encoder_decoder_state = {
        'encoder': model.encoder.state_dict(),
        'decoder': model.decoder.state_dict(),
        'quant_conv': model.quant_conv.state_dict(),
        'post_quant_conv': model.post_quant_conv.state_dict(),
    }
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config_info = {
        'embed_dim': model.embed_dim if hasattr(model, 'embed_dim') else 32,
        'z_channels': model.encoder.z_channels if hasattr(model.encoder, 'z_channels') else 32,
    }
    
    checkpoint = {
        'state_dict': encoder_decoder_state,
        'config': config_info
    }
    
    torch.save(checkpoint, save_path)
    print(f"âœ… ç¼–ç å™¨å·²å¯¼å‡ºè‡³: {save_path}")
    
    return save_path


def comprehensive_evaluation(model, dataset_root, split_file, device='cuda'):
    """ç»¼åˆè¯„ä¼°å¹¶ç”ŸæˆæŠ¥å‘Š"""
    print("\n" + "="*50)
    print("ğŸ“Š VA-VAE Stage 3 ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
    print("="*50)
    
    scores = {}
    
    # 1. é‡å»ºè´¨é‡
    mse, psnr = validate_reconstruction(model, dataset_root, split_file, device=device)
    if psnr:
        scores['reconstruction'] = min(100, psnr / 30 * 100)  # PSNR 30dB = 100åˆ†
    
    # 2. VFå¯¹é½
    vf_sim = test_vf_alignment(model, dataset_root, split_file, device=device)
    if vf_sim:
        scores['vf_alignment'] = vf_sim * 100
    
    # 3. ç”¨æˆ·åŒºåˆ†
    silhouette, sep_ratio = test_user_discrimination(model, dataset_root, split_file, device=device)
    if silhouette is not None:
        scores['user_discrimination'] = max(0, (silhouette + 1) * 50)  # [-1,1] -> [0,100]
    
    # 4. æ½œåœ¨ç©ºé—´ç»Ÿè®¡
    stats = extract_latent_statistics(model, dataset_root, split_file, device=device)
    
    # 5. å¯¼å‡ºæ¨¡å‹
    export_path = export_for_dit(model)
    
    # è®¡ç®—æ€»åˆ†
    if scores:
        total_score = np.mean(list(scores.values()))
        
        print("\n" + "="*50)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
        print("="*50)
        
        for key, score in scores.items():
            print(f"   {key}: {score:.2f}/100")
        
        print(f"\nğŸ“Š ç»¼åˆå¾—åˆ†: {total_score:.2f}/100")
        
        # è¯„çº§
        if total_score >= 90:
            grade = "A+ (å“è¶Š)"
        elif total_score >= 85:
            grade = "A (ä¼˜ç§€)"
        elif total_score >= 80:
            grade = "A- (è‰¯å¥½)"
        elif total_score >= 75:
            grade = "B+ (åˆæ ¼)"
        else:
            grade = "B (éœ€æ”¹è¿›)"
        
        print(f"ğŸ† æœ€ç»ˆè¯„çº§: {grade}")
        print("="*50)
    
    return scores


def main():
    parser = argparse.ArgumentParser(description='VA-VAEéªŒè¯ä¸å¯¼å‡º')
    parser.add_argument('--checkpoint', type=str, 
                       default='/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt',
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--dataset_root', type=str, 
                       default='/kaggle/input/dataset',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split_file', type=str,
                       default='/kaggle/working/data_split/dataset_split.json',
                       help='æ•°æ®åˆ’åˆ†æ–‡ä»¶')
    parser.add_argument('--device', type=str, default='cuda')
    
    # åŠŸèƒ½é€‰æ‹©
    parser.add_argument('--validate', action='store_true', help='éªŒè¯é‡å»ºè´¨é‡')
    parser.add_argument('--vf_test', action='store_true', help='æµ‹è¯•VFå¯¹é½')
    parser.add_argument('--user_test', action='store_true', help='æµ‹è¯•ç”¨æˆ·åŒºåˆ†')
    parser.add_argument('--extract_stats', action='store_true', help='æå–æ½œåœ¨ç»Ÿè®¡')
    parser.add_argument('--export_dit', action='store_true', help='å¯¼å‡ºDiTç¼–ç å™¨')
    parser.add_argument('--comprehensive', action='store_true', help='ç»¼åˆè¯„ä¼°(æ¨è)')
    parser.add_argument('--all', action='store_true', help='è¿è¡Œæ‰€æœ‰åŠŸèƒ½')
    
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, device=args.device)
    
    # æ‰§è¡Œé€‰å®šçš„åŠŸèƒ½
    if args.comprehensive or args.all:
        comprehensive_evaluation(model, args.dataset_root, args.split_file, args.device)
    else:
        if args.validate:
            validate_reconstruction(model, args.dataset_root, args.split_file, device=args.device)
        
        if args.vf_test:
            test_vf_alignment(model, args.dataset_root, args.split_file, device=args.device)
        
        if args.user_test:
            test_user_discrimination(model, args.dataset_root, args.split_file, device=args.device)
        
        if args.extract_stats:
            extract_latent_statistics(model, args.dataset_root, args.split_file, device=args.device)
        
        if args.export_dit:
            export_for_dit(model)
    
    print("\nâœ… éªŒè¯å®Œæˆ!")


if __name__ == '__main__':
    main()

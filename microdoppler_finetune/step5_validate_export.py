#!/usr/bin/env python3
"""
Step 5: éªŒè¯è®­ç»ƒç»“æœå¹¶å¯¼å‡ºæœ€ç»ˆæ¨¡å‹
ç”¨äºKaggleè¾“å‡ºå’Œä¸‹æ¸¸DiTè®­ç»ƒ
"""

import os
import sys
import argparse
from pathlib import Path
import json
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

# æ·»åŠ LightningDiTè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))

from omegaconf import OmegaConf
from ldm.util import instantiate_from_config


def load_model(checkpoint_path, config_path=None):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    
    # åŠ è½½é…ç½®
    if config_path and Path(config_path).exists():
        config = OmegaConf.load(config_path)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = OmegaConf.create({
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'embed_dim': 32,
                'use_vf': 'dinov2',
                'reverse_proj': True,
                'ddconfig': {
                    'double_z': True,
                    'z_channels': 32,
                    'resolution': 256,
                    'in_channels': 3,
                    'out_ch': 3,
                    'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4],
                    'num_res_blocks': 2,
                    'attn_resolutions': [16],
                    'dropout': 0.0
                },
                'lossconfig': {
                    'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                    'params': {
                        'disc_start': 1,
                        'vf_weight': 0.1,
                        'distmat_weight': 1.0,
                        'cos_weight': 1.0
                    }
                }
            }
        })
    
    # å®ä¾‹åŒ–æ¨¡å‹
    model = instantiate_from_config(config if isinstance(config, dict) else config.model)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    if 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    model.eval()
    return model


def validate_reconstruction(model, data_root, split_file, num_samples=16, device='cuda'):
    """éªŒè¯é‡å»ºè´¨é‡"""
    
    print("\nğŸ” éªŒè¯é‡å»ºè´¨é‡...")
    
    # åŠ è½½æ•°æ®
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    val_data = split_data['val'][:num_samples]
    
    # å‡†å¤‡å›¾åƒ
    images = []
    reconstructions = []
    
    model = model.to(device)
    
    with torch.no_grad():
        for item in tqdm(val_data, desc="å¤„ç†å›¾åƒ"):
            # åŠ è½½å›¾åƒ
            img_path = Path(data_root) / item['path']
            if not img_path.exists():
                continue
                
            img = Image.open(img_path).convert('RGB')
            img = img.resize((256, 256), Image.LANCZOS)
            
            # è½¬æ¢ä¸ºtensor
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            # é‡å»º
            reconstructed, _, _, _ = model(img_tensor)
            
            images.append(img_tensor.cpu())
            reconstructions.append(reconstructed.cpu())
    
    # è®¡ç®—æŒ‡æ ‡
    images_cat = torch.cat(images, dim=0)
    recons_cat = torch.cat(reconstructions, dim=0)
    
    # MSE
    mse = torch.mean((images_cat - recons_cat) ** 2).item()
    
    # PSNR
    psnr = 20 * np.log10(2.0) - 10 * np.log10(mse)
    
    print(f"âœ… é‡å»ºæŒ‡æ ‡:")
    print(f"   MSE: {mse:.6f}")
    print(f"   PSNR: {psnr:.2f} dB")
    
    # ä¿å­˜å¯è§†åŒ–
    save_reconstruction_grid(images_cat, recons_cat, 'reconstruction_results.png')
    
    return mse, psnr


def save_reconstruction_grid(images, reconstructions, save_path, num_show=8):
    """ä¿å­˜é‡å»ºå¯¹æ¯”å›¾"""
    
    num_show = min(num_show, len(images))
    
    fig, axes = plt.subplots(2, num_show, figsize=(num_show * 2, 4))
    
    for i in range(num_show):
        # åŸå›¾
        img = images[i].permute(1, 2, 0).numpy()
        img = (img + 1) / 2  # [-1,1] -> [0,1]
        axes[0, i].imshow(np.clip(img, 0, 1))
        axes[0, i].axis('off')
        if i == 0:
            axes[0, i].set_title('Original')
        
        # é‡å»º
        rec = reconstructions[i].permute(1, 2, 0).numpy()
        rec = (rec + 1) / 2
        axes[1, i].imshow(np.clip(rec, 0, 1))
        axes[1, i].axis('off')
        if i == 0:
            axes[1, i].set_title('Reconstructed')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š ä¿å­˜é‡å»ºå¯¹æ¯”å›¾: {save_path}")


def extract_latent_statistics(model, data_root, split_file, device='cuda'):
    """æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯"""
    
    print("\nğŸ“ˆ æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡...")
    
    # åŠ è½½æ•°æ®
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    train_data = split_data['train']
    
    model = model.to(device)
    
    # æ”¶é›†æ½œåœ¨å‘é‡
    all_latents = []
    user_latents = {}
    
    with torch.no_grad():
        for item in tqdm(train_data, desc="ç¼–ç å›¾åƒ"):
            img_path = Path(data_root) / item['path']
            if not img_path.exists():
                continue
            
            # åŠ è½½å›¾åƒ
            img = Image.open(img_path).convert('RGB')
            img = img.resize((256, 256), Image.LANCZOS)
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            # ç¼–ç 
            posterior = model.encode(img_tensor)
            z = posterior.sample()
            
            all_latents.append(z.cpu())
            
            # æŒ‰ç”¨æˆ·åˆ†ç»„
            user_id = item['user_id']
            if user_id not in user_latents:
                user_latents[user_id] = []
            user_latents[user_id].append(z.cpu())
    
    # è®¡ç®—å…¨å±€ç»Ÿè®¡
    all_latents = torch.cat(all_latents, dim=0)
    mean = all_latents.mean(dim=[0, 2, 3])  # [C]
    std = all_latents.std(dim=[0, 2, 3])    # [C]
    
    # è®¡ç®—ç”¨æˆ·é—´å·®å¼‚
    user_means = {}
    for user_id, latents in user_latents.items():
        user_latents_cat = torch.cat(latents, dim=0)
        user_means[user_id] = user_latents_cat.mean(dim=[0, 2, 3])
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'global_mean': mean.numpy().tolist(),
        'global_std': std.numpy().tolist(),
        'num_samples': len(all_latents),
        'latent_dim': all_latents.shape[1],
        'spatial_size': [all_latents.shape[2], all_latents.shape[3]]
    }
    
    stats_path = 'latent_statistics.json'
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"âœ… æ½œåœ¨ç©ºé—´ç»Ÿè®¡:")
    print(f"   ç»´åº¦: {stats['latent_dim']} x {stats['spatial_size'][0]} x {stats['spatial_size'][1]}")
    print(f"   æ ·æœ¬æ•°: {stats['num_samples']}")
    print(f"   å‡å€¼èŒƒå›´: [{min(mean):.3f}, {max(mean):.3f}]")
    print(f"   æ ‡å‡†å·®èŒƒå›´: [{min(std):.3f}, {max(std):.3f}]")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ä¿å­˜è‡³: {stats_path}")
    
    return stats


def export_for_dit(checkpoint_path, output_path):
    """å¯¼å‡ºæ¨¡å‹ç”¨äºDiTè®­ç»ƒ"""
    
    print(f"\nğŸ“¦ å¯¼å‡ºæ¨¡å‹ç”¨äºDiTè®­ç»ƒ...")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # æå–å¿…è¦çš„ç»„ä»¶
    state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint
    
    # åªä¿ç•™ç¼–ç å™¨å’Œé‡åŒ–å±‚ï¼ˆDiTåªéœ€è¦ç¼–ç å™¨ï¼‰
    encoder_keys = [k for k in state_dict.keys() if 
                   k.startswith('encoder.') or 
                   k.startswith('quant_conv.') or
                   k.startswith('linear_proj.')]
    
    encoder_state = {k: state_dict[k] for k in encoder_keys}
    
    # ä¿å­˜DiTç‰ˆæœ¬
    dit_checkpoint = {
        'encoder_state_dict': encoder_state,
        'full_state_dict': state_dict,  # ä¹Ÿä¿ç•™å®Œæ•´ç‰ˆæœ¬
        'config': {
            'embed_dim': 32,
            'z_channels': 32,
            'use_vf': 'dinov2',
            'reverse_proj': True,
            'resolution': 256
        },
        'type': 'vavae_encoder_for_dit'
    }
    
    torch.save(dit_checkpoint, output_path)
    print(f"âœ… DiTç¼–ç å™¨å¯¼å‡ºè‡³: {output_path}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = Path(output_path).stat().st_size / (1024 * 1024)
    print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser()
    
    # è·¯å¾„å‚æ•°
    parser.add_argument('--checkpoint', type=str, 
                       default='checkpoints/stage3/last.ckpt',
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--config', type=str,
                       default='checkpoints/stage3/config.yaml',
                       help='æ¨¡å‹é…ç½®æ–‡ä»¶')
    parser.add_argument('--data_root', type=str,
                       default='/kaggle/input/micro-doppler-data',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split_file', type=str,
                       default='dataset_split.json',
                       help='æ•°æ®åˆ’åˆ†æ–‡ä»¶')
    
    # åŠŸèƒ½é€‰æ‹©
    parser.add_argument('--validate', action='store_true',
                       help='éªŒè¯é‡å»ºè´¨é‡')
    parser.add_argument('--extract_stats', action='store_true',
                       help='æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡')
    parser.add_argument('--export_dit', action='store_true',
                       help='å¯¼å‡ºDiTç¼–ç å™¨')
    parser.add_argument('--all', action='store_true',
                       help='æ‰§è¡Œæ‰€æœ‰åŠŸèƒ½')
    
    # Kaggleæ ‡å¿—
    parser.add_argument('--kaggle', action='store_true',
                       help='Kaggleç¯å¢ƒæ ‡å¿—')
    
    args = parser.parse_args()
    
    # Kaggleç¯å¢ƒæ£€æµ‹
    if args.kaggle:
        kaggle_input = Path('/kaggle/input')
        kaggle_working = Path('/kaggle/working')
        if kaggle_input.exists():
            print("âœ… æ£€æµ‹åˆ°Kaggleç¯å¢ƒ")
            # æŸ¥æ‰¾checkpoint
            if (kaggle_working / 'checkpoints').exists():
                ckpt_dir = kaggle_working / 'checkpoints'
                # æŸ¥æ‰¾æœ€æ–°é˜¶æ®µ
                for stage in [3, 2, 1]:
                    stage_dir = ckpt_dir / f'stage{stage}'
                    if stage_dir.exists() and (stage_dir / 'last.ckpt').exists():
                        args.checkpoint = str(stage_dir / 'last.ckpt')
                        args.config = str(stage_dir / 'config.yaml')
                        print(f"ä½¿ç”¨ç¬¬{stage}é˜¶æ®µcheckpoint")
                        break
    
    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, args.config)
    
    # æ‰§è¡ŒåŠŸèƒ½
    if args.all:
        args.validate = args.extract_stats = args.export_dit = True
    
    if args.validate:
        validate_reconstruction(model, args.data_root, args.split_file, device=device)
    
    if args.extract_stats:
        extract_latent_statistics(model, args.data_root, args.split_file, device=device)
    
    if args.export_dit:
        output_path = 'vavae_encoder_for_dit.pt'
        export_for_dit(args.checkpoint, output_path)
    
    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")


if __name__ == '__main__':
    main()

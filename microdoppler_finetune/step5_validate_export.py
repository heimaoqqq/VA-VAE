#!/usr/bin/env python
"""
VA-VAEæ¨¡å‹ç»¼åˆéªŒè¯è„šæœ¬
é›†æˆæ‰€æœ‰éªŒè¯åŠŸèƒ½äºä¸€ä¸ªæ–‡ä»¶
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
import json
import argparse
from omegaconf import OmegaConf
from datetime import datetime

# æ·»åŠ LightningDiTè·¯å¾„
if os.path.exists('/kaggle/working'):
    sys.path.insert(0, '/kaggle/working/VA-VAE/LightningDiT/vavae')
    sys.path.insert(0, '/kaggle/working/VA-VAE/LightningDiT')
else:
    vavae_path = Path(__file__).parent / 'LightningDiT' / 'vavae'
    if vavae_path.exists():
        sys.path.insert(0, str(vavae_path))
        sys.path.insert(0, str(vavae_path.parent))

# å¯¼å…¥æ¨¡å‹æ¨¡å—
try:
    from ldm.models.autoencoder import AutoencoderKL
    from ldm.util import instantiate_from_config
    print("âœ… æˆåŠŸå¯¼å…¥VA-VAEæ¨¡å‹æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿LightningDiT/vavaeé¡¹ç›®åœ¨æ­£ç¡®ä½ç½®")
    sys.exit(1)


def load_model(checkpoint_path, device='cuda'):
    """åŠ è½½è®­ç»ƒå¥½çš„VA-VAEæ¨¡å‹"""
    print(f"\nğŸ“‚ Loading model from: {checkpoint_path}")
    
    # åŠ è½½checkpoint
    ckpt = torch.load(checkpoint_path, map_location=device)
    
    # è·å–é…ç½®
    if 'config' in ckpt:
        config = ckpt['config']
        if isinstance(config, dict):
            config = OmegaConf.create(config)
    else:
        # é»˜è®¤é…ç½®
        config = OmegaConf.create({
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'embed_dim': 32,
                'use_vf': 'dinov2',
                'ddconfig': {
                    'double_z': False,
                    'z_channels': 32,
                    'resolution': 256,
                    'in_channels': 3,
                    'out_ch': 3,
                    'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4],
                    'num_res_blocks': 2,
                    'attn_resolutions': [],
                    'dropout': 0.0
                }
            }
        })
    
    # å®ä¾‹åŒ–æ¨¡å‹
    if hasattr(config, 'target'):
        model = instantiate_from_config(config)
    else:
        model = instantiate_from_config(config.model if hasattr(config, 'model') else config)
    
    # åŠ è½½æƒé‡
    if 'state_dict' in ckpt:
        model.load_state_dict(ckpt['state_dict'], strict=False)
    else:
        model.load_state_dict(ckpt, strict=False)
    
    model = model.to(device)
    model.eval()
    
    print("âœ… Model loaded successfully!")
    return model


def test_reconstruction(model, data_root, num_samples=30, device='cuda'):
    """æµ‹è¯•é‡å»ºè´¨é‡"""
    print("\n" + "="*50)
    print("ğŸ” Testing Reconstruction Quality")
    print("="*50)
    
    data_path = Path(data_root)
    mse_scores = []
    psnr_scores = []
    
    sample_count = 0
    pbar = tqdm(total=num_samples, desc="Processing images")
    
    for user_id in range(1, 32):
        if sample_count >= num_samples:
            break
            
        user_folder = data_path / f"ID_{user_id}"
        if not user_folder.exists():
            continue
            
        images = list(user_folder.glob("*.jpg"))
        if images:
            img_path = images[0]  # å–ç¬¬ä¸€å¼ 
            
            # åŠ è½½å’Œé¢„å¤„ç†
            img = Image.open(img_path).convert('RGB').resize((256, 256))
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            with torch.no_grad():
                # ç¼–ç -è§£ç 
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                rec = model.decode(z)
                
                # è®¡ç®—MSEå’ŒPSNR
                mse = F.mse_loss(rec, img_tensor).item()
                mse_scores.append(mse)
                psnr = 10 * np.log10(4.0 / mse)  # èŒƒå›´[-1, 1]
                psnr_scores.append(psnr)
            
            sample_count += 1
            pbar.update(1)
    
    pbar.close()
    
    # ç»Ÿè®¡ç»“æœ
    avg_mse = np.mean(mse_scores)
    std_mse = np.std(mse_scores)
    avg_psnr = np.mean(psnr_scores)
    std_psnr = np.std(psnr_scores)
    
    print(f"ğŸ“Š Results:")
    print(f"  MSE: {avg_mse:.6f} Â± {std_mse:.6f}")
    print(f"  PSNR: {avg_psnr:.2f} Â± {std_psnr:.2f} dB")
    
    # è¯„çº§
    if avg_mse < 0.01:
        grade = "Excellent â­â­â­"
    elif avg_mse < 0.02:
        grade = "Good â­â­"
    else:
        grade = "Fair â­"
    print(f"  Grade: {grade}")
    
    return {
        'mse': avg_mse,
        'mse_std': std_mse,
        'psnr': avg_psnr,
        'psnr_std': std_psnr,
        'grade': grade
    }


def test_vf_alignment(model, data_root, num_samples=20, device='cuda'):
    """æµ‹è¯•VFå¯¹é½"""
    print("\n" + "="*50)
    print("ğŸ” Testing Vision Foundation Alignment")
    print("="*50)
    
    # æ£€æŸ¥VFæ¨¡å‹
    if not hasattr(model, 'vf_model') or model.vf_model is None:
        print("âš ï¸ No VF model found, skipping test")
        return None
    
    data_path = Path(data_root)
    similarities = []
    
    sample_count = 0
    pbar = tqdm(total=num_samples, desc="Processing VF alignment")
    
    for user_id in range(1, 32):
        if sample_count >= num_samples:
            break
            
        user_folder = data_path / f"ID_{user_id}"
        if not user_folder.exists():
            continue
            
        images = list(user_folder.glob("*.jpg"))
        if images:
            img_path = images[0]
            
            img = Image.open(img_path).convert('RGB').resize((256, 256))
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            with torch.no_grad():
                # åŸå§‹å›¾åƒçš„VFç‰¹å¾
                vf_input = F.interpolate(img_tensor, size=(224, 224), mode='bilinear', align_corners=False)
                vf_input = (vf_input + 1.0) / 2.0
                orig_features = model.vf_model.forward_features(vf_input)['x_norm_clstoken']
                
                # é‡å»ºå›¾åƒ
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                rec = model.decode(z)
                
                # é‡å»ºå›¾åƒçš„VFç‰¹å¾
                rec_vf_input = F.interpolate(rec, size=(224, 224), mode='bilinear', align_corners=False)
                rec_vf_input = (rec_vf_input + 1.0) / 2.0
                rec_features = model.vf_model.forward_features(rec_vf_input)['x_norm_clstoken']
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                cos_sim = F.cosine_similarity(orig_features, rec_features, dim=1).mean().item()
                similarities.append(cos_sim)
            
            sample_count += 1
            pbar.update(1)
    
    pbar.close()
    
    if similarities:
        avg_sim = np.mean(similarities)
        std_sim = np.std(similarities)
        
        print(f"ğŸ“Š Results:")
        print(f"  VF Similarity: {avg_sim:.4f} Â± {std_sim:.4f}")
        
        # è¯„çº§
        if avg_sim > 0.95:
            grade = "Excellent â­â­â­"
        elif avg_sim > 0.90:
            grade = "Good â­â­"
        else:
            grade = "Fair â­"
        print(f"  Grade: {grade}")
        
        return {
            'similarity': avg_sim,
            'similarity_std': std_sim,
            'grade': grade
        }
    
    return None


def test_user_discrimination(model, data_root, samples_per_user=10, device='cuda'):
    """æµ‹è¯•ç”¨æˆ·åŒºåˆ†èƒ½åŠ›"""
    print("\n" + "="*50)
    print("ğŸ” Testing User Discrimination")
    print("="*50)
    
    data_path = Path(data_root)
    user_features = {}
    all_features = []
    all_labels = []
    
    # æå–æ¯ä¸ªç”¨æˆ·çš„ç‰¹å¾
    for user_id in tqdm(range(1, 32), desc="Extracting user features"):
        user_folder = data_path / f"ID_{user_id}"
        if not user_folder.exists():
            continue
        
        features = []
        images = list(user_folder.glob("*.jpg"))[:samples_per_user]
        
        for img_path in images:
            img = Image.open(img_path).convert('RGB').resize((256, 256))
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            with torch.no_grad():
                # è·å–æ½œåœ¨ç‰¹å¾
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                
                # å…¨å±€æ± åŒ–
                z_pooled = z.mean(dim=[2, 3]).cpu().numpy().flatten()
                features.append(z_pooled)
                all_features.append(z_pooled)
                all_labels.append(user_id)
        
        if features:
            user_features[user_id] = np.mean(features, axis=0)
    
    if len(user_features) < 2:
        print("âš ï¸ Not enough users for discrimination analysis")
        return None
    
    # è®¡ç®—Silhouetteåˆ†æ•°
    all_features_array = np.array(all_features)
    all_labels_array = np.array(all_labels)
    
    silhouette = 0
    if len(np.unique(all_labels_array)) > 1:
        silhouette = silhouette_score(all_features_array, all_labels_array)
    
    # è®¡ç®—ç±»é—´/ç±»å†…è·ç¦»æ¯”
    intra_distances = []
    inter_distances = []
    
    # ç±»é—´è·ç¦»
    users = list(user_features.keys())
    for i in range(len(users)):
        for j in range(i+1, len(users)):
            feat_i = user_features[users[i]]
            feat_j = user_features[users[j]]
            dist = np.linalg.norm(feat_i - feat_j)
            inter_distances.append(dist)
    
    # ç±»å†…è·ç¦»
    for user_id in np.unique(all_labels_array):
        user_mask = all_labels_array == user_id
        user_feats = all_features_array[user_mask]
        if len(user_feats) > 1:
            for i in range(len(user_feats)):
                for j in range(i+1, len(user_feats)):
                    dist = np.linalg.norm(user_feats[i] - user_feats[j])
                    intra_distances.append(dist)
    
    avg_intra = np.mean(intra_distances) if intra_distances else 0
    avg_inter = np.mean(inter_distances) if inter_distances else 0
    ratio = avg_inter / avg_intra if avg_intra > 0 else 0
    
    print(f"ğŸ“Š Results:")
    print(f"  Silhouette Score: {silhouette:.4f}")
    print(f"  Intra-class distance: {avg_intra:.4f}")
    print(f"  Inter-class distance: {avg_inter:.4f}")
    print(f"  Ratio (inter/intra): {ratio:.4f}")
    print(f"  Number of users: {len(user_features)}")
    
    # è¯„çº§
    if ratio > 2.0:
        grade = "Excellent â­â­â­"
    elif ratio > 1.5:
        grade = "Good â­â­"
    else:
        grade = "Fair â­"
    print(f"  Grade: {grade}")
    
    return {
        'silhouette_score': silhouette,
        'intra_distance': avg_intra,
        'inter_distance': avg_inter,
        'ratio': ratio,
        'num_users': len(user_features),
        'grade': grade
    }


def extract_latent_statistics(model, data_root, num_samples=100, device='cuda'):
    """æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡"""
    print("\n" + "="*50)
    print("ğŸ“Š Extracting Latent Space Statistics")
    print("="*50)
    
    data_path = Path(data_root)
    all_latents = []
    
    sample_count = 0
    for user_id in tqdm(range(1, 32), desc="Processing latents"):
        if sample_count >= num_samples:
            break
            
        user_folder = data_path / f"ID_{user_id}"
        if not user_folder.exists():
            continue
            
        images = list(user_folder.glob("*.jpg"))[:5]
        
        for img_path in images:
            if sample_count >= num_samples:
                break
                
            img = Image.open(img_path).convert('RGB').resize((256, 256))
            img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
            
            with torch.no_grad():
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                all_latents.append(z.cpu().numpy())
            
            sample_count += 1
    
    # è®¡ç®—ç»Ÿè®¡
    all_latents = np.concatenate(all_latents, axis=0)
    mean = np.mean(all_latents)
    std = np.std(all_latents)
    
    print(f"  Latent mean: {mean:.6f}")
    print(f"  Latent std: {std:.6f}")
    print(f"  Latent shape: {all_latents.shape}")
    
    # è¯„ä¼°
    if abs(mean) < 0.1 and 0.8 < std < 1.2:
        status = "âœ… Well-normalized"
    else:
        status = "âš ï¸ Needs regularization"
    print(f"  Status: {status}")
    
    return {
        'mean': float(mean),
        'std': float(std),
        'shape': list(all_latents.shape),
        'status': status
    }


def export_encoder_decoder(model, checkpoint_path):
    """å¯¼å‡ºç¼–ç å™¨å’Œè§£ç å™¨"""
    print("\n" + "="*50)
    print("ğŸ’¾ Exporting Encoder and Decoder")
    print("="*50)
    
    # å¯¼å‡ºç¼–ç å™¨
    encoder_path = checkpoint_path.replace('.pt', '_encoder.pt')
    encoder_state = {
        'encoder': model.encoder.state_dict(),
        'quant_conv': model.quant_conv.state_dict() if hasattr(model, 'quant_conv') else None,
        'embed_dim': model.embed_dim if hasattr(model, 'embed_dim') else 32,
        'config': {
            'z_channels': 32,
            'resolution': 256,
            'ch_mult': [1, 1, 2, 2, 4]
        }
    }
    torch.save(encoder_state, encoder_path)
    print(f"âœ… Encoder exported to: {encoder_path}")
    
    # å¯¼å‡ºè§£ç å™¨
    decoder_path = checkpoint_path.replace('.pt', '_decoder.pt')
    decoder_state = {
        'decoder': model.decoder.state_dict(),
        'post_quant_conv': model.post_quant_conv.state_dict() if hasattr(model, 'post_quant_conv') else None,
        'embed_dim': model.embed_dim if hasattr(model, 'embed_dim') else 32
    }
    torch.save(decoder_state, decoder_path)
    print(f"âœ… Decoder exported to: {decoder_path}")
    
    return encoder_path, decoder_path


def generate_report(results, checkpoint_path):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ COMPREHENSIVE VALIDATION REPORT")
    print("="*60)
    
    # é‡å»ºè´¨é‡
    if 'reconstruction' in results:
        rec = results['reconstruction']
        print(f"\n1ï¸âƒ£ Reconstruction Quality:")
        print(f"   MSE: {rec['mse']:.6f} Â± {rec['mse_std']:.6f}")
        print(f"   PSNR: {rec['psnr']:.2f} Â± {rec['psnr_std']:.2f} dB")
        print(f"   {rec['grade']}")
    
    # VFå¯¹é½
    if 'vf_alignment' in results and results['vf_alignment']:
        vf = results['vf_alignment']
        print(f"\n2ï¸âƒ£ Vision Foundation Alignment:")
        print(f"   Similarity: {vf['similarity']:.4f} Â± {vf['similarity_std']:.4f}")
        print(f"   {vf['grade']}")
    
    # ç”¨æˆ·åŒºåˆ†
    if 'user_discrimination' in results and results['user_discrimination']:
        disc = results['user_discrimination']
        print(f"\n3ï¸âƒ£ User Discrimination:")
        print(f"   Silhouette Score: {disc['silhouette_score']:.4f}")
        print(f"   Inter/Intra Ratio: {disc['ratio']:.4f}")
        print(f"   {disc['grade']}")
    
    # æ½œåœ¨ç»Ÿè®¡
    if 'latent_statistics' in results:
        lat = results['latent_statistics']
        print(f"\n4ï¸âƒ£ Latent Space Statistics:")
        print(f"   Mean: {lat['mean']:.6f}, Std: {lat['std']:.6f}")
        print(f"   {lat['status']}")
    
    # æ€»ä½“è¯„ä¼°
    print("\n" + "="*60)
    print("ğŸ¯ OVERALL ASSESSMENT")
    print("="*60)
    
    rec_ok = results.get('reconstruction', {}).get('mse', 1.0) < 0.02
    disc_ok = results.get('user_discrimination', {}).get('ratio', 0) > 1.5
    
    if rec_ok and disc_ok:
        print("âœ… Model is ready for DiT training!")
        print("   Both reconstruction and user discrimination meet requirements")
    elif rec_ok:
        print("âš ï¸ Model has good reconstruction but weak user discrimination")
        print("   Consider training with stronger user contrastive loss")
    elif disc_ok:
        print("âš ï¸ Model has good user discrimination but poor reconstruction")
        print("   Consider adjusting reconstruction loss weights")
    else:
        print("âŒ Model needs more training")
        print("   Both metrics need improvement")
    
    # ä¿å­˜JSONæŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"validation_report_{timestamp}.json"
    with open(report_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nğŸ“ Report saved to: {report_path}")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='VA-VAE Model Validation')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to model checkpoint')
    parser.add_argument('--data_root', type=str, default='/kaggle/input/dataset',
                       help='Path to dataset')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (cuda/cpu)')
    parser.add_argument('--test_reconstruction', action='store_true', default=True,
                       help='Test reconstruction quality')
    parser.add_argument('--test_vf', action='store_true',
                       help='Test VF alignment')
    parser.add_argument('--test_discrimination', action='store_true',
                       help='Test user discrimination')
    parser.add_argument('--test_latents', action='store_true',
                       help='Extract latent statistics')
    parser.add_argument('--export_models', action='store_true',
                       help='Export encoder/decoder for DiT')
    parser.add_argument('--full_test', action='store_true',
                       help='Run all tests')
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šfull_testï¼Œå¯ç”¨æ‰€æœ‰æµ‹è¯•
    if args.full_test:
        args.test_vf = True
        args.test_discrimination = True
        args.test_latents = True
        args.export_models = True
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, args.device)
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    
    # 1. é‡å»ºè´¨é‡æµ‹è¯•ï¼ˆé»˜è®¤å¼€å¯ï¼‰
    if args.test_reconstruction:
        results['reconstruction'] = test_reconstruction(
            model, args.data_root, device=args.device
        )
    
    # 2. VFå¯¹é½æµ‹è¯•
    if args.test_vf:
        results['vf_alignment'] = test_vf_alignment(
            model, args.data_root, device=args.device
        )
    
    # 3. ç”¨æˆ·åŒºåˆ†æµ‹è¯•
    if args.test_discrimination:
        results['user_discrimination'] = test_user_discrimination(
            model, args.data_root, device=args.device
        )
    
    # 4. æ½œåœ¨ç©ºé—´ç»Ÿè®¡
    if args.test_latents:
        results['latent_statistics'] = extract_latent_statistics(
            model, args.data_root, device=args.device
        )
    
    # 5. å¯¼å‡ºæ¨¡å‹
    if args.export_models:
        encoder_path, decoder_path = export_encoder_decoder(model, args.checkpoint)
        results['exported_models'] = {
            'encoder': encoder_path,
            'decoder': decoder_path
        }
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results, args.checkpoint)
    
    # ä½¿ç”¨æç¤º
    if not args.full_test:
        print("\nğŸ’¡ Tips:")
        print("  â€¢ Use --full_test to run all validation tests")
        print("  â€¢ Use --test_vf to test VF alignment")
        print("  â€¢ Use --test_discrimination to test user discrimination")
        print("  â€¢ Use --export_models to export for DiT training")


if __name__ == '__main__':
    main()

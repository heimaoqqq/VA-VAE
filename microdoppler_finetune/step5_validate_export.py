#!/usr/bin/env python
"""VA-VAEæ¨¡å‹éªŒè¯ä¸å¯¼å‡ºè„šæœ¬ - åŸºäºå®˜æ–¹LightningDiTé¡¹ç›®æ ‡å‡†"""

import os
import sys
from pathlib import Path

# æ·»åŠ LightningDiTè·¯å¾„ - å¿…é¡»åœ¨æ‰€æœ‰å¯¼å…¥ä¹‹å‰
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))
sys.path.insert(0, str(project_root))

# å…³é”®ï¼šåœ¨å¯¼å…¥ldmä¹‹å‰è®¾ç½®tamingè·¯å¾„ï¼
def setup_taming_path():
    """è®¾ç½®tamingè·¯å¾„ï¼Œå¿…é¡»åœ¨å¯¼å…¥ldmä¹‹å‰è°ƒç”¨"""
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥tamingä½ç½®
    taming_locations = [
        Path('/kaggle/working/taming-transformers'),  # Kaggleæ ‡å‡†ä½ç½®
        Path('/kaggle/working/.taming_path'),  # è·¯å¾„æ–‡ä»¶
        Path.cwd().parent / 'taming-transformers',  # é¡¹ç›®æ ¹ç›®å½•
        Path.cwd() / '.taming_path'  # å½“å‰ç›®å½•è·¯å¾„æ–‡ä»¶
    ]
    
    for location in taming_locations:
        if location.name == '.taming_path' and location.exists():
            # è¯»å–è·¯å¾„æ–‡ä»¶
            try:
                with open(location, 'r') as f:
                    taming_path = f.read().strip()
                if Path(taming_path).exists() and taming_path not in sys.path:
                    sys.path.insert(0, taming_path)
                    print(f"ğŸ“‚ å·²åŠ è½½tamingè·¯å¾„: {taming_path}")
                    return True
            except Exception as e:
                continue
        elif location.name == 'taming-transformers' and location.exists():
            # ç›´æ¥è·¯å¾„
            taming_path = str(location.absolute())
            if taming_path not in sys.path:
                sys.path.insert(0, taming_path)
                print(f"ğŸ“‚ å‘ç°å¹¶åŠ è½½taming: {taming_path}")
                return True
    
    # é™é»˜å¤±è´¥ï¼Œå› ä¸ºå¯èƒ½å·²ç»é€šè¿‡å…¶ä»–æ–¹å¼åŠ è½½
    return False

# åœ¨ä»»ä½•å¯¼å…¥ldmä¹‹å‰è®¾ç½®tamingè·¯å¾„
setup_taming_path()

# ç°åœ¨å®‰å…¨å¯¼å…¥å…¶ä»–æ¨¡å—
import json
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
import argparse
from omegaconf import OmegaConf
from datetime import datetime
import torch.nn.functional as F

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from omegaconf import OmegaConf
from ldm.util import instantiate_from_config
from ldm.models.autoencoder import AutoencoderKL
print("âœ… æˆåŠŸå¯¼å…¥VA-VAEæ¨¡å—")

# å°è¯•å¯¼å…¥å¯é€‰æ¨¡å—
try:
    from sklearn.metrics import silhouette_score
    from sklearn.manifold import TSNE
    from sklearn.decomposition import PCA
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learnæœªå®‰è£…ï¼Œéƒ¨åˆ†åˆ†æåŠŸèƒ½å°†å—é™")

try:
    from lpips import LPIPS
    LPIPS_AVAILABLE = True
except ImportError:
    LPIPS_AVAILABLE = False
    print("âš ï¸ LPIPSæœªå®‰è£…ï¼Œæ„ŸçŸ¥æŸå¤±è¯„ä¼°å°†è·³è¿‡")

def get_training_vae_config():
    """ä» step4_train_vavae.py æå–çš„å®Œæ•´VA-VAEé…ç½®"""
    # ç›´æ¥ä»è®­ç»ƒè„šæœ¬å¤åˆ¶çš„å®Œæ•´é…ç½®ï¼ˆè¡Œ 518-544ï¼‰
    return {
        'target': 'ldm.models.autoencoder.AutoencoderKL',
        'params': {
            'monitor': 'val/rec_loss',
            'embed_dim': 32,
            'use_vf': 'dinov2',  # VA-VAEç‰¹æœ‰å‚æ•°
            'reverse_proj': True,  # VA-VAEç‰¹æœ‰å‚æ•°
            'ddconfig': {
                'double_z': True, 
                'z_channels': 32, 
                'resolution': 256,
                'in_channels': 3, 
                'out_ch': 3, 
                'ch': 128,
                'ch_mult': [1, 1, 2, 2, 4], 
                'num_res_blocks': 2,
                'attn_resolutions': [16],  # è¿™æ˜¯å…³é”®å·®å¼‚ï¼
                'dropout': 0.0
            },
            'lossconfig': {
                'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                'params': {
                    # ä»è®­ç»ƒè„šæœ¬å¤åˆ¶çš„å®Œæ•´é…ç½®ï¼ˆè¡Œ533-543ï¼‰
                    'disc_start': 1, 'disc_num_layers': 3,
                    'disc_weight': 0.5, 'disc_factor': 1.0,
                    'disc_in_channels': 3, 'disc_conditional': False, 'disc_loss': 'hinge',
                    'pixelloss_weight': 1.0, 'perceptual_weight': 1.0,
                    'kl_weight': 1e-6, 'logvar_init': 0.0,
                    'use_actnorm': False, 'pp_style': False,
                    'vf_weight': 0.1, 'adaptive_vf': False,
                    'distmat_weight': 1.0, 'cos_weight': 1.0,
                    'distmat_margin': 0.0, 'cos_margin': 0.0  # Stage 3çš„marginå€¼
                }
            }
        }
    }

def infer_vae_config_from_checkpoint(checkpoint):
    """ä½¿ç”¨è®­ç»ƒè„šæœ¬çš„å®Œæ•´VA-VAEé…ç½®"""
    print("ä½¿ç”¨step4_train_vavae.pyçš„å®Œæ•´é…ç½®: embed_dim=32, use_vf=dinov2, reverse_proj=True")
    
    # è°ƒè¯•ï¼šåˆ†æcheckpointç»“æ„
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    # æŒ‰å‰ç¼€åˆ†ç»„åˆ†æ
    key_prefixes = {}
    for key in state_dict.keys():
        prefix = key.split('.')[0] if '.' in key else 'root'
        key_prefixes[prefix] = key_prefixes.get(prefix, 0) + 1
    
    print(f"  ğŸ“Š Checkpointå‚æ•°åˆ†å¸ƒ:")
    for prefix, count in sorted(key_prefixes.items()):
        print(f"    {prefix}: {count}ä¸ªå‚æ•°")
    
    # æ˜¾ç¤ºå‰10ä¸ªé”®
    sample_keys = list(state_dict.keys())[:10]
    print(f"  ğŸ“ ç¤ºä¾‹é”®: {sample_keys}")
    
    return get_training_vae_config()

def load_model(checkpoint_path, config_path=None, device='cuda'):
    """åŠ è½½VA-VAEæ¨¡å‹ï¼ˆè‡ªé€‚åº”æ¶æ„ï¼‰"""
    print(f"\nğŸ“‚ åŠ è½½VA-VAEæ¨¡å‹...")
    print(f"  Checkpoint: {checkpoint_path}")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # è·å–æˆ–åˆ›å»ºé…ç½®
    if config_path and Path(config_path).exists():
        print(f"  Config: {config_path}")
        config = OmegaConf.load(config_path)
    elif 'config' in checkpoint:
        config = OmegaConf.create(checkpoint['config'])
        print("  ä½¿ç”¨checkpointä¸­çš„é…ç½®")
    else:
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç¡®åˆ‡é…ç½®
        print("  ä½¿ç”¨è®­ç»ƒæ—¶çš„VAEé…ç½®")
        inferred_config = get_training_vae_config()
        config = OmegaConf.create(inferred_config)
    
    # å®ä¾‹åŒ–æ¨¡å‹
    model_config = config.model if hasattr(config, 'model') else config
    model = instantiate_from_config(model_config)
    
    # åŠ è½½state_dict
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    # å…ˆå°è¯•åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™è°ƒæ•´é…ç½®
    # è°ƒè¯•ï¼šæ£€æŸ¥checkpointä¸­çš„é”®
    print(f"  ğŸ“Š CheckpointåŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºLightningè®­ç»ƒçš„checkpointæ ¼å¼
    if any(k.startswith('model.') for k in state_dict.keys()):
        print("  ğŸ”§ æ£€æµ‹åˆ°Lightningæ ¼å¼ï¼Œåªæå–model.*çš„æƒé‡")
        # åªä¿ç•™ä»¥'model.'å¼€å¤´çš„å‚æ•°
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('model.'):
                new_key = k[6:]  # ç§»é™¤'model.'å‰ç¼€
                new_state_dict[new_key] = v
        
        print(f"  âœ‚ï¸ è¿‡æ»¤åå‰©ä½™ {len(new_state_dict)} ä¸ªæ¨¡å‹å‚æ•°")
        state_dict = new_state_dict
        
        # å†æ¬¡åˆ†æè¿‡æ»¤åçš„é”®
        key_prefixes_filtered = {}
        for key in state_dict.keys():
            prefix = key.split('.')[0] if '.' in key else 'root'
            key_prefixes_filtered[prefix] = key_prefixes_filtered.get(prefix, 0) + 1
        print(f"  ğŸ“‹ è¿‡æ»¤åå‚æ•°åˆ†å¸ƒ: {key_prefixes_filtered}")
    
    try:
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        if len(missing) > 10 or len(unexpected) > 100:  # è¿›ä¸€æ­¥æ”¾å®½é˜ˆå€¼
            raise RuntimeError("æ¶æ„ä¸åŒ¹é…")
    except RuntimeError as e:
        if "æ¶æ„ä¸åŒ¹é…" in str(e) or "size mismatch" in str(e):
            print("  âš ï¸ æ¶æ„ä¸åŒ¹é…ï¼Œä½¿ç”¨è®­ç»ƒé…ç½®...")
            config = OmegaConf.create(get_training_vae_config())
            model_config = config.model if hasattr(config, 'model') else config
            model = instantiate_from_config(model_config)
            missing, unexpected = model.load_state_dict(state_dict, strict=False)
        else:
            raise e
    
    if missing:
        print(f"  âš ï¸ Missing keys: {len(missing)} (å‰5ä¸ª: {list(missing)[:5]})")
    if unexpected:
        # åˆ†æunexpected keysçš„ç±»å‹
        unexpected_prefixes = {}
        for key in unexpected:
            prefix = key.split('.')[0] if '.' in key else 'root'
            unexpected_prefixes[prefix] = unexpected_prefixes.get(prefix, 0) + 1
        
        print(f"  âš ï¸ Unexpected keys: {len(unexpected)}")
        print(f"    åˆ†å¸ƒ: {unexpected_prefixes}")
        print(f"    å‰5ä¸ª: {list(unexpected)[:5]}")
    
    model = model.to(device)
    model.eval()
    
    # æ£€æŸ¥VA-VAEç‰¹æ€§ - æ›´å…¨é¢çš„æ£€æµ‹
    has_vf = False
    has_proj = False
    
    # æ£€æŸ¥å„ç§å¯èƒ½çš„ä½ç½® - æ ¹æ®AutoencoderKLæºç ä¿®æ­£
    if hasattr(model, 'foundation_model') and model.foundation_model is not None:
        has_vf = True
    elif hasattr(model, 'vf_model') and model.vf_model is not None:
        has_vf = True
    elif hasattr(model, 'aux_model') and model.aux_model is not None:
        has_vf = True
    elif hasattr(model, 'loss') and hasattr(model.loss, 'aux_model') and model.loss.aux_model is not None:
        has_vf = True
        
    # æ£€æŸ¥VFæŠ•å½±å±‚ - æ ¹æ®æºç ä½¿ç”¨linear_proj
    if hasattr(model, 'linear_proj') and model.linear_proj is not None:
        has_proj = True
    elif hasattr(model, 'vf_proj') and model.vf_proj is not None:
        has_proj = True
    elif hasattr(model, 'aux_proj') and model.aux_proj is not None:
        has_proj = True
    elif hasattr(model, 'loss') and hasattr(model.loss, 'aux_proj') and model.loss.aux_proj is not None:
        has_proj = True
    
    # è°ƒè¯•ï¼šæ˜¾ç¤ºæ¨¡å‹çš„å®é™…å±æ€§
    model_attrs = [attr for attr in dir(model) if not attr.startswith('_') and ('vf' in attr.lower() or 'aux' in attr.lower() or 'dinov2' in attr.lower())]
    print(f"  ğŸ” æ¨¡å‹VFç›¸å…³å±æ€§: {model_attrs}")
    
    # æ£€æŸ¥use_vfå±æ€§
    if hasattr(model, 'use_vf') and getattr(model, 'use_vf') is not None:
        use_vf_val = getattr(model, 'use_vf')
        print(f"  âœ“ æ¨¡å‹å¯ç”¨VF: use_vf={use_vf_val}")
        if use_vf_val == 'dinov2' or use_vf_val is True:
            has_vf = True
    
    # æ›´è¯¦ç»†çš„æ£€æŸ¥ - VA-VAEçš„VFç»„ä»¶é€šå¸¸åœ¨lossæ¨¡å—ä¸­
    if hasattr(model, 'loss'):
        loss_attrs = [attr for attr in dir(model.loss) if 'vf' in attr.lower() or 'aux' in attr.lower() or 'dinov2' in attr.lower()]
        print(f"  ğŸ” Lossæ¨¡å—VFå±æ€§: {loss_attrs}")
        
        # æ£€æŸ¥foundation_model (æ ¹æ®AutoencoderKLæºç )
        if hasattr(model, 'foundation_model') and getattr(model, 'foundation_model') is not None:
            print(f"  âœ“ å‘ç°model.foundation_model: {type(model.foundation_model)}")
            has_vf = True
            
        # æ£€æŸ¥linear_proj (æŠ•å½±å±‚)
        if hasattr(model, 'linear_proj') and getattr(model, 'linear_proj') is not None:
            print(f"  âœ“ å‘ç°model.linear_proj: {type(model.linear_proj)}")
            has_proj = True
            
        # æ£€æŸ¥aux_model (DINOv2) - å…¼å®¹æ—§ç‰ˆ
        if hasattr(model.loss, 'aux_model') and getattr(model.loss, 'aux_model') is not None:
            print(f"  âœ“ å‘ç°model.loss.aux_model: {type(model.loss.aux_model)}")
            has_vf = True
            
        # æ£€æŸ¥aux_proj (åå‘æŠ•å½±) - å…¼å®¹æ—§ç‰ˆ
        if hasattr(model.loss, 'aux_proj') and getattr(model.loss, 'aux_proj') is not None:
            print(f"  âœ“ å‘ç°model.loss.aux_proj: {type(model.loss.aux_proj)}")
            has_proj = True
            
        # æ£€æŸ¥VF weight - å¦‚æœæœ‰vf_weightä¸”>0è¯´æ˜VFç»„ä»¶åœ¨å·¥ä½œ
        if hasattr(model.loss, 'vf_weight') and getattr(model.loss, 'vf_weight', 0) > 0:
            print(f"  âœ“ VFæŸå¤±æ¿€æ´»: vf_weight={model.loss.vf_weight}")
            has_vf = True
    
    # ç‰¹æ®Šæƒ…å†µå¤„ç†ï¼šå¦‚æœè¾“å‡ºæ˜¾ç¤º"Using dinov2 as auxiliary feature"ä½†æœªæ£€æµ‹åˆ°VFç»„ä»¶
    if not has_vf and hasattr(model, 'use_vf') and getattr(model, 'use_vf') is not None:
        print(f"  âš ï¸ VFç»„ä»¶æœªæ­£ç¡®æ£€æµ‹ï¼Œä½†use_vf={getattr(model, 'use_vf')}")
        print(f"  â„¹ï¸ è¿™å¯èƒ½æ˜¯VFç»„ä»¶åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­ä½†æ²¡æœ‰è¢«æ­£ç¡®åŠ è½½")
        has_vf = True  # åŸºäºé…ç½®åˆ¤æ–­åº”è¯¥æœ‰VFç»„ä»¶
    
    print(f"\nâœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"  VA-VAEç‰¹æ€§: VF={'âœ“' if has_vf else 'âœ—'}, Proj={'âœ“' if has_proj else 'âœ—'}")
    
    return model

# åˆ é™¤äº†infer_vae_config_from_checkpoint_detailedå‡½æ•°ï¼Œä½¿ç”¨è®­ç»ƒé…ç½®


def load_and_preprocess_image(img_path, device='cuda'):
    """åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ"""
    try:
        img = Image.open(img_path).convert('RGB')
        # è°ƒæ•´åˆ°256x256
        img = img.resize((256, 256), Image.LANCZOS)
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = np.array(img).astype(np.float32)
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        img_array = img_array / 127.5 - 1.0
        # è½¬æ¢ä¸ºtensor [H,W,C] -> [C,H,W]
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)
        # æ·»åŠ batchç»´åº¦
        img_tensor = img_tensor.unsqueeze(0).to(device)
        return img_tensor
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•åŠ è½½å›¾åƒ {img_path}: {e}")
        return None


def test_reconstruction(model, data_root, split_file=None, device='cuda'):
    """æµ‹è¯•é‡å»ºè´¨é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    return evaluate_reconstruction_quality(model, data_root, split_file, device=device)


def evaluate_reconstruction_quality(model, data_root, split_file=None, num_samples=50, device='cuda'):
    """è¯„ä¼°VA-VAEé‡å»ºè´¨é‡ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ“Š è¯„ä¼°é‡å»ºè´¨é‡ (Reconstruction Quality)")
    print("="*60)
    
    data_path = Path(data_root)
    mse_scores = []
    psnr_scores = []
    
    # åŠ è½½æ•°æ®åˆ’åˆ†
    all_images = []
    if split_file and os.path.exists(split_file):
        print(f"  ğŸ“‚ ä½¿ç”¨æ•°æ®åˆ†å‰²æ–‡ä»¶: {split_file}")
        with open(split_file, 'r') as f:
            split_data = json.load(f)
        
        # æ£€æŸ¥split_fileçš„ç»“æ„
        print(f"  ğŸ“Š Splitæ–‡ä»¶ç»“æ„: {list(split_data.keys())[:5]}")
        
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šæ–°æ ¼å¼(valåˆ—è¡¨) å’Œ æ—§æ ¼å¼(ç”¨æˆ·å­—å…¸)
        if 'val' in split_data:  # æ–°æ ¼å¼ï¼š{"train": [...], "val": [...], "test": [...]}
            val_data = split_data['val']
            print(f"  ğŸ“Š Valæ•°æ®ç±»å‹: {type(val_data)}, é•¿åº¦: {len(val_data) if hasattr(val_data, '__len__') else 'N/A'}")
            
            # å¤„ç†ä¸åŒçš„æ•°æ®ç±»å‹
            if isinstance(val_data, list):
                val_images = val_data[:num_samples]
            elif isinstance(val_data, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå¯èƒ½æ˜¯ {user_id: [files]}çš„æ ¼å¼
                val_images = []
                for user_files in val_data.values():
                    if isinstance(user_files, list):
                        val_images.extend(user_files[:3])  # æ¯ä¸ªç”¨æˆ·å–3å¼ 
                val_images = val_images[:num_samples]
            else:
                print(f"  âš ï¸ ä¸æ”¯æŒçš„valæ•°æ®æ ¼å¼: {type(val_data)}")
                val_images = []
            
            for img_path in val_images:
                # ä»è·¯å¾„æ¨æ–­ç”¨æˆ·ID
                user_id = 1  # é»˜è®¤
                if 'user' in str(img_path):
                    try:
                        user_id = int(str(img_path).split('user')[1].split('/')[0])
                    except:
                        pass
                full_path = os.path.join(data_root, str(img_path))
                if os.path.exists(full_path):
                    all_images.append((full_path, user_id))
        else:  # æ—§æ ¼å¼ï¼š{"user1": {"val": [...]}, ...}
            for user_id, user_data in split_data.items():
                val_images = user_data.get('val', [])
                for img_path in val_images[:5]:  # æ¯ä¸ªç”¨æˆ·å–5å¼ 
                    full_path = os.path.join(data_root, img_path)
                    if os.path.exists(full_path):
                        all_images.append((full_path, int(user_id.replace('user', ''))))
    else:
        # å…¼å®¹æ—§ç‰ˆï¼šç›´æ¥ä»æ–‡ä»¶å¤¹è¯»å–
        print(f"  ğŸ“‚ ç›´æ¥æ‰«ææ•°æ®ç›®å½•: {data_root}")
        for user_id in range(1, 32):
            user_folder = data_path / f'user{user_id}'
            if user_folder.exists():
                images = sorted(user_folder.glob('*.jpg'))[:5]
                all_images.extend([(str(img), user_id) for img in images])

    print(f"  ğŸ“Š æ‰¾åˆ° {len(all_images)} ä¸ªå›¾ç‰‡æ–‡ä»¶")

    if len(all_images) == 0:
        print(f"  âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼Œè·³è¿‡é‡å»ºæµ‹è¯•")
        return {
            'mse': 0.0, 'psnr': 0.0, 'lpips': 0.0 if LPIPS_AVAILABLE else None,
            'samples_count': 0, 'grade': 'éœ€æ”¹è¿› âš ï¸'
        }

    # éšæœºé‡‡æ ·
    if len(all_images) > num_samples:
        import random
        random.seed(42)
        all_images = random.sample(all_images, num_samples)

    
    lpips_fn = None
    if LPIPS_AVAILABLE:
        lpips_fn = LPIPS(net='alex').to(device)
        lpips_scores = []
    
    for img_path, user_id in tqdm(all_images, desc="è¯„ä¼°é‡å»º"):
        img = load_and_preprocess_image(img_path, device)
        if img is None:
            continue
        
        with torch.no_grad():
            # ç¼–ç -è§£ç 
            posterior = model.encode(img)
            z = posterior.sample()
            rec = model.decode(z)
            
            # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
            img_norm = (img + 1.0) / 2.0
            rec_norm = (rec + 1.0) / 2.0
            rec_norm = torch.clamp(rec_norm, 0, 1)
            
            # MSEï¼ˆåœ¨[0,1]èŒƒå›´å†…ï¼‰
            mse = F.mse_loss(rec_norm, img_norm).item()
            mse_scores.append(mse)
            
            # PSNR
            psnr = 20 * np.log10(1.0 / np.sqrt(mse)) if mse > 0 else 100
            psnr_scores.append(psnr)
            
            # LPIPS
            if lpips_fn is not None:
                lpips_val = lpips_fn(img, rec).item()
                lpips_scores.append(lpips_val)
    
    # ç»Ÿè®¡ç»“æœ
    avg_mse = np.mean(mse_scores)
    std_mse = np.std(mse_scores)
    results = {
        'avg_mse': np.mean(mse_scores) if mse_scores else 0,
        'std_mse': np.std(mse_scores) if mse_scores else 0,
        'avg_psnr': np.mean(psnr_scores) if psnr_scores else 0,
        'std_psnr': np.std(psnr_scores) if psnr_scores else 0,
        'num_samples': len(mse_scores)
    }
    
    if LPIPS_AVAILABLE and lpips_scores:
        results['avg_lpips'] = np.mean(lpips_scores)
        results['std_lpips'] = np.std(lpips_scores)
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    print(f"\nğŸ“Š é‡å»ºè´¨é‡è¯„ä¼°ç»“æœ:")
    print(f"  MSE: {results['avg_mse']:.6f} Â± {results['std_mse']:.6f}")
    print(f"  PSNR: {results['avg_psnr']:.2f} Â± {results['std_psnr']:.2f} dB")
    if 'avg_lpips' in results:
        print(f"  LPIPS: {results['avg_lpips']:.4f} Â± {results['std_lpips']:.4f}")
    print(f"  æ ·æœ¬æ•°: {results['num_samples']}")
    
    # è´¨é‡è¯„çº§ï¼ˆåŸºäºPSNRå’ŒLPIPSï¼‰
    score = 0
    if results['avg_psnr'] >= 30:
        score += 3
    elif results['avg_psnr'] >= 25:
        score += 2
    elif results['avg_psnr'] >= 20:
        score += 1
    
    if 'avg_lpips' in results:
        if results['avg_lpips'] <= 0.1:
            score += 2
        elif results['avg_lpips'] <= 0.2:
            score += 1
    
    if score >= 4:
        grade = "ä¼˜ç§€ â­â­â­"
    elif score >= 3:
        grade = "è‰¯å¥½ â­â­"
    elif score >= 2:
        grade = "åˆæ ¼ â­"
    else:
        grade = "éœ€æ”¹è¿› âš ï¸"
    
    print(f"\nğŸ† é‡å»ºè´¨é‡è¯„çº§: {grade}")
    results['grade'] = grade
    
    return results


def test_vf_alignment(model, data_root, split_file=None, num_samples=50, device='cuda'):
    """è¯„ä¼°Vision Foundationå¯¹é½åº¦ï¼ˆVA-VAEæ ¸å¿ƒåˆ›æ–°ï¼‰"""
    return evaluate_vf_alignment(model, data_root, split_file, num_samples, device)


def evaluate_vf_alignment(model, data_root, split_file=None, num_samples=30, device='cuda'):
    """è¯„ä¼°Vision Foundationå¯¹é½åº¦ï¼ˆVA-VAEæ ¸å¿ƒåˆ›æ–°ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ¯ è¯„ä¼°VFè¯­ä¹‰å¯¹é½ (Vision Foundation Alignment)")
    print("="*60)
    
    # æ£€æŸ¥VFæ¨¡å‹ - æ ¹æ®AutoencoderKLæºç çš„å®é™…å®ç°
    has_vf = False
    
    # æ ¹æ®æºç ï¼ŒVFç»„ä»¶å­˜å‚¨åœ¨foundation_modelä¸­
    if hasattr(model, 'foundation_model') and model.foundation_model is not None:
        has_vf = True
        print(f"  âœ… æ£€æµ‹åˆ°foundation_model: {type(model.foundation_model)}")
    elif hasattr(model, 'use_vf') and getattr(model, 'use_vf') is not None:
        # å¯èƒ½åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­ä½†æœªåŠ è½½
        has_vf = True
        print(f"  âœ… æ£€æµ‹åˆ°use_vfé…ç½®: {getattr(model, 'use_vf')}")
    
    if not has_vf:
        print("âš ï¸ æ¨¡å‹æœªé…ç½®VFç»„ä»¶ï¼Œè·³è¿‡æ­¤è¯„ä¼°")
        print(f"  æ£€æŸ¥ç»“æœ: foundation_model={hasattr(model, 'foundation_model')}, use_vf={hasattr(model, 'use_vf')}")
        return None
        
    print("âœ… æ£€æµ‹åˆ°VA-VAEçš„VFç»„ä»¶ï¼Œå¼€å§‹å¯¹é½è¯„ä¼°")
    
    data_path = Path(data_root)
    cosine_sims = []
    feature_dists = []
    
    # æ”¶é›†æµ‹è¯•æ ·æœ¬ - æ”¯æŒsplit_fileå’Œç›´æ¥æ‰«æ
    test_samples = []
    
    if split_file and os.path.exists(split_file):
        print(f"  ğŸ“‚ ä½¿ç”¨splitæ–‡ä»¶: {split_file}")
        with open(split_file, 'r') as f:
            split_data = json.load(f)
        
        if 'val' in split_data:
            val_data = split_data['val']
            if isinstance(val_data, dict):
                # æ ¼å¼: {"user1": [files], "user2": [files]}
                for user_files in val_data.values():
                    if isinstance(user_files, list):
                        for file_path in user_files[:2]:  # æ¯ä¸ªç”¨æˆ·2å¼ 
                            full_path = os.path.join(data_root, str(file_path))
                            if os.path.exists(full_path):
                                test_samples.append(Path(full_path))
            elif isinstance(val_data, list):
                # æ ¼å¼: ["user1/file1.jpg", ...]
                for file_path in val_data[:num_samples]:
                    full_path = os.path.join(data_root, str(file_path))
                    if os.path.exists(full_path):
                        test_samples.append(Path(full_path))
    else:
        # ç›´æ¥æ‰«æç›®å½• - ä¿®å¤æ–‡ä»¶æ‰©å±•å
        for user_id in range(1, 32):
            user_folder = data_path / f'user{user_id}'
            if user_folder.exists():
                # æœç´¢jpgè€Œä¸æ˜¯png
                images = sorted(user_folder.glob('*.jpg'))[:2]
                test_samples.extend(images)
    
    if len(test_samples) > num_samples:
        import random
        random.seed(42)
        test_samples = random.sample(test_samples, num_samples)
    
    print(f"  è¯„ä¼° {len(test_samples)} ä¸ªæ ·æœ¬çš„VFå¯¹é½åº¦...")
    
    for img_path in tqdm(test_samples, desc="è®¡ç®—VFå¯¹é½"):
        img = load_and_preprocess_image(img_path, device)
        if img is None:
            continue
        
        with torch.no_grad():
            # ç¼–ç -è§£ç 
            posterior = model.encode(img)
            z = posterior.sample()
            rec = model.decode(z)
            
            # è°ƒæ•´å›¾åƒèŒƒå›´åˆ°VFæ¨¡å‹æœŸæœ›çš„è¾“å…¥
            # DINOv2æœŸæœ›[0,1]èŒƒå›´çš„è¾“å…¥
            img_vf = (img + 1.0) / 2.0
            rec_vf = (rec + 1.0) / 2.0
            rec_vf = torch.clamp(rec_vf, 0, 1)
            
            # æå–VFç‰¹å¾
            if hasattr(model.vf_model, 'forward_features'):
                # DINOv2é£æ ¼çš„æ¨¡å‹
                orig_feat = model.vf_model.forward_features(img_vf)
                rec_feat = model.vf_model.forward_features(rec_vf)
            else:
                # æ ‡å‡†æ¨¡å‹
                orig_feat = model.vf_model(img_vf)
                rec_feat = model.vf_model(rec_vf)
            
            # å¦‚æœæ˜¯å¤šå±‚ç‰¹å¾ï¼Œå–æœ€åä¸€å±‚
            if isinstance(orig_feat, (list, tuple)):
                orig_feat = orig_feat[-1]
                rec_feat = rec_feat[-1]
            
            # å±•å¹³ç‰¹å¾
            orig_feat = orig_feat.flatten(1)
            rec_feat = rec_feat.flatten(1)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            cos_sim = F.cosine_similarity(orig_feat, rec_feat, dim=1).mean().item()
            cosine_sims.append(cos_sim)
            
            # è®¡ç®—L2è·ç¦»
            l2_dist = F.mse_loss(orig_feat, rec_feat).item()
            feature_dists.append(l2_dist)
    
    # ç»Ÿè®¡ç»“æœ
    avg_cos_sim = np.mean(cosine_sims)
    std_cos_sim = np.std(cosine_sims)
    avg_feature_dist = np.mean(feature_dists)
    std_feature_dist = np.std(feature_dists)
    
    print(f"\nğŸ“Š VFå¯¹é½åº¦è¯„ä¼°ç»“æœ:")
    print(f"  ä½™å¼¦ç›¸ä¼¼åº¦: {avg_cos_sim:.4f} Â± {std_cos_sim:.4f}")
    print(f"  ç‰¹å¾è·ç¦»: {avg_feature_dist:.4f} Â± {std_feature_dist:.4f}")
    
    # è¯„çº§
    if avg_cos_sim >= 0.95:
        grade = "ä¼˜ç§€ â­â­â­"
    elif avg_cos_sim >= 0.90:
        grade = "è‰¯å¥½ â­â­"
    else:
        grade = "ä¸€èˆ¬ â­"
    
    print(f"\nğŸ† VFå¯¹é½åº¦è¯„çº§: {grade}")
    
    return {
        'avg_cos_sim': avg_cos_sim,
        'std_cos_sim': std_cos_sim,
        'avg_feature_dist': avg_feature_dist,
        'std_feature_dist': std_feature_dist,
        'grade': grade
    }


def test_user_discrimination(model, data_root, split_file=None, num_users=10, samples_per_user=10, device='cuda'):
    """è¯„ä¼°ç”¨æˆ·åŒºåˆ†èƒ½åŠ›ï¼ˆå¾®å¤šæ™®å‹’ç‰¹å®šï¼‰"""
    return evaluate_user_discrimination(model, data_root, split_file, num_users, samples_per_user, device)


def evaluate_user_discrimination(model, data_root, split_file=None, num_users=10, samples_per_user=10, device='cuda'):
    """è¯„ä¼°ç”¨æˆ·åŒºåˆ†èƒ½åŠ›ï¼ˆå¾®å¤šæ™®å‹’ç‰¹å®šï¼‰"""
    print("\n" + "="*60)
    print("ğŸ‘¥ è¯„ä¼°ç”¨æˆ·åŒºåˆ†èƒ½åŠ› (User Discrimination)")
    print("="*60)
    
    data_path = Path(data_root)
    user_features = {}
    all_features = []
    all_labels = []
    
    # æå–æ¯ä¸ªç”¨æˆ·çš„ç‰¹å¾
    for user_id in tqdm(range(1, 32), desc="æå–ç”¨æˆ·ç‰¹å¾"):
        user_folder = data_path / f"user{user_id}"
        if not user_folder.exists():
            continue
        
        features = []
        images = list(user_folder.glob("*.png"))[:samples_per_user]
        
        for img_path in images:
            img_tensor = load_and_preprocess_image(img_path, device)
            if img_tensor is None:
                continue
            
            with torch.no_grad():
                # è·å–æ½œåœ¨ç‰¹å¾
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                
                # å…¨å±€æ± åŒ–å¾—åˆ°ç‰¹å¾å‘é‡
                z_pooled = z.mean(dim=[2, 3]).cpu().numpy().flatten()
                features.append(z_pooled)
                all_features.append(z_pooled)
                all_labels.append(user_id)
        
        if features:
            user_features[user_id] = np.mean(features, axis=0)
    
    if len(user_features) < 2:
        print("âš ï¸ ç”¨æˆ·æ•°ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒåŒºåˆ†åˆ†æ")
        return None
    
    # è®¡ç®—Silhouetteåˆ†æ•°ï¼ˆå¦‚æœsklearnå¯ç”¨ï¼‰
    silhouette = 0
    if SKLEARN_AVAILABLE and len(np.unique(all_labels)) > 1:
        all_features_array = np.array(all_features)
        all_labels_array = np.array(all_labels)
        silhouette = silhouette_score(all_features_array, all_labels_array)
    
    # è®¡ç®—ç±»é—´/ç±»å†…è·ç¦»æ¯”
    intra_distances = []
    inter_distances = []
    
    # ç±»é—´è·ç¦»
    users = list(user_features.keys())
    for i in range(len(users)):
        for j in range(i+1, len(users)):
            feat_i = user_features[users[i]]
            feat_j = user_features[users[j]]
            dist = np.linalg.norm(feat_i - feat_j)
            inter_distances.append(dist)
    
    # ç±»å†…è·ç¦»
    if all_features:
        all_features_array = np.array(all_features)
        all_labels_array = np.array(all_labels)
        for user_id in np.unique(all_labels_array):
            user_mask = all_labels_array == user_id
            user_feats = all_features_array[user_mask]
            if len(user_feats) > 1:
                for i in range(len(user_feats)):
                    for j in range(i+1, len(user_feats)):
                        dist = np.linalg.norm(user_feats[i] - user_feats[j])
                        intra_distances.append(dist)
    
    avg_intra = np.mean(intra_distances) if intra_distances else 0
    avg_inter = np.mean(inter_distances) if inter_distances else 0
    ratio = avg_inter / avg_intra if avg_intra > 0 else 0
    
    print(f"ğŸ“Š ç»“æœ:")
    if SKLEARN_AVAILABLE:
        print(f"  Silhouetteåˆ†æ•°: {silhouette:.4f}")
    print(f"  ç±»é—´è·ç¦»: {avg_inter:.4f}")
    print(f"  ç±»å†…è·ç¦»: {avg_intra:.4f}")
    print(f"  ç±»é—´/ç±»å†…æ¯”: {ratio:.4f}")
    print(f"  ç”¨æˆ·æ•°: {len(user_features)}")
    
    # è¯„çº§
    if ratio > 2.0:
        grade = "ä¼˜ç§€ â­â­â­"
    elif ratio > 1.5:
        grade = "è‰¯å¥½ â­â­"
    else:
        grade = "ä¸€èˆ¬ â­"
    print(f"  è¯„çº§: {grade}")
    
    return {
        'silhouette_score': silhouette,
        'intra_distance': avg_intra,
        'inter_distance': avg_inter,
        'ratio': ratio,
        'num_users': len(user_features),
        'grade': grade
    }


def compute_latent_statistics(model, data_root, num_samples=100, device='cuda'):
    """è®¡ç®—æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºDiTè®­ç»ƒï¼‰"""
    print("\n" + "="*60)
    print("ğŸ“¡ è®¡ç®—æ½œåœ¨ç©ºé—´ç»Ÿè®¡ (Latent Statistics)")
    print("="*60)
    
    data_path = Path(data_root)
    all_latents = []
    
    # æ”¶é›†æ ·æœ¬
    sample_count = 0
    for user_id in range(1, 32):
        if sample_count >= num_samples:
            break
        user_folder = data_path / f'user{user_id}'
        if not user_folder.exists():
            continue
        
        images = sorted(user_folder.glob('*.png'))[:5]
        for img_path in images:
            if sample_count >= num_samples:
                break
            
            img = load_and_preprocess_image(img_path, device)
            if img is None:
                continue
            
            with torch.no_grad():
                posterior = model.encode(img)
                z = posterior.sample()
                all_latents.append(z.cpu())
                sample_count += 1
    
    if not all_latents:
        print("  âš ï¸ æ— æ³•æ”¶é›†æ½œåœ¨ç‰¹å¾")
        return None
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    all_latents = torch.cat(all_latents, dim=0)
    mean = all_latents.mean(dim=0)
    std = all_latents.std(dim=0)
    
    print(f"  æ½œåœ¨ç©ºé—´ç»´åº¦: {list(mean.shape)}")
    print(f"  å‡å€¼èŒƒå›´: [{mean.min():.4f}, {mean.max():.4f}]")
    print(f"  æ ‡å‡†å·®èŒƒå›´: [{std.min():.4f}, {std.max():.4f}]")
    print(f"  æ ·æœ¬æ•°: {len(all_latents)}")
    
    return {
        'mean': mean,
        'std': std,
        'shape': list(mean.shape),
        'num_samples': len(all_latents)
    }


def export_encoder_decoder(model, checkpoint_path, output_dir):
    """å¯¼å‡ºç¼–ç å™¨å’Œè§£ç å™¨ç”¨äºDiTè®­ç»ƒ"""
    print("\n" + "="*60)
    print("ğŸ’¾ å¯¼å‡ºç¼–ç å™¨å’Œè§£ç å™¨ (Export for DiT)")
    print("="*60)
    
    # å¯¼å‡ºç¼–ç å™¨
    encoder_path = checkpoint_path.replace('.pt', '_encoder.pt')
    encoder_state = {
        'encoder': model.encoder.state_dict(),
        'quant_conv': model.quant_conv.state_dict() if hasattr(model, 'quant_conv') else None,
        'embed_dim': model.embed_dim if hasattr(model, 'embed_dim') else 32,
        'config': {
            'z_channels': 32,
            'resolution': 256,
            'ch_mult': [1, 1, 2, 2, 4]
        }
    }
    torch.save(encoder_state, encoder_path)
    print(f"âœ… ç¼–ç å™¨å·²å¯¼å‡º: {encoder_path}")
    
    # å¯¼å‡ºè§£ç å™¨
    decoder_path = checkpoint_path.replace('.pt', '_decoder.pt')
    decoder_state = {
        'decoder': model.decoder.state_dict(),
        'post_quant_conv': model.post_quant_conv.state_dict() if hasattr(model, 'post_quant_conv') else None,
        'embed_dim': model.embed_dim if hasattr(model, 'embed_dim') else 32
    }
    torch.save(decoder_state, decoder_path)
    print(f"âœ… è§£ç å™¨å·²å¯¼å‡º: {decoder_path}")
    
    return encoder_path, decoder_path


def generate_report(results):
    """ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ ç»¼åˆéªŒè¯æŠ¥å‘Š (Validation Report)")
    print("="*60)
    
    # é‡å»ºè´¨é‡
    if 'reconstruction' in results:
        rec = results['reconstruction']
        print(f"\n1ï¸âƒ£ é‡å»ºè´¨é‡ (Reconstruction):")
        print(f"   MSE: {rec.get('avg_mse', 0):.6f} Â± {rec.get('std_mse', 0):.6f}")
        print(f"   PSNR: {rec.get('avg_psnr', 0):.2f} Â± {rec.get('std_psnr', 0):.2f} dB")
        if 'avg_lpips' in rec:
            print(f"   LPIPS: {rec['avg_lpips']:.4f} Â± {rec['std_lpips']:.4f}")
        print(f"   è¯„çº§: {rec.get('grade', 'N/A')}")
    
    # VFå¯¹é½
    if 'vf_alignment' in results and results['vf_alignment']:
        vf = results['vf_alignment']
        print(f"\n2ï¸âƒ£ Vision Foundationå¯¹é½:")
        print(f"   ä½™å¼¦ç›¸ä¼¼åº¦: {vf.get('avg_cos_sim', 0):.4f} Â± {vf.get('std_cos_sim', 0):.4f}")
        print(f"   L2è·ç¦»: {vf.get('avg_feature_dist', 0):.6f} Â± {vf.get('std_feature_dist', 0):.6f}")
        print(f"   è¯„çº§: {vf.get('grade', 'N/A')}")
    
    # ç”¨æˆ·åŒºåˆ†
    if 'user_discrimination' in results and results['user_discrimination']:
        disc = results['user_discrimination']
        print(f"\n3ï¸âƒ£ ç”¨æˆ·åŒºåˆ†èƒ½åŠ› (User Discrimination):")
        if SKLEARN_AVAILABLE and 'silhouette_score' in disc:
            print(f"   Silhouetteåˆ†æ•°: {disc['silhouette_score']:.4f}")
        print(f"   ç±»é—´è·ç¦»: {disc.get('inter_distance', 0):.4f}")
        print(f"   ç±»å†…è·ç¦»: {disc.get('intra_distance', 0):.4f}")
        print(f"   ç±»é—´/ç±»å†…æ¯”: {disc.get('ratio', 0):.4f}")
        print(f"   è¯„çº§: {disc.get('grade', 'N/A')}")
    
    # æ€»ä½“è¯„ä¼°
    print("\n" + "="*60)
    print("ğŸ¯ æ€»ä½“è¯„ä¼°")
    print("="*60)
    
    rec_ok = results.get('reconstruction', {}).get('mse', 1.0) < 0.02
    disc_ok = results.get('user_discrimination', {}).get('ratio', 0) > 1.5
    
    if rec_ok and disc_ok:
        print("âœ… æ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡ŒDiTè®­ç»ƒ!")
        print("   é‡å»ºè´¨é‡å’Œç”¨æˆ·åŒºåˆ†èƒ½åŠ›å‡è¾¾æ ‡")
    elif rec_ok:
        print("âš ï¸ æ¨¡å‹é‡å»ºè´¨é‡è‰¯å¥½ä½†ç”¨æˆ·åŒºåˆ†èƒ½åŠ›è¾ƒå¼±")
        print("   å»ºè®®å¢å¼ºç”¨æˆ·å¯¹æ¯”æŸå¤±è¿›è¡Œè®­ç»ƒ")
    elif disc_ok:
        print("âš ï¸ æ¨¡å‹ç”¨æˆ·åŒºåˆ†èƒ½åŠ›è‰¯å¥½ä½†é‡å»ºè´¨é‡è¾ƒå·®")
        print("   å»ºè®®è°ƒæ•´é‡å»ºæŸå¤±æƒé‡")
    else:
        print("âŒ æ¨¡å‹éœ€è¦æ›´å¤šè®­ç»ƒ")
        print("   å„é¡¹æŒ‡æ ‡å‡éœ€æå‡")
    
    # ä¿å­˜JSONæŠ¥å‘Š
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"validation_report_{timestamp}.json"
    with open(report_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='VA-VAEæ¨¡å‹éªŒè¯ä¸å¯¼å‡º')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--data_root', type=str, default='/kaggle/input/dataset',
                       help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--split_file', type=str, default='/kaggle/working/data_split/dataset_split.json',
                       help='æ•°æ®åˆ’åˆ†æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--test_reconstruction', action='store_true', default=True,
                       help='æµ‹è¯•é‡å»ºè´¨é‡')
    parser.add_argument('--test_vf', action='store_true',
                       help='æµ‹è¯•VFå¯¹é½')
    parser.add_argument('--test_discrimination', action='store_true',
                       help='æµ‹è¯•ç”¨æˆ·åŒºåˆ†')
    parser.add_argument('--export_models', action='store_true',
                       help='å¯¼å‡ºç¼–ç å™¨/è§£ç å™¨')
    parser.add_argument('--full_test', action='store_true',
                       help='è¿è¡Œæ‰€æœ‰æµ‹è¯•')
    parser.add_argument('--comprehensive', action='store_true',
                       help='è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼ˆåŒ--full_testï¼‰')
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šfull_testæˆ–comprehensiveï¼Œå¯ç”¨æ‰€æœ‰æµ‹è¯•
    if args.full_test or args.comprehensive:
        args.test_vf = True
        args.test_discrimination = True
        args.export_models = True
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, args.device)
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    
    # 1. é‡å»ºè´¨é‡æµ‹è¯•ï¼ˆé»˜è®¤å¼€å¯ï¼‰
    if args.test_reconstruction:
        results['reconstruction'] = test_reconstruction(
            model, args.data_root, args.split_file, device=args.device
        )
    
    # 2. VFå¯¹é½æµ‹è¯•
    if args.test_vf:
        results['vf_alignment'] = test_vf_alignment(
            model, args.data_root, args.split_file, device=args.device
        )
    
    # 3. ç”¨æˆ·åŒºåˆ†æµ‹è¯•
    if args.test_discrimination:
        results['user_discrimination'] = test_user_discrimination(
            model, args.data_root, args.split_file, device=args.device
        )
    
    # 4. å¯¼å‡ºæ¨¡å‹
    if args.export_models:
        # è®¾ç½®å¯¼å‡ºç›®å½•
        export_dir = Path(args.checkpoint).parent / 'exported_models'
        encoder_path, decoder_path = export_encoder_decoder(model, args.checkpoint, str(export_dir))
        results['exported_models'] = {
            'encoder': encoder_path,
            'decoder': decoder_path
        }
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results)
    
    # ä½¿ç”¨æç¤º
    if not args.full_test:
        print("\nğŸ’¡ æç¤º:")
        print("  â€¢ ä½¿ç”¨ --full_test è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•")
        print("  â€¢ ä½¿ç”¨ --test_vf æµ‹è¯•VFå¯¹é½")
        print("  â€¢ ä½¿ç”¨ --test_discrimination æµ‹è¯•ç”¨æˆ·åŒºåˆ†")
        print("  â€¢ ä½¿ç”¨ --export_models å¯¼å‡ºç”¨äºDiTè®­ç»ƒ")


if __name__ == '__main__':
    main()


def extract_latent_statistics(model, dataset_root, split_file, device='cuda'):
    """æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“ˆ æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡...")
    
    model = model.to(device)
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # ä½¿ç”¨è®­ç»ƒé›†æ•°æ®
    if isinstance(split_data['train'], list):
        train_data = split_data['train']
    else:
        train_data = list(split_data['train'].values())
    
    all_latents = []
    
    with torch.no_grad():
        processed_count = 0
        
        for user_paths in tqdm(train_data, desc="ç¼–ç å›¾åƒ"):
            if processed_count >= 500:
                break
                
            if isinstance(user_paths, list):
                selected_paths = user_paths[:min(5, len(user_paths))]  # æ¯ç”¨æˆ·æœ€å¤š5å¼ 
            else:
                selected_paths = [user_paths]
            
            for img_path_str in selected_paths:
                if processed_count >= 500:
                    break
                    
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                
                img = Image.open(img_path).convert('RGB').resize((256, 256))
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                
                all_latents.append(z.cpu())
                processed_count += 1
    
    # è®¡ç®—ç»Ÿè®¡
    all_latents = torch.cat(all_latents, dim=0)
    mean = all_latents.mean(dim=[0, 2, 3])
    std = all_latents.std(dim=[0, 2, 3])
    
    stats = {
        'global_mean': mean.numpy().tolist(),
        'global_std': std.numpy().tolist(),
        'num_samples': len(all_latents),
        'latent_dim': all_latents.shape[1],
        'spatial_size': [all_latents.shape[2], all_latents.shape[3]]
    }
    
    stats_path = 'latent_statistics.json'
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"âœ… æ½œåœ¨ç©ºé—´ç»Ÿè®¡å·²ä¿å­˜è‡³: {stats_path}")
    print(f"   æ½œåœ¨ç»´åº¦: {stats['latent_dim']}")
    print(f"   ç©ºé—´å°ºå¯¸: {stats['spatial_size']}")
    print(f"   æ ·æœ¬æ•°é‡: {stats['num_samples']}")
    
    return stats


def export_encoder_for_dit(model, checkpoint_path, output_path=None):
    """å¯¼å‡ºç¼–ç å™¨ä¾›DiTè®­ç»ƒä½¿ç”¨"""
    
    print("\nğŸ¯ å¯¼å‡ºç¼–ç å™¨ä¾›DiTè®­ç»ƒ...")
    
    if output_path is None:
        checkpoint_name = Path(checkpoint_path).stem
        output_path = f"encoder_decoder_{checkpoint_name}.pt"
    
    # æå–ç¼–ç å™¨å’Œè§£ç å™¨çŠ¶æ€
    state_dict = model.state_dict()
    encoder_decoder_state = {k: v for k, v in state_dict.items() 
                             if 'encoder' in k or 'decoder' in k or 'quant' in k}
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config_info = {
        'embed_dim': getattr(model, 'embed_dim', 32),
        'z_channels': getattr(model, 'z_channels', 32),
        'use_vf': getattr(model, 'use_vf', 'dinov2'),
        'reverse_proj': getattr(model, 'reverse_proj', True),
        'resolution': 256,
        'type': 'vavae_encoder_decoder'
    }
    
    checkpoint = {
        'state_dict': encoder_decoder_state,
        'config': config_info
    }
    
    torch.save(checkpoint, output_path)
    print(f"âœ… ç¼–ç å™¨å·²å¯¼å‡ºè‡³: {output_path}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = Path(output_path).stat().st_size / (1024 * 1024)
    print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    
    return output_path


# ä»¥ä¸‹å‡½æ•°å·²è¢«å‰é¢çš„main()å‡½æ•°æ›¿ä»£ï¼Œè¿™é‡Œæ˜¯æ‰©å±•åŠŸèƒ½
def validate_reconstruction_extended(model, data_root, split_file=None, device='cuda'):
    """æ‰©å±•çš„é‡å»ºéªŒè¯åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ” æ‰§è¡Œæ‰©å±•é‡å»ºè´¨é‡éªŒè¯")
    print("="*60)
    
    # å¦‚æœæ²¡æœ‰split_fileï¼Œä½¿ç”¨æ‰€æœ‰ç”¨æˆ·æ•°æ®
    user_dirs = sorted([d for d in Path(data_root).iterdir() if d.is_dir() and d.name.startswith('user')])
    # åŠ è½½æ•°æ®é›† - æ”¯æŒsplit_fileå’Œç›´æ¥ç›®å½•æ‰«æ
    image_files = []
    
    if split_file and os.path.exists(split_file):
        print(f"  ğŸ“‚ ä½¿ç”¨æ•°æ®åˆ†å‰²æ–‡ä»¶: {split_file}")
        with open(split_file, 'r') as f:
            split_data = json.load(f)
        # ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæµ‹è¯•
        val_files = split_data.get('val', [])
        image_files = [os.path.join(data_root, f) for f in val_files if os.path.exists(os.path.join(data_root, f))]
    else:
        print(f"  ğŸ“‚ æ‰«ææ•°æ®ç›®å½•: {data_root}")
        if not os.path.exists(data_root):
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
            return None, None
        
        # é€’å½’æ‰«æå›¾ç‰‡æ–‡ä»¶
        for ext in ['*.jpg', '*.jpeg', '*.png']:
            image_files.extend(glob.glob(os.path.join(data_root, '**', ext), recursive=True))
    
    if len(image_files) == 0:
        print(f"âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return {'similarity': 0.0, 'alignment_score': 0.0}
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    num_samples = 100
    selected_files = random.sample(image_files, min(num_samples, len(image_files)))
    print(f"  ğŸ“Š å°†æµ‹è¯• {len(selected_files)} ä¸ªæ ·æœ¬")
        
    all_mse = []
    all_psnr = []
    
    for img_path in selected_files:
        img = load_and_preprocess_image(str(img_path), device)
        
        # ç¼–ç å’Œè§£ç 
        with torch.no_grad():
            posterior = model.encode(img)
            z = posterior.sample()
            rec = model.decode(z)
        
        # è®¡ç®—æŒ‡æ ‡
        mse = F.mse_loss(rec, img).item()
        psnr = 10 * torch.log10(4.0 / torch.tensor(mse)).item()
        
        all_mse.append(mse)
        all_psnr.append(psnr)
        
        print(f"   {Path(img_path).name}: MSE={mse:.6f}, PSNR={psnr:.2f}dB")
    
    # è®¡ç®—å¹³å‡å€¼
    avg_mse = np.mean(all_mse) if all_mse else 0
    avg_psnr = np.mean(all_psnr) if all_psnr else 0
    
    print(f"\nğŸ“ˆ æ€»ä½“ç»“æœ:")
    print(f"   å¹³å‡MSE: {avg_mse:.6f}")
    print(f"   å¹³å‡PSNR: {avg_psnr:.2f} dB")
    
    return avg_mse, avg_psnr
    
def export_encoder_for_dit_extended(model, checkpoint_path, output_path):
    """å¯¼å‡ºç¼–ç å™¨ä¾›DiTè®­ç»ƒä½¿ç”¨ - æ‰©å±•ç‰ˆæœ¬"""
    print("\n" + "="*60)
    print("ğŸ’¾ å¯¼å‡ºVA-VAEç¼–ç å™¨ (ç”¨äºDiTè®­ç»ƒ)")
    print("="*60)
    
    # æå–ç¼–ç å™¨ç›¸å…³æƒé‡
    state_dict = model.state_dict()
    encoder_state = {}
    
    for key, value in state_dict.items():
        if 'encoder' in key or 'quant_conv' in key:
            encoder_state[key] = value
            print(f"   âœ“ å¯¼å‡º: {key} [{list(value.shape)}]")
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config_info = {
        'model_type': 'VA-VAE',
        'embed_dim': getattr(model, 'embed_dim', 32),
        'z_channels': getattr(model, 'z_channels', 32),
        'use_vf': True,
        'resolution': 256,
        'checkpoint_source': checkpoint_path
    }
    
    # æ‰“åŒ…ä¿å­˜
    checkpoint = {
        'state_dict': encoder_state,
        'config': config_info,
        'export_time': datetime.now().isoformat()
    }
    
    torch.save(checkpoint, output_path)
    print(f"\nâœ… ç¼–ç å™¨å·²å¯¼å‡ºè‡³: {output_path}")
    
    # æ–‡ä»¶ä¿¡æ¯
    file_size = Path(output_path).stat().st_size / (1024 * 1024)
    print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    print(f"   åŒ…å« {len(encoder_state)} ä¸ªå‚æ•°ç»„")
    
    return output_path
    
# æ³¨æ„ï¼šä¸»æ‰§è¡Œé€»è¾‘å·²åœ¨å‰é¢çš„main()å‡½æ•°ä¸­å®ç°
# è¿™é‡Œå¯ä»¥æ·»åŠ é¢å¤–çš„è¾…åŠ©å‡½æ•°
        
def print_usage_instructions():
    """æ‰“å°ä½¿ç”¨è¯´æ˜"""
    print("\n" + "="*60)
    print("ğŸ“š VA-VAEéªŒè¯å·¥å…·ä½¿ç”¨è¯´æ˜")
    print("="*60)
    print("\nåŸºç¡€ç”¨æ³•:")
    print("  python step5_validate_export.py --checkpoint <path> [options]")
    print("\nå¸¸ç”¨é€‰é¡¹:")
    print("  --checkpoint PATH       : æ¨¡å‹checkpointè·¯å¾„ (å¿…éœ€)")
    print("  --data_root PATH       : æ•°æ®é›†è·¯å¾„ (é»˜è®¤: /kaggle/input/dataset)")
    print("  --full_test            : è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    print("  --test_reconstruction  : æµ‹è¯•é‡å»ºè´¨é‡")
    print("  --test_vf              : æµ‹è¯•VFå¯¹é½")
    print("  --test_discrimination  : æµ‹è¯•ç”¨æˆ·åŒºåˆ†")
    print("  --export_models        : å¯¼å‡ºç¼–ç å™¨/è§£ç å™¨")
    print("\nç¤ºä¾‹:")
    print("  # è¿è¡Œå®Œæ•´æµ‹è¯•")
    print("  python step5_validate_export.py --checkpoint model.pt --full_test")
    print("  \n  # åªæµ‹è¯•é‡å»ºè´¨é‡")
    print("  python step5_validate_export.py --checkpoint model.pt --test_reconstruction")
    print("="*60)

# è„šæœ¬ç»“æŸ

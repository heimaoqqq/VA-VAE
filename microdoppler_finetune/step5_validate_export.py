#!/usr/bin/env python
"""VA-VAEæ¨¡å‹éªŒè¯ä¸å¯¼å‡ºè„šæœ¬"""

import os
import sys
import json
import torch
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import argparse
from omegaconf import OmegaConf

def setup_taming_path():
    """è®¾ç½®taming-transformersè·¯å¾„"""
    taming_locations = [
        Path('/kaggle/working/taming-transformers'),
        Path('/kaggle/working/.taming_path'),
        Path.cwd().parent / 'taming-transformers',
        Path.cwd() / '.taming_path'
    ]
    
    for location in taming_locations:
        if location.name == '.taming_path' and location.exists():
            try:
                with open(location, 'r') as f:
                    taming_path = f.read().strip()
                if Path(taming_path).exists() and taming_path not in sys.path:
                    sys.path.insert(0, taming_path)
                    print(f"âœ… Tamingè·¯å¾„å·²æ·»åŠ : {taming_path}")
                    return True
            except Exception:
                continue
        elif location.name == 'taming-transformers' and location.exists():
            taming_path = str(location.absolute())
            if taming_path not in sys.path:
                sys.path.insert(0, taming_path)
                print(f"âœ… Tamingè·¯å¾„å·²æ·»åŠ : {taming_path}")
                return True
    
    print("âš ï¸ æœªæ‰¾åˆ°taming-transformersè·¯å¾„ï¼Œå°è¯•ç›´æ¥å¯¼å…¥...")
    return False

# è®¾ç½®tamingè·¯å¾„
setup_taming_path()

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from ldm.util import instantiate_from_config
    from ldm.models.autoencoder import AutoencoderKL
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿taming-transformerså’Œlatent-diffusionå·²æ­£ç¡®å®‰è£…")
    sys.exit(1)

def load_model(checkpoint_path, device='cuda'):
    """åŠ è½½VA-VAEæ¨¡å‹"""
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # è·å–é…ç½®
    if 'config' in checkpoint:
        config = checkpoint['config']
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config = OmegaConf.create({
            'model': {
                'target': 'ldm.models.autoencoder.AutoencoderKL',
                'params': {
                    'embed_dim': 32,
                    'use_vf': 'dinov2',
                    'reverse_proj': True,
                    'ddconfig': {
                        'double_z': True,
                        'z_channels': 32,
                        'resolution': 256,
                        'in_channels': 3,
                        'out_ch': 3,
                        'ch': 128,
                        'ch_mult': [1, 1, 2, 2, 4],
                        'num_res_blocks': 2,
                        'attn_resolutions': [16],
                        'dropout': 0.0
                    },
                    'lossconfig': {
                        'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                        'params': {
                            'disc_start': 1,
                            'kl_weight': 1e-6,
                            'disc_weight': 0.5
                        }
                    }
                }
            }
        })
    
    # å®ä¾‹åŒ–æ¨¡å‹
    model = instantiate_from_config(config.model)
    
    # åŠ è½½æƒé‡
    if 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    model = model.to(device)
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return model


def validate_reconstruction(model, data_root, split_file, num_samples=16, device='cuda'):
    """éªŒè¯é‡å»ºè´¨é‡"""
    
    print("\nğŸ” éªŒè¯é‡å»ºè´¨é‡...")
    
    # åŠ è½½æ•°æ®
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„æ ¼å¼
    if isinstance(split_data['val'], list):
        val_data = split_data['val'][:num_samples]
    else:
        # å¦‚æœvalæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        val_data = list(split_data['val'].values())[:num_samples]
    
    # æ£€æŸ¥æ•°æ®é¡¹æ ¼å¼å¹¶è°ƒè¯•
    if val_data and len(val_data) > 0:
        print(f"ğŸ“Š æ•°æ®é¡¹ç¤ºä¾‹: {val_data[0]}")
        print(f"ğŸ“Š æ•°æ®é¡¹ç±»å‹: {type(val_data[0])}")
    
    # å‡†å¤‡å›¾åƒ
    images = []
    reconstructions = []
    
    model = model.to(device)
    
    with torch.no_grad():
        processed_count = 0
        for idx, user_paths in enumerate(tqdm(val_data, desc="å¤„ç†ç”¨æˆ·")):
            # æ¯ä¸ªuser_pathsæ˜¯ä¸€ä¸ªç”¨æˆ·çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            if isinstance(user_paths, list):
                # ä»æ¯ä¸ªç”¨æˆ·é€‰æ‹©å‡ å¼ å›¾ç‰‡
                num_to_select = min(2, len(user_paths), num_samples - processed_count)
                selected_paths = user_paths[:num_to_select]
            else:
                selected_paths = [user_paths]  # å¦‚æœæ˜¯å•ä¸ªè·¯å¾„
                
            for img_path_str in selected_paths:
                if processed_count >= num_samples:
                    break
                    
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                    
                img = Image.open(img_path).convert('RGB')
                img = img.resize((256, 256), Image.LANCZOS)
                
                # è½¬æ¢ä¸ºtensor
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                # é‡å»º
                reconstructed, _, _, _ = model(img_tensor)
                
                images.append(img_tensor.cpu())
                reconstructions.append(reconstructed.cpu())
                processed_count += 1
                
            if processed_count >= num_samples:
                break
    
    # è®¡ç®—æŒ‡æ ‡
    images_cat = torch.cat(images, dim=0)
    recons_cat = torch.cat(reconstructions, dim=0)
    
    # MSE
    mse = torch.mean((images_cat - recons_cat) ** 2).item()
    
    # PSNR
    psnr = 20 * np.log10(2.0) - 10 * np.log10(mse)
    
    print(f"âœ… é‡å»ºæŒ‡æ ‡:")
    print(f"   MSE: {mse:.6f}")
    print(f"   PSNR: {psnr:.2f} dB")
    
    # ä¿å­˜å¯è§†åŒ–
    save_reconstruction_grid(images_cat, recons_cat, 'reconstruction_results.png')
    
    return mse, psnr


def save_reconstruction_grid(images, reconstructions, save_path, num_show=8):
    """ä¿å­˜é‡å»ºå¯¹æ¯”å›¾"""
    
    num_show = min(num_show, len(images))
    
    fig, axes = plt.subplots(2, num_show, figsize=(num_show * 2, 4))
    
    for i in range(num_show):
        # åŸå›¾
        img = images[i].permute(1, 2, 0).numpy()
        img = (img + 1) / 2  # [-1,1] -> [0,1]
        axes[0, i].imshow(np.clip(img, 0, 1))
        axes[0, i].axis('off')
        if i == 0:
            axes[0, i].set_title('Original')
        
        # é‡å»º
        rec = reconstructions[i].permute(1, 2, 0).numpy()
        rec = (rec + 1) / 2
        axes[1, i].imshow(np.clip(rec, 0, 1))
        axes[1, i].axis('off')
        if i == 0:
            axes[1, i].set_title('Reconstructed')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š ä¿å­˜é‡å»ºå¯¹æ¯”å›¾: {save_path}")


def test_vf_alignment(model, data_root, split_file, device='cuda'):
    """æµ‹è¯•Vision Foundationå¯¹é½èƒ½åŠ› - VA-VAEæ ¸å¿ƒåˆ›æ–°"""
    
    print("\nğŸ¨ Vision Foundationå¯¹é½èƒ½åŠ›éªŒè¯...")
    
    model = model.to(device)
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„æ ¼å¼
    if isinstance(split_data['val'], list):
        test_samples = split_data['val'][:20]
    else:
        test_samples = list(split_data['val'].values())[:20]
    vf_similarities, reconstruction_errors = [], []
    
    with torch.no_grad():
        processed_count = 0
        for idx, user_paths in enumerate(tqdm(test_samples, desc="VFå¯¹é½æµ‹è¯•")):
            if isinstance(user_paths, list):
                num_to_select = min(2, len(user_paths), 20 - processed_count)
                selected_paths = user_paths[:num_to_select]
            else:
                selected_paths = [user_paths]
                
            for img_path_str in selected_paths:
                if processed_count >= 20:
                    break
                    
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                    
                img = Image.open(img_path).convert('RGB').resize((256, 256))
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                reconstructed, posterior, aux_feature, z = model(img_tensor)
                
                if hasattr(model, 'foundation_model'):
                    with torch.no_grad():
                        # è·å–åŸå§‹å›¾åƒå’Œé‡å»ºå›¾åƒçš„VFç‰¹å¾
                        orig_vf = model.foundation_model(img_tensor)
                        recon_vf = model.foundation_model(reconstructed)
                        
                    # æ¯”è¾ƒåŸå§‹å›¾åƒå’Œé‡å»ºå›¾åƒçš„VFç‰¹å¾ç›¸ä¼¼åº¦
                    similarity = torch.cosine_similarity(
                        orig_vf.flatten(), recon_vf.flatten(), dim=0).item()
                    
                    vf_similarities.append(similarity)
                    reconstruction_errors.append(torch.mean((img_tensor - reconstructed) ** 2).item())
                    processed_count += 1
                    
            if processed_count >= 20:
                break
    
    avg_vf_similarity = np.mean(vf_similarities)
    
    print(f"âœ… Vision Foundationå¯¹é½ç»“æœ:")
    print(f"   å¹³å‡VFè¯­ä¹‰ç›¸ä¼¼åº¦: {avg_vf_similarity:.4f}")
    print(f"   VFç›¸ä¼¼åº¦æ ‡å‡†å·®: {np.std(vf_similarities):.4f}")
    
    if avg_vf_similarity > 0.95:
        print(f"   ğŸ† VFå¯¹é½è´¨é‡: ä¼˜ç§€ (>0.95)")
    elif avg_vf_similarity > 0.85:
        print(f"   âœ… VFå¯¹é½è´¨é‡: è‰¯å¥½ (>0.85)")
    else:
        print(f"   âš ï¸ VFå¯¹é½è´¨é‡: éœ€è¦æ”¹è¿› (<0.85)")
    
    return avg_vf_similarity


def test_user_discrimination(model, data_root, split_file, device='cuda'):
    """æµ‹è¯•ç”¨æˆ·åŒºåˆ†èƒ½åŠ› - VA-VAE Stage3åˆ›æ–°"""
    
    print("\nğŸ‘¥ ç”¨æˆ·åŒºåˆ†èƒ½åŠ›éªŒè¯...")
    
    model = model.to(device)
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # ä½¿ç”¨éªŒè¯é›†æ•°æ®
    if isinstance(split_data['val'], dict):
        # å­—å…¸æ ¼å¼ï¼šé”®æ˜¯ç”¨æˆ·ID
        val_data = split_data['val']
        user_items = [(uid, paths) for uid, paths in val_data.items()]
    else:
        # åˆ—è¡¨æ ¼å¼ï¼šç´¢å¼•ä½œä¸ºç”¨æˆ·ID
        val_data = split_data['val']
        user_items = [(f"user_{idx}", paths) for idx, paths in enumerate(val_data)]
    
    user_features = {}
    
    with torch.no_grad():
        # ä¸ºæ¯ä¸ªç”¨æˆ·æå–ç‰¹å¾
        for user_id, user_paths in tqdm(user_items[:31], desc="æå–ç”¨æˆ·ç‰¹å¾"):
            features = []
            
            if isinstance(user_paths, list):
                selected_paths = user_paths[:min(10, len(user_paths))]  # æ¯ç”¨æˆ·æœ€å¤š10å¼ 
            else:
                selected_paths = [user_paths]
            
            for img_path_str in selected_paths[:10]:  # é™åˆ¶æ¯ç”¨æˆ·æ ·æœ¬æ•°
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                    
                img = Image.open(img_path).convert('RGB').resize((256, 256))
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                # ç¼–ç è·å–ç‰¹å¾
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                
                # ä½¿ç”¨å¹³å‡æ± åŒ–è·å–å…¨å±€ç‰¹å¾
                z_pooled = torch.mean(z, dim=[2, 3])  # [B, C]
                features.append(z_pooled.flatten().cpu().numpy())
            
            if features:
                user_features[user_id] = features
    
    # è®¡ç®—åˆ†ç¦»åº¦æŒ‡æ ‡
    if len(user_features) > 1:
        # å‡†å¤‡æ•°æ®ç”¨äºèšç±»åˆ†æ
        all_features = []
        all_labels = []
        
        for idx, (user_id, feats) in enumerate(user_features.items()):
            all_features.extend(feats)
            all_labels.extend([idx] * len(feats))
        
        all_features = np.array(all_features)
        all_labels = np.array(all_labels)
        
        # è®¡ç®—Silhouette Score
        if len(np.unique(all_labels)) > 1:
            from sklearn.metrics import silhouette_score
            silhouette = silhouette_score(all_features, all_labels)
            print(f"âœ… ç”¨æˆ·åˆ†ç¦»åº¦ (Silhouette): {silhouette:.4f}")
        else:
            silhouette = 0
            print("âš ï¸ ç”¨æˆ·æ•°ä¸è¶³ï¼Œæ— æ³•è®¡ç®—Silhouetteåˆ†æ•°")
        
        # è®¡ç®—ç±»é—´/ç±»å†…è·ç¦»æ¯”
        inter_distances = []
        intra_distances = []
        
        user_keys = list(user_features.keys())
        for i, user_i in enumerate(user_keys):
            user_i_feats = np.array(user_features[user_i])
            
            # ç±»å†…è·ç¦»
            if len(user_i_feats) > 1:
                for j in range(len(user_i_feats)):
                    for k in range(j+1, len(user_i_feats)):
                        dist = np.linalg.norm(user_i_feats[j] - user_i_feats[k])
                        intra_distances.append(dist)
            
            # ç±»é—´è·ç¦»ï¼ˆåªè®¡ç®—éƒ¨åˆ†é¿å…è®¡ç®—é‡è¿‡å¤§ï¼‰
            for j in range(i+1, min(i+5, len(user_keys))):
                user_j = user_keys[j]
                user_j_feats = np.array(user_features[user_j])
                
                # é‡‡æ ·è®¡ç®—
                for feat_i in user_i_feats[:3]:
                    for feat_j in user_j_feats[:3]:
                        dist = np.linalg.norm(feat_i - feat_j)
                        inter_distances.append(dist)
        
        # è®¡ç®—å¹³å‡è·ç¦»
        avg_intra = np.mean(intra_distances) if intra_distances else 0
        avg_inter = np.mean(inter_distances) if inter_distances else 0
        separation_ratio = avg_inter / avg_intra if avg_intra > 0 else 0
        
        print(f"âœ… ç±»é—´/ç±»å†…è·ç¦»æ¯”: {separation_ratio:.4f}")
        
        # å¯è§†åŒ–
        visualize_user_distribution(user_features)
        
        return silhouette, separation_ratio
    
    return None, None


def visualize_user_distribution(user_features, save_path='user_distribution.png'):
    """å¯è§†åŒ–ç”¨æˆ·ç‰¹å¾åˆ†å¸ƒ"""
    from sklearn.manifold import TSNE
    
    all_features = []
    all_labels = []
    
    for idx, (user_id, feats) in enumerate(user_features.items()):
        all_features.extend(feats)
        all_labels.extend([idx] * len(feats))
    
    all_features = np.array(all_features)
    
    if len(all_features) > 50:
        # ä½¿ç”¨t-SNEé™ç»´
        tsne = TSNE(n_components=2, random_state=42)
        features_2d = tsne.fit_transform(all_features)
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], 
                            c=all_labels, cmap='tab20', alpha=0.6)
        plt.colorbar(scatter)
        plt.title('ç”¨æˆ·ç‰¹å¾åˆ†å¸ƒ (t-SNE)')
        plt.xlabel('Dimension 1')
        plt.ylabel('Dimension 2')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ç”¨æˆ·åˆ†å¸ƒå›¾ä¿å­˜è‡³: {save_path}")


def extract_latent_statistics(model, dataset_root, split_file, device='cuda'):
    """æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“ˆ æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡...")
    
    model = model.to(device)
    
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    # ä½¿ç”¨è®­ç»ƒé›†æ•°æ®
    if isinstance(split_data['train'], list):
        train_data = split_data['train']
    else:
        train_data = list(split_data['train'].values())
    
    all_latents = []
    
    with torch.no_grad():
        processed_count = 0
        
        for user_paths in tqdm(train_data, desc="ç¼–ç å›¾åƒ"):
            if processed_count >= 500:
                break
                
            if isinstance(user_paths, list):
                selected_paths = user_paths[:min(5, len(user_paths))]  # æ¯ç”¨æˆ·æœ€å¤š5å¼ 
            else:
                selected_paths = [user_paths]
            
            for img_path_str in selected_paths:
                if processed_count >= 500:
                    break
                    
                img_path = Path(img_path_str)
                if not img_path.exists():
                    continue
                
                img = Image.open(img_path).convert('RGB').resize((256, 256))
                img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0).to(device)
                
                posterior = model.encode(img_tensor)
                z = posterior.sample()
                
                all_latents.append(z.cpu())
                processed_count += 1
    
    # è®¡ç®—ç»Ÿè®¡
    all_latents = torch.cat(all_latents, dim=0)
    mean = all_latents.mean(dim=[0, 2, 3])
    std = all_latents.std(dim=[0, 2, 3])
    
    stats = {
        'global_mean': mean.numpy().tolist(),
        'global_std': std.numpy().tolist(),
        'num_samples': len(all_latents),
        'latent_dim': all_latents.shape[1],
        'spatial_size': [all_latents.shape[2], all_latents.shape[3]]
    }
    
    stats_path = 'latent_statistics.json'
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"âœ… æ½œåœ¨ç©ºé—´ç»Ÿè®¡å·²ä¿å­˜è‡³: {stats_path}")
    print(f"   æ½œåœ¨ç»´åº¦: {stats['latent_dim']}")
    print(f"   ç©ºé—´å°ºå¯¸: {stats['spatial_size']}")
    print(f"   æ ·æœ¬æ•°é‡: {stats['num_samples']}")
    
    return stats


def export_encoder_for_dit(model, checkpoint_path, output_path=None):
    """å¯¼å‡ºç¼–ç å™¨ä¾›DiTè®­ç»ƒä½¿ç”¨"""
    
    print("\nğŸ¯ å¯¼å‡ºç¼–ç å™¨ä¾›DiTè®­ç»ƒ...")
    
    if output_path is None:
        checkpoint_name = Path(checkpoint_path).stem
        output_path = f"encoder_decoder_{checkpoint_name}.pt"
    
    # æå–ç¼–ç å™¨å’Œè§£ç å™¨çŠ¶æ€
    state_dict = model.state_dict()
    encoder_decoder_state = {k: v for k, v in state_dict.items() 
                             if 'encoder' in k or 'decoder' in k or 'quant' in k}
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config_info = {
        'embed_dim': getattr(model, 'embed_dim', 32),
        'z_channels': getattr(model, 'z_channels', 32),
        'use_vf': getattr(model, 'use_vf', 'dinov2'),
        'reverse_proj': getattr(model, 'reverse_proj', True),
        'resolution': 256,
        'type': 'vavae_encoder_decoder'
    }
    
    checkpoint = {
        'state_dict': encoder_decoder_state,
        'config': config_info
    }
    
    torch.save(checkpoint, output_path)
    print(f"âœ… ç¼–ç å™¨å·²å¯¼å‡ºè‡³: {output_path}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = Path(output_path).stat().st_size / (1024 * 1024)
    print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    
    return output_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='VA-VAEæ¨¡å‹éªŒè¯å’Œå¯¼å‡ºå·¥å…·')
    
    # è·¯å¾„å‚æ•°
    parser.add_argument('--checkpoint', type=str, 
                       default='checkpoints/stage3/last.ckpt',
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--config', type=str,
                       default='checkpoints/stage3/config.yaml',
                       help='æ¨¡å‹é…ç½®æ–‡ä»¶')
    parser.add_argument('--data_root', type=str,
                       default='/kaggle/input/dataset',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split_file', type=str,
                       default='/kaggle/working/data_split/dataset_split.json',
                       help='æ•°æ®åˆ’åˆ†æ–‡ä»¶')
    
    # åŠŸèƒ½é€‰æ‹©
    parser.add_argument('--validate', action='store_true',
                       help='éªŒè¯é‡å»ºè´¨é‡')
    parser.add_argument('--vf_test', action='store_true',
                       help='æµ‹è¯•VFå¯¹é½èƒ½åŠ›')
    parser.add_argument('--user_test', action='store_true',
                       help='æµ‹è¯•ç”¨æˆ·åŒºåˆ†èƒ½åŠ›')
    parser.add_argument('--extract_stats', action='store_true',
                       help='æå–æ½œåœ¨ç©ºé—´ç»Ÿè®¡')
    parser.add_argument('--export_dit', action='store_true',
                       help='å¯¼å‡ºDiTç¼–ç å™¨')
    parser.add_argument('--comprehensive', action='store_true',
                       help='æ‰§è¡Œç»¼åˆVA-VAEéªŒè¯ (æ¨è)')
    parser.add_argument('--all', action='store_true',
                       help='æ‰§è¡Œæ‰€æœ‰åŠŸèƒ½')
    
    # Kaggleæ ‡å¿—
    parser.add_argument('--kaggle', action='store_true',
                       help='Kaggleç¯å¢ƒæ ‡å¿—')
    
    args = parser.parse_args()
    
    # Kaggleç¯å¢ƒæ£€æµ‹
    if args.kaggle:
        kaggle_input = Path('/kaggle/input')
        kaggle_working = Path('/kaggle/working')
        if kaggle_input.exists():
            print("âœ… æ£€æµ‹åˆ°Kaggleç¯å¢ƒ")
            # æŸ¥æ‰¾checkpoint
            if (kaggle_working / 'checkpoints').exists():
                ckpt_dir = kaggle_working / 'checkpoints'
                # æŸ¥æ‰¾æœ€æ–°é˜¶æ®µ
                for stage in [3, 2, 1]:
                    stage_dir = ckpt_dir / f'stage{stage}'
                    if stage_dir.exists() and (stage_dir / 'last.ckpt').exists():
                        args.checkpoint = str(stage_dir / 'last.ckpt')
                        args.config = str(stage_dir / 'config.yaml')
                        print(f"ä½¿ç”¨ç¬¬{stage}é˜¶æ®µcheckpoint")
                        break
    
    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, device)
    
    # æ‰§è¡ŒåŠŸèƒ½
    if args.comprehensive or (not any([args.validate, args.vf_test, args.user_test, 
                                      args.extract_stats, args.export_dit, args.all])):
        # é»˜è®¤æ‰§è¡Œç»¼åˆéªŒè¯
        print("ğŸš€ VA-VAEç»¼åˆéªŒè¯æµ‹è¯•")
        print("="*60)
        
        results = {}
        
        # åŸºç¡€é‡å»ºéªŒè¯
        mse, psnr = validate_reconstruction(model, args.data_root, args.split_file, device=device)
        results['mse'] = mse
        results['psnr'] = psnr
        
        # VA-VAEç‰¹æœ‰åŠŸèƒ½éªŒè¯
        vf_score = test_vf_alignment(model, args.data_root, args.split_file, device)
        results['vf_alignment'] = vf_score
        
        silhouette, separation_ratio = test_user_discrimination(
            model, args.data_root, args.split_file, device)
        results['user_discrimination'] = silhouette
        results['feature_separation'] = separation_ratio
        
        # æ½œåœ¨ç©ºé—´ç»Ÿè®¡
        stats = extract_latent_statistics(model, args.data_root, args.split_file, device)
        results['latent_stats'] = stats
        
        # å¯¼å‡ºDiTç¼–ç å™¨
        export_encoder_for_dit(model, args.checkpoint, 'vavae_encoder_for_dit.pt')
        
        # ç»¼åˆè¯„ä¼°
        print(f"\nğŸ† VA-VAEç»¼åˆè¯„ä¼°æŠ¥å‘Š:")
        print(f"="*60)
        
        # è¯„åˆ†ç³»ç»Ÿ
        mse_grade = "A+" if mse < 0.005 else "A" if mse < 0.01 else "B+" if mse < 0.02 else "B"
        vf_grade = "A+" if vf_score > 0.95 else "A" if vf_score > 0.90 else "B+" if vf_score > 0.85 else "B"
        user_grade = "A+" if silhouette > 0.3 else "A" if silhouette > 0.2 else "B+" if silhouette > 0.1 else "B"
        sep_grade = "A+" if separation_ratio > 2.0 else "A" if separation_ratio > 1.5 else "B+" if separation_ratio > 1.2 else "B"
        
        print(f"ğŸ“Š é‡å»ºè´¨é‡ (MSE): {mse:.6f} (ç­‰çº§: {mse_grade})")
        print(f"ğŸ“Š é‡å»ºè´¨é‡ (PSNR): {psnr:.2f} dB")
        print(f"ğŸ¨ Vision Foundationå¯¹é½: {vf_score:.4f} (ç­‰çº§: {vf_grade})")
        print(f"ğŸ‘¥ ç”¨æˆ·åŒºåˆ†èƒ½åŠ›: {silhouette:.4f} (ç­‰çº§: {user_grade})")
        print(f"ğŸ¯ ç‰¹å¾åˆ†ç¦»åº¦: {separation_ratio:.4f} (ç­‰çº§: {sep_grade})")
        
        # æ•´ä½“è¯„ä»·
        grades = [mse_grade, vf_grade, user_grade, sep_grade]
        grade_scores = {"A+": 4, "A": 3, "B+": 2, "B": 1, "C": 0}
        avg_score = np.mean([grade_scores[g] for g in grades])
        
        if avg_score >= 3.5:
            overall = "ä¼˜ç§€ - å®Œå…¨èƒœä»»å¾®å¤šæ™®å‹’ç”¨æˆ·åŒºåˆ†ä»»åŠ¡"
        elif avg_score >= 2.5:
            overall = "è‰¯å¥½ - åŸºæœ¬èƒœä»»ï¼Œæœ‰æ”¹è¿›ç©ºé—´"
        else:
            overall = "ä¸€èˆ¬ - éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–"
        
        print(f"\nğŸ–ï¸ æ•´ä½“è¯„ä»·: {overall}")
        print(f"="*60)
        
    else:
        # åˆ†åˆ«æ‰§è¡ŒæŒ‡å®šåŠŸèƒ½
        if args.all:
            args.validate = args.vf_test = args.user_test = True
            args.extract_stats = args.export_dit = True
        
        if args.validate:
            validate_reconstruction(model, args.data_root, args.split_file, device=device)
        
        if args.vf_test:
            test_vf_alignment(model, args.data_root, args.split_file, device)
        
        if args.user_test:
            test_user_discrimination(model, args.data_root, args.split_file, device)
        
        if args.extract_stats:
            extract_latent_statistics(model, args.data_root, args.split_file, device=device)
        
        if args.export_dit:
            output_path = 'vavae_encoder_for_dit.pt'
            export_encoder_for_dit(model, args.checkpoint, output_path)
    
    print("\nâœ… æ‰€æœ‰éªŒè¯ä»»åŠ¡å®Œæˆ!")


if __name__ == '__main__':
    main()

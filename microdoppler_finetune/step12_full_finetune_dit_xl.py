#!/usr/bin/env python3
"""
DiT XLå®Œæ•´å¾®è°ƒ - Kaggle T4x2 DDPç‰ˆæœ¬
åŸºäºå®˜æ–¹é¢„è®­ç»ƒæƒé‡çš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œå¾®è°ƒ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import GradScaler, autocast
from torch.optim import AdamW
import sys
import os
from pathlib import Path
import time
import yaml
from tqdm import tqdm
import numpy as np
import safetensors.torch as safetensors
import argparse

# ğŸ”§ å…³é”®æ˜¾å­˜ä¼˜åŒ–è®¾ç½®
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

# æ·»åŠ è·¯å¾„
sys.path.append('/kaggle/working/VA-VAE')
sys.path.append('/kaggle/working/VA-VAE/LightningDiT')

from models.lightningdit import LightningDiT_models

class LatentDataset(torch.utils.data.Dataset):
    """Latentå‘é‡æ•°æ®é›†"""
    def __init__(self, latents, labels):
        self.latents = latents
        self.labels = labels
    
    def __len__(self):
        return len(self.latents)
    
    def __getitem__(self, idx):
        return self.latents[idx], self.labels[idx]

def load_dit_xl_for_full_finetune(checkpoint_path, rank=None):
    """DDPä¼˜åŒ–ç‰ˆDiT XLæ¨¡å‹åŠ è½½ - CPUå…ˆåŠ è½½å†è½¬ç§»GPU"""
    if rank is None or rank == 0:
        print(f"ğŸ“‚ åŠ è½½DiT XLæ¨¡å‹: {checkpoint_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(checkpoint_path):
        if rank is None or rank == 0:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return None
    
    # ğŸ”§ DDPå…³é”®ä¼˜åŒ–ï¼šå®Œå…¨åœ¨CPUä¸Šåˆå§‹åŒ–ï¼Œé¿å…å¤šè¿›ç¨‹GPUç«äº‰
    if rank is None or rank == 0:
        print(f"ğŸ“Š å¼€å§‹CPUåŠ è½½ï¼Œé¿å…DDPæ˜¾å­˜ç«äº‰...")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    if rank is None or rank == 0:
        print(f"âœ… Checkpointå·²åœ¨CPUåŠ è½½ ({os.path.getsize(checkpoint_path)/1e9:.1f}GB)")
    
    if 'model' in checkpoint:
        state_dict = checkpoint['model']
    elif 'ema' in checkpoint:
        state_dict = checkpoint['ema']
    else:
        state_dict = checkpoint
    
    # å¤„ç†DataParallelæƒé‡é”®
    clean_state_dict = {}
    for key, value in state_dict.items():
        clean_key = key.replace('module.', '') if key.startswith('module.') else key
        clean_state_dict[clean_key] = value
    
    # æ¨æ–­å‚æ•°
    pos_embed_shape = None
    y_embed_shape = None
    final_layer_shape = None
    has_swiglu = False
    
    for key, tensor in clean_state_dict.items():
        if key == 'pos_embed':
            pos_embed_shape = tensor.shape
        elif key == 'y_embedder.embedding_table.weight':
            y_embed_shape = tensor.shape
        elif key == 'final_layer.linear.weight':
            final_layer_shape = tensor.shape
        elif 'mlp.w12' in key:
            has_swiglu = True
    
    input_size = int(pos_embed_shape[1]**0.5) if pos_embed_shape else 16
    num_classes = y_embed_shape[0] if y_embed_shape else 1001
    out_channels = final_layer_shape[0] if final_layer_shape else 32
    
    print(f"ğŸ“‹ æ¨¡å‹é…ç½®:")
    print(f"   è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
    print(f"   ç±»åˆ«æ•°é‡: {num_classes}")
    print(f"   è¾“å‡ºé€šé“: {out_channels}")
    print(f"   MLPç±»å‹: {'SwiGLU' if has_swiglu else 'GELU'}")
    
    # ğŸ”§ å…³é”®ï¼šåœ¨CPUä¸Šå®Œæˆæ‰€æœ‰æ¨¡å‹åˆå§‹åŒ– + å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    model = LightningDiT_models['LightningDiT-XL/1'](
        input_size=input_size,
        num_classes=num_classes,
        class_dropout_prob=0.0,
        use_qknorm=False,
        use_swiglu=has_swiglu,
        use_rope=True,
        use_rmsnorm=True,
        wo_shift=False,
        in_channels=out_channels,
        use_checkpoint=True,  # ğŸ”§ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜ï¼
    )
    
    # ç¡®ä¿æ¨¡å‹åœ¨CPUä¸Š
    model = model.cpu()
    
    # åœ¨CPUä¸ŠåŠ è½½æƒé‡
    missing_keys, unexpected_keys = model.load_state_dict(clean_state_dict, strict=False)
    
    if (rank is None or rank == 0) and missing_keys:
        print(f"âš ï¸ ç¼ºå¤±æƒé‡é”®: {len(missing_keys)}")
    if (rank is None or rank == 0) and unexpected_keys:
        print(f"âš ï¸ å¤šä½™æƒé‡é”®: {len(unexpected_keys)}")
    
    param_count = sum(p.numel() for p in model.parameters())
    trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    if rank is None or rank == 0:
        print(f"âœ… DiT XLæ¨¡å‹åœ¨CPUåˆå§‹åŒ–å®Œæˆ")
        print(f"   æ€»å‚æ•°: {param_count/1e6:.1f}M")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_count/1e6:.1f}M ({trainable_count/param_count*100:.1f}%)")
        print(f"ğŸ“¦ æ¨¡å‹ä¿æŒåœ¨CPUï¼Œç­‰å¾…å®‰å…¨è½¬ç§»åˆ°GPU...")
    
    # è¿”å›CPUä¸Šçš„æ¨¡å‹ï¼Œç¨åå†è½¬ç§»åˆ°æŒ‡å®šGPU
    return model

def setup_ddp(rank, world_size):
    """åˆå§‹åŒ–DDPè¿›ç¨‹ç»„"""
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '12355'
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=world_size,
        rank=rank
    )
    
    # è®¾ç½®å½“å‰GPU
    torch.cuda.set_device(rank)
    
    # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–é…ç½®
    torch.cuda.empty_cache()
    if hasattr(torch.cuda, 'memory_stats'):
        torch.cuda.reset_peak_memory_stats(rank)

def cleanup_ddp():
    """æ¸…ç†DDPè¿›ç¨‹ç»„"""
    dist.destroy_process_group()

def evaluate_model(model, vae_model, device, rank):
    """éªŒè¯æ¨¡å‹æ€§èƒ½"""
    model.eval()
    val_loss = 0.0
    num_samples = 20  # éªŒè¯æ ·æœ¬æ•°
    
    with torch.no_grad():
        # ä½¿ç”¨æ­£ç¡®çš„éªŒè¯é›†æ•°æ®è·¯å¾„
        from pathlib import Path
        official_val_dir = Path("/kaggle/working/latents_official/vavae_config_for_dit/microdoppler_val_256")
        latent_path = official_val_dir / "latents_rank00_shard000.safetensors"
        
        if not latent_path.exists():
            if rank == 0:
                print(f"âš ï¸ éªŒè¯æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡éªŒè¯: {latent_path}")
            return 0.0
            
        data = safetensors.load_file(str(latent_path))
        latents = data['latents'][:num_samples]
        labels = data['labels'][:num_samples]
        
        # ğŸ”§ åº”ç”¨ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„å®˜æ–¹LightningDiTå½’ä¸€åŒ–
        # æ£€æŸ¥æ˜¯å¦æœ‰latents_stats.ptæ–‡ä»¶
        stats_file = official_val_dir / "latents_stats.pt"
        train_stats_file = Path("/kaggle/working/latents_official/vavae_config_for_dit/microdoppler_train_256/latents_stats.pt")
        
        if stats_file.exists():
            stats = torch.load(stats_file, map_location='cpu')
            latent_mean = stats['mean']
            latent_std = stats['std']
        elif train_stats_file.exists():
            # ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯
            stats = torch.load(train_stats_file, map_location='cpu')
            latent_mean = stats['mean']
            latent_std = stats['std']
        else:
            # è®¡ç®—å½“å‰éªŒè¯æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
            latent_mean = latents.mean(dim=0, keepdim=True)
            latent_std = latents.std(dim=0, keepdim=True)
        
        # åº”ç”¨å®˜æ–¹å½’ä¸€åŒ–: (latent - mean) / std * multiplier
        latents = (latents - latent_mean) / latent_std * 1.0
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        latents = latents.to(device)
        labels = labels.to(device)
        
        for i in range(num_samples):
            # æ¨¡æ‹Ÿæ‰©æ•£è¿‡ç¨‹
            noise = torch.randn_like(latents[i:i+1])
            timesteps = torch.randint(0, 1000, (1,), device=device).float()
            
            alpha_t = 1 - timesteps / 1000
            alpha_t = alpha_t.view(-1, 1, 1, 1)
            noisy_latents = torch.sqrt(alpha_t) * latents[i:i+1] + torch.sqrt(1 - alpha_t) * noise
            
            # å‰å‘ä¼ æ’­
            pred_noise = model(noisy_latents, timesteps, labels[i:i+1])
            loss = nn.MSELoss()(pred_noise, noise)
            val_loss += loss.item()
    
    model.train()
    return val_loss / num_samples

def generate_validation_samples(model, vae_model, device, epoch):
    """å¿«é€Ÿç”Ÿæˆæ¡ä»¶æ‰©æ•£æ ·æœ¬è¿›è¡Œå¯è§†åŒ–éªŒè¯"""
    model.eval()
    
    with torch.no_grad():
        print(f"   ğŸ” å¼€å§‹ç”ŸæˆéªŒè¯æ ·æœ¬ï¼Œè®¾å¤‡: {device}")
        
        # ç”Ÿæˆ4ä¸ªä¸åŒç±»åˆ«çš„æ ·æœ¬
        num_samples = 4
        sample_labels = torch.arange(num_samples, device=device)
        print(f"   ğŸ·ï¸ æ ·æœ¬æ ‡ç­¾: {sample_labels.tolist()}")
        
        # ä»çº¯å™ªå£°å¼€å§‹ - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        latent_shape = (num_samples, 32, 16, 16)
        z = torch.randn(latent_shape, device=device, dtype=torch.float16)
        print(f"   ğŸ“ å™ªå£°å½¢çŠ¶: {z.shape}, è®¾å¤‡: {z.device}, ç±»å‹: {z.dtype}")
        
        # å®˜æ–¹LightningDiT ODEé‡‡æ ·è®¾ç½®
        cfg_scale = 10.0  # å®˜æ–¹æ ‡å‡†CFGå¼ºåº¦
        timestep_shift = 0.3  # å®˜æ–¹æ ‡å‡†æ—¶é—´æ­¥åç§»
        
        # ODEæ±‚è§£ - å®˜æ–¹dopri5é£æ ¼
        t_start, t_end = 1.0, 1e-4
        num_steps = 50  # å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
        
        def ode_fn(t, x):
            """ODEå‡½æ•°ï¼šdx/dt = f(x,t) - å®˜æ–¹LightningDiTå®ç°"""
            print(f"     ODEæ­¥éª¤: t={t:.4f}")
            
            # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨GPUä¸Š
            t_batch = torch.full((num_samples,), t, device=device, dtype=torch.float32)
            
            if cfg_scale > 1.0:
                # CFG: æ¡ä»¶ + æ— æ¡ä»¶é¢„æµ‹ (å®˜æ–¹å®ç°)
                uncond_labels = torch.full_like(sample_labels, -1)  # -1è¡¨ç¤ºæ— æ¡ä»¶
                x_cond = torch.cat([x, x], dim=0)
                t_cond = torch.cat([t_batch, t_batch], dim=0) 
                labels_cond = torch.cat([sample_labels, uncond_labels], dim=0)
                
                with torch.cuda.amp.autocast():
                    pred_both = model(x_cond, t_cond, labels_cond)
                pred_cond, pred_uncond = pred_both.chunk(2, dim=0)
                
                # CFGç»„åˆ
                pred = pred_uncond + cfg_scale * (pred_cond - pred_uncond)
            else:
                with torch.cuda.amp.autocast():
                    pred = model(x, t_batch, sample_labels)
            
            return -pred  # è´Ÿå·ï¼šä»å™ªå£°åˆ°å¹²å‡€å›¾åƒ
        
        print(f"   â±ï¸ å¼€å§‹å®˜æ–¹ODEé‡‡æ · ({num_steps}æ­¥)")
        
        # æ‰‹åŠ¨Eulerç§¯åˆ†ï¼ˆdopri5ç®€åŒ–ç‰ˆï¼Œå®˜æ–¹é£æ ¼ï¼‰
        dt = (t_end - t_start) / num_steps
        t = t_start
        x = z.clone()
        
        for step in range(num_steps):
            print(f"     æ­¥éª¤ {step+1}/{num_steps}, t={t:.4f}")
            
            # æ—¶é—´æ­¥åç§»è°ƒæ•´ (å®˜æ–¹å®ç°)
            t_shifted = t + timestep_shift
            t_shifted = max(t_shifted, t_end)
            
            # Euleræ­¥éª¤
            dx_dt = ode_fn(t_shifted, x)
            x = x + dt * dx_dt
            t = t + dt
            
            if t <= t_end:
                break
        
        latents = x.float()  # è½¬æ¢ä¸ºfloat32ç”¨äºVAEè§£ç 
        print(f"   ğŸ¯ æœ€ç»ˆlatentså½¢çŠ¶: {latents.shape}, è®¾å¤‡: {latents.device}")
        
        # VA-VAEè§£ç 
        print(f"   ğŸ”„ å¼€å§‹VA-VAEè§£ç ...")
        generated_images = vae_model.decode(latents)
        print(f"   ğŸ“· è§£ç åå›¾åƒ: {generated_images.shape}, èŒƒå›´: [{generated_images.min():.3f}, {generated_images.max():.3f}]")
        
        # åˆ†æVA-VAEè¾“å‡ºèŒƒå›´å¹¶æ­£ç¡®å½’ä¸€åŒ–
        # å¤§å¤šæ•°VAEè¾“å‡ºèŒƒå›´æ˜¯ [-1, 1]ï¼Œä½†éœ€è¦ç¡®è®¤
        if generated_images.min() >= -1.1 and generated_images.max() <= 1.1:
            # èŒƒå›´æ˜¯ [-1, 1]ï¼Œè½¬æ¢åˆ° [0, 1]
            generated_images = (generated_images + 1.0) / 2.0
            print(f"   âœ… æ£€æµ‹åˆ°[-1,1]èŒƒå›´ï¼Œè½¬æ¢åˆ°[0,1]")
        elif generated_images.min() >= -0.1 and generated_images.max() <= 1.1:
            # èŒƒå›´å·²ç»æ˜¯ [0, 1] å·¦å³
            print(f"   âœ… æ£€æµ‹åˆ°[0,1]èŒƒå›´ï¼Œç›´æ¥ä½¿ç”¨")
        else:
            # å…¶ä»–èŒƒå›´ï¼Œä½¿ç”¨min-maxå½’ä¸€åŒ–
            min_val = generated_images.min()
            max_val = generated_images.max()
            generated_images = (generated_images - min_val) / (max_val - min_val)
            print(f"   âš ï¸ å¼‚å¸¸èŒƒå›´[{min_val:.3f},{max_val:.3f}]ï¼Œä½¿ç”¨min-maxå½’ä¸€åŒ–")
        
        # ç¡®ä¿æœ€ç»ˆèŒƒå›´æ­£ç¡®
        generated_images = torch.clamp(generated_images, 0.0, 1.0)
        print(f"   ğŸ“Š æœ€ç»ˆå›¾åƒèŒƒå›´: [{generated_images.min():.3f}, {generated_images.max():.3f}]")
        
        # ä¿å­˜æ ·æœ¬
        save_path = f"/kaggle/working/validation_samples_epoch_{epoch}.png"
        import torchvision.utils as vutils
        vutils.save_image(generated_images, save_path, nrow=2, normalize=False)
        print(f"   âœ… ä¿å­˜éªŒè¯æ ·æœ¬: {save_path}")
    
    model.train()

def train_ddp_worker(rank, world_size, config):
    """DDPå·¥ä½œè¿›ç¨‹ - æ¯ä¸ªGPUè¿è¡Œæ­¤å‡½æ•°"""
    print(f"ğŸš€ å¯åŠ¨DDP Worker - Rank {rank}/{world_size}")
    
    # åˆå§‹åŒ–DDP
    setup_ddp(rank, world_size)
    device = torch.device(f'cuda:{rank}')
    
    # è®­ç»ƒå‚æ•°
    num_epochs = config.get('num_epochs', 20)
    learning_rate = config.get('learning_rate', 1e-5) 
    save_interval = config.get('save_interval', 5)
    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 4)
    batch_size = config.get('batch_size', 1)
    
    if rank == 0:
        print(f"ğŸ“‹ DDPè®­ç»ƒé…ç½®:")
        print(f"   è®¾å¤‡: {device} (Rank {rank})")
        print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    
    # ğŸ”§ DDPä¼˜åŒ–åŠ è½½æµç¨‹ï¼šCPU â†’ GPU â†’ DDP
    dit_xl_path = "/kaggle/working/VA-VAE/models/lightningdit-xl-imagenet256-64ep.pt"
    
    if rank == 0:
        print(f"ğŸ”„ ç¬¬1æ­¥ï¼šåœ¨CPUåŠ è½½DiT XLæ¨¡å‹...")
    
    # Step 1: åœ¨CPUåŠ è½½æ¨¡å‹
    model = load_dit_xl_for_full_finetune(dit_xl_path, rank)
    if model is None:
        cleanup_ddp()
        return
    
    if rank == 0:
        print(f"ğŸ”„ ç¬¬2æ­¥ï¼šè½¬ç§»æ¨¡å‹åˆ°GPU{rank}...")
        
    # Step 2: åˆ†é˜¶æ®µæ¨¡å‹è½¬ç§»åˆ°GPU (å‡å°‘è¿›ç¨‹èµ„æºç«äº‰)
    torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜
    pre_gpu_memory = torch.cuda.memory_allocated(device) / (1024**3)
    
    # ğŸ”„ é”™å³°è½¬ç§»ç­–ç•¥ï¼šå‡å°‘ç³»ç»Ÿèµ„æºç«äº‰
    if world_size > 1:
        # DDPç¯å¢ƒï¼šæŒ‰ranké”™å³°è½¬ç§»
        time.sleep(rank * 3)  # rank 0å…ˆè½¬ï¼Œrank 1ç­‰3ç§’åè½¬
        if rank == 0:
            print(f"ğŸ”„ Rank {rank}: å¼€å§‹è½¬ç§»æ¨¡å‹åˆ°GPU...")
        model = model.to(device)
        if rank == 0:
            print(f"âœ… Rank {rank}: æ¨¡å‹è½¬ç§»å®Œæˆ")
        # ç­‰å¾…æ‰€æœ‰rankå®Œæˆè½¬ç§»
        dist.barrier()
    else:
        # å•GPUç¯å¢ƒ
        model = model.to(device)
    
    post_gpu_memory = torch.cuda.memory_allocated(device) / (1024**3)
    
    if rank == 0:
        print(f"âœ… æ¨¡å‹è½¬ç§»å®Œæˆ - GPU{rank}æ˜¾å­˜: {pre_gpu_memory:.1f}GB â†’ {post_gpu_memory:.1f}GB")
        print(f"ğŸ”„ ç¬¬3æ­¥ï¼šåŠ è½½VA-VAEæ¨¡å‹...")
    
    # Step 3: åŠ è½½VA-VAE (æ¯ä¸ªrankç‹¬ç«‹åŠ è½½)
    vae_model = load_vae_model(device)
    
    if rank == 0:
        print(f"ğŸ”„ ç¬¬4æ­¥ï¼šDDPåŒ…è£…æ¨¡å‹...")
    
    # Step 4: åŒ…è£…ä¸ºDDP - ä¿®å¤æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
    model = DDP(
        model, 
        device_ids=[device], 
        find_unused_parameters=True,  # è§£å†³æœªä½¿ç”¨å‚æ•°é”™è¯¯
        gradient_as_bucket_view=True  # ä¼˜åŒ–æ¢¯åº¦åŒæ­¥
    )
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šè§£å†³æ¢¯åº¦æ£€æŸ¥ç‚¹+DDPå†²çª
    model._set_static_graph()  # å‘Šè¯‰DDPå›¾ç»“æ„ä¸å˜
    
    if rank == 0:
        print(f"âœ… DDPåŒ…è£…å®Œæˆ - æ¨¡å‹å‡†å¤‡å°±ç»ª")
    
    # åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨
    dataloader = create_distributed_dataloader(config, rank, world_size)
    
    # ğŸ”§ æ¿€è¿›æ˜¾å­˜ä¼˜åŒ–é…ç½®
    scaler = torch.amp.GradScaler('cuda',
        init_scale=2**10,    # æ›´ä½åˆå§‹ç¼©æ”¾
        growth_factor=1.5,   # æ›´ä¿å®ˆå¢é•¿
        backoff_factor=0.25, # æ›´æ¿€è¿›å›é€€
        growth_interval=2000 # æ›´é•¿æ£€æŸ¥é—´éš”
    )
    
    # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–çš„ä¼˜åŒ–å™¨é…ç½®
    # æ³¨æ„ï¼šKaggleç¯å¢ƒå¯èƒ½æ²¡æœ‰bitsandbytesï¼Œä½¿ç”¨æ ‡å‡†AdamW
    optimizer = AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        betas=(0.9, 0.95),
        weight_decay=0.1,
        eps=1e-4
    )
    if rank == 0:
        print("âš ï¸ ä½¿ç”¨æ ‡å‡†AdamWä¼˜åŒ–å™¨ (Kaggleç¯å¢ƒé™åˆ¶)")
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    criterion = nn.MSELoss()
    
    if rank == 0:
        param_count = sum(p.numel() for p in model.parameters())
        effective_batch = batch_size * gradient_accumulation_steps * world_size
        print(f"âœ… DDPæ¨¡å‹å°±ç»ª - å‚æ•°é‡: {param_count/1e6:.1f}M")
        print(f"ğŸ”§ æ¢¯åº¦æ£€æŸ¥ç‚¹: å·²å¯ç”¨ (èŠ‚çœæ˜¾å­˜)")
        print(f"ğŸ”§ æ··åˆç²¾åº¦: ç¨³å®šå‹FP16 (èŠ‚çœæ˜¾å­˜)")
        print(f"ğŸ“Š æ‰¹æ¬¡é…ç½®: {batch_size}Ã—{gradient_accumulation_steps}Ã—{world_size} = {effective_batch}")
        if vae_model is not None:
            print(f"âœ… VA-VAEæ¨¡å‹å°±ç»ª")
        else:
            print(f"âš ï¸ VA-VAEä¸å¯ç”¨")
    
    # æ˜¾å­˜ç»Ÿè®¡
    torch.cuda.empty_cache()
    memory_used = torch.cuda.memory_allocated(device) / (1024**3)
    
    if rank == 0:
        print(f"ğŸ“Š æ˜¾å­˜ä½¿ç”¨: {memory_used:.1f}GB (GPU{rank})")
    
    # æœ€ä½³æ¨¡å‹è·Ÿè¸ªå˜é‡
    best_val_loss = float('inf')
    best_model_path = None
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0
        optimizer.zero_grad()
        
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{num_epochs}")
        
        if dataloader is None:
            print("   âŒ æ— æ•°æ®åŠ è½½å™¨ï¼Œé€€å‡ºè®­ç»ƒ")
            break
            
        # è®¾ç½®samplerçš„epoch (é‡è¦!)
        if hasattr(dataloader.sampler, 'set_epoch'):
            dataloader.sampler.set_epoch(epoch)
            
        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}') if rank == 0 else dataloader
        
        for step, (latents, labels) in enumerate(progress_bar):
            try:
                # ğŸ”§ æ•°æ®ç±»å‹ä¼˜åŒ– (ä¿æŒè¾“å…¥FP32æé«˜ç¨³å®šæ€§)
                latents = latents.to(device, dtype=torch.float32)  # ä¿æŒFP32è¾“å…¥
                labels = labels.to(device)
                
                batch_size = latents.shape[0]
                
                # ğŸ”§ æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with torch.amp.autocast('cuda'):  # ä¿®å¤åºŸå¼ƒAPIè­¦å‘Š
                    # æ‰©æ•£è¿‡ç¨‹
                    noise = torch.randn_like(latents)
                    timesteps = torch.randint(0, 1000, (batch_size,), device=device).float()
                    
                    # æ·»åŠ å™ªå£° (DDPM forward process)
                    alpha_t = 1 - timesteps / 1000
                    alpha_t = alpha_t.view(-1, 1, 1, 1)
                    noisy_latents = torch.sqrt(alpha_t) * latents + torch.sqrt(1 - alpha_t) * noise
                    
                    # å‰å‘ä¼ æ’­ - FP16è‡ªåŠ¨æ··åˆç²¾åº¦
                    pred_noise = model(noisy_latents, timesteps, labels)
                    loss = criterion(pred_noise, noise) / gradient_accumulation_steps
                
                # ğŸ”§ å¼ºåŒ–çš„æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 100.0:
                    print(f"\nâš ï¸ æ£€æµ‹åˆ°ä¸ç¨³å®šæŸå¤±: {loss.item():.4f}ï¼Œè·³è¿‡æ‰¹æ¬¡")
                    # é™ä½ç¼©æ”¾å› å­
                    scaler._scale = scaler._scale * 0.5
                    optimizer.zero_grad()
                    torch.cuda.empty_cache()
                    continue
                
                # ğŸ”§ æ··åˆç²¾åº¦åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                epoch_loss += loss.item() * gradient_accumulation_steps
                
                if (step + 1) % gradient_accumulation_steps == 0:
                    # ğŸ”§ ç¨³å®šçš„æ··åˆç²¾åº¦æ¢¯åº¦æ›´æ–°
                    scaler.unscale_(optimizer)
                    
                    # ğŸ”§ æ¿€è¿›æ¢¯åº¦è£å‰ªå’Œç¨³å®šæ€§æ£€æŸ¥
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['max_grad_norm'])
                    
                    # ğŸ›¡ï¸ æ¢¯åº¦çˆ†ç‚¸æ£€æŸ¥ï¼ˆæ”¾å®½é˜ˆå€¼ï¼‰
                    if grad_norm > 100.0 or torch.isnan(grad_norm):
                        if rank == 0:
                            print(f"âš ï¸ æ¢¯åº¦çˆ†ç‚¸: {grad_norm:.2f}, è·³è¿‡æ›´æ–°")
                        scaler.update()
                        optimizer.zero_grad(set_to_none=True)  # é‡Šæ”¾æ¢¯åº¦å†…å­˜
                        continue
                    
                    # ğŸ§¹ å®šæœŸæ¸…ç†æ˜¾å­˜ç¢ç‰‡
                    if step % config['empty_cache_freq'] == 0:
                        torch.cuda.empty_cache()
                        if rank == 0 and step % 50 == 0:
                            mem_used = torch.cuda.memory_allocated(device) / 1024**3
                            print(f"ğŸ§¹ æ˜¾å­˜æ¸…ç† - å½“å‰ä½¿ç”¨: {mem_used:.1f}GB")
                    
                    # ğŸ“ˆ æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad(set_to_none=True)  # set_to_none=Trueé‡Šæ”¾æ›´å¤šå†…å­˜
                    
                    # æ›´æ–°è¿›åº¦æ¡å’Œæ¢¯åº¦ç›‘æ§ (åªåœ¨rank 0)
                    if rank == 0:
                        current_scale = scaler.get_scale()
                        progress_bar.set_postfix({
                            'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',
                            'grad_norm': f'{grad_norm:.2f}',
                            'scale': f'{current_scale:.0f}',
                            'gpu_mem': f'{torch.cuda.memory_allocated(device) / (1024**3):.1f}GB'
                        })
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"\nâš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œæ‰§è¡Œç´§æ€¥æ¸…ç†: {e}")
                    # ğŸ”§ ç´§æ€¥æ˜¾å­˜æ¸…ç†
                    if 'pred_noise' in locals(): del pred_noise
                    if 'noisy_latents' in locals(): del noisy_latents
                    if 'noise' in locals(): del noise
                    optimizer.zero_grad()
                    torch.cuda.empty_cache()
                    continue
                else:
                    raise e
        
        # Epochå®Œæˆ - åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        avg_loss = epoch_loss / max(len(dataloader), 1)
        scheduler.step()
        
        # å…¨å±€æŸå¤±å¹³å‡ (è·¨æ‰€æœ‰GPUs)
        if world_size > 1:
            loss_tensor = torch.tensor(avg_loss, device=device)
            dist.all_reduce(loss_tensor, op=dist.ReduceOp.AVG)
            avg_loss = loss_tensor.item()
        
        if rank == 0:
            print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"   å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
        
        # ğŸ” æ¯ä¸ªepochéªŒè¯è¯„ä¼° 
        if rank == 0:
            print(f"\nğŸ” Epoch {epoch+1} éªŒè¯è¯„ä¼°:")
            validation_loss = evaluate_model(model, vae_model, device, rank)
            print(f"   éªŒè¯æŸå¤±: {validation_loss:.4f}")
            print(f"   è®­ç»ƒæŸå¤±: {avg_loss:.4f}")
            print(f"   å·®å¼‚: {abs(validation_loss - avg_loss):.4f}")
            
            # ğŸ† æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹å¹¶ä¿å­˜
            if validation_loss < best_val_loss:
                best_val_loss = validation_loss
                
                # ğŸ—‘ï¸ åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹
                if best_model_path and os.path.exists(best_model_path):
                    os.remove(best_model_path)
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æœ€ä½³æ¨¡å‹: {os.path.basename(best_model_path)}")
                
                # ğŸ’¾ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
                best_model_path = f"/kaggle/working/best_dit_xl_ddp_epoch_{epoch+1}.pt"
                torch.save({
                    'model_state_dict': model.module.state_dict(),  # æ³¨æ„: model.module
                    'optimizer_state_dict': optimizer.state_dict(),
                    'epoch': epoch + 1,
                    'train_loss': avg_loss,
                    'val_loss': validation_loss,
                    'best_val_loss': best_val_loss,
                }, best_model_path)
                print(f"ğŸ† æ–°æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {validation_loss:.4f}")
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {os.path.basename(best_model_path)}")
            
            # ğŸ–¼ï¸ æ¯ä¸ªepochç”Ÿæˆæ¡ä»¶æ‰©æ•£æ ·æœ¬
            print(f"ğŸ¨ ç”ŸæˆéªŒè¯æ ·æœ¬...")
            generate_validation_samples(model, vae_model, device, epoch+1)
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if world_size > 1:
            dist.barrier()
        
        torch.cuda.empty_cache()
    
    # è®­ç»ƒå®Œæˆ
    if rank == 0:
        print("âœ… DDPè®­ç»ƒå®Œæˆ")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_save_path = "/kaggle/working/dit_xl_ddp_final.pt"
        torch.save({
            'model_state_dict': model.module.state_dict(),
            'config': config,
        }, final_save_path)
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹: {final_save_path}")
    
    # æ¸…ç†DDP
    cleanup_ddp()
    return model

def load_vae_model(device):
    """åŠ è½½VA-VAEæ¨¡å‹ - ä¼˜åŒ–ç‰ˆ"""
    rank = device.index if hasattr(device, 'index') else 0
    if rank == 0:
        print(f"ğŸ“‚ åŠ è½½VA-VAEæ¨¡å‹:")
    
    # å¯¼å…¥VA-VAE
    try:
        from LightningDiT.tokenizer.autoencoder import AutoencoderKL
        if rank == 0:
            print(f"   ğŸ“‹ ä½¿ç”¨çœŸæ­£çš„VA-VAEå®ç°")
    except ImportError as e:
        if rank == 0:
            print(f"   âŒ æ— æ³•å¯¼å…¥AutoencoderKL: {e}")
        return None
    
    vae_checkpoint_path = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
    if not os.path.exists(vae_checkpoint_path):
        if rank == 0:
            print(f"   âŒ æœªæ‰¾åˆ°VA-VAEæƒé‡æ–‡ä»¶: {vae_checkpoint_path}")
        return None
    
    try:
        # VA-VAEä¹Ÿé‡‡ç”¨CPUå…ˆåŠ è½½ç­–ç•¥
        if rank == 0:
            print(f"   ğŸ”„ CPUå…ˆåŠ è½½VA-VAEï¼Œå†è½¬ç§»åˆ°GPU{rank}...")
            
        vae_model = AutoencoderKL(
            embed_dim=32,
            ch_mult=(1, 1, 2, 2, 4),
            use_variational=True,
            ckpt_path=vae_checkpoint_path,
            model_type='vavae'
        )
        vae_model.eval().to(device)  # CPU â†’ GPUè½¬ç§»
        
        if rank == 0:
            print(f"   âœ… VA-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   ğŸ“Š æ¨¡å‹å‚æ•°: embed_dim=32, ä¸‹é‡‡æ ·=16x")
        return vae_model
    except Exception as e:
        if rank == 0:
            print(f"   âŒ VA-VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def create_distributed_dataloader(config, rank, world_size):
    """åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨"""
    if rank == 0:
        print(f"ğŸ“Š åˆ›å»ºåˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨:")
    
    batch_size = config.get('batch_size', 1)
    num_workers = config.get('num_workers', 2)  # æ¯GPU 2ä¸ªworker
    
    # åŠ è½½æ•°æ®
    official_train_dir = Path("/kaggle/working/latents_official/vavae_config_for_dit/microdoppler_train_256")
    official_train_file = official_train_dir / "latents_rank00_shard000.safetensors"
    
    if not official_train_file.exists():
        if rank == 0:
            print(f"   âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®: {official_train_file}")
        return None
        
    try:
        latents_data = safetensors.load_file(str(official_train_file))
        latents = latents_data['latents']  
        labels = latents_data['labels']

        # ğŸ”§ åº”ç”¨å®˜æ–¹LightningDiTå½’ä¸€åŒ–ç­–ç•¥
        # latent_norm=true: æ ‡å‡†åŒ–åˆ°mean=0, std=1
        # latent_multiplier=1.0: ä¿æŒæ ‡å‡†å·®ä¸º1.0

        # æ£€æŸ¥æ˜¯å¦æœ‰latents_stats.ptæ–‡ä»¶ï¼ˆå®˜æ–¹é¢„å¤„ç†çš„ç»Ÿè®¡ä¿¡æ¯ï¼‰
        stats_file = official_train_dir / "latents_stats.pt"
        if stats_file.exists():
            if rank == 0:
                print(f"   ğŸ“Š åŠ è½½å®˜æ–¹latentç»Ÿè®¡ä¿¡æ¯: {stats_file}")
            stats = torch.load(stats_file, map_location='cpu')
            latent_mean = stats['mean']
            latent_std = stats['std']
        else:
            # è®¡ç®—å½“å‰æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
            if rank == 0:
                print(f"   ğŸ“Š è®¡ç®—latentç»Ÿè®¡ä¿¡æ¯...")
            latent_mean = latents.mean(dim=0, keepdim=True)
            latent_std = latents.std(dim=0, keepdim=True)

        # åº”ç”¨å®˜æ–¹å½’ä¸€åŒ–: (latent - mean) / std * multiplier
        if rank == 0:
            print(f"   ğŸ”§ åº”ç”¨å®˜æ–¹latent_norm=true, latent_multiplier=1.0")
            print(f"   ğŸ“ˆ åŸå§‹latentèŒƒå›´: [{latents.min():.4f}, {latents.max():.4f}]")

        latents = (latents - latent_mean) / latent_std  # æ ‡å‡†åŒ–
        latents = latents * 1.0  # latent_multiplier=1.0

        if rank == 0:
            print(f"   ğŸ“ˆ å½’ä¸€åŒ–åèŒƒå›´: [{latents.min():.4f}, {latents.max():.4f}]")
            print(f"   âœ… æˆåŠŸåŠ è½½å¹¶å½’ä¸€åŒ–è®­ç»ƒæ•°æ®")
            print(f"   ğŸ“Š Latentså½¢çŠ¶: {latents.shape}")
            print(f"   ğŸ“Š Labelså½¢çŠ¶: {labels.shape}")
            print(f"   ğŸ“Š ç±»åˆ«èŒƒå›´: {labels.min().item()} - {labels.max().item()}")

        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        dataset = MicroDopplerLatentDataset(latents, labels)
        sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True) if world_size > 1 else None
        dataloader = DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=(sampler is None), 
            sampler=sampler,
            num_workers=0, 
            pin_memory=True
        )
        return dataloader

    except Exception as e:
        if rank == 0:
            print(f"   âš ï¸ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def main():
    """ä¸»å‡½æ•° - å¯åŠ¨DDPè®­ç»ƒ"""
    print("ğŸ¯ DiT XL DDPå¾®è°ƒè„šæœ¬ (Kaggle T4x2)")
    print("="*50)
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDA")
        return
    
    world_size = torch.cuda.device_count()
    print(f"ğŸ”§ æ£€æµ‹åˆ° {world_size} ä¸ªGPU")
    
    if world_size < 2:
        print("âš ï¸ å»ºè®®ä½¿ç”¨2ä¸ªGPUè¿›è¡ŒDDPè®­ç»ƒ")
        world_size = 1  # å•GPUå›é€€
    
    for i in range(world_size):
        props = torch.cuda.get_device_properties(i)
        print(f"   GPU{i}: {props.name} - {props.total_memory/1024**3:.1f}GB")
    
    # ğŸ”§ ç¨³å®šæ€§ä¼˜å…ˆçš„DDPé…ç½®
    config = {
        'learning_rate': 5e-6,  # ç¨³å®šçš„å­¦ä¹ ç‡
        'num_epochs': 15,
        'batch_size': 1,        # æ¯GPUæ‰¹æ¬¡å¤§å°
        'save_interval': 5,     # å‡å°‘IOå¼€é”€
        'gradient_accumulation_steps': 12, # æ›´å¤§ç´¯ç§¯æ­¥æ•°å‡å°‘æ˜¾å­˜å‹åŠ›
        'num_workers': 0,       # ç¦ç”¨å¤šè¿›ç¨‹å‡å°‘å†…å­˜
        'warmup_steps': 100,    # æ·»åŠ çƒ­èº«é˜¶æ®µ
        'use_fp32': False,      # è®¾ä¸ºTrueå³å¯å›é€€FP32è®­ç»ƒ
        'max_grad_norm': 5.0,   # æ”¾å®½æ¢¯åº¦è£å‰ªé˜ˆå€¼
        'empty_cache_freq': 10, # æ¯10æ­¥æ¸…ç†ç¼“å­˜
    }
    
    print(f"\nğŸ“‹ DDPé…ç½® (ç¨³å®šæ€§ä¼˜å…ˆ):")
    print(f"   World Size: {world_size}")
    print(f"   Backend: NCCL")
    effective_batch = config['batch_size'] * config['gradient_accumulation_steps'] * world_size
    print(f"   æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch} ({config['batch_size']}Ã—{config['gradient_accumulation_steps']}Ã—{world_size})")
    for key, value in config.items():
        if key != 'batch_size' and key != 'gradient_accumulation_steps':
            print(f"   {key}: {value}")
    
    if world_size > 1:
        # å¯åŠ¨DDPå¤šè¿›ç¨‹è®­ç»ƒ
        print(f"\nğŸš€ å¯åŠ¨DDPå¤šè¿›ç¨‹è®­ç»ƒ...")
        mp.spawn(
            train_ddp_worker,
            args=(world_size, config),
            nprocs=world_size,
            join=True
        )
    else:
        # å•GPUè®­ç»ƒ
        print(f"\nğŸš€ å¯åŠ¨å•GPUè®­ç»ƒ...")
        train_ddp_worker(0, 1, config)
    
    print(f"\nğŸ‰ DDPè®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()

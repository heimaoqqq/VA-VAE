#!/usr/bin/env python3
"""
æ··åˆç­–ç•¥ï¼šæ ‡å‡†DiTè®­ç»ƒ + å¯é€‰çš„ç”¨æˆ·åŒºåˆ†å¢å¼º
å…¼é¡¾åŸé¡¹ç›®æ€æƒ³å’Œä»»åŠ¡ç‰¹å®šéœ€æ±‚
ä¼˜åŒ–for Kaggle T4*2 GPUåˆ†å¸ƒå¼è®­ç»ƒ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
from torch.optim.lr_scheduler import CosineAnnealingLR

# T4*2 GPUä¼˜åŒ–é…ç½® - å¯ç”¨Tritonç¼–è¯‘å™¨å’ŒTensorCore
torch.backends.cuda.matmul.allow_tf32 = True  # T4 TensorCoreä¼˜åŒ–
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–å›ºå®šè¾“å…¥å¤§å°

# Kaggle T4*2 åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

from datetime import datetime
from tqdm import tqdm
from PIL import Image
import yaml
import numpy as np
import argparse
import logging
from pathlib import Path

# æ·»åŠ LightningDiTåˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'LightningDiT'))

# å…ˆå¤„ç†fairscaleä¾èµ–é—®é¢˜
import torch.nn as nn

# åˆ›å»ºfairscaleçš„mockæ›¿ä»£ï¼Œé¿å…å¯¼å…¥é”™è¯¯
class MockFairscaleModule:
    def __init__(self, *args, **kwargs):
        pass
    
    def __call__(self, *args, **kwargs):
        return None

# Mockç¼ºå¤±çš„ä¾èµ–æ¨¡å—
import sys
import types

# Mock torchdiffeq
def mock_odeint(func, y0, t, **kwargs):
    """ç®€å•çš„æ¬§æ‹‰æ³•mockå®ç°"""
    result = [y0]
    for i in range(1, len(t)):
        dt = t[i] - t[i-1]
        dy = func(t[i-1], result[-1])
        result.append(result[-1] + dt * dy)
    return torch.stack(result)

if 'torchdiffeq' not in sys.modules:
    torchdiffeq_mock = types.ModuleType('torchdiffeq')
    torchdiffeq_mock.odeint = mock_odeint
    sys.modules['torchdiffeq'] = torchdiffeq_mock

if 'fairscale' not in sys.modules:
    fairscale_mock = types.ModuleType('fairscale')
    fairscale_mock.nn = types.ModuleType('nn')
    fairscale_mock.nn.model_parallel = types.ModuleType('model_parallel')
    fairscale_mock.nn.model_parallel.initialize = MockFairscaleModule()
    fairscale_mock.nn.model_parallel.layers = types.ModuleType('layers')
    fairscale_mock.nn.model_parallel.layers.ColumnParallelLinear = nn.Linear
    fairscale_mock.nn.model_parallel.layers.RowParallelLinear = nn.Linear
    fairscale_mock.nn.model_parallel.layers.ParallelEmbedding = nn.Embedding
    sys.modules['fairscale'] = fairscale_mock
    sys.modules['fairscale.nn'] = fairscale_mock.nn
    sys.modules['fairscale.nn.model_parallel'] = fairscale_mock.nn.model_parallel
    sys.modules['fairscale.nn.model_parallel.initialize'] = fairscale_mock.nn.model_parallel.initialize
    sys.modules['fairscale.nn.model_parallel.layers'] = fairscale_mock.nn.model_parallel.layers

try:
    from models.lightningdit import LightningDiT_models
    from tokenizer.vavae import VA_VAE
    from transport import create_transport
    print("âœ… æˆåŠŸå¯¼å…¥LightningDiTæ¨¡å‹")
except ImportError as e:
    print(f"Error importing LightningDiT models: {e}")
    exit(1)

# ç›´æ¥åœ¨æ­¤æ–‡ä»¶ä¸­å®šä¹‰ï¼Œé¿å…å¯¼å…¥ä¾èµ–é—®é¢˜
# from step6_standard_dit_training import MicroDopplerLatentDataset, create_logger

from torch.utils.data import Dataset
import logging


class MicroDopplerDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›†"""
    
    def __init__(self, data_dir, image_size=256, split="train", train_ratio=0.8, vae=None, device=None):
        self.data_dir = Path(data_dir)
        self.split = split
        self.samples = []
        
        # è·å–æ‰€æœ‰ç”¨æˆ·çš„å›¾ç‰‡
        for user_dir in sorted(self.data_dir.iterdir()):
            if user_dir.is_dir() and user_dir.name.startswith("ID_"):
                user_id = int(user_dir.name.split("_")[1]) - 1  # 0-based indexing
                images = list(user_dir.glob("*.jpg"))
                
                # åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
                n_train = int(len(images) * train_ratio)
                if split == "train":
                    selected = images[:n_train]
                else:
                    selected = images[n_train:]
                
                for img_path in selected:
                    self.samples.append({
                        "path": img_path,
                        "class_id": user_id  # 0-basedç”¨æˆ·ID
                    })
        
        print(f"{split}é›†: {len(self.samples)}ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡
        img = Image.open(sample["path"]).convert("RGB")
        img = transforms.ToTensor()(img)  # [3, H, W], èŒƒå›´[0,1]
        img = (img * 2.0) - 1.0  # è½¬æ¢åˆ°[-1, 1]
        
        # è¿”å›åŸå›¾ï¼ŒVAEç¼–ç å°†åœ¨ä¸»è¿›ç¨‹GPUä¸Šè¿›è¡Œ
        return {
            "image": img,
            "class_id": sample["class_id"]
        }


def create_logger(logging_dir):
    """åˆ›å»ºæ—¥å¿—è®°å½•å™¨"""
    logging.basicConfig(
        level=logging.INFO,
        format='[\033[34m%(asctime)s\033[0m] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f"{logging_dir}/log.txt")
        ]
    )
    logger = logging.getLogger(__name__)
    return logger


class UserDiscriminationLoss(torch.nn.Module):
    """å¯é€‰çš„ç”¨æˆ·åŒºåˆ†æŸå¤± - è½»é‡çº§å®ç°"""
    
    def __init__(self, temperature=0.1, weight=0.1):
        super().__init__()
        self.temperature = temperature
        self.weight = weight
    
    def forward(self, features, user_ids):
        """
        ç®€å•çš„InfoNCEæŸå¤±
        features: (B, D) ç‰¹å¾å‘é‡
        user_ids: (B,) ç”¨æˆ·ID
        """
        if features.dim() > 2:
            features = features.flatten(1)  # å±•å¹³ç‰¹å¾
        
        # å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1)
        
        # ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.mm(features, features.t()) / self.temperature
        
        # åˆ›å»ºæ­£æ ·æœ¬maskï¼ˆåŒç”¨æˆ·ï¼‰
        user_ids = user_ids.unsqueeze(0)
        pos_mask = (user_ids == user_ids.t()).float()
        
        # ç§»é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±å’Œè‡ªå·±ï¼‰
        pos_mask.fill_diagonal_(0)
        
        # å¦‚æœbatchä¸­æ²¡æœ‰åŒç”¨æˆ·çš„å…¶ä»–æ ·æœ¬ï¼Œè¿”å›0
        if pos_mask.sum() == 0:
            return torch.tensor(0.0, device=features.device, requires_grad=True)
        
        # ç®€åŒ–çš„å¯¹æ¯”æŸå¤±
        pos_logits = sim_matrix * pos_mask
        neg_logits = sim_matrix * (1 - pos_mask)
        
        # è®¡ç®—æŸå¤±
        pos_loss = -torch.log(torch.exp(pos_logits).sum() / 
                             (torch.exp(pos_logits).sum() + torch.exp(neg_logits).sum()))
        
        return pos_loss * self.weight


# Kaggle T4*2 åˆ†å¸ƒå¼è®­ç»ƒè¾…åŠ©å‡½æ•°
def setup_distributed_training(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ - Kaggle T4*2ä¼˜åŒ–"""
    if world_size > 1:
        # Kaggleç¯å¢ƒç‰¹æ®Šé…ç½®
        os.environ['MASTER_ADDR'] = '127.0.0.1'  # ä½¿ç”¨127.0.0.1æ›´ç¨³å®š
        os.environ['MASTER_PORT'] = '29500'  # ä½¿ç”¨è¾ƒé«˜ç«¯å£é¿å…å†²çª
        os.environ['NCCL_DEBUG'] = 'WARN'  # å‡å°‘NCCLæ—¥å¿—
        os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'OFF'  # å…³é—­è°ƒè¯•æ—¥å¿—
        
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        ) 
        torch.cuda.set_device(rank)


def cleanup_distributed_training():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def hybrid_dit_train_worker(rank, world_size, config_path, use_user_loss=False, user_loss_weight=0.1):
    """åˆ†å¸ƒå¼è®­ç»ƒworkerè¿›ç¨‹ - Kaggle T4x2ä¼˜åŒ–"""
    # Kaggle T4x2æ ‡å‡†ç¯å¢ƒå˜é‡è®¾ç½®
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29500'
    os.environ['NCCL_DEBUG'] = 'WARN'
    os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'OFF'
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=world_size,
        rank=rank
    )
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU
    torch.cuda.set_device(rank)
    device = torch.device(f'cuda:{rank}')
    
    print(f"[GPU {rank}] åœ¨è®¾å¤‡ {device} ä¸Šå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ")
    print(f"[GPU {rank}] è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸ")
    
    # è°ƒç”¨åŸè®­ç»ƒå‡½æ•°ä½†æ·»åŠ åˆ†å¸ƒå¼æ”¯æŒ
    hybrid_dit_train(config_path, use_user_loss, user_loss_weight, rank, world_size, device)
    
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    cleanup_distributed_training()


def hybrid_dit_train(config_path='../configs/microdoppler_finetune.yaml', 
                     use_user_loss=True, user_loss_weight=0.1, 
                     rank=0, world_size=1, device=None):
    """
    æ··åˆè®­ç»ƒç­–ç•¥ï¼š
    - ä¸»ä½“ï¼šæ ‡å‡†LightningDiTè®­ç»ƒ
    - å¯é€‰ï¼šè½»é‡çº§ç”¨æˆ·åŒºåˆ†æŸå¤±
    """
    
    # åŠ è½½é…ç½® - ç›´æ¥æ„å»ºæ­£ç¡®çš„ç»å¯¹è·¯å¾„
    if not os.path.isabs(config_path):
        # ç›¸å¯¹äºå½“å‰è„šæœ¬æ–‡ä»¶çš„è·¯å¾„ï¼Œå‘ä¸Šä¸€çº§åˆ°VA-VAEæ ¹ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))  # /kaggle/working/VA-VAE/microdoppler_finetune
        va_vae_root = os.path.dirname(script_dir)  # /kaggle/working/VA-VAE
        config_path = os.path.join(va_vae_root, 'configs', 'microdoppler_finetune.yaml')
    
    print(f"Script directory: {os.path.dirname(os.path.abspath(__file__))}")
    print(f"VA-VAE root: {os.path.dirname(os.path.dirname(os.path.abspath(__file__)))}")
    print(f"Loading config from: {config_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        # å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•å¦ä¸€ä¸ªå¯èƒ½çš„è·¯å¾„
        alt_config_path = '/kaggle/working/VA-VAE/configs/microdoppler_finetune.yaml'
        print(f"Config file not found at {config_path}")
        print(f"Trying alternative path: {alt_config_path}")
        if os.path.exists(alt_config_path):
            config_path = alt_config_path
        else:
            raise FileNotFoundError(f"Config file not found at either {config_path} or {alt_config_path}")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # è®¾å¤‡é…ç½® - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    is_distributed = world_size > 1
    is_main_process = rank == 0
    
    if is_main_process:
        print(f"Using device: {device}")
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ”— åˆ†å¸ƒå¼è®­ç»ƒ: {'å¼€å¯' if world_size > 1 else 'å…³é—­'}")
    if world_size > 1:
        print(f"ğŸ“Š é›†ç¾¤å¤§å°: {world_size} GPUs")
        print(f"ğŸ·ï¸ å½“å‰è¿›ç¨‹: Rank {rank}")
    print(f"ğŸ‘¤ ç”¨æˆ·åŒºåˆ†æŸå¤±: {'å¼€å¯' if use_user_loss else 'å…³é—­'}")
    print("=" * 50)
    
    # åˆ›å»ºå®éªŒç›®å½•
    exp_name = f"hybrid_dit_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    exp_dir = Path(f"experiments/{exp_name}")
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    logger = create_logger(exp_dir)
    logger.info(f"Experiment directory: {exp_dir}")
    logger.info(f"User loss enabled: {use_user_loss}, weight: {user_loss_weight}")
    
    # åŠ è½½å¾®è°ƒåçš„VA-VAE - ä½¿ç”¨Stage 3è®­ç»ƒå¥½çš„æ¨¡å‹
    logger.info("=== åŠ è½½å¾®è°ƒåçš„VA-VAEç¼–ç å™¨ ===")
    
    # ä½¿ç”¨å¾®è°ƒåçš„checkpointè·¯å¾„
    vae_checkpoint_path = '/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(vae_checkpoint_path):
        logger.error(f"å¾®è°ƒåçš„VA-VAEæ¨¡å‹ä¸å­˜åœ¨: {vae_checkpoint_path}")
        logger.error("è¯·ç¡®ä¿å·²æ·»åŠ stage3æ•°æ®é›†ä½œä¸ºè¾“å…¥")
        raise FileNotFoundError(f"VA-VAE checkpoint not found: {vae_checkpoint_path}")
    
    logger.info(f"åŠ è½½å¾®è°ƒåçš„VA-VAE: {vae_checkpoint_path}")
    
    # å‡†å¤‡VA-VAEé…ç½®æ–‡ä»¶è·¯å¾„
    vae_config_path = os.path.join(va_vae_root, 'LightningDiT', 'tokenizer', 'configs', 'vavae_f16d32.yaml')
    
    # ä¿®å¤é…ç½®ä¸­çš„checkpointè·¯å¾„ - ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
    import tempfile
    from omegaconf import OmegaConf
    
    # åŠ è½½å¹¶ä¿®æ”¹é…ç½®
    vae_config = OmegaConf.load(vae_config_path)
    vae_config.ckpt_path = vae_checkpoint_path  # ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as tmp_config:
        OmegaConf.save(vae_config, tmp_config.name)
        tmp_config_path = tmp_config.name
    
    # åˆå§‹åŒ–VA-VAE - æŒ‰ç…§å®˜æ–¹æ–¹å¼
    vae = VA_VAE(tmp_config_path, img_size=256, fp16=True)
    vae.model.eval()  # VA-VAEå†…éƒ¨å·²ç»è°ƒç”¨äº†.cuda()ï¼Œä¸éœ€è¦é¢å¤–çš„.to(device)
    
    logger.info("âœ… æˆåŠŸåŠ è½½å¾®è°ƒåçš„VA-VAEæ¨¡å‹")
    logger.info("  - è¯¥æ¨¡å‹ç»è¿‡3é˜¶æ®µè®­ç»ƒä¼˜åŒ–")
    logger.info("  - VFè¯­ä¹‰ç›¸ä¼¼åº¦ > 0.987")
    logger.info("  - ä¸“é—¨é’ˆå¯¹å¾®å¤šæ™®å‹’æ•°æ®ä¼˜åŒ–")
    
    # åŠ è½½DiTæ¨¡å‹ - éµå¾ªå®˜æ–¹LightningDiTé¡¹ç›®ç»“æ„
    logger.info("=== åŠ è½½LightningDiTæ¨¡å‹ ===")
    pretrained_path = os.path.join(va_vae_root, 'LightningDiT', 'models', 'lightningdit-xl-imagenet256-64ep.pt')
    
    if not os.path.exists(pretrained_path):
        logger.warning(f"é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {pretrained_path}")
        logger.warning("è¯·ç¡®ä¿å·²ä¸‹è½½LightningDiTé¢„è®­ç»ƒæƒé‡")
    
    # åˆå§‹åŒ–DiT - éµå¾ªå®˜æ–¹LightningDiTæ ‡å‡†
    logger.info("=== åˆå§‹åŒ–LightningDiT ===")
    num_classes = config['model']['params']['num_users']  # 31ä¸ªç”¨æˆ·
    
    # ä½¿ç”¨å®˜æ–¹LightningDiTæ¨¡å‹å·¥å‚
    model = LightningDiT_models["LightningDiT-XL/1"](
        num_classes=num_classes,
        in_channels=32,  # VA-VAEæ½œç©ºé—´é€šé“æ•°
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if os.path.exists(pretrained_path):
        logger.info(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
        checkpoint = torch.load(pretrained_path, map_location='cpu')
        state_dict = checkpoint.get('model', checkpoint)
        # æ¸…ç†state_dicté”®å
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        if is_main_process:
            logger.info(f"ç¼ºå¤±é”®: {len(missing)}, æ„å¤–é”®: {len(unexpected)}")
    else:
        logger.warning(f"é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {pretrained_path}")
        logger.warning("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿›è¡Œè®­ç»ƒ")
    
    model.to(device)
    
    # æ··åˆç²¾åº¦è®­ç»ƒé…ç½® - T4ä¼˜åŒ–
    use_amp = True  # T4 GPUæ”¯æŒFP16
    scaler = torch.amp.GradScaler('cuda') if use_amp else None  # ä¿®å¤åºŸå¼ƒè­¦å‘Š
    
    # åˆ†å¸ƒå¼æ¨¡å‹åŒ…è£… - ç¬¦åˆå®˜æ–¹DDPæœ€ä½³å®è·µ
    if is_distributed:
        model = torch.nn.parallel.DistributedDataParallel(
            model,
            device_ids=[rank],
            output_device=rank,
            find_unused_parameters=False  # æå‡æ€§èƒ½
        )
    if is_main_process:
        logger.info(f"Model wrapped with DDP on rank {rank}")
    
    # å¯é€‰çš„ç”¨æˆ·åŒºåˆ†æŸå¤±
    user_loss_fn = UserDiscriminationLoss(weight=user_loss_weight) if use_user_loss else None
    
    # åˆ›å»ºEMA
    from copy import deepcopy
    def requires_grad(model, flag=True):
        for p in model.parameters():
            p.requires_grad = flag
    
    ema = deepcopy(model).to(device)
    requires_grad(ema, False)
    
    # åˆ›å»ºTransportå¯¹è±¡ - éµå¾ªå®˜æ–¹LightningDiTæµåŒ¹é…æŸå¤±é…ç½®
    transport = create_transport(
        path_type='Linear',
        prediction="velocity",  # å®˜æ–¹æ ‡å‡†ï¼šé€Ÿåº¦é¢„æµ‹
        loss_weight=None,
        train_eps=None,
        sample_eps=None,
        use_cosine_loss=True,  # å®˜æ–¹æ¨èè®¾ç½®
        use_lognorm=True,  # å®˜æ–¹æ¨èè®¾ç½®
    )
    
    # æ•°æ®é›†
    logger.info("=== å‡†å¤‡æ•°æ®é›† ===")
    # åˆ›å»ºæ•°æ®é›† - ä¸åœ¨Datasetä¸­è¿›è¡ŒVAEç¼–ç ï¼Œæ”¯æŒå¤šè¿›ç¨‹åŠ è½½
    data_dir = config['data']['params']['data_dir']
    val_split = config['data']['params']['val_split']
    train_ratio = 1.0 - val_split
    
    train_dataset = MicroDopplerDataset(
        data_dir=data_dir, 
        split="train", 
        train_ratio=train_ratio
    )
    val_dataset = MicroDopplerDataset(
        data_dir=data_dir, 
        split="val", 
        train_ratio=train_ratio
    )
    
    # T4*2ä¼˜åŒ–çš„batch_sizeé…ç½®
    base_batch_size = config['data']['params']['batch_size']  # ä»é…ç½®æ–‡ä»¶è¯»å–
    batch_size = base_batch_size // world_size  # åˆ†å¸ƒå¼è®­ç»ƒæ—¶æ¯ä¸ªGPUçš„batch_size
    
    if is_main_process:
        logger.info(f"Total batch_size: {base_batch_size}, Per-GPU batch_size: {batch_size}")
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨é…ç½®
    train_sampler = DistributedSampler(
        train_dataset, 
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    ) if is_distributed else None
    
    val_sampler = DistributedSampler(
        val_dataset,
        num_replicas=world_size, 
        rank=rank,
        shuffle=False
    ) if is_distributed else None
    
    # ç‰¹æ®Šé‡‡æ ·ï¼šç¡®ä¿æ¯ä¸ªbatchåŒ…å«å¤šä¸ªç”¨æˆ·ï¼ˆå¯¹æ¯”æŸå¤±éœ€è¦ï¼‰
    if use_user_loss and not is_distributed:
        from torch.utils.data.sampler import BatchSampler, RandomSampler
        
        # åˆ›å»ºå¹³è¡¡é‡‡æ ·å™¨
        class BalancedBatchSampler:
            def __init__(self, dataset, batch_size):
                self.dataset = dataset
                self.batch_size = batch_size
                
                # æŒ‰ç”¨æˆ·åˆ†ç»„æ ·æœ¬ç´¢å¼•
                self.user_to_indices = {}
                for idx, sample in enumerate(dataset.samples):
                    user_id = sample['class_id']
                    if user_id not in self.user_to_indices:
                        self.user_to_indices[user_id] = []
                    self.user_to_indices[user_id].append(idx)
                
                self.num_users = len(self.user_to_indices)
            
            def __iter__(self):
                while True:
                    # æ¯ä¸ªbatchå°è¯•åŒ…å«å¤šä¸ªä¸åŒç”¨æˆ·
                    batch_indices = []
                    users_in_batch = min(self.batch_size, self.num_users)
                    selected_users = np.random.choice(list(self.user_to_indices.keys()), 
                                                    users_in_batch, replace=False)
                    
                    samples_per_user = self.batch_size // users_in_batch
                    for user_id in selected_users:
                        user_indices = self.user_to_indices[user_id]
                        selected = np.random.choice(user_indices, samples_per_user, replace=True)
                        batch_indices.extend(selected)
                    
                    # å¡«å……åˆ°batch_size
                    while len(batch_indices) < self.batch_size:
                        random_user = np.random.choice(list(self.user_to_indices.keys()))
                        random_idx = np.random.choice(self.user_to_indices[random_user])
                        batch_indices.append(random_idx)
                    
                    yield batch_indices[:self.batch_size]
            
            def __len__(self):
                return len(self.dataset) // self.batch_size
        
        train_sampler = BalancedBatchSampler(train_dataset, batch_size)
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_sampler=train_sampler,
            num_workers=0,
            pin_memory=True
        )
    else:
        # æ ‡å‡†éšæœºé‡‡æ · - æ¢å¤å¤šè¿›ç¨‹æ•°æ®åŠ è½½
        num_workers = config['data']['params'].get('num_workers', 4)
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=train_sampler,
            shuffle=(train_sampler is None),  # æ²¡æœ‰sampleræ—¶æ‰shuffle
            num_workers=num_workers // world_size if is_distributed else num_workers,
            pin_memory=True,
            persistent_workers=(num_workers > 0)
        )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        sampler=val_sampler,
        shuffle=False,
        num_workers=num_workers // world_size if is_distributed else num_workers,
        pin_memory=True,
        persistent_workers=(num_workers > 0)
    )
    
    # ä¼˜åŒ–å™¨ - éµå¾ªå®˜æ–¹LightningDiTé…ç½®
    lr = 1e-4  # å®˜æ–¹å¾®è°ƒå­¦ä¹ ç‡
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=lr,
        weight_decay=0,  # å®˜æ–¹æ ‡å‡†ï¼šweight_decay=0
        betas=(0.9, 0.95)  # å®˜æ–¹æ ‡å‡†beta2=0.95
    )
    logger.info(f"Optimizer: AdamW, lr={lr}, beta2=0.95, weight_decay=0")
    
    # è®­ç»ƒå¾ªç¯
    logger.info("=== å¼€å§‹è®­ç»ƒ ===")
    num_epochs = config['trainer']['max_epochs']
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        logger.info(f"--- Epoch {epoch+1}/{num_epochs} ---")
        
        # è®­ç»ƒ
        model.train()
        train_losses = {'total': [], 'diffusion': [], 'user': []}
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc="è®­ç»ƒä¸­")):
            # è·å–å›¾åƒå¹¶è¿›è¡ŒVAEç¼–ç 
            images = batch['image'].to(device)  # [B, 3, 256, 256]
            class_ids = batch['class_id'].to(device)
            
            # VA-VAEç¼–ç  - åœ¨ä¸»è¿›ç¨‹GPUä¸Šæ‰§è¡Œ
            with torch.no_grad():
                if rank == 0 or not is_distributed:
                    # rank 0æˆ–å•GPUæ¨¡å¼ï¼šç›´æ¥ç¼–ç 
                    latents = vae.encode_images(images)  # [B, 32, 16, 16]
                else:
                    # å…¶ä»–rankï¼šéœ€è¦å°†æ•°æ®å‘åˆ°cuda:0ç¼–ç åå†è¿”å›
                    images_cuda0 = images.to('cuda:0')
                    latents = vae.encode_images(images_cuda0)
                    latents = latents.to(device)  # ç§»å›å½“å‰è®¾å¤‡
            
            # æ ‡å‡†æµåŒ¹é…è®­ç»ƒ - ä½¿ç”¨å®˜æ–¹Transport API
            with torch.cuda.amp.autocast(enabled=use_amp):
                model_kwargs = dict(y=class_ids)
                loss_dict = transport.training_losses(model, latents, model_kwargs)
                
                # å¤„ç†cos_lossï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if 'cos_loss' in loss_dict:
                    diffusion_loss = loss_dict["loss"].mean()
                    cos_loss = loss_dict["cos_loss"].mean()
                    diffusion_loss = diffusion_loss + cos_loss
                else:
                    diffusion_loss = loss_dict["loss"].mean()
            
            # å¯é€‰çš„ç”¨æˆ·åŒºåˆ†æŸå¤±
            total_loss = diffusion_loss
            user_loss_value = 0.0
            
            if use_user_loss and user_loss_fn is not None:
                # è·å–æ¨¡å‹ç‰¹å¾ç”¨äºç”¨æˆ·åŒºåˆ†
                # éœ€è¦å•ç‹¬å‰å‘ä¼ æ’­è·å–ä¸­é—´ç‰¹å¾
                with torch.cuda.amp.autocast(enabled=use_amp):
                    t = torch.rand(latents.shape[0], device=device)
                    x_1 = torch.randn_like(latents)
                    x_t = t.view(-1, 1, 1, 1) * x_1 + (1 - t.view(-1, 1, 1, 1)) * latents
                    model_features = model(x_t, t * 1000, class_ids)
                    user_loss_value = user_loss_fn(model_features, class_ids)
                total_loss = diffusion_loss + user_loss_value
            
            # åå‘ä¼ æ’­ - æ”¯æŒæ··åˆç²¾åº¦
            optimizer.zero_grad()
            if use_amp:
                scaler.scale(total_loss).backward()
                scaler.unscale_(optimizer)
                # æ¢¯åº¦è£å‰ª
                if config['trainer']['gradient_clip_val'] > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['trainer']['gradient_clip_val'])
                scaler.step(optimizer)
                scaler.update()
            else:
                total_loss.backward()
                # æ¢¯åº¦è£å‰ª
                if config['trainer']['gradient_clip_val'] > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['trainer']['gradient_clip_val'])
                optimizer.step()
            
            # EMAæ›´æ–°
            with torch.no_grad():
                for ema_param, param in zip(ema.parameters(), model.parameters()):
                    ema_param.data.mul_(0.9999).add_(param.data, alpha=0.0001)
            
            # è®°å½•æŸå¤±
            train_losses['total'].append(total_loss.item())
            train_losses['diffusion'].append(diffusion_loss.item())
            train_losses['user'].append(user_loss_value.item() if isinstance(user_loss_value, torch.Tensor) else user_loss_value)
            
            if batch_idx % 50 == 0:
                if use_user_loss:
                    logger.info(f"  Batch {batch_idx}: Total={total_loss.item():.4f} "
                              f"(Diff={diffusion_loss.item():.4f}, User={user_loss_value:.4f})")
                else:
                    logger.info(f"  Batch {batch_idx}: Loss={diffusion_loss.item():.4f}")
        
        # è®­ç»ƒç»Ÿè®¡
        avg_total = np.mean(train_losses['total'])
        avg_diff = np.mean(train_losses['diffusion'])
        avg_user = np.mean(train_losses['user'])
        
        logger.info(f"Train - Total: {avg_total:.4f}, Diffusion: {avg_diff:.4f}, User: {avg_user:.4f}")
        
        # éªŒè¯ï¼ˆä»…ä½¿ç”¨æ‰©æ•£æŸå¤±ï¼‰
        if epoch % config['trainer']['check_val_every_n_epoch'] == 0:
            model.eval()
            val_losses = []
            
            with torch.no_grad():
                for batch in tqdm(val_loader, desc="Validation"):
                    # è·å–å›¾åƒå¹¶è¿›è¡ŒVAEç¼–ç 
                    images = batch['image'].to(device)
                    class_ids = batch['class_id'].to(device)
                    
                    # VA-VAEç¼–ç 
                    if rank == 0 or not is_distributed:
                        latents = vae.encode_images(images)
                    else:
                        images_cuda0 = images.to('cuda:0')
                        latents = vae.encode_images(images_cuda0)
                        latents = latents.to(device)
                    
                    t = torch.rand(latents.shape[0], device=device)
                    x_1 = torch.randn_like(latents)
                    x_t = t.view(-1, 1, 1, 1) * x_1 + (1 - t.view(-1, 1, 1, 1)) * latents
                    target = x_1 - latents
                    
                    model_output = model(x_t, t * 1000, class_ids)
                    loss = F.mse_loss(model_output, target)
                    val_losses.append(loss.item())
            
            avg_val_loss = np.mean(val_losses)
            logger.info(f"Validation Loss: {avg_val_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save({
                    'model': model.state_dict(),
                    'ema': ema.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'epoch': epoch,
                    'val_loss': avg_val_loss,
                    'config': config,
                    'use_user_loss': use_user_loss,
                    'user_loss_weight': user_loss_weight
                }, exp_dir / 'best_model.pt')
                logger.info(f"âœ… Saved best model (val_loss={avg_val_loss:.4f})")
        
        # å®šæœŸä¿å­˜
        if (epoch + 1) % 10 == 0 and is_main_process:
            torch.save({
                'model': model.module.state_dict() if is_distributed else model.state_dict(),
                'ema': ema.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch,
                'config': config
            }, exp_dir / f'checkpoint_epoch_{epoch+1}.pt')
    
    if is_main_process:
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    return exp_dir


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Hybrid DiT Training for Micro-Doppler")
    parser.add_argument("--config", type=str, default="../configs/microdoppler_finetune.yaml")
    parser.add_argument("--use_user_loss", action="store_true", help="Enable user discrimination loss")
    parser.add_argument("--user_loss_weight", type=float, default=0.1, help="Weight for user loss")
    parser.add_argument("--distributed", action="store_true", help="Enable T4*2 distributed training")
    parser.add_argument("--world_size", type=int, default=2, help="Number of GPUs for distributed training")
    args = parser.parse_args()
    
    # æ£€æµ‹GPUæ•°é‡å¹¶è‡ªåŠ¨é…ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    num_gpus = torch.cuda.device_count()
    print(f"ğŸ” æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU")
    
    if args.distributed and num_gpus >= 2:
        print("ğŸš€ å¯åŠ¨Kaggle T4x2åˆ†å¸ƒå¼è®­ç»ƒ...")
        print(f"ğŸ“Š GPUé…ç½®:")
        for i in range(num_gpus):
            props = torch.cuda.get_device_properties(i)
            print(f"   GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f}GB)")
        print(f"ğŸŒ è¿›ç¨‹æ•°: {args.world_size}")
        print("=" * 50)
        
        # è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(i) for i in range(num_gpus))
        
        # å¯åŠ¨T4*2åˆ†å¸ƒå¼è®­ç»ƒ
        mp.spawn(
            hybrid_dit_train_worker,
            args=(args.world_size, args.config, args.use_user_loss, args.user_loss_weight),
            nprocs=args.world_size,
            join=True
        )
    else:
        # å•GPUè®­ç»ƒ
        if args.distributed:
            print("âš ï¸ Distributed training requested but insufficient GPUs available")
            print("Falling back to single GPU training...")
        else:
            print("ğŸ¯ Starting Single GPU Training...")
        
        hybrid_dit_train(args.config, args.use_user_loss, args.user_loss_weight)

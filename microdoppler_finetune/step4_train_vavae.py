#!/usr/bin/env python3
"""
Step 4: VA-VAE å¾®å¤šæ™®å‹’å¾®è°ƒè®­ç»ƒ
åŸºäºLightningDiTåŸé¡¹ç›®çš„å®Œæ•´å®ç°
åŒ…å«ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å’ŒVision Foundationå¯¹é½
"""

import os
import sys
import argparse
from pathlib import Path
import json
import yaml
from datetime import datetime

# æ·»åŠ LightningDiTè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))
sys.path.insert(0, str(project_root))  # æ·»åŠ æ ¹ç›®å½•ä»¥å¯¼å…¥è‡ªå®šä¹‰æ•°æ®é›†

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from PIL import Image
import pytorch_lightning as pl
from pytorch_lightning import seed_everything
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.strategies import DDPStrategy

from omegaconf import OmegaConf
from ldm.util import instantiate_from_config
from ldm.models.autoencoder import AutoencoderKL
from main import DataModuleFromConfig  # ä»åŸé¡¹ç›®æ­£ç¡®å¯¼å…¥


class MicroDopplerDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›† - å…¼å®¹åŸé¡¹ç›®æ ¼å¼"""
    
    def __init__(self, data_root, split_file, split='train', image_size=256):
        self.data_root = Path(data_root)
        self.image_size = image_size
        self.split = split
        
        # åŠ è½½æ•°æ®åˆ’åˆ†
        with open(split_file, 'r') as f:
            split_data = json.load(f)
        
        self.samples = []
        data_list = split_data['train'] if split == 'train' else split_data['val']
        
        for item in data_list:
            img_path = self.data_root / item['path']
            if img_path.exists():
                self.samples.append({
                    'path': img_path,
                    'user_id': item['user_id']
                })
        
        print(f"âœ… {split}é›†: {len(self.samples)} å¼ å›¾åƒ")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        img = Image.open(sample['path']).convert('RGB')
        img = img.resize((self.image_size, self.image_size), Image.LANCZOS)
        
        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–åˆ°[-1, 1] (åŸé¡¹ç›®æ ‡å‡†)
        img_array = np.array(img).astype(np.float32) / 127.5 - 1.0
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)
        
        # è¿”å›åŸé¡¹ç›®æ ¼å¼
        return {'image': img_tensor}


# æ³¨æ„ï¼šåŸVA-VAEä¸åŒ…å«ç”¨æˆ·å¯¹æ¯”æŸå¤±
# VFå¯¹é½æœºåˆ¶ï¼ˆDINOv2ï¼‰å·²ç»æä¾›äº†è¶³å¤Ÿçš„è¯­ä¹‰åŒºåˆ†èƒ½åŠ›
# æ·»åŠ é¢å¤–çš„ç”¨æˆ·å¯¹æ¯”æŸå¤±å¯èƒ½ä¼šå¹²æ‰°åŸå§‹è®­ç»ƒç›®æ ‡


def get_training_strategy(args):
    """æ ¹æ®GPUé…ç½®é€‰æ‹©è®­ç»ƒç­–ç•¥"""
    if not torch.cuda.is_available():
        return 'auto'
    
    num_gpus = torch.cuda.device_count()
    
    if num_gpus == 1:
        return 'auto'
    elif num_gpus == 2 and args.kaggle_t4:
        # Kaggle T4Ã—2ç‰¹æ®Šé…ç½®
        print("ğŸ”§ ä½¿ç”¨Kaggle T4Ã—2 DDPç­–ç•¥")
        return DDPStrategy(
            find_unused_parameters=True,
            static_graph=False,  # T4å¯èƒ½éœ€è¦åŠ¨æ€å›¾
            gradient_as_bucket_view=True
        )
    else:
        # é€šç”¨å¤šGPUé…ç½®
        return 'ddp_find_unused_parameters_true'


def create_stage_config(args, stage, checkpoint_path=None):
    """åˆ›å»ºé˜¶æ®µé…ç½® - å®Œå…¨å…¼å®¹åŸé¡¹ç›®"""
    
    # åŸºäºåŸé¡¹ç›®çš„ä¸‰é˜¶æ®µå‚æ•°
    stage_params = {
        1: {  # è¯­ä¹‰å¯¹é½é˜¶æ®µ
            'disc_start': 5001,  # åŸé¡¹ç›®é»˜è®¤å€¼ï¼Œå»¶è¿Ÿåˆ¤åˆ«å™¨å¯åŠ¨
            'disc_weight': 0.5,  # åŸé¡¹ç›®é»˜è®¤å€¼
            'vf_weight': 0.5,  # é«˜æƒé‡è¿›è¡Œè¯­ä¹‰å¯¹é½
            'distmat_margin': 0.0,
            'cos_margin': 0.0,
            'learning_rate': 1e-4,
            'max_epochs': 30  # é€‚åº”å°æ•°æ®é›†
        },
        2: {  # é‡å»ºä¼˜åŒ–é˜¶æ®µ
            'disc_start': 1,  # å¯ç”¨åˆ¤åˆ«å™¨
            'disc_weight': 0.5,  # åŸé¡¹ç›®é»˜è®¤å€¼
            'vf_weight': 0.1,  # é™ä½VFæƒé‡
            'distmat_margin': 0.0,
            'cos_margin': 0.0,
            'learning_rate': 5e-5,
            'max_epochs': 15
        },
        3: {  # è¾¹è·ä¼˜åŒ–é˜¶æ®µ
            'disc_start': 1,
            'disc_weight': 0.5,  # åŸé¡¹ç›®é»˜è®¤å€¼
            'vf_weight': 0.1,
            'distmat_margin': 0.25,  # åŸé¡¹ç›®é»˜è®¤å€¼
            'cos_margin': 0.5,  # åŸé¡¹ç›®é»˜è®¤å€¼
            'learning_rate': 2e-5,
            'max_epochs': 10
        }
    }
    
    params = stage_params[stage]
    
    config = OmegaConf.create({
        'model': {
            'base_learning_rate': params['learning_rate'],
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'monitor': 'val/rec_loss',
                'embed_dim': 32,
                'ckpt_path': args.pretrained_path if stage == 1 else checkpoint_path,
                
                # Vision Foundationé…ç½® - åŸé¡¹ç›®æ ¸å¿ƒ
                'use_vf': 'dinov2',
                'reverse_proj': True,  # 32D -> 1024DæŠ•å½±
                
                # æ¶æ„é…ç½® - ä¸åŸé¡¹ç›®ä¸€è‡´
                'ddconfig': {
                    'double_z': True,  # KL-VAEéœ€è¦
                    'z_channels': 32,
                    'resolution': 256,
                    'in_channels': 3,
                    'out_ch': 3,
                    'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4],
                    'num_res_blocks': 2,
                    'attn_resolutions': [16],
                    'dropout': 0.0
                },
                
                # æŸå¤±é…ç½® - åŸé¡¹ç›®æ ¸å¿ƒ
                'lossconfig': {
                    'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                    'params': {
                        # åˆ¤åˆ«å™¨å‚æ•° - ä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´
                        'disc_start': params['disc_start'],
                        'disc_num_layers': 3,
                        'disc_weight': params['disc_weight'],  # ä½¿ç”¨é˜¶æ®µç‰¹å®šå€¼
                        'disc_factor': 1.0,
                        'disc_in_channels': 3,
                        'disc_conditional': False,
                        'disc_loss': 'hinge',  # åŸé¡¹ç›®é»˜è®¤
                        
                        # é‡å»ºæŸå¤± - ä¸åŸé¡¹ç›®ä¸€è‡´
                        'pixelloss_weight': 1.0,
                        'perceptual_weight': 0.0,  # åŸé¡¹ç›®VA-VAEä¸ç”¨æ„ŸçŸ¥æŸå¤±
                        'kl_weight': 1e-6,  # åŸé¡¹ç›®å€¼
                        'logvar_init': 0.0,  # åŸé¡¹ç›®é»˜è®¤
                        
                        # VFå¯¹é½æŸå¤± - åŸé¡¹ç›®æ ¸å¿ƒå‚æ•°
                        'vf_weight': params['vf_weight'],
                        'adaptive_vf': True,  # è‡ªé€‚åº”æƒé‡å¹³è¡¡
                        'distmat_weight': 1.0,  # è·ç¦»çŸ©é˜µæƒé‡
                        'cos_weight': 1.0,  # ä½™å¼¦ç›¸ä¼¼åº¦æƒé‡
                        'distmat_margin': params['distmat_margin'],
                        'cos_margin': params['cos_margin'],
                        'pp_style': False,  # åŸé¡¹ç›®é»˜è®¤
                        'use_actnorm': False  # åŸé¡¹ç›®é»˜è®¤
                    }
                }
            }
        },
        
        'data': {
            'target': 'main.DataModuleFromConfig',
            'params': {
                'batch_size': args.batch_size,
                'num_workers': args.num_workers,
                'wrap': False,  # åŸé¡¹ç›®å‚æ•°
                'train': {
                    'target': 'microdoppler_finetune.step4_train_vavae.MicroDopplerDataset',
                    'params': {
                        'data_root': args.data_root,
                        'split_file': args.split_file,
                        'split': 'train'
                    }
                },
                'validation': {
                    'target': 'microdoppler_finetune.step4_train_vavae.MicroDopplerDataset',
                    'params': {
                        'data_root': args.data_root,
                        'split_file': args.split_file,
                        'split': 'val'
                    }
                }
            }
        },
        
        'lightning': {
            'trainer': {
                'devices': args.devices if args.devices else 'auto',
                'accelerator': 'gpu' if torch.cuda.is_available() else 'cpu',
                'max_epochs': params['max_epochs'],
                'precision': 32,  # åŸé¡¹ç›®ä½¿ç”¨32ä½ï¼Œé¿å…FP16çš„NaNé—®é¢˜
                'strategy': get_training_strategy(args),  # æ ¹æ®GPUé…ç½®é€‰æ‹©ç­–ç•¥
                'accumulate_grad_batches': args.gradient_accumulation,
                'gradient_clip_val': 0.5,  # æ›´ä¿å®ˆçš„æ¢¯åº¦è£å‰ª
                'log_every_n_steps': 10,
                'val_check_interval': 0.5,  # å‡å°‘éªŒè¯é¢‘ç‡ä»¥åŠ é€Ÿè®­ç»ƒ
                'num_sanity_val_steps': 0,
                'detect_anomaly': args.detect_anomaly  # è°ƒè¯•NaNé—®é¢˜
            }
        }
    })
    
    return config


def train_stage(args, stage):
    """è®­ç»ƒå•ä¸ªé˜¶æ®µ"""
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ VA-VAE ç¬¬{stage}é˜¶æ®µè®­ç»ƒ")
    print(f"{'='*60}")
    
    # è®¾ç½®éšæœºç§å­
    seed_everything(args.seed, workers=True)
    
    # è·å–ä¸Šä¸€é˜¶æ®µcheckpoint
    checkpoint_path = None
    if stage > 1:
        prev_ckpt_dir = Path(f'checkpoints/stage{stage-1}')
        if prev_ckpt_dir.exists():
            # æŸ¥æ‰¾æœ€æ–°çš„checkpoint
            ckpt_files = list(prev_ckpt_dir.glob('*.ckpt'))
            if ckpt_files:
                checkpoint_path = str(max(ckpt_files, key=lambda x: x.stat().st_mtime))
                print(f"ğŸ“¦ åŠ è½½ç¬¬{stage-1}é˜¶æ®µcheckpoint: {checkpoint_path}")
    
    # åˆ›å»ºé…ç½®
    config = create_stage_config(args, stage, checkpoint_path)
    
    # ä¿å­˜é…ç½®
    config_dir = Path('checkpoints') / f'stage{stage}'
    config_dir.mkdir(parents=True, exist_ok=True)
    OmegaConf.save(config, config_dir / 'config.yaml')
    
    # å®ä¾‹åŒ–æ¨¡å‹
    model = instantiate_from_config(config.model)
    
    # å®ä¾‹åŒ–æ•°æ®æ¨¡å— - ä½¿ç”¨åŸé¡¹ç›®çš„DataModuleFromConfig
    data_module = instantiate_from_config(config.data)
    
    # é…ç½®å›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        dirpath=f'checkpoints/stage{stage}',
        filename=f'vavae-stage{stage}-{{epoch:02d}}-{{val_rec_loss:.4f}}',
        monitor='val/rec_loss',
        mode='min',
        save_top_k=2,
        save_last=True
    )
    
    lr_monitor = LearningRateMonitor(logging_interval='epoch')
    
    # é…ç½®è®­ç»ƒå™¨
    trainer = pl.Trainer(
        **config.lightning.trainer,
        callbacks=[checkpoint_callback, lr_monitor]
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹ç¬¬{stage}é˜¶æ®µè®­ç»ƒ...")
    print(f"   åˆ¤åˆ«å™¨å¯åŠ¨: {config.model.params.lossconfig.params.disc_start}")
    print(f"   VFæƒé‡: {config.model.params.lossconfig.params.vf_weight}")
    print(f"   è·ç¦»è¾¹è·: {config.model.params.lossconfig.params.distmat_margin}")
    print(f"   ä½™å¼¦è¾¹è·: {config.model.params.lossconfig.params.cos_margin}")
    
    trainer.fit(model, data_module)
    
    return trainer.checkpoint_callback.best_model_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser()
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_root', type=str, default='/kaggle/input/micro-doppler-data',
                       help='å¾®å¤šæ™®å‹’æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split_file', type=str, default='dataset_split.json',
                       help='æ•°æ®åˆ’åˆ†æ–‡ä»¶')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--pretrained_path', type=str,
                       default='/kaggle/input/vavae-pretrained/vavae-imagenet256-f16d32-dinov2.pt',
                       help='é¢„è®­ç»ƒVA-VAEæ¨¡å‹è·¯å¾„')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--gradient_accumulation', type=int, default=2)
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--seed', type=int, default=42)
    
    # GPUé…ç½®
    parser.add_argument('--devices', type=str, default=None,
                       help='GPUè®¾å¤‡ï¼Œä¾‹å¦‚"0,1"æˆ–"1"')
    parser.add_argument('--kaggle_t4', action='store_true',
                       help='ä½¿ç”¨Kaggle T4Ã—2é…ç½®')
    parser.add_argument('--detect_anomaly', action='store_true',
                       help='å¯ç”¨å¼‚å¸¸æ£€æµ‹ï¼ˆè°ƒè¯•NaNï¼‰')
    
    # é˜¶æ®µé€‰æ‹©
    parser.add_argument('--stages', type=str, default='1,2,3',
                       help='è¦è®­ç»ƒçš„é˜¶æ®µï¼Œé€—å·åˆ†éš”')
    parser.add_argument('--kaggle', action='store_true',
                       help='Kaggleç¯å¢ƒæ ‡å¿—')
    
    args = parser.parse_args()
    
    # éªŒè¯ç¯å¢ƒ
    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
    
    if args.kaggle:
        print("ğŸŒ Kaggleç¯å¢ƒæ£€æµ‹")
        kaggle_input = Path('/kaggle/input')
        if kaggle_input.exists():
            print("âœ… æ£€æµ‹åˆ°Kaggleç¯å¢ƒ")
            # è‡ªåŠ¨è®¾ç½®è·¯å¾„
            if (kaggle_input / 'micro-doppler-data').exists():
                args.data_root = '/kaggle/input/micro-doppler-data'
            if (kaggle_input / 'vavae-pretrained').exists():
                args.pretrained_path = '/kaggle/input/vavae-pretrained/vavae-imagenet256-f16d32-dinov2.pt'
    
    # è®¾ç½®ç§å­
    seed_everything(args.seed)
    
    # è§£æé˜¶æ®µ
    stages_to_train = [int(s) for s in args.stages.split(',')]
    
    print("="*60)
    print("ğŸš€ VA-VAE å¾®å¤šæ™®å‹’å¾®è°ƒ - LightningDiTå…¼å®¹ç‰ˆ")
    print("="*60)
    print(f"ğŸ“Š æ•°æ®é›†: {args.data_root}")
    print(f"ğŸ“¦ é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_path}")
    print(f"ğŸ¯ è®­ç»ƒé˜¶æ®µ: {stages_to_train}")
    print(f"âš™ï¸  è®¾ç½®:")
    print(f"   - Batch Size: {args.batch_size}")
    print(f"   - Gradient Accumulation: {args.gradient_accumulation}")
    print(f"   - æœ‰æ•ˆBatch Size: {args.batch_size * args.gradient_accumulation}")
    print("="*60)
    
    # è®­ç»ƒå„é˜¶æ®µ
    best_checkpoints = []
    for stage in stages_to_train:
        best_ckpt = train_stage(args, stage)
        best_checkpoints.append(best_ckpt)
        print(f"\nâœ… ç¬¬{stage}é˜¶æ®µå®Œæˆ")
        print(f"ğŸ“¦ æœ€ä½³checkpoint: {best_ckpt}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if best_checkpoints:
        final_ckpt = best_checkpoints[-1]
        checkpoint = torch.load(final_ckpt, map_location='cpu')
        
        # æå–state_dict
        state_dict = checkpoint['state_dict']
        
        # ä¿å­˜ä¸º.ptæ ¼å¼ï¼ˆå…¼å®¹åŸé¡¹ç›®ï¼‰
        final_path = Path('checkpoints') / 'vavae_microdoppler_final.pt'
        torch.save({
            'state_dict': state_dict,
            'stages_trained': stages_to_train,
            'config': {
                'embed_dim': 32,
                'use_vf': 'dinov2',
                'reverse_proj': True,
                'resolution': 256
            }
        }, final_path)
        
        print(f"\n{'='*60}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“¦ æœ€ç»ˆæ¨¡å‹: {final_path}")
        print(f"{'='*60}")


if __name__ == '__main__':
    main()

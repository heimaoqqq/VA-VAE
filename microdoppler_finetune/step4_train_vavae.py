#!/usr/bin/env python3
"""
Step 4: VA-VAE å¾®å¤šæ™®å‹’å¾®è°ƒè®­ç»ƒ
åŸºäºLightningDiTåŸé¡¹ç›®çš„å®Œæ•´å®ç°
åŒ…å«ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å’ŒVision Foundationå¯¹é½
"""

import os
import sys
import argparse
from pathlib import Path
import json
import yaml
from datetime import datetime

# æ·»åŠ LightningDiTè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root / 'LightningDiT'))
sys.path.insert(0, str(project_root))  # æ·»åŠ æ ¹ç›®å½•ä»¥å¯¼å…¥è‡ªå®šä¹‰æ•°æ®é›†

# å…³é”®ï¼šåœ¨å¯¼å…¥ldmä¹‹å‰è®¾ç½®tamingè·¯å¾„ï¼
def setup_taming_path():
    """è®¾ç½®tamingè·¯å¾„ï¼Œå¿…é¡»åœ¨å¯¼å…¥ldmä¹‹å‰è°ƒç”¨"""
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥tamingä½ç½®
    taming_locations = [
        Path('/kaggle/working/taming-transformers'),  # Kaggleæ ‡å‡†ä½ç½®
        Path('/kaggle/working/.taming_path'),  # è·¯å¾„æ–‡ä»¶
        Path.cwd().parent / 'taming-transformers',  # é¡¹ç›®æ ¹ç›®å½•
        Path.cwd() / '.taming_path'  # å½“å‰ç›®å½•è·¯å¾„æ–‡ä»¶
    ]
    
    for location in taming_locations:
        if location.name == '.taming_path' and location.exists():
            # è¯»å–è·¯å¾„æ–‡ä»¶
            try:
                with open(location, 'r') as f:
                    taming_path = f.read().strip()
                if Path(taming_path).exists() and taming_path not in sys.path:
                    sys.path.insert(0, taming_path)
                    print(f"ğŸ“‚ å·²åŠ è½½tamingè·¯å¾„: {taming_path}")
                    return True
            except Exception as e:
                continue
        elif location.name == 'taming-transformers' and location.exists():
            # ç›´æ¥è·¯å¾„
            taming_path = str(location.absolute())
            if taming_path not in sys.path:
                sys.path.insert(0, taming_path)
                print(f"ğŸ“‚ å‘ç°å¹¶åŠ è½½taming: {taming_path}")
                return True
    
    # é™é»˜å¤±è´¥ï¼Œå› ä¸ºå¯èƒ½å·²ç»é€šè¿‡å…¶ä»–æ–¹å¼åŠ è½½
    return False

# åœ¨ä»»ä½•å¯¼å…¥ldmä¹‹å‰è®¾ç½®tamingè·¯å¾„
setup_taming_path()

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from PIL import Image
import pytorch_lightning as pl
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # æ— GUIç¯å¢ƒ
from pytorch_lightning import seed_everything
from pytorch_lightning.callbacks import ModelCheckpoint, Callback, LearningRateMonitor
from pytorch_lightning.strategies import DDPStrategy

from omegaconf import OmegaConf
from ldm.util import instantiate_from_config
from ldm.models.autoencoder import AutoencoderKL
# from main import DataModuleFromConfig  # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®æ¨¡å—


class MicroDopplerDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ•°æ®é›† - å…¼å®¹åŸé¡¹ç›®æ ¼å¼"""
    
    def __init__(self, data_root, split_file, split='train', image_size=256):
        self.data_root = Path(data_root)
        self.image_size = image_size
        self.split = split
        
        # åŠ è½½æ•°æ®åˆ’åˆ†
        with open(split_file, 'r') as f:
            split_data = json.load(f)
        
        self.samples = []
        data_dict = split_data['train'] if split == 'train' else split_data['val']
        
        # step3ç”Ÿæˆæ ¼å¼ï¼š{"ID_1": [img_paths], "ID_2": [img_paths], ...}
        for user_id, img_paths in data_dict.items():
            for img_path in img_paths:
                if Path(img_path).exists():
                    self.samples.append({
                        'path': Path(img_path),
                        'user_id': user_id
                    })
        
        print(f"âœ… {split}é›†: {len(self.samples)} å¼ å›¾åƒ")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        img = Image.open(sample['path']).convert('RGB')
        img = img.resize((self.image_size, self.image_size), Image.LANCZOS)
        
        # æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼šæ¨¡å‹get_inputéœ€è¦HWCæ ¼å¼ï¼
        # éªŒè¯ï¼šget_inputä¼šè°ƒç”¨permute(0,3,1,2)å°†BHWCè½¬ä¸ºBCHW
        
        # æ–¹æ³•1ï¼šä½¿ç”¨numpyç›´æ¥åˆ›å»ºHWCæ ¼å¼
        img_array = np.array(img).astype(np.float32)  # HWCæ ¼å¼ [256,256,3]
        img_array = img_array / 127.5 - 1.0  # å½’ä¸€åŒ–åˆ°[-1,1]
        
        # è½¬ä¸ºtensorï¼Œä¿æŒHWCæ ¼å¼
        img_tensor = torch.from_numpy(img_array)  # [256,256,3]
        
        return {
            'image': img_tensor,  # HWCæ ¼å¼ï¼Œæ­£ç¡®åŒ¹é…get_inputæœŸæœ›
            'user_id': int(sample['user_id'].split('_')[1])
        }


class TrainingMonitorCallback(Callback):
    """è®­ç»ƒç›‘æ§å›è°ƒ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, stage):
        super().__init__()
        self.stage = stage
        self.best_val_loss = float('inf')
        self.loss_history = []
        # åˆ›å»ºé‡å»ºå›¾åƒä¿å­˜ç›®å½•
        self.save_dir = Path(f'logs/stage{stage}/reconstructions')
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
    def on_validation_epoch_end(self, trainer, pl_module):
        metrics = trainer.callback_metrics
        epoch = trainer.current_epoch
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„è®­ç»ƒæŒ‡æ ‡
        if epoch <= 2:  # å‰3ä¸ªepochæ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            train_keys = [k for k in metrics.keys() if k.startswith('train/')]
            print(f"ğŸ” è°ƒè¯• - å¯ç”¨çš„è®­ç»ƒæŒ‡æ ‡: {train_keys}")
        
        # è·å–å…³é”®æŸå¤±æŒ‡æ ‡ - æ£€æŸ¥æŸå¤±å‡½æ•°å®é™…è¿”å›çš„æŒ‡æ ‡åç§°
        val_rec_loss = metrics.get('val/rec_loss', 0)
        val_kl_loss = metrics.get('val/kl_loss', 0)
        val_vf_loss = metrics.get('val/vf_loss', 0)  # ä¿®æ­£ä¸ºå®é™…åç§°
        
        # VA-VAEå®é™…è®°å½•çš„è®­ç»ƒæŸå¤±æŒ‡æ ‡
        train_ae_loss = metrics.get('train/aeloss', 0)  # AutoEncoderæ€»æŸå¤±
        train_disc_loss = metrics.get('train/discloss', 0)  # åˆ¤åˆ«å™¨æŸå¤±
        
        # å°è¯•è·å–è¯¦ç»†æŸå¤±åˆ†è§£ - ä½¿ç”¨æŸå¤±å‡½æ•°å®é™…çš„æŒ‡æ ‡åç§°
        train_total_loss = metrics.get('train/total_loss', 0)  # æ€»æŸå¤±
        train_rec_loss = metrics.get('train/rec_loss', 0)      # é‡å»ºæŸå¤±
        train_kl_loss = metrics.get('train/kl_loss', 0)        # KLæŸå¤±
        train_vf_loss = metrics.get('train/vf_loss', 0)        # VFå¯¹é½æŸå¤±
        train_g_loss = metrics.get('train/g_loss', 0)          # ç”Ÿæˆå™¨æŸå¤±
        
        # è°ƒè¯•ï¼šå¦‚æœæŸå¤±å¼‚å¸¸é«˜ï¼Œæ‰“å°æ‰€æœ‰å¯ç”¨çš„metrics
        if train_ae_loss > 1000:
            print(f"\nâš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸é«˜æŸå¤±ï¼Œè¯¦ç»†metrics:")
            for key, value in metrics.items():
                if 'train/' in key and value != 0:
                    print(f"   {key}: {value:.4f}")
        
        # è·å–å­¦ä¹ ç‡
        current_lr = 0
        if hasattr(pl_module, 'optimizers'):
            opts = pl_module.optimizers()
            if opts and len(opts) > 0:
                current_lr = opts[0].param_groups[0]['lr']
        
        # åˆ¤æ–­è®­ç»ƒç¨³å®šæ€§
        is_stable = self._check_training_stability(val_rec_loss, train_ae_loss)
        stability_icon = "âœ…" if is_stable else "âš ï¸"
        
        # æ›´æ–°æœ€ä½³æŸå¤±
        if val_rec_loss < self.best_val_loss:
            self.best_val_loss = val_rec_loss
            best_icon = "ğŸ†"
        else:
            best_icon = ""
        
        print(f"\n{stability_icon} Stage {self.stage} - Epoch {epoch + 1} {best_icon}")
        print(f"ğŸ“Š éªŒè¯æŸå¤±:")
        print(f"   é‡å»º: {val_rec_loss:.4f} | KL: {val_kl_loss:.4f} | VF: {val_vf_loss:.4f}")
        print(f"ğŸ¯ è®­ç»ƒæŸå¤±:")
        print(f"   AutoEncoder: {train_ae_loss:.4f} | åˆ¤åˆ«å™¨: {train_disc_loss:.4f}")
        
        # æ˜¾ç¤ºè¯¦ç»†æŸå¤±åˆ†è§£ - ç”¨äºè¯Šæ–­ (å§‹ç»ˆå°è¯•æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ç¬¬ä¸€ä¸ªepoch)
        print(f"\nğŸ“Š è®­ç»ƒæŸå¤±è¯¦æƒ… (é«˜ç²¾åº¦):")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è¯¦ç»†æŸå¤±è¢«è®°å½•
        has_detailed_loss = (train_total_loss != 0 or train_rec_loss != 0 or 
                           train_kl_loss != 0 or train_vf_loss != 0 or train_g_loss != 0)
        
        if has_detailed_loss:
            print(f"   - Total Loss: {train_total_loss:.6f}")
            print(f"   - Rec Loss: {train_rec_loss:.6f}")
            
            # æ˜¾ç¤ºKLæŸå¤±çš„ç²¾ç¡®å€¼ï¼ˆæ˜¾ç¤º12ä½å°æ•°ä»¥è§‚å¯Ÿå¾®å°å˜åŒ–ï¼‰
            if train_kl_loss == 0:
                print(f"   - KL Loss: 0.000000000000 (å®Œå…¨ä¸ºé›¶)")
            else:
                # æ˜¾ç¤ºå®é™…KLå€¼å’ŒåŠ æƒåçš„å€¼
                raw_kl = train_kl_loss / 1e-6 if train_kl_loss > 0 else 0
                print(f"   - KL Loss: {train_kl_loss:.12f} (åŸå§‹KL={raw_kl:.6f}, æƒé‡=1e-6)")
                
            # æ˜¾ç¤ºVFæŸå¤±çš„ç²¾ç¡®å€¼ï¼ˆæ˜¾ç¤º12ä½å°æ•°ï¼‰
            if train_vf_loss == 0:
                print(f"   - VF Loss: 0.000000000000 (å®Œå…¨ä¸ºé›¶)")
            else:
                vf_weight = metrics.get('train/vf_weight', 0.5)
                raw_vf = train_vf_loss / vf_weight if vf_weight > 0 else train_vf_loss
                print(f"   - VF Loss: {train_vf_loss:.12f} (åŸå§‹VF={raw_vf:.6f}, æƒé‡={vf_weight})")
                
            print(f"   - Disc Loss: {train_disc_loss:.6f}")
            print(f"   - Generator Loss: {train_g_loss:.6f}")
        else:
            # å¦‚æœæ²¡æœ‰è¯¦ç»†æŸå¤±ï¼Œå°è¯•ä»autoencoderå’ŒdiscriminatoræŸå¤±æ¨æ–­
            print(f"   - AE Loss (èšåˆ): {train_ae_loss:.6f}")
            print(f"   - Disc Loss (èšåˆ): {train_disc_loss:.6f}")
            print(f"   â„¹ï¸ è¯¦ç»†æŸå¤±åˆ†è§£å°†åœ¨ä¸‹ä¸ªepochå¼€å§‹è®°å½•")
        
        print(f"âš™ï¸  å­¦ä¹ ç‡: {current_lr:.2e}")
        
        # é˜¶æ®µç‰¹å®šå…³æ³¨ç‚¹
        if self.stage == 1:
            if train_vf_loss > 0:
                print(f"ğŸ¨ Stage 1 é‡ç‚¹: VFå¯¹é½æ•ˆæœ = {train_vf_loss:.4f}")
            else:
                print(f"ğŸ¨ Stage 1 é‡ç‚¹: AEæŸå¤±(å«VF) = {train_ae_loss:.4f}")
        elif self.stage == 2:
            print(f"ğŸ—ï¸  Stage 2 é‡ç‚¹: åˆ¤åˆ«å™¨å¹³è¡¡ = {train_disc_loss:.4f}")
        elif self.stage == 3:
            print(f"ğŸ¯ Stage 3 é‡ç‚¹: ç”¨æˆ·åŒºåˆ†ä¼˜åŒ–")
        
        # å¼‚å¸¸è­¦å‘Š
        self._check_anomalies(val_rec_loss, train_ae_loss, current_lr)
        
        # ğŸ¯ æ–°å¢åŠŸèƒ½1: VFè¯­ä¹‰å¯¹é½æ£€æŸ¥
        self._check_vf_alignment(trainer, pl_module)
        
        # ğŸ¯ æ–°å¢åŠŸèƒ½2: æ¯ä¸ªepochç”Ÿæˆé‡å»ºå›¾åƒ
        self._generate_reconstruction_images(trainer, pl_module, epoch)
        
        print("-" * 50)
        
    def _check_training_stability(self, val_loss, train_loss):
        """æ£€æŸ¥è®­ç»ƒç¨³å®šæ€§"""
        if torch.isnan(torch.tensor([val_loss, train_loss])).any():
            return False
        if val_loss > 10.0 or train_loss > 10.0:  # æŸå¤±è¿‡å¤§
            return False
        return True
        
    def _check_anomalies(self, val_loss, train_loss, lr):
        """æ£€æŸ¥è®­ç»ƒå¼‚å¸¸"""
        warnings = []
        
        if torch.isnan(torch.tensor([val_loss, train_loss])).any():
            warnings.append("ğŸš¨ æ£€æµ‹åˆ°NaNæŸå¤±!")
        if val_loss > 5.0:
            warnings.append("âš ï¸  éªŒè¯æŸå¤±è¿‡é«˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
        if train_loss > 10.0:
            warnings.append("âš ï¸  è®­ç»ƒæŸå¤±å¼‚å¸¸é«˜")
        if lr < 1e-7:
            warnings.append("âš ï¸  å­¦ä¹ ç‡è¿‡ä½ï¼Œè®­ç»ƒå¯èƒ½åœæ»")
        if len(self.loss_history) > 5:
            recent_losses = self.loss_history[-5:]
            if all(abs(recent_losses[i] - recent_losses[i-1]) < 1e-5 for i in range(1, 5)):
                warnings.append("âš ï¸  æŸå¤±æ”¶æ•›åœæ»")
                
        self.loss_history.append(val_loss)
        if len(self.loss_history) > 10:
            self.loss_history.pop(0)
            
        for warning in warnings:
            print(warning)
    
    def _check_vf_alignment(self, trainer, pl_module):
        """æ£€æŸ¥VFè¯­ä¹‰å¯¹é½è´¨é‡"""
        try:
            if not hasattr(pl_module, 'foundation_model') or pl_module.foundation_model is None:
                print("âš ï¸ VFæ¨¡å—æœªåˆå§‹åŒ–")
                return
            
            # è·å–éªŒè¯æ•°æ®æ‰¹æ¬¡è¿›è¡ŒVFæ£€æŸ¥
            val_dataloader = trainer.val_dataloaders[0] if isinstance(trainer.val_dataloaders, list) else trainer.val_dataloaders
            
            with torch.no_grad():
                for batch in val_dataloader:
                    inputs = pl_module.get_input(batch, pl_module.image_key)
                    inputs = inputs[:4].to(pl_module.device)  # åªç”¨å‰4ä¸ªæ ·æœ¬
                    
                    # å‰å‘ä¼ æ’­è·å–ç‰¹å¾
                    reconstructions, posterior, z, aux_feature = pl_module(inputs)
                    
                    if aux_feature is not None and z is not None:
                        # è®¡ç®—VFç‰¹å¾èŒƒæ•°
                        vf_norm = torch.norm(aux_feature, dim=1).mean().item()
                        z_norm = torch.norm(z, dim=1).mean().item()
                        
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ - ä½¿ç”¨reshapeé¿å…tensor strideé—®é¢˜
                        aux_flat = aux_feature.reshape(aux_feature.size(0), -1)
                        z_flat = z.reshape(z.size(0), -1)
                        similarity = torch.nn.functional.cosine_similarity(aux_flat, z_flat, dim=1).mean().item()
                        
                        print(f"\nğŸ” VFè¯­ä¹‰å¯¹é½æ£€æŸ¥:")
                        print(f"   VFç‰¹å¾èŒƒæ•°: {vf_norm:.4f}")
                        print(f"   æ½œåœ¨ç¼–ç èŒƒæ•°: {z_norm:.4f}")
                        print(f"   ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")
                        
                        if similarity > 0.3:
                            print(f"   âœ… VFè¯­ä¹‰å¯¹é½è‰¯å¥½ (ç›¸ä¼¼åº¦ > 0.3)")
                        elif similarity > 0.1:
                            print(f"   âš ï¸ VFè¯­ä¹‰å¯¹é½ä¸­ç­‰ (éœ€è¦æ›´å¤šè®­ç»ƒ)")
                        else:
                            print(f"   âŒ VFè¯­ä¹‰å¯¹é½è¾ƒå·® (éœ€è¦æ£€æŸ¥é…ç½®)")
                        
                        if vf_norm > 0.1:
                            print(f"   âœ… VFç‰¹å¾æ­£å¸¸å·¥ä½œ (èŒƒæ•° > 0.1)")
                        else:
                            print(f"   âŒ VFç‰¹å¾å¯èƒ½æœªæ¿€æ´»")
                    else:
                        print("âš ï¸ VFç‰¹å¾æˆ–æ½œåœ¨ç¼–ç ä¸ºNone")
                    
                    break  # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
                    
        except Exception as e:
            print(f"âš ï¸ VFå¯¹é½æ£€æŸ¥å¤±è´¥: {e}")
    
    def _generate_reconstruction_images(self, trainer, pl_module, epoch):
        """ç”Ÿæˆå¹¶ä¿å­˜é‡å»ºå›¾åƒå¯è§†åŒ–"""
        try:
            pl_module.eval()
            val_dataloader = trainer.val_dataloaders[0] if isinstance(trainer.val_dataloaders, list) else trainer.val_dataloaders
            
            with torch.no_grad():
                for batch in val_dataloader:
                    inputs = pl_module.get_input(batch, pl_module.image_key)
                    inputs = inputs[:8].to(pl_module.device)  # åªå¤„ç†å‰8ä¸ªæ ·æœ¬
                    
                    # ä½¿ç”¨æ­£ç¡®çš„æ–¹å¼ç”Ÿæˆé‡å»ºï¼šå…ˆç¼–ç åè§£ç 
                    posterior = pl_module.encode(inputs)
                    z = posterior.sample()
                    reconstructions = pl_module.decode(z)
                    
                    # ç¡®ä¿é‡å»ºä¸æ˜¯Noneä¸”å½¢çŠ¶æ­£ç¡®
                    if reconstructions is None:
                        print(f"   âš ï¸ è§£ç å™¨è¿”å›Noneï¼Œå°è¯•ä½¿ç”¨å®Œæ•´å‰å‘ä¼ æ’­")
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å®Œæ•´çš„å‰å‘ä¼ æ’­
                        outputs, posterior = pl_module(inputs)
                        reconstructions = outputs
                    
                    # è°ƒè¯•ï¼šæ‰“å°å¼ é‡å½¢çŠ¶å’ŒèŒƒå›´
                    print(f"   ğŸ“ è¾“å…¥å½¢çŠ¶: {inputs.shape}, èŒƒå›´: [{inputs.min():.2f}, {inputs.max():.2f}]")
                    print(f"   ğŸ“ æ½œåœ¨ç¼–ç å½¢çŠ¶: {z.shape}, èŒƒå›´: [{z.min():.2f}, {z.max():.2f}]")
                    print(f"   ğŸ“ é‡å»ºå½¢çŠ¶: {reconstructions.shape}, èŒƒå›´: [{reconstructions.min():.2f}, {reconstructions.max():.2f}]")
                    
                    # éªŒè¯é‡å»ºæ˜¯å¦çœŸçš„ä¸åŒ
                    input_mean = inputs.mean().item()
                    recon_mean = reconstructions.mean().item()
                    mse = ((inputs - reconstructions) ** 2).mean().item()
                    print(f"   ğŸ” è¾“å…¥å‡å€¼: {input_mean:.4f}, é‡å»ºå‡å€¼: {recon_mean:.4f}")
                    print(f"   ğŸ” MSEå·®å¼‚: {mse:.6f}")
                    
                    # æ ¹æ®å®é™…batchå¤§å°åˆ›å»ºå¯è§†åŒ–
                    num_samples = min(8, inputs.shape[0])  # æœ€å¤šæ˜¾ç¤º8å¼ 
                    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 2, 4))
                    fig.suptitle(f'Stage {self.stage} - Epoch {epoch + 1} é‡å»ºæ•ˆæœå¯¹æ¯”')
                    
                    # ç¡®ä¿axesæ˜¯äºŒç»´æ•°ç»„ï¼Œå³ä½¿åªæœ‰ä¸€åˆ—
                    if num_samples == 1:
                        axes = axes.reshape(2, 1)
                    
                    for i in range(num_samples):
                        # åŸå§‹å›¾åƒ (è½¬æ¢ä¸ºnumpyæ˜¾ç¤ºæ ¼å¼)
                        orig = inputs[i].cpu().detach().numpy()
                        if orig.shape[0] == 3:  # RGB
                            orig = np.transpose(orig, (1, 2, 0))
                            orig = (orig + 1.0) / 2.0  # ä»[-1,1]è½¬ä¸º[0,1]
                            orig = np.clip(orig, 0, 1)
                        else:  # å•é€šé“
                            orig = orig[0]
                            orig = (orig + 1.0) / 2.0
                            orig = np.clip(orig, 0, 1)
                        
                        # é‡å»ºå›¾åƒ - ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„é‡å»ºç»“æœ
                        recon = reconstructions[i].cpu().detach().numpy()
                        if recon.shape[0] == 3:  # RGB
                            recon = np.transpose(recon, (1, 2, 0))
                            recon = (recon + 1.0) / 2.0
                            recon = np.clip(recon, 0, 1)
                        else:  # å•é€šé“
                            recon = recon[0]
                            recon = (recon + 1.0) / 2.0
                            recon = np.clip(recon, 0, 1)
                        
                        # æ˜¾ç¤ºåŸå§‹å›¾åƒï¼ˆç¬¬ä¸€è¡Œï¼‰
                        axes[0, i].imshow(orig, cmap='viridis' if orig.ndim == 2 else None)
                        axes[0, i].axis('off')
                        if i == 0:
                            axes[0, i].set_title('åŸå§‹å›¾åƒ', fontsize=10)
                        
                        # æ˜¾ç¤ºé‡å»ºå›¾åƒï¼ˆç¬¬äºŒè¡Œï¼‰- ç¡®ä¿æ˜¯é‡å»ºè€Œä¸æ˜¯åŸå§‹
                        axes[1, i].imshow(recon, cmap='viridis' if recon.ndim == 2 else None)
                        axes[1, i].axis('off')
                        if i == 0:
                            axes[1, i].set_title('é‡å»ºå›¾åƒ', fontsize=10)
                        
                        # è°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„ä¸åŒ
                        if i == 0:
                            diff = np.abs(orig - recon).mean()
                            print(f"   ğŸ“Š ç¬¬ä¸€å¼ å›¾çš„å¹³å‡å·®å¼‚: {diff:.4f}")
                    
                    # ä¿å­˜å›¾åƒ
                    save_path = self.save_dir / f'stage{self.stage}_epoch{epoch + 1:03d}.png'
                    plt.savefig(save_path, dpi=100, bbox_inches='tight', facecolor='white')
                    plt.close()
                    
                    print(f"ğŸ’¾ é‡å»ºå›¾åƒå·²ä¿å­˜: {save_path}")
                    break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
                    
            pl_module.train()
        except Exception as e:
            print(f"âš ï¸ å›¾åƒç”Ÿæˆå¤±è´¥: {e}")


class MicroDopplerDataModule(pl.LightningDataModule):
    """å¾®å¤šæ™®å‹’æ•°æ®æ¨¡å—"""
    
    def __init__(self, data_root, split_file, batch_size=8, num_workers=4, image_size=256):
        super().__init__()
        self.data_root = data_root
        self.split_file = split_file
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.image_size = image_size
    
    def setup(self, stage=None):
        if stage == 'fit' or stage is None:
            self.train_dataset = MicroDopplerDataset(
                data_root=self.data_root,
                split_file=self.split_file,
                split='train',
                image_size=self.image_size
            )
            self.val_dataset = MicroDopplerDataset(
                data_root=self.data_root,
                split_file=self.split_file,
                split='val',
                image_size=self.image_size
            )
    
    def train_dataloader(self):
        return torch.utils.data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            persistent_workers=True,
            pin_memory=True
        )
    
    def val_dataloader(self):
        return torch.utils.data.DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            persistent_workers=True,
            pin_memory=True
        )


def create_stage_config(args, stage, checkpoint_path=None):
    """åˆ›å»ºé˜¶æ®µé…ç½®"""
    
    stage_params = {
        1: {'disc_start': 5001, 'disc_weight': 0.5, 'vf_weight': 0.5, 'distmat_margin': 0.0, 'cos_margin': 0.0, 'learning_rate': 1e-4, 'max_epochs': 50},
        2: {'disc_start': 1, 'disc_weight': 0.5, 'vf_weight': 0.1, 'distmat_margin': 0.0, 'cos_margin': 0.0, 'learning_rate': 5e-5, 'max_epochs': 15},
        3: {'disc_start': 1, 'disc_weight': 0.5, 'vf_weight': 0.1, 'distmat_margin': 0.25, 'cos_margin': 0.5, 'learning_rate': 2e-5, 'max_epochs': 15}
    }
    
    params = stage_params[stage]
    
    config = OmegaConf.create({
        'model': {
            'base_learning_rate': params['learning_rate'],
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'monitor': 'val/rec_loss',
                'embed_dim': 32,
                'ckpt_path': args.pretrained_path if stage == 1 else checkpoint_path,
                'use_vf': 'dinov2',
                'reverse_proj': True,
                'ddconfig': {
                    'double_z': True, 'z_channels': 32, 'resolution': 256,
                    'in_channels': 3, 'out_ch': 3, 'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4], 'num_res_blocks': 2,
                    'attn_resolutions': [16], 'dropout': 0.0
                },
                'lossconfig': {
                    'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                    'params': {
                        'disc_start': params['disc_start'], 'disc_num_layers': 3,
                        'disc_weight': params['disc_weight'], 'disc_factor': 1.0,
                        'disc_in_channels': 3, 'disc_conditional': False, 'disc_loss': 'hinge',
                        'pixelloss_weight': 1.0, 'perceptual_weight': 1.0,  # é‡è¦ï¼šåŸé¡¹ç›®ä½¿ç”¨æ„ŸçŸ¥æŸå¤±ï¼
                        'kl_weight': 1e-6, 'logvar_init': 0.0,
                        'use_actnorm': False,  # åˆ¤åˆ«å™¨ä¸­ä¸ä½¿ç”¨ActNorm
                        'pp_style': False,  # ä¸ä½¿ç”¨pp_styleçš„nllæŸå¤±è®¡ç®—
                        'vf_weight': params['vf_weight'], 'adaptive_vf': False,  # ç¦ç”¨è‡ªé€‚åº”é¿å…æƒé‡å¤±æ§
                        'distmat_weight': 1.0, 'cos_weight': 1.0,
                        'distmat_margin': params['distmat_margin'],
                        'cos_margin': params['cos_margin']
                    }
                }
            }
        }
    })
    
    return config


def train_stage(args, stage):
    """è®­ç»ƒé˜¶æ®µ"""
    
    seed_everything(args.seed, workers=True)
    
    checkpoint_path = None
    if stage > 1:
        prev_ckpt_dir = Path(f'checkpoints/stage{stage-1}')
        if prev_ckpt_dir.exists():
            ckpt_files = list(prev_ckpt_dir.glob('*.ckpt'))
            if ckpt_files:
                checkpoint_path = str(max(ckpt_files, key=lambda x: x.stat().st_mtime))
                print(f"åŠ è½½checkpoint: {checkpoint_path}")

    config = create_stage_config(args, stage, checkpoint_path)
    params = config.model.params.lossconfig.params

    model = instantiate_from_config(config.model)
    model.learning_rate = config.model.base_learning_rate

    # å…¨é¢éªŒè¯VFæ¨¡å—å’Œæƒé‡åŠ è½½
    if hasattr(model, 'use_vf'):
        print(f"ğŸ” VFæ¨¡å—çŠ¶æ€: use_vf={model.use_vf}")
        if model.use_vf and hasattr(model, 'foundation_model'):
            print(f"âœ… DINOv2æ¨¡å‹å·²åŠ è½½")
            # æ£€æŸ¥å…³é”®æƒé‡æ˜¯å¦å­˜åœ¨
            has_vf_weights = any('foundation_model' in k for k in model.state_dict().keys())
            has_proj_weights = any('linear_proj' in k for k in model.state_dict().keys())
            print(f"   - Foundationæƒé‡: {'âœ… å·²åŠ è½½' if has_vf_weights else 'âŒ ç¼ºå¤±'}")
            print(f"   - Projectionæƒé‡: {'âœ… å·²åŠ è½½' if has_proj_weights else 'âŒ ç¼ºå¤±'}")
            
            if not has_vf_weights:
                print(f"âš ï¸  è­¦å‘Šï¼šDINOv2æƒé‡æœªä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼")
                print(f"   è¿™ä¼šå¯¼è‡´VFæŸå¤±ä¸º0ï¼ŒStage 1è®­ç»ƒæ— æ•ˆ")
                print(f"   è¯·ç¡®ä¿é¢„è®­ç»ƒæ–‡ä»¶åŒ…å«foundation_modelæƒé‡")
        else:
            print(f"âš ï¸  DINOv2æ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–ï¼")
    else:
        print(f"âŒ æ¨¡å‹ç¼ºå°‘use_vfå±æ€§ï¼")
    print(f"å­¦ä¹ ç‡: {model.learning_rate:.2e}")

    data_module = MicroDopplerDataModule(
        data_root=args.data_root,
        split_file=args.split_file,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        image_size=256
    )
    
    checkpoint_dir = Path(f'checkpoints/stage{stage}')
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename=f'vavae-stage{stage}-{{epoch:02d}}-{{val_rec_loss:.4f}}',
        monitor='val/rec_loss',
        mode='min',
        save_top_k=1,
        save_last=False,
        verbose=True
    )
    
    training_monitor = TrainingMonitorCallback(stage)
    
    trainer = pl.Trainer(
        devices='auto',
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        max_epochs=params.get('max_epochs', 50),
        precision=32,
        callbacks=[checkpoint_callback, training_monitor],
        enable_progress_bar=True,  # å¯ç”¨é»˜è®¤è¿›åº¦æ¡
        enable_model_summary=False,  # ç¦ç”¨æ¨¡å‹æ‘˜è¦è¾“å‡º
        log_every_n_steps=50,  # å¢åŠ æ—¥å¿—æ­¥é•¿å‡å°‘è¾“å‡ºé¢‘ç‡
        enable_checkpointing=True,
        num_sanity_val_steps=0,  # è·³è¿‡sanity checké¿å…é¢å¤–çš„éªŒè¯è¾“å‡º
        logger=False  # ç¦ç”¨é»˜è®¤loggerå‡å°‘è¾“å‡º
    )
    
    print(f"\nç¬¬{stage}é˜¶æ®µè®­ç»ƒ - LR: {config.model.base_learning_rate:.2e}")
    
    trainer.fit(model, data_module)
    
    return trainer.checkpoint_callback.best_model_path


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, default='/kaggle/input/micro-doppler-data')
    parser.add_argument('--split_file', type=str, default='/kaggle/working/data_split/dataset_split.json')
    parser.add_argument('--pretrained_path', type=str, default='/kaggle/input/vavae-pretrained/vavae-imagenet256-f16d32-dinov2.pt')
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--stages', type=str, default='1,2,3')
    parser.add_argument('--gradient_accumulation', type=int, default=1)
    parser.add_argument('--kaggle', action='store_true')
    
    args = parser.parse_args()
    stages_to_train = [int(s) for s in args.stages.split(',')]
    
    print("å¼€å§‹VA-VAEå¾®å¤šæ™®å‹’å¾®è°ƒè®­ç»ƒ")
    print(f"æ•°æ®: {args.data_root}")
    print(f"è®­ç»ƒé˜¶æ®µ: {stages_to_train}")
    
    data_module = MicroDopplerDataModule(
        data_root=args.data_root,
        split_file=args.split_file,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        image_size=256
    )
    data_module.setup()
    
    print(f"è®­ç»ƒé›†: {len(data_module.train_dataset)} å¼ å›¾åƒ, éªŒè¯é›†: {len(data_module.val_dataset)} å¼ å›¾åƒ")
    
    best_checkpoints = []
    for stage in stages_to_train:
        best_ckpt = train_stage(args, stage)
        best_checkpoints.append(best_ckpt)
        print(f"ç¬¬{stage}é˜¶æ®µå®Œæˆ")
    
    print(f"è®­ç»ƒå®Œæˆ! æœ€ä½³checkpoints: {best_checkpoints}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if best_checkpoints:
        final_ckpt = best_checkpoints[-1]
        checkpoint = torch.load(final_ckpt, map_location='cpu')
        
        # æå–state_dict
        state_dict = checkpoint['state_dict']
        
        # ä¿å­˜ä¸º.ptæ ¼å¼ï¼ˆå…¼å®¹åŸé¡¹ç›®ï¼‰
        final_path = Path('checkpoints') / 'vavae_microdoppler_final.pt'
        torch.save({
            'state_dict': state_dict,
            'stages_trained': stages_to_train,
            'config': {
                'embed_dim': 32,
                'use_vf': 'dinov2',
                'reverse_proj': True,
                'resolution': 256
            }
        }, final_path)
        
        print(f"\n{'='*60}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“¦ æœ€ç»ˆæ¨¡å‹: {final_path}")
        print(f"{'='*60}")


if __name__ == '__main__':
    main()

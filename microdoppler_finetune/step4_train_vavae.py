#!/usr/bin/env python3
"""
VA-VAE è®­ç»ƒè„šæœ¬ - å¾®å¤šæ™®å‹’æ•°æ®å¾®è°ƒ

å…³é”®ç‰¹æ€§ï¼š
1. å›ºå®šVFæƒé‡ï¼ˆadaptive_vf=Falseï¼‰é¿å…åŸŸå·®å¼‚å¯¼è‡´çš„æƒé‡å¤±æ§
   - Stage 1: vf_weight=0.5 (åˆå§‹å¯¹é½)
   - Stage 2/3: vf_weight=0.1 (ç²¾ç»†è°ƒæ•´)
2. è¯¦ç»†æŸå¤±ç›‘æ§ - æ˜¾ç¤ºæ‰€æœ‰æŸå¤±åˆ†é‡æ˜ç»†
3. VFè¯­ä¹‰å¯¹é½æ£€æµ‹ - ç›‘æ§ç‰¹å¾ç›¸ä¼¼åº¦
4. è‡ªåŠ¨å¯è§†åŒ– - æ¯ä¸ªepochä¿å­˜é‡å»ºå¯¹æ¯”å›¾

ä½¿ç”¨æ–¹æ³•ï¼š
python step4_train_vavae.py --stage 1 --batch_size 4
"""

import os
import sys
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, Callback
from pytorch_lightning.loggers import TensorBoardLogger
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'LightningDiT', 'vavae'))

# æ·»åŠ taming-transformersè·¯å¾„ï¼ˆè§£å†³tamingæ¨¡å—å¯¼å…¥é—®é¢˜ï¼‰
def setup_taming_path():
    """è®¾ç½®taming-transformersè·¯å¾„"""
    # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„tamingä½ç½®
    possible_paths = [
        os.path.join(os.path.dirname(__file__), '..', 'taming-transformers'),  # ç›¸å¯¹è·¯å¾„
        '/kaggle/working/taming-transformers',  # Kaggleç¯å¢ƒ
        os.path.join(os.getcwd(), 'taming-transformers'),  # å½“å‰ç›®å½•
    ]
    
    # æ£€æŸ¥.taming_pathæ–‡ä»¶
    taming_path_file = os.path.join(os.path.dirname(__file__), '..', '.taming_path')
    if os.path.exists(taming_path_file):
        with open(taming_path_file, 'r') as f:
            possible_paths.insert(0, f.read().strip())
    
    for taming_path in possible_paths:
        if os.path.exists(taming_path):
            if taming_path not in sys.path:
                sys.path.insert(0, taming_path)
                print(f"âœ… å·²æ·»åŠ tamingè·¯å¾„: {taming_path}")
            return True
    
    print("âŒ æœªæ‰¾åˆ°taming-transformersï¼Œè¯·å…ˆè¿è¡Œstep1_setup_environment.py")
    return False

# è®¾ç½®tamingè·¯å¾„
setup_taming_path()

from ldm.util import instantiate_from_config
from ldm.data.microdoppler import MicroDopplerDataset
from ldm.models.autoencoder import AutoencoderKL
from torch.utils.data import DataLoader, random_split

class MicroDopplerDataModule(pl.LightningDataModule):
    """PyTorch Lightningæ•°æ®æ¨¡å—"""
    def __init__(self, data_dir, batch_size=4, num_workers=2, val_split=0.1):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.val_split = val_split
        self.dataset = None
        self.train_dataset = None
        self.val_dataset = None
        
    def setup(self, stage=None):
        if self.dataset is None:
            self.dataset = MicroDopplerDataset(self.data_dir)
            
            # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
            total_size = len(self.dataset)
            val_size = int(total_size * self.val_split)
            train_size = total_size - val_size
            
            self.train_dataset, self.val_dataset = random_split(
                self.dataset, [train_size, val_size],
                generator=torch.Generator().manual_seed(42)
            )
            
            print(f"æ•°æ®é›†åˆ†å‰²: è®­ç»ƒ{train_size}, éªŒè¯{val_size}")
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            persistent_workers=True if self.num_workers > 0 else False,
            pin_memory=True
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            persistent_workers=True if self.num_workers > 0 else False,
            pin_memory=True
        )

class DetailedLossMonitor(Callback):
    """è¯¦ç»†æŸå¤±ç›‘æ§å›è°ƒ"""
    def __init__(self, stage, save_dir='logs/reconstructions'):
        self.stage = stage
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        self.epoch_losses = {
            'train': {'rec': [], 'kl': [], 'vf': [], 'disc': [], 'g': [], 'total': []},
            'val': {'rec': [], 'kl': [], 'vf': [], 'total': []}
        }
        
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        """è®°å½•è®­ç»ƒæ‰¹æ¬¡çš„è¯¦ç»†æŸå¤±"""
        # ä»æ—¥å¿—ä¸­æå–è¯¦ç»†æŸå¤±
        logs = trainer.logged_metrics
        
        if 'train/rec_loss' in logs:
            self.epoch_losses['train']['rec'].append(logs['train/rec_loss'].item())
        if 'train/kl_loss' in logs:
            self.epoch_losses['train']['kl'].append(logs['train/kl_loss'].item())
        if 'train/vf_loss' in logs:
            self.epoch_losses['train']['vf'].append(logs['train/vf_loss'].item())
        if 'train/disc_loss' in logs:
            self.epoch_losses['train']['disc'].append(logs['train/disc_loss'].item())
        if 'train/g_loss' in logs:
            self.epoch_losses['train']['g'].append(logs['train/g_loss'].item())
        if 'train/aeloss' in logs:
            self.epoch_losses['train']['total'].append(logs['train/aeloss'].item())
    
    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        """è®°å½•éªŒè¯æ‰¹æ¬¡çš„è¯¦ç»†æŸå¤±"""
        logs = trainer.logged_metrics
        
        if 'val/rec_loss' in logs:
            self.epoch_losses['val']['rec'].append(logs['val/rec_loss'].item())
        if 'val/kl_loss' in logs:
            self.epoch_losses['val']['kl'].append(logs['val/kl_loss'].item())
        if 'val/vf_loss' in logs:
            self.epoch_losses['val']['vf'].append(logs['val/vf_loss'].item())
        if 'val/total_loss' in logs:
            self.epoch_losses['val']['total'].append(logs['val/total_loss'].item())
    
    def on_epoch_end(self, trainer, pl_module):
        """Epochç»“æŸæ—¶æ‰“å°è¯¦ç»†æŸå¤±å¹¶ç”Ÿæˆé‡å»ºå›¾åƒ"""
        epoch = trainer.current_epoch
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Stage {self.stage} - Epoch {epoch} è¯¦ç»†æŸå¤±æŠ¥å‘Š")
        print(f"{'='*60}")
        
        # æ‰“å°è®­ç»ƒæŸå¤±
        print(f"\nğŸ¯ è®­ç»ƒæŸå¤±æ˜ç»†:")
        if self.epoch_losses['train']['rec']:
            print(f"   é‡å»ºæŸå¤±: {np.mean(self.epoch_losses['train']['rec']):.4f}")
        if self.epoch_losses['train']['kl']:
            print(f"   KLæŸå¤±: {np.mean(self.epoch_losses['train']['kl']):.6f}")
        if self.epoch_losses['train']['vf']:
            vf_loss = np.mean(self.epoch_losses['train']['vf'])
            print(f"   VFæŸå¤±: {vf_loss:.4f}")
            if vf_loss > 0:
                print(f"   âœ… VFæ­£åœ¨å·¥ä½œ - è¯­ä¹‰å¯¹é½æŸå¤±: {vf_loss:.4f}")
            else:
                print(f"   âš ï¸ VFæœªæ¿€æ´»æˆ–æŸå¤±ä¸º0")
        if self.epoch_losses['train']['disc']:
            print(f"   åˆ¤åˆ«å™¨æŸå¤±: {np.mean(self.epoch_losses['train']['disc']):.4f}")
        if self.epoch_losses['train']['g']:
            print(f"   ç”Ÿæˆå™¨æŸå¤±: {np.mean(self.epoch_losses['train']['g']):.4f}")
        if self.epoch_losses['train']['total']:
            total_loss = np.mean(self.epoch_losses['train']['total'])
            print(f"   æ€»æŸå¤±: {total_loss:.2f}")
            if total_loss > 1000:
                print(f"   âš ï¸ æ€»æŸå¤±å¼‚å¸¸é«˜ï¼Œå¯èƒ½æ˜¯VFæƒé‡é—®é¢˜")
        
        # æ‰“å°éªŒè¯æŸå¤±
        print(f"\nğŸ“Š éªŒè¯æŸå¤±æ˜ç»†:")
        if self.epoch_losses['val']['rec']:
            print(f"   é‡å»ºæŸå¤±: {np.mean(self.epoch_losses['val']['rec']):.4f}")
        if self.epoch_losses['val']['kl']:
            print(f"   KLæŸå¤±: {np.mean(self.epoch_losses['val']['kl']):.6f}")
        if self.epoch_losses['val']['vf']:
            print(f"   VFæŸå¤±: {np.mean(self.epoch_losses['val']['vf']):.4f}")
        if self.epoch_losses['val']['total']:
            print(f"   æ€»æŸå¤±: {np.mean(self.epoch_losses['val']['total']):.4f}")
        
        # ç”Ÿæˆé‡å»ºå›¾åƒ
        self.generate_reconstructions(trainer, pl_module, epoch)
        
        # æ¸…ç©ºæŸå¤±è®°å½•
        for split in self.epoch_losses:
            for key in self.epoch_losses[split]:
                self.epoch_losses[split][key] = []
        
        print(f"{'='*60}\n")
    
    def generate_reconstructions(self, trainer, pl_module, epoch):
        """ç”Ÿæˆå¹¶ä¿å­˜é‡å»ºå›¾åƒ"""
        pl_module.eval()
        dataloader = trainer.val_dataloaders[0] if isinstance(trainer.val_dataloaders, list) else trainer.val_dataloaders
        
        with torch.no_grad():
            # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
            for batch in dataloader:
                inputs = pl_module.get_input(batch, pl_module.image_key)
                inputs = inputs[:8].to(pl_module.device)  # åªå¤„ç†å‰8ä¸ªæ ·æœ¬
                
                # ç”Ÿæˆé‡å»º
                reconstructions, posterior, z, aux_feature = pl_module(inputs)
                
                # æ£€æŸ¥VFç‰¹å¾
                if aux_feature is not None:
                    vf_norm = torch.norm(aux_feature, dim=1).mean().item()
                    z_norm = torch.norm(z, dim=1).mean().item()
                    similarity = torch.nn.functional.cosine_similarity(
                        aux_feature.view(aux_feature.size(0), -1),
                        z.view(z.size(0), -1)
                    ).mean().item()
                    print(f"\nğŸ” VFè¯­ä¹‰å¯¹é½æ£€æŸ¥:")
                    print(f"   VFç‰¹å¾èŒƒæ•°: {vf_norm:.4f}")
                    print(f"   æ½œåœ¨ç¼–ç èŒƒæ•°: {z_norm:.4f}")
                    print(f"   ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")
                    if similarity > 0.5:
                        print(f"   âœ… VFè¯­ä¹‰å¯¹é½è‰¯å¥½")
                    else:
                        print(f"   âš ï¸ VFè¯­ä¹‰å¯¹é½è¾ƒå·®ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒ")
                
                # åˆ›å»ºå¯è§†åŒ–
                fig, axes = plt.subplots(2, 8, figsize=(16, 4))
                
                for i in range(min(8, inputs.shape[0])):
                    # åŸå§‹å›¾åƒ
                    orig = inputs[i].cpu().numpy()
                    if orig.shape[0] == 3:  # RGB
                        orig = np.transpose(orig, (1, 2, 0))
                    else:  # å•é€šé“
                        orig = orig[0]
                    
                    # é‡å»ºå›¾åƒ
                    recon = reconstructions[i].cpu().numpy()
                    if recon.shape[0] == 3:  # RGB
                        recon = np.transpose(recon, (1, 2, 0))
                    else:  # å•é€šé“
                        recon = recon[0]
                    
                    # æ˜¾ç¤º
                    axes[0, i].imshow(orig, cmap='viridis' if orig.ndim == 2 else None)
                    axes[0, i].axis('off')
                    if i == 0:
                        axes[0, i].set_title('åŸå§‹')
                    
                    axes[1, i].imshow(recon, cmap='viridis' if recon.ndim == 2 else None)
                    axes[1, i].axis('off')
                    if i == 0:
                        axes[1, i].set_title('é‡å»º')
                
                plt.suptitle(f'Stage {self.stage} - Epoch {epoch} é‡å»ºæ•ˆæœ')
                save_path = os.path.join(self.save_dir, f'stage{self.stage}_epoch{epoch:03d}.png')
                plt.savefig(save_path, dpi=100, bbox_inches='tight')
                plt.close()
                print(f"   ğŸ’¾ é‡å»ºå›¾åƒå·²ä¿å­˜: {save_path}")
                
                break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        
        pl_module.train()

def create_stage_config(stage, checkpoint_path=None):
    """åˆ›å»ºé˜¶æ®µé…ç½® - ä¿®å¤ç‰ˆ"""
    # ä¸‰é˜¶æ®µè®­ç»ƒé…ç½®
    stage_configs = {
        1: {
            'disc_start': 5001, 
            'disc_weight': 0.5, 
            'vf_weight': 0.5,  # ä¿æŒåŸå§‹å€¼
            'distmat_margin': 0.0, 
            'cos_margin': 0.0, 
            'learning_rate': 1e-4, 
            'max_epochs': 50
        },
        2: {
            'disc_start': 1, 
            'disc_weight': 0.5, 
            'vf_weight': 0.1,  # Stage 2é™ä½VFæƒé‡
            'distmat_margin': 0.0, 
            'cos_margin': 0.0, 
            'learning_rate': 5e-5, 
            'max_epochs': 15
        },
        3: {
            'disc_start': 1, 
            'disc_weight': 0.5, 
            'vf_weight': 0.1, 
            'distmat_margin': 0.25,  # Stage 3å¯ç”¨margin
            'cos_margin': 0.5, 
            'learning_rate': 2e-5, 
            'max_epochs': 15
        }
    }
    
    params = stage_configs[stage]
    
    config = {
        'base_learning_rate': params['learning_rate'],
        'target': 'ldm.models.autoencoder.AutoencoderKL',
        'params': {
            'embed_dim': 32,
            'monitor': 'val/rec_loss',
            'use_vf': 'dinov2',
            'ddconfig': {
                'double_z': True,
                'z_channels': 32,
                'resolution': 256,
                'in_channels': 3,
                'out_ch': 3,
                'ch': 128,
                'ch_mult': [1, 1, 2, 2, 4],
                'num_res_blocks': 2,
                'attn_resolutions': [16],
                'dropout': 0.0
            },
            'lossconfig': {
                'target': 'ldm.modules.losses.contperceptual.LPIPSWithDiscriminator',
                'params': {
                    'disc_start': params['disc_start'],
                    'disc_weight': params['disc_weight'],
                    'disc_num_layers': 3,
                    'kl_weight': 1e-6,
                    'pixelloss_weight': 1.0,
                    'perceptual_weight': 1.0,  # å¯ç”¨LPIPS
                    'disc_in_channels': 3,
                    'disc_conditional': False,
                    'vf_weight': params['vf_weight'], 
                    'adaptive_vf': False,  # å…³é”®ä¿®å¤ï¼šç¦ç”¨è‡ªé€‚åº”VFé¿å…æƒé‡å¤±æ§
                    'distmat_weight': 1.0,
                    'cos_weight': 1.0,
                    'distmat_margin': params['distmat_margin'],
                    'cos_margin': params['cos_margin'],
                    'use_actnorm': False,
                    'pp_style': False
                }
            },
            'ckpt_path': checkpoint_path,
            'ignore_keys': [],
            'image_key': 'image',
            'colorize_nlabels': None,
            'proj_fix': False
        }
    }
    
    return config, params

def main():
    # è§£æå‚æ•°
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--stage', type=int, default=1, choices=[1, 2, 3])
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--data_dir', type=str, default='data/microdoppler_dataset')
    parser.add_argument('--checkpoint', type=str, default=None, help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--pretrained', type=str, 
                       default='pretrained_models/vavae/dinov2_f16d32_res256x256.pth',
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    args = parser.parse_args()
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"ğŸš€ VA-VAE å¢å¼ºç‰ˆè®­ç»ƒ - Stage {args.stage}")
    print(f"{'='*60}")
    print(f"ğŸ“Š é…ç½®:")
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"   æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"   é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained}")
    print(f"   æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"   âš ï¸ adaptive_vf: False (é¿å…æƒé‡å¤±æ§)")
    print(f"{'='*60}\n")
    
    # ç¡®å®šæ£€æŸ¥ç‚¹è·¯å¾„
    checkpoint_path = args.checkpoint
    if checkpoint_path is None and args.stage == 1:
        checkpoint_path = args.pretrained
    elif checkpoint_path is None and args.stage > 1:
        # è‡ªåŠ¨æŸ¥æ‰¾å‰ä¸€é˜¶æ®µçš„æœ€ä½³æ£€æŸ¥ç‚¹
        prev_stage = args.stage - 1
        ckpt_dir = f'logs/stage{prev_stage}/checkpoints'
        if os.path.exists(ckpt_dir):
            checkpoints = [f for f in os.listdir(ckpt_dir) if f.endswith('.ckpt')]
            if checkpoints:
                checkpoint_path = os.path.join(ckpt_dir, sorted(checkpoints)[-1])
                print(f"âœ… è‡ªåŠ¨åŠ è½½Stage {prev_stage}æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # åˆ›å»ºé…ç½®
    config, params = create_stage_config(args.stage, checkpoint_path)
    
    # åˆ›å»ºæ¨¡å‹
    model = instantiate_from_config(config)
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    data_module = MicroDopplerDataModule(
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )
    
    # åˆ›å»ºå›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        dirpath=f'logs/stage{args.stage}/checkpoints',
        filename='epoch{epoch:02d}-val_loss{val/rec_loss:.4f}',
        monitor='val/rec_loss',
        mode='min',
        save_top_k=3,
        save_last=True,
        every_n_epochs=1,
        verbose=True
    )
    
    loss_monitor = DetailedLossMonitor(
        stage=args.stage,
        save_dir=f'logs/stage{args.stage}/reconstructions'
    )
    
    # åˆ›å»ºlogger
    logger = TensorBoardLogger(
        save_dir='logs',
        name=f'stage{args.stage}',
        version=f'run_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = pl.Trainer(
        devices='auto',
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        max_epochs=params['max_epochs'],
        precision=32,  # ä½¿ç”¨FP32ä¿è¯ç¨³å®šæ€§
        callbacks=[checkpoint_callback, loss_monitor],
        logger=logger,
        log_every_n_steps=10,
        gradient_clip_val=1.0,  # æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
        enable_progress_bar=True
    )
    
    print(f"\nğŸ¯ å¼€å§‹Stage {args.stage}è®­ç»ƒ")
    print(f"   å­¦ä¹ ç‡: {config['base_learning_rate']:.2e}")
    print(f"   VFæƒé‡: {params['vf_weight']}")
    print(f"   åˆ¤åˆ«å™¨èµ·å§‹: {params['disc_start']}")
    print(f"   æœ€å¤§è½®æ•°: {params['max_epochs']}")
    print(f"{'='*60}\n")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.fit(model, data_module)
    
    print(f"\nâœ… Stage {args.stage}è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: logs/stage{args.stage}/checkpoints/")
    print(f"   é‡å»ºå›¾åƒä¿å­˜åœ¨: logs/stage{args.stage}/reconstructions/")
    
    # å¦‚æœä¸æ˜¯æœ€åé˜¶æ®µï¼Œæç¤ºä¸‹ä¸€æ­¥
    if args.stage < 3:
        print(f"\nğŸ“Œ ä¸‹ä¸€æ­¥:")
        print(f"   python step4_train_vavae_enhanced.py --stage {args.stage + 1}")

if __name__ == '__main__':
    main()

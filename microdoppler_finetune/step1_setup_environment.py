#!/usr/bin/env python3
"""
æ­¥éª¤1: VA-VAEå¾®è°ƒç¯å¢ƒé…ç½®
é…ç½®å¾®å¤šæ™®å‹’æ•°æ®å¾®è°ƒæ‰€éœ€çš„ç¯å¢ƒ
"""

import os
import sys
import subprocess
from pathlib import Path

def setup_taming_transformers(base_path):
    """è®¾ç½®taming-transformersï¼ˆå…‹éš†æ–¹å¼ï¼‰"""
    print("\nğŸ“¥ è®¾ç½®taming-transformers...")
    
    taming_dir = base_path / "taming-transformers"
    if not taming_dir.exists():
        print("ğŸ“¥ å…‹éš†taming-transformers...")
        try:
            subprocess.check_call([
                "git", "clone", 
                "https://github.com/CompVis/taming-transformers.git",
                str(taming_dir)
            ])
            print("âœ… taming-transformers å…‹éš†æˆåŠŸ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ å…‹éš†å¤±è´¥: {e}")
            return False
    else:
        print("âœ… taming-transformerså·²å­˜åœ¨")
    
    # ä¿®å¤torchå…¼å®¹æ€§é—®é¢˜
    utils_file = taming_dir / "taming" / "data" / "utils.py"
    if utils_file.exists():
        print("ğŸ”§ ä¿®å¤torchå…¼å®¹æ€§...")
        try:
            content = utils_file.read_text()
            if "from torch._six import string_classes" in content:
                content = content.replace(
                    "from torch._six import string_classes",
                    "from six import string_types as string_classes"
                )
                utils_file.write_text(content)
                print("âœ… å…¼å®¹æ€§ä¿®å¤å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å…¼å®¹æ€§ä¿®å¤å¤±è´¥: {e}")
    
    # æ·»åŠ åˆ°Pythonè·¯å¾„
    taming_path = str(taming_dir.absolute())
    if taming_path not in sys.path:
        sys.path.insert(0, taming_path)
        print(f"ğŸ“‚ å·²æ·»åŠ tamingè·¯å¾„: {taming_path}")
    
    # ä¿å­˜è·¯å¾„ä¿¡æ¯
    path_file = base_path / ".taming_path"
    with open(path_file, "w") as f:
        f.write(taming_path)
    
    return True

def setup_vavae_environment():
    """é…ç½®VA-VAEå¾®è°ƒç¯å¢ƒ"""
    print("ğŸ”§ é…ç½®VA-VAEå¾®è°ƒç¯å¢ƒ")
    print("="*60)
    
    # æ£€æµ‹è¿è¡Œç¯å¢ƒ
    if os.path.exists('/kaggle/working'):
        print("ğŸ“ æ£€æµ‹åˆ°Kaggleç¯å¢ƒ")
        base_path = Path('/kaggle/working')
    else:
        print("ğŸ“ æ£€æµ‹åˆ°æœ¬åœ°ç¯å¢ƒ")
        base_path = Path.cwd()
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
    dirs_to_create = [
        'models',           # å­˜æ”¾é¢„è®­ç»ƒæ¨¡å‹
        'checkpoints',      # å­˜æ”¾è®­ç»ƒæ£€æŸ¥ç‚¹
        'logs',            # è®­ç»ƒæ—¥å¿—
        'configs',         # é…ç½®æ–‡ä»¶
        'data_split',      # æ•°æ®é›†åˆ’åˆ†ä¿¡æ¯
        'visualizations'   # å¯è§†åŒ–ç»“æœ
    ]
    
    for dir_name in dirs_to_create:
        dir_path = base_path / dir_name
        dir_path.mkdir(exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {dir_path}")
    
    print("\nğŸ“¦ å®‰è£…å¿…è¦çš„PythonåŒ…...")
    
    # åŸºç¡€ä¾èµ–
    packages = [
        "torch>=2.0.0",
        "torchvision>=0.15.0", 
        "pytorch-lightning>=2.0.0",
        "transformers>=4.30.0",  # ç”¨äºDINOv2
        "einops>=0.6.0",
        "omegaconf>=2.3.0",
        "Pillow>=9.5.0",
        "numpy<2.0",  # é¿å…ç‰ˆæœ¬å†²çª
        "pandas",
        "matplotlib",
        "seaborn",
        "tensorboard",
        "tqdm",
        # LightningDiT/VA-VAEç‰¹å®šä¾èµ–
        "diffusers>=0.20.0",
        "accelerate>=0.20.0",
        "lpips>=0.1.4",  # æ„ŸçŸ¥æŸå¤±
        "timm>=0.9.0",   # Vision Transformeræ¨¡å‹
        # LightningDiT transportæ¨¡å—ä¾èµ–
        "torchdiffeq",   # ODEæ±‚è§£å™¨
        "torchsde",      # SDEæ±‚è§£å™¨
        "scipy",         # ç§‘å­¦è®¡ç®—
        # LightningDiTæ¨¡å‹ä¾èµ–
        "fairscale",     # æ¨¡å‹å¹¶è¡ŒåŒ–
        "einops",        # å¼ é‡æ“ä½œ
        "safetensors",   # å®‰å…¨æ¨¡å‹å­˜å‚¨
        "pytorch_fid",   # FIDè®¡ç®—
        "tensorboard",   # è®­ç»ƒç›‘æ§
        "omegaconf",     # é…ç½®ç®¡ç†
        # æ³¨æ„ï¼šä¸è¦åŒ…å«clip-by-openaiï¼Œæœ‰ä¾èµ–å†²çª
        # æ³¨æ„ï¼šä¸è¦åŒ…å«tamingï¼Œä½¿ç”¨å…‹éš†æ–¹å¼
    ]
    
    # å®‰è£…åŸºç¡€åŒ…
    for package in packages:
        try:
            # åªè·³è¿‡çœŸæ­£çš„torchæ ¸å¿ƒåŒ…ï¼Œä¸è·³è¿‡torchdiffeqç­‰
            torch_core_packages = ['torch>=', 'torchvision>=', 'pytorch-lightning>=']
            if any(package.startswith(core) for core in torch_core_packages) and os.path.exists('/kaggle/working'):
                # Kaggleå·²é¢„è£…PyTorchæ ¸å¿ƒåŒ…
                print(f"â­ï¸ è·³è¿‡ {package} (Kaggleé¢„è£…)")
                continue
            
            print(f"ğŸ“¥ å®‰è£… {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸ å®‰è£… {package} å¤±è´¥: {e}")
            print("   å°è¯•ç»§ç»­...")
    
    # è®¾ç½®taming-transformersï¼ˆä½¿ç”¨å…‹éš†æ–¹å¼ï¼‰
    setup_taming_transformers(base_path)
    
    # å…‹éš†æˆ–é“¾æ¥LightningDiTä»£ç 
    print("\nğŸ“‚ é…ç½®LightningDiTä»£ç åº“...")
    
    if os.path.exists('/kaggle/working'):
        # Kaggleç¯å¢ƒï¼šå‡è®¾ä»£ç å·²ä¸Šä¼ 
        lightningdit_path = base_path / 'LightningDiT'
        if not lightningdit_path.exists():
            print("âš ï¸ è¯·ç¡®ä¿å·²ä¸Šä¼ LightningDiTä»£ç åˆ°Kaggle")
            print("   æˆ–ä½¿ç”¨git clone: https://github.com/hustvl/LightningDiT.git")
    else:
        # æœ¬åœ°ç¯å¢ƒï¼šæ£€æŸ¥çˆ¶ç›®å½•
        parent_lightningdit = base_path.parent / 'LightningDiT'
        if parent_lightningdit.exists():
            print(f"âœ… æ‰¾åˆ°LightningDiT: {parent_lightningdit}")
            # æ·»åŠ åˆ°Pythonè·¯å¾„
            sys.path.insert(0, str(parent_lightningdit / 'vavae'))
            sys.path.insert(0, str(parent_lightningdit))
        else:
            print("âš ï¸ æœªæ‰¾åˆ°LightningDiTï¼Œéœ€è¦å…‹éš†ä»£ç åº“")
            clone_cmd = "git clone https://github.com/hustvl/LightningDiT.git"
            print(f"   è¿è¡Œ: {clone_cmd}")
    
    # æ£€æŸ¥GPU
    print("\nğŸ–¥ï¸ æ£€æŸ¥GPUé…ç½®...")
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    except ImportError:
        print("âŒ PyTorchæœªæ­£ç¡®å®‰è£…")
    
    # ä¿å­˜ç¯å¢ƒä¿¡æ¯
    env_info = {
        'base_path': str(base_path),
        'python_version': sys.version,
        'platform': sys.platform,
        'cuda_available': torch.cuda.is_available() if 'torch' in sys.modules else False
    }
    
    import json
    with open(base_path / 'environment_info.json', 'w') as f:
        json.dump(env_info, f, indent=2)
    
    print("\nâœ… ç¯å¢ƒé…ç½®å®Œæˆ!")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. è¿è¡Œ python step2_download_models.py ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹")
    print("2. è¿è¡Œ python step3_prepare_dataset.py å‡†å¤‡æ•°æ®é›†")
    print("3. è¿è¡Œ python step4_train_stage1.py å¼€å§‹ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ")
    
    return base_path

if __name__ == "__main__":
    setup_vavae_environment()

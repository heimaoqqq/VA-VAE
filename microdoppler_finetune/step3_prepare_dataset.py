#!/usr/bin/env python3
"""
æ­¥éª¤3: å‡†å¤‡å¾®å¤šæ™®å‹’æ•°æ®é›†
åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»º80/20åˆ’åˆ†
"""

import os
import json
import random
from pathlib import Path
from typing import Dict, List, Tuple
import shutil

def split_user_data(user_folder: Path, train_ratio: float = 0.8, seed: int = 42) -> Tuple[List[str], List[str]]:
    """ä¸ºå•ä¸ªç”¨æˆ·åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
    random.seed(seed)
    
    # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒ
    images = sorted([str(img) for img in user_folder.glob("*.jpg")])
    
    if not images:
        print(f"âš ï¸ {user_folder.name} æ²¡æœ‰æ‰¾åˆ°å›¾åƒ")
        return [], []
    
    # éšæœºæ‰“ä¹±å¹¶åˆ’åˆ†
    random.shuffle(images)
    split_idx = int(len(images) * train_ratio)
    
    train_images = images[:split_idx]
    val_images = images[split_idx:]
    
    return train_images, val_images

def prepare_microdoppler_dataset():
    """å‡†å¤‡å¾®å¤šæ™®å‹’æ•°æ®é›†"""
    print("ğŸ“Š å‡†å¤‡å¾®å¤šæ™®å‹’æ•°æ®é›†")
    print("="*60)
    
    # æ£€æµ‹ç¯å¢ƒ
    if os.path.exists('/kaggle/input/dataset'):
        data_root = Path('/kaggle/input/dataset')
        output_root = Path('/kaggle/working/data_split')
        print("ğŸ“ Kaggleç¯å¢ƒï¼šä½¿ç”¨/kaggle/input/dataset")
    else:
        # æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
        data_root = Path('G:/micro-doppler-dataset')  # è¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹
        output_root = Path.cwd() / 'data_split'
        print(f"ğŸ“ æœ¬åœ°ç¯å¢ƒï¼šæ•°æ®è·¯å¾„ {data_root}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not data_root.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        print("è¯·ç¡®ä¿æ•°æ®é›†ä½äºæ­£ç¡®ä½ç½®")
        print("Kaggle: /kaggle/input/dataset/")
        print("æœ¬åœ°: ä¿®æ”¹è„šæœ¬ä¸­çš„data_rootè·¯å¾„")
        return None
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_root.mkdir(exist_ok=True)
    
    # ç»Ÿè®¡æ•°æ®
    user_stats = {}
    total_images = 0
    
    print("\nğŸ“‚ æ‰«æç”¨æˆ·æ•°æ®...")
    for user_id in range(1, 32):
        user_folder = data_root / f"ID_{user_id}"
        if user_folder.exists():
            image_count = len(list(user_folder.glob("*.jpg")))
            user_stats[f"ID_{user_id}"] = image_count
            total_images += image_count
            print(f"   ID_{user_id}: {image_count} å¼ å›¾åƒ")
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»ç”¨æˆ·æ•°: {len(user_stats)}")
    print(f"   æ€»å›¾åƒæ•°: {total_images}")
    print(f"   å¹³å‡æ¯ç”¨æˆ·: {total_images/len(user_stats):.1f} å¼ ")
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ’åˆ†æ•°æ®
    print("\nâœ‚ï¸ åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† (80/20)...")
    
    dataset_split = {
        "train": {},
        "val": {},
        "test": {},  # é¢„ç•™æµ‹è¯•é›†
        "statistics": {
            "total_users": len(user_stats),
            "total_images": total_images,
            "train_images": 0,
            "val_images": 0,
            "split_ratio": "80/20",
            "random_seed": 42
        }
    }
    
    # æ¯ä¸ªç”¨æˆ·å†…éƒ¨åˆ’åˆ†ï¼ˆ80%è®­ç»ƒï¼Œ20%éªŒè¯ï¼‰
    print(f"\nğŸ¯ æ¯ä¸ªç”¨æˆ·å†…éƒ¨åˆ’åˆ† 80/20")
    
    for user_id in range(1, 32):
        user_folder = data_root / f"ID_{user_id}"
        if not user_folder.exists():
            continue
        
        user_key = f"ID_{user_id}"
        
        # æ¯ä¸ªç”¨æˆ·å†…éƒ¨80/20åˆ’åˆ†
        train_images, val_images = split_user_data(user_folder, train_ratio=0.8)
        dataset_split["train"][user_key] = train_images
        dataset_split["val"][user_key] = val_images
        dataset_split["statistics"]["train_images"] += len(train_images)
        dataset_split["statistics"]["val_images"] += len(val_images)
        print(f"   {user_key}: {len(train_images)} è®­ç»ƒ, {len(val_images)} éªŒè¯")
    
    # ä¿å­˜åˆ’åˆ†ä¿¡æ¯
    split_file = output_root / "dataset_split.json"
    with open(split_file, 'w') as f:
        json.dump(dataset_split, f, indent=2)
    print(f"\nâœ… æ•°æ®åˆ’åˆ†å·²ä¿å­˜åˆ°: {split_file}")
    
    # åˆ›å»ºç”¨æˆ·æ ‡ç­¾æ˜ å°„
    user_labels = {f"ID_{i}": i-1 for i in range(1, 32)}  # 0-30
    labels_file = output_root / "user_labels.json"
    with open(labels_file, 'w') as f:
        json.dump(user_labels, f, indent=2)
    print(f"âœ… ç”¨æˆ·æ ‡ç­¾å·²ä¿å­˜åˆ°: {labels_file}")
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    train_config = {
        "data_root": str(data_root),
        "split_file": str(split_file),
        "labels_file": str(labels_file),
        "num_users": len(user_stats),
        "split_strategy": "per_user_80_20",  # æ¯ä¸ªç”¨æˆ·å†…éƒ¨80/20åˆ’åˆ†
        "image_size": 256,
        "batch_size": 8,
        "num_workers": 4
    }
    
    config_file = output_root / "train_config.json"
    with open(config_file, 'w') as f:
        json.dump(train_config, f, indent=2)
    print(f"âœ… è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_file}")
    
    # æ˜¾ç¤ºæ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æ•°æ®é›†å‡†å¤‡å®Œæˆ!")
    print(f"   è®­ç»ƒå›¾åƒ: {dataset_split['statistics']['train_images']}")
    print(f"   éªŒè¯å›¾åƒ: {dataset_split['statistics']['val_images']}")
    print(f"   æ€»ç”¨æˆ·æ•°: {len(user_stats)} ä¸ª")
    print(f"   æ¯ä¸ªç”¨æˆ·éƒ½æœ‰è®­ç»ƒå’ŒéªŒè¯æ•°æ®ï¼ˆ80/20åˆ’åˆ†ï¼‰")
    
    print("\nä¸‹ä¸€æ­¥:")
    print("1. è¿è¡Œ python step4_train_stage1.py å¼€å§‹ç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼ˆè¯­ä¹‰å¯¹é½ï¼‰")
    print("2. å®Œæˆåè¿è¡Œ python step5_train_stage2.py è¿›è¡Œç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼ˆæ•´ä½“å¾®è°ƒï¼‰")
    
    return output_root

def verify_dataset_structure(data_root: Path):
    """éªŒè¯æ•°æ®é›†ç»“æ„"""
    print("\nğŸ” éªŒè¯æ•°æ®é›†ç»“æ„...")
    
    issues = []
    
    # æ£€æŸ¥ç”¨æˆ·æ–‡ä»¶å¤¹
    for user_id in range(1, 32):
        user_folder = data_root / f"ID_{user_id}"
        if not user_folder.exists():
            issues.append(f"ç¼ºå°‘ç”¨æˆ·æ–‡ä»¶å¤¹: ID_{user_id}")
        else:
            images = list(user_folder.glob("*.jpg"))
            if len(images) == 0:
                issues.append(f"ID_{user_id} æ²¡æœ‰å›¾åƒ")
            elif len(images) < 50:
                issues.append(f"ID_{user_id} å›¾åƒè¿‡å°‘: {len(images)}")
    
    if issues:
        print("âš ï¸ å‘ç°ä»¥ä¸‹é—®é¢˜:")
        for issue in issues:
            print(f"   - {issue}")
    else:
        print("âœ… æ•°æ®é›†ç»“æ„æ­£å¸¸")
    
    return len(issues) == 0

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_root', type=str, default=None, help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--verify_only', action='store_true', help='ä»…éªŒè¯æ•°æ®é›†')
    args = parser.parse_args()
    
    if args.data_root:
        data_root = Path(args.data_root)
        if args.verify_only:
            verify_dataset_structure(data_root)
        else:
            # è‡ªå®šä¹‰è·¯å¾„
            prepare_microdoppler_dataset()
    else:
        # è‡ªåŠ¨æ£€æµ‹
        prepare_microdoppler_dataset()

#!/usr/bin/env python3
"""å¿«é€ŸéªŒè¯é‡åŒ–æ¨¡å‹æ˜¯å¦çœŸå®é‡åŒ–"""

import torch
from pathlib import Path
import sys
import os

# æ·»åŠ å¿…è¦çš„è·¯å¾„
sys.path.append('/kaggle/working/VA-VAE')
sys.path.append('/kaggle/working/VA-VAE/LightningDiT')

try:
    from models.lightningdit import LightningDiT_models
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥LightningDiTæ¨¡å‹ï¼Œè·³è¿‡æ¨¡å‹ç»“æ„éªŒè¯")

def quick_check():
    quantized_path = "/kaggle/working/dit_xl_quantized.pt"
    
    print(f"ğŸ” æ£€æŸ¥é‡åŒ–æ¨¡å‹: {quantized_path}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if Path(quantized_path).exists():
        file_size_mb = Path(quantized_path).stat().st_size / (1024**2)
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size_mb:.1f}MB")
        
        if file_size_mb < 1000:  # å°äº1GB
            print("âš ï¸ æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½é‡åŒ–å¤±è´¥")
        else:
            print("âœ… æ–‡ä»¶å¤§å°æ­£å¸¸ï¼Œå¯èƒ½æ˜¯çœŸå®é‡åŒ–")
    else:
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
    
    if not Path(quantized_path).exists():
        print("âŒ é‡åŒ–æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # åŠ è½½æ¨¡å‹ï¼ˆPyTorch 2.6+ éœ€è¦ weights_only=Falseï¼‰
    try:
        data = torch.load(quantized_path, map_location='cpu', weights_only=False)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # æ£€æŸ¥åŠ è½½çš„æ•°æ®ç±»å‹
    print(f"ğŸ“‹ åŠ è½½æ•°æ®ç±»å‹: {type(data)}")
    
    if isinstance(data, dict):
        print("ğŸ“¦ åŠ è½½çš„æ˜¯ç»“æ„åŒ–å­—å…¸")
        
        # æ£€æŸ¥å­—å…¸é”®
        print(f"   åŒ…å«é”®: {list(data.keys())}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡åŒ–æ¨¡å‹
        if 'quantized_model' in data:
            print("âœ… æ‰¾åˆ° 'quantized_model' é”®")
            quantized_model = data['quantized_model']
            print(f"   é‡åŒ–æ¨¡å‹ç±»å‹: {type(quantized_model)}")
            
            if hasattr(quantized_model, 'named_modules'):
                print("âœ… é‡åŒ–æ¨¡å‹æ˜¯å®Œæ•´æ¨¡å‹å¯¹è±¡")
                
                # æ£€æŸ¥é‡åŒ–å±‚
                quantized_layers = 0
                total_modules = 0
                
                for name, module in quantized_model.named_modules():
                    total_modules += 1
                    # æ£€æŸ¥å¤šç§é‡åŒ–æ ‡è¯†
                    if (hasattr(module, '_packed_params') or 
                        'quantized' in str(type(module)).lower() or
                        hasattr(module, 'scale') and hasattr(module, 'zero_point')):
                        quantized_layers += 1
                        print(f"   âœ… é‡åŒ–å±‚: {name} | ç±»å‹: {type(module).__name__}")
                
                print(f"\nğŸ“Š é‡åŒ–ç»Ÿè®¡:")
                print(f"   æ€»æ¨¡å—æ•°: {total_modules}")
                print(f"   é‡åŒ–å±‚æ•°: {quantized_layers}")
                
                if quantized_layers == 0:
                    print("âŒ æœªæ‰¾åˆ°é‡åŒ–å±‚ï¼æ£€æŸ¥æƒé‡ç±»å‹...")
                    # æ£€æŸ¥æƒé‡ç²¾åº¦
                    param_count = 0
                    for name, param in quantized_model.named_parameters():
                        if param_count >= 3:
                            break
                        print(f"   {name[:40]:40} | dtype: {param.dtype} | shape: {param.shape}")
                        param_count += 1
                else:
                    print(f"âœ… é‡åŒ–æˆåŠŸï¼æ‰¾åˆ° {quantized_layers} ä¸ªé‡åŒ–å±‚")
                    
            elif isinstance(quantized_model, dict):
                print("âš ï¸ é‡åŒ–æ¨¡å‹æ˜¯state_dictï¼Œä¸æ˜¯å®Œæ•´æ¨¡å‹")
                print(f"   æƒé‡æ•°é‡: {len(quantized_model)}")
                print("âŒ é‡åŒ–å¤±è´¥ï¼šä¿å­˜çš„æ˜¯æƒé‡è€Œéé‡åŒ–æ¨¡å‹ç»“æ„")
            else:
                print(f"â“ æœªçŸ¥é‡åŒ–æ¨¡å‹ç±»å‹: {type(quantized_model)}")
        else:
            print("âŒ æœªæ‰¾åˆ° 'quantized_model' é”®")
            print("   å¯èƒ½çš„é”®:", list(data.keys()))
            print("âŒ é‡åŒ–å¤±è´¥ï¼šæ–‡ä»¶ç»“æ„ä¸æ­£ç¡®")
        
    elif hasattr(data, 'named_modules'):
        print("âœ… åŠ è½½çš„æ˜¯å®Œæ•´æ¨¡å‹")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡åŒ–å±‚
        quantized_layers = 0
        total_modules = 0
        
        for name, module in data.named_modules():
            total_modules += 1
            if hasattr(module, '_packed_params'):
                quantized_layers += 1
                print(f"âœ… æ‰¾åˆ°é‡åŒ–å±‚: {name}")
        
        print(f"\nğŸ“Š é‡åŒ–ç»Ÿè®¡:")
        print(f"   æ€»æ¨¡å—æ•°: {total_modules}")
        print(f"   é‡åŒ–å±‚æ•°: {quantized_layers}")
        
        if quantized_layers == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é‡åŒ–å±‚ï¼é‡åŒ–å¤±è´¥")
        else:
            print(f"âœ… æ‰¾åˆ° {quantized_layers} ä¸ªé‡åŒ–å±‚")
            
    else:
        print(f"â“ æœªçŸ¥æ•°æ®ç±»å‹: {type(data)}")

if __name__ == "__main__":
    quick_check()

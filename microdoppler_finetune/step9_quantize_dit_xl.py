#!/usr/bin/env python3
"""
æ­¥éª¤9: é‡åŒ–å·²è®­ç»ƒçš„LightningDiT XLæ¨¡å‹
å¯¹å·²å®Œæˆè®­ç»ƒçš„DiT XLæ¨¡å‹è¿›è¡Œæ¨ç†ä¼˜åŒ–ï¼Œå‡å°‘å†…å­˜å ç”¨å’ŒåŠ é€Ÿç”Ÿæˆ
"""

import torch
import torch.quantization as quantization
import time
import os
import sys
from pathlib import Path
import psutil
import numpy as np
from tqdm import tqdm

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('/kaggle/working/VA-VAE/LightningDiT')
sys.path.append('/kaggle/working/LightningDiT')

from models.lightningdit import LightningDiT_models
from transport import create_transport, Sampler

def get_gpu_free_memory(gpu_id):
    """è·å–æŒ‡å®šGPUçš„ç©ºé—²æ˜¾å­˜(GB)"""
    torch.cuda.set_device(gpu_id)
    torch.cuda.empty_cache()
    props = torch.cuda.get_device_properties(gpu_id)
    total = props.total_memory / (1024**3)
    allocated = torch.cuda.memory_allocated(gpu_id) / (1024**3)
    return total - allocated

def select_best_gpu_for_xl_model(use_cpu_loading=True):
    """æ™ºèƒ½é€‰æ‹©æœ€ä½³GPUè®¾å¤‡å¤„ç†T4x2ä¸å‡åŒ€æ˜¾å­˜åˆ†é…"""
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return 'cpu', 'cpu'
    
    num_gpus = torch.cuda.device_count()
    print(f"ğŸ” æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU")
    
    # æ£€æŸ¥æ¯ä¸ªGPUçš„æ˜¾å­˜æƒ…å†µ
    best_gpu = 0
    max_free_memory = 0
    gpu_info = []
    
    for i in range(num_gpus):
        props = torch.cuda.get_device_properties(i)
        total_memory = props.total_memory / (1024**3)
        
        # è·å–å½“å‰ç©ºé—²æ˜¾å­˜
        torch.cuda.set_device(i)
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated(i) / (1024**3)
        free_memory = total_memory - allocated
        
        gpu_info.append({
            'id': i,
            'name': props.name,
            'total': total_memory,
            'allocated': allocated, 
            'free': free_memory
        })
        
        print(f"   GPU {i}: {props.name}")
        print(f"     æ€»æ˜¾å­˜: {total_memory:.1f}GB")
        print(f"     å·²ç”¨æ˜¾å­˜: {allocated:.1f}GB") 
        print(f"     ç©ºé—²æ˜¾å­˜: {free_memory:.1f}GB")
        
        if free_memory > max_free_memory:
            max_free_memory = free_memory
            best_gpu = i
    
    # å†³å®šåŠ è½½ç­–ç•¥
    xl_model_requirement = 12.0  # XLæ¨¡å‹å¤§çº¦éœ€è¦12GB
    
    print(f"\nğŸ’¡ XLæ¨¡å‹æ˜¾å­˜éœ€æ±‚: ~{xl_model_requirement:.1f}GB")
    print(f"   æœ€ä½³GPU: GPU {best_gpu} (ç©ºé—² {max_free_memory:.1f}GB)")
    
    if max_free_memory >= xl_model_requirement:
        # æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥ç›´æ¥åœ¨GPUä¸ŠåŠ è½½
        device = f'cuda:{best_gpu}'
        load_device = device
        print(f"âœ… æ˜¾å­˜å……è¶³ï¼Œç›´æ¥åœ¨ {device} ä¸ŠåŠ è½½XLæ¨¡å‹")
    elif max_free_memory >= 6.0 and use_cpu_loading:
        # æ˜¾å­˜ä¸è¶³åŠ è½½XLï¼Œä½†è¶³å¤Ÿé‡åŒ–åçš„æ¨¡å‹ï¼Œä½¿ç”¨CPUåŠ è½½ç­–ç•¥  
        device = f'cuda:{best_gpu}'
        load_device = 'cpu'
        print(f"âš ï¸ æ˜¾å­˜ä¸è¶³åŠ è½½XLæ¨¡å‹ï¼Œä½¿ç”¨CPUåŠ è½½ç­–ç•¥")
        print(f"   åŠ è½½è®¾å¤‡: CPU")
        print(f"   é‡åŒ–åç§»è‡³: {device}")
    else:
        # æ˜¾å­˜ä¸¥é‡ä¸è¶³ï¼Œå…¨CPUæ“ä½œ
        device = 'cpu'
        load_device = 'cpu'
        print(f"âŒ æ˜¾å­˜ä¸è¶³ï¼Œä½¿ç”¨CPUè¿›è¡Œæ‰€æœ‰æ“ä½œ")
    
    # è®¾ç½®ä¸»GPU
    if device != 'cpu':
        torch.cuda.set_device(best_gpu)
    
    return device, load_device

def load_trained_dit_xl(checkpoint_path, device):
    """åŠ è½½å·²è®­ç»ƒçš„DiT XLæ¨¡å‹"""
    print(f"ğŸ“‚ åŠ è½½è®­ç»ƒå®Œæˆçš„DiT XLæ¨¡å‹: {checkpoint_path}")
    
    # é¦–å…ˆåŠ è½½checkpointæ¥æ£€æŸ¥æ¨¡å‹æ¶æ„
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    if 'model' in checkpoint:
        state_dict = checkpoint['model']
    elif 'ema' in checkpoint:
        state_dict = checkpoint['ema']
    else:
        state_dict = checkpoint
    
    # ä»æƒé‡æ¨æ–­æ¨¡å‹é…ç½®
    pos_embed_shape = None
    y_embed_shape = None
    final_layer_shape = None
    has_swiglu = False
    
    print(f"ğŸ” åˆ†æcheckpointæƒé‡é”®...")
    print(f"   CheckpointåŒ…å« {len(state_dict)} ä¸ªæƒé‡é”®")
    
    # åˆ—å‡ºå…³é”®æƒé‡é”®ä»¥ä¾¿è°ƒè¯•
    key_patterns = ['pos_embed', 'y_embedder', 'final_layer', 'mlp.w12']
    for pattern in key_patterns:
        matching_keys = [k for k in state_dict.keys() if pattern in k]
        if matching_keys:
            print(f"   {pattern} ç›¸å…³é”®: {matching_keys}")
    
    for key, tensor in state_dict.items():
        # å¤„ç†DataParallelä¿å­˜çš„æ¨¡å‹æƒé‡é”®ï¼ˆå»é™¤module.å‰ç¼€ï¼‰
        clean_key = key.replace('module.', '') if key.startswith('module.') else key
        
        if clean_key == 'pos_embed':
            pos_embed_shape = tensor.shape  # [1, seq_len, dim]
            print(f"   âœ“ æ‰¾åˆ°pos_embed: {pos_embed_shape} (é”®: {key})")
        elif clean_key == 'y_embedder.embedding_table.weight':
            y_embed_shape = tensor.shape    # [num_classes, dim]
            print(f"   âœ“ æ‰¾åˆ°y_embedder: {y_embed_shape} (é”®: {key})")
        elif clean_key == 'final_layer.linear.weight':
            final_layer_shape = tensor.shape  # [out_channels, dim]
            print(f"   âœ“ æ‰¾åˆ°final_layer: {final_layer_shape} (é”®: {key})")
        elif 'mlp.w12' in clean_key and not has_swiglu:
            has_swiglu = True
            print(f"   âœ“ æ£€æµ‹åˆ°SwiGLU: {key}")
    
    # æ¨æ–­å‚æ•° - ç²¾ç¡®åŒ¹é…checkpointä¸­çš„å®é™…é…ç½®
    if pos_embed_shape:
        seq_len = pos_embed_shape[1]  # åºåˆ—é•¿åº¦
        input_size = int(seq_len**0.5)  # input_size = sqrt(seq_len)
        print(f"   åºåˆ—é•¿åº¦: {seq_len} -> input_size: {input_size}")
    else:
        input_size = 16  # é»˜è®¤å€¼
        
    num_classes = y_embed_shape[0] if y_embed_shape else 1000  # ä»checkpointè¯»å–å®é™…ç±»åˆ«æ•°
    out_channels = final_layer_shape[0] if final_layer_shape else 32
    patch_size = 1  # å®˜æ–¹XLé¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨patch_size=1 (LightningDiT-XL/1)
    
    # è°ƒè¯•è¾“å‡ºå˜é‡çŠ¶æ€
    print(f"   ğŸ“Š å˜é‡æ£€æŸ¥:")
    print(f"     pos_embed_shape: {pos_embed_shape}")
    print(f"     y_embed_shape: {y_embed_shape}")
    print(f"     final_layer_shape: {final_layer_shape}")
    print(f"     has_swiglu: {has_swiglu}")
    
    print(f"ğŸ“‹ æ£€æµ‹åˆ°çš„æ¨¡å‹é…ç½®:")
    print(f"   è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
    print(f"   ç±»åˆ«æ•°é‡: {num_classes}")
    print(f"   è¾“å‡ºé€šé“: {out_channels}")
    print(f"   MLPç±»å‹: {'SwiGLU' if has_swiglu else 'Standard'}")
    print(f"   è¡¥ä¸å¤§å°: {patch_size}")
    
    # åˆ›å»ºæ¨¡å‹æ¶æ„ - ä½¿ç”¨å®˜æ–¹XL/1é…ç½®ï¼ˆä¸é¢„è®­ç»ƒæƒé‡åŒ¹é…ï¼‰
    model = LightningDiT_models['LightningDiT-XL/1'](
        input_size=input_size,      # ä»æƒé‡æ¨æ–­
        num_classes=num_classes,    # ä»æƒé‡æ¨æ–­
        class_dropout_prob=0.0,     # æ¨ç†æ—¶ä¸ä½¿ç”¨dropout
        use_qknorm=False,           # å®˜æ–¹é…ç½®
        use_swiglu=has_swiglu,      # ä»æƒé‡æ£€æµ‹
        use_rope=True,              # å®˜æ–¹é…ç½®
        use_rmsnorm=True,           # å®˜æ–¹é…ç½® 
        wo_shift=False,             # å®˜æ–¹é…ç½®
        in_channels=out_channels,   # ä»æƒé‡æ¨æ–­
        use_checkpoint=False,       # æ¨ç†æ—¶ä¸ä½¿ç”¨checkpoint
    )
    
    # å¤„ç†DataParallelä¿å­˜çš„æƒé‡ï¼ˆå»é™¤module.å‰ç¼€ï¼‰
    if any(key.startswith('module.') for key in state_dict.keys()):
        print(f"ğŸ”§ æ£€æµ‹åˆ°DataParallelæƒé‡ï¼Œå»é™¤module.å‰ç¼€...")
        clean_state_dict = {}
        for key, value in state_dict.items():
            clean_key = key.replace('module.', '') if key.startswith('module.') else key
            clean_state_dict[clean_key] = value
        state_dict = clean_state_dict
        print(f"   å¤„ç†å®Œæˆï¼Œæƒé‡é”®æ•°é‡: {len(state_dict)}")
    
    # åŠ è½½æƒé‡
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹æƒé‡...")
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    
    if missing_keys:
        print(f"âš ï¸ ç¼ºå¤±çš„æƒé‡é”® ({len(missing_keys)}ä¸ª):")
        for key in missing_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   - {key}")
        if len(missing_keys) > 5:
            print(f"   ... å’Œå…¶ä»– {len(missing_keys)-5} ä¸ª")
    
    if unexpected_keys:
        print(f"âš ï¸ æ„å¤–çš„æƒé‡é”® ({len(unexpected_keys)}ä¸ª):")
        for key in unexpected_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   - {key}")
        if len(unexpected_keys) > 5:
            print(f"   ... å’Œå…¶ä»– {len(unexpected_keys)-5} ä¸ª")
    
    model = model.to(device)
    model.eval()
    
    # è®¡ç®—æˆåŠŸåŠ è½½çš„æƒé‡æ¯”ä¾‹
    total_params = len(state_dict)
    loaded_params = total_params - len(missing_keys)
    load_ratio = loaded_params / total_params * 100 if total_params > 0 else 0
    
    print(f"âœ… DiT XLæ¨¡å‹åŠ è½½å®Œæˆ ({load_ratio:.1f}%æƒé‡åŒ¹é…)")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
    print(f"   æ¨¡å‹å¤§å°: {sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2):.1f}MB")
    
    # å¦‚æœæƒé‡åŒ¹é…ç‡å¤ªä½ï¼Œç»™å‡ºè­¦å‘Š
    if load_ratio < 80:
        print(f"âš ï¸ æƒé‡åŒ¹é…ç‡è¾ƒä½ ({load_ratio:.1f}%)ï¼Œé‡åŒ–åçš„æ€§èƒ½å¯èƒ½å—å½±å“")
        print(f"   å»ºè®®ä½¿ç”¨ä¸checkpointå®Œå…¨åŒ¹é…çš„æ¨¡å‹æ¶æ„")
    
    return model

def apply_dynamic_quantization(model):
    """åº”ç”¨åŠ¨æ€é‡åŒ–åˆ°æ¨¡å‹ï¼ˆå¸¦å…¼å®¹æ€§æ£€æŸ¥ï¼‰"""
    print(f"\nğŸ”§ å¼€å§‹åº”ç”¨åŠ¨æ€é‡åŒ–...")
    
    # å°†æ¨¡å‹ç§»åˆ°CPUè¿›è¡Œé‡åŒ–
    model_cpu = model.cpu()
    
    try:
        # å°è¯•åº”ç”¨åŠ¨æ€é‡åŒ– - åªé‡åŒ–Linearå±‚
        quantized_model = torch.quantization.quantize_dynamic(
            model_cpu,           # åŸå§‹æ¨¡å‹ï¼ˆCPUï¼‰
            {torch.nn.Linear},   # è¦é‡åŒ–çš„å±‚ç±»å‹
            dtype=torch.qint8,   # é‡åŒ–ç²¾åº¦
            inplace=False         # ä¸ä¿®æ”¹åŸæ¨¡å‹
        )
        
        # éªŒè¯é‡åŒ–æ˜¯å¦çœŸæ­£ç”Ÿæ•ˆ
        quantized_layers = 0
        for name, module in quantized_model.named_modules():
            if hasattr(module, '_packed_params'):
                quantized_layers += 1
        
        print("âœ… åŠ¨æ€é‡åŒ–å®Œæˆ")
        print("   é‡åŒ–ç›®æ ‡: Linear layers only")
        print("   é‡åŒ–ç²¾åº¦: INT8")
        print("   é‡åŒ–æ–¹å¼: åŠ¨æ€é‡åŒ–ï¼ˆæ¨ç†æ—¶å®æ—¶é‡åŒ–activationï¼‰")
        print(f"   âœ… æ£€æµ‹åˆ° {quantized_layers} ä¸ªé‡åŒ–å±‚")
        
        if quantized_layers == 0:
            print("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°é‡åŒ–å±‚ï¼Œé‡åŒ–å¯èƒ½å¤±è´¥")
            print("   å›é€€åˆ°åŸå§‹æ¨¡å‹")
            return model_cpu
        
        return quantized_model
        
    except (RuntimeError, NotImplementedError) as e:
        print(f"âš ï¸ åŠ¨æ€é‡åŒ–å¤±è´¥: {str(e)}")
        print("   å›é€€åˆ°åŸå§‹æ¨¡å‹ï¼ˆCPUç‰ˆæœ¬ï¼‰")
        print("   è¿™å¯èƒ½æ˜¯ç”±äºæ¨¡å‹æ¶æ„å…¼å®¹æ€§é—®é¢˜")
        
        # è¿”å›CPUç‰ˆæœ¬çš„åŸå§‹æ¨¡å‹ä½œä¸ºfallback
        return model_cpu

def measure_model_size(model, model_name):
    """æµ‹é‡æ¨¡å‹å¤§å°"""
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶æµ‹é‡å¤§å°
    temp_path = f'temp_{model_name}.pt'
    torch.save(model, temp_path)  # ä¿å­˜å®Œæ•´æ¨¡å‹è€Œéstate_dict
    size_mb = os.path.getsize(temp_path) / (1024**2)
    os.remove(temp_path)
    return size_mb

def benchmark_inference_speed(model, model_name, device, num_runs=50):
    """åŸºå‡†æµ‹è¯•æ¨ç†é€Ÿåº¦ï¼ˆå¸¦OOMä¿æŠ¤ï¼‰"""
    print(f"\nâ±ï¸ æµ‹è¯• {model_name} æ¨ç†é€Ÿåº¦...")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé‡åŒ–æ¨¡å‹
    is_quantized = any(hasattr(module, '_packed_params') for module in model.modules())
    
    if is_quantized:
        # é‡åŒ–æ¨¡å‹æŒ‰PyTorchå®˜æ–¹æ ‡å‡†åœ¨CPUä¸Šæ¨ç†
        actual_device = 'cpu'
        model_device = next(model.parameters()).device
        if model_device.type != 'cpu':
            model = model.cpu()  # ç¡®ä¿é‡åŒ–æ¨¡å‹åœ¨CPUä¸Š
        batch_size = 2
        num_runs = min(num_runs, 10)  # CPUæµ‹è¯•æ¬¡æ•°å°‘ä¸€äº›
        print(f"   é‡åŒ–æ¨¡å‹æŒ‰PyTorchå®˜æ–¹æ ‡å‡†ä½¿ç”¨CPUæ¨ç†")
        print(f"   CPU batch size: {batch_size}")
    elif device == 'cpu':
        actual_device = 'cpu'
        batch_size = 2  # CPUä½¿ç”¨æ›´å°batch
        num_runs = min(num_runs, 10)  # CPUæµ‹è¯•æ¬¡æ•°å°‘ä¸€äº›
    else:
        # åŸå§‹æ¨¡å‹å¯ä»¥ä½¿ç”¨GPU
        actual_device = device
        # æ ¹æ®GPUæ˜¾å­˜åŠ¨æ€è°ƒæ•´batch size
        gpu_id = int(device.split(':')[1]) if ':' in device else 0
        free_memory = get_gpu_free_memory(gpu_id)
        
        if free_memory > 10:
            batch_size = 4
        elif free_memory > 6:
            batch_size = 2
        else:
            batch_size = 1
            
        print(f"   è‡ªé€‚åº”batch size: {batch_size} (åŸºäº {free_memory:.1f}GB ç©ºé—²æ˜¾å­˜)")
    
    try:
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_latents = torch.randn(batch_size, 32, 16, 16).to(actual_device)  # VA-VAE latent format
        test_timesteps = torch.randint(0, 1000, (batch_size,)).to(actual_device)
        test_labels = torch.randint(0, 1001, (batch_size,)).to(actual_device)  # 1001 classes
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print(f"   âŒ åˆ›å»ºæµ‹è¯•æ•°æ®OOMï¼Œè·³è¿‡æ€§èƒ½æµ‹è¯•")
            return 0.1, 0.0  # è¿”å›é»˜è®¤å€¼
        else:
            raise e
    
    model.eval()
    times = []
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(5):
            _ = model(test_latents, test_timesteps, y=test_labels)
    
    # æ­£å¼æµ‹è¯•ï¼ˆå¸¦OOMä¿æŠ¤ï¼‰
    if actual_device != 'cpu':
        torch.cuda.synchronize()
        
    with torch.no_grad():
        for i in tqdm(range(num_runs), desc=f"Testing {model_name}"):
            try:
                start_time = time.time()
                _ = model(test_latents, test_timesteps, y=test_labels)
                if actual_device != 'cpu':
                    torch.cuda.synchronize()
                times.append(time.time() - start_time)
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"\n   âš ï¸ æ¨ç†è¿‡ç¨‹OOMï¼Œä½¿ç”¨å·²æœ‰ {len(times)} æ¬¡æµ‹è¯•ç»“æœ")
                    torch.cuda.empty_cache()
                    break
                else:
                    raise e
    
    avg_time = np.mean(times)
    std_time = np.std(times)
    
    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f}ms Â± {std_time*1000:.2f}ms")
    print(f"   ååé‡: {batch_size/avg_time:.2f} samples/sec")
    
    return avg_time, std_time

def benchmark_memory_usage(model, model_name, device):
    """åŸºå‡†æµ‹è¯•å†…å­˜ä½¿ç”¨"""
    print(f"\nğŸ’¾ æµ‹è¯• {model_name} å†…å­˜ä½¿ç”¨...")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé‡åŒ–æ¨¡å‹ï¼ˆåœ¨CPUä¸Šï¼‰
    is_quantized = any(hasattr(module, '_packed_params') for module in model.modules())
    model_device = next(model.parameters()).device
    
    if is_quantized or model_device.type == 'cpu':
        # é‡åŒ–æ¨¡å‹ä½¿ç”¨CPU
        actual_device = 'cpu'
        print(f"   é‡åŒ–æ¨¡å‹ä½¿ç”¨CPUå†…å­˜æµ‹è¯•")
        memory_mb = 0  # CPUå†…å­˜ä¸æ˜“ç²¾ç¡®æµ‹é‡ï¼Œè®¾ä¸º0
    else:
        # åŸå§‹æ¨¡å‹ä½¿ç”¨GPU
        actual_device = device
        # æ¸…ç©ºç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    
    # æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨æ¨¡å‹å®é™…æ‰€åœ¨è®¾å¤‡ï¼‰
    batch_size = 4
    test_latents = torch.randn(batch_size, 32, 16, 16).to(actual_device)
    test_timesteps = torch.randint(0, 1000, (batch_size,)).to(actual_device)
    test_labels = torch.randint(0, 1001, (batch_size,)).to(actual_device)  # 1001 classes
    
    # æ‰§è¡Œæ¨ç†
    model.eval()
    with torch.no_grad():
        _ = model(test_latents, test_timesteps, y=test_labels)
    
    # è·å–å†…å­˜ä½¿ç”¨
    if actual_device != 'cpu':
        memory_mb = torch.cuda.max_memory_allocated() / (1024**2)
        print(f"   å³°å€¼æ˜¾å­˜ä½¿ç”¨: {memory_mb:.1f}MB")
    else:
        print(f"   CPUå†…å­˜ä½¿ç”¨: ä¸æ˜“ç²¾ç¡®æµ‹é‡")
        
    model_size_mb = measure_model_size(model, model_name)
    print(f"   æ¨¡å‹æ–‡ä»¶å¤§å°: {model_size_mb:.1f}MB")
    
    return memory_mb, model_size_mb

def comprehensive_benchmark(original_model, quantized_model, device):
    """å…¨é¢çš„æ€§èƒ½å¯¹æ¯”åŸºå‡†æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ” å¼€å§‹å…¨é¢æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*60)
    
    results = {}
    
    # 1. æ¨ç†é€Ÿåº¦æµ‹è¯•
    print("\nğŸ“Š 1. æ¨ç†é€Ÿåº¦å¯¹æ¯”")
    original_time, _ = benchmark_inference_speed(original_model, "Original DiT XL", device)
    quantized_time, _ = benchmark_inference_speed(quantized_model, "Quantized DiT XL", device)
    
    speedup = original_time / quantized_time
    print(f"\n   åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    # 2. å†…å­˜ä½¿ç”¨æµ‹è¯•
    print("\nğŸ“Š 2. å†…å­˜ä½¿ç”¨å¯¹æ¯”")
    orig_memory, orig_size = benchmark_memory_usage(original_model, "original", device)
    quant_memory, quant_size = benchmark_memory_usage(quantized_model, "quantized", device)
    
    memory_reduction = (orig_memory - quant_memory) / orig_memory * 100
    size_reduction = (orig_size - quant_size) / orig_size * 100
    
    print(f"\n   æ˜¾å­˜èŠ‚çœ: {memory_reduction:.1f}%")
    print(f"   æ–‡ä»¶å¤§å°å‡å°‘: {size_reduction:.1f}%")
    
    # 3. æ±‡æ€»ç»“æœ
    results = {
        'speedup': speedup,
        'memory_reduction': memory_reduction,
        'size_reduction': size_reduction,
        'original_time': original_time,
        'quantized_time': quantized_time,
        'original_memory': orig_memory,
        'quantized_memory': quant_memory,
        'original_size': orig_size,
        'quantized_size': quant_size
    }
    
    print("\n" + "="*60)
    print("ğŸ“ˆ é‡åŒ–æ•ˆæœæ±‡æ€»:")
    print(f"   ğŸš€ æ¨ç†åŠ é€Ÿ:     {speedup:.2f}x")
    print(f"   ğŸ’¾ æ˜¾å­˜èŠ‚çœ:     {memory_reduction:.1f}%")
    print(f"   ğŸ“¦ æ¨¡å‹å‹ç¼©:     {size_reduction:.1f}%")
    print(f"   â±ï¸  å»¶è¿Ÿæ”¹å–„:     {original_time*1000:.1f}ms â†’ {quantized_time*1000:.1f}ms")
    print("="*60)
    
    return results

def test_generation_quality(model, vae, device, num_samples=4):
    """æµ‹è¯•ç”Ÿæˆè´¨é‡"""
    print(f"\nğŸ¨ æµ‹è¯•ç”Ÿæˆè´¨é‡ ({num_samples} samples)...")
    
    model.eval()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé‡åŒ–æ¨¡å‹ï¼Œç¡®å®šå®é™…è®¾å¤‡
    is_quantized = any(hasattr(module, '_packed_params') for module in model.modules())
    model_device = next(model.parameters()).device
    
    if is_quantized or model_device.type == 'cpu':
        actual_device = 'cpu'
        print(f"   é‡åŒ–æ¨¡å‹ä½¿ç”¨CPUç”Ÿæˆ")
    else:
        actual_device = device
    
    # åˆ›å»ºtransportç”¨äºé‡‡æ ·
    transport = create_transport(
        path_type='Linear',
        prediction='velocity',
        loss_weight=None,
        train_eps=1e-5,
        sample_eps=1e-5,
    )
    
    # ç”Ÿæˆæ ·æœ¬
    with torch.no_grad():
        # éšæœºå™ªå£°ï¼ˆä½¿ç”¨æ¨¡å‹æ‰€åœ¨è®¾å¤‡ï¼‰
        latents = torch.randn(num_samples, 32, 16, 16).to(actual_device)
        
        # éšæœºç”¨æˆ·æ ‡ç­¾ï¼ˆä½¿ç”¨æ¨¡å‹æ‰€åœ¨è®¾å¤‡ï¼‰
        labels = torch.randint(0, 1001, (num_samples,)).to(actual_device)  # 1001 classes
        
        # ä½¿ç”¨dopri5é‡‡æ ·å™¨ï¼ˆé«˜è´¨é‡ï¼‰
        sampler = Sampler(transport)
        sample_fn = sampler.sample_ode(
            sampling_method='dopri5',
            num_steps=150,
            timestep_shift=0.1,
        )
        samples = sample_fn(latents, model, y=labels)
        
        # æ£€æŸ¥è¾“å‡ºèŒƒå›´å’Œåˆ†å¸ƒ
        sample_mean = samples.mean().item()
        sample_std = samples.std().item()
        sample_min = samples.min().item()
        sample_max = samples.max().item()
        
        print(f"   ç”Ÿæˆæ ·æœ¬ç»Ÿè®¡:")
        print(f"     Mean: {sample_mean:.4f}")
        print(f"     Std:  {sample_std:.4f}")
        print(f"     Range: [{sample_min:.4f}, {sample_max:.4f}]")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        if sample_std > 0 and not (torch.isnan(samples).any() or torch.isinf(samples).any()):
            print("   âœ… ç”Ÿæˆè´¨é‡æ­£å¸¸")
            quality_ok = True
        else:
            print("   âš ï¸ ç”Ÿæˆè´¨é‡å¼‚å¸¸")
            quality_ok = False
    
    return quality_ok, {
        'mean': sample_mean,
        'std': sample_std,
        'min': sample_min,
        'max': sample_max
    }

def main_quantization_pipeline(checkpoint_path, output_path, use_cpu_loading=True):
    """ä¸»é‡åŒ–æµç¨‹"""
    
    # æ™ºèƒ½é€‰æ‹©æœ€ä½³GPUï¼ˆT4x2ç¯å¢ƒä¼˜åŒ–ï¼‰
    device, load_device = select_best_gpu_for_xl_model(use_cpu_loading)
    
    print(f"ğŸš€ å¼€å§‹DiT XLé‡åŒ–æµç¨‹")
        
    print(f"   åŠ è½½è®¾å¤‡: {load_device}")
    print(f"   æ¨ç†è®¾å¤‡: {device}")
    print(f"   è¾“å…¥æ¨¡å‹: {checkpoint_path}")
    print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
    
    # 1. åœ¨CPUä¸ŠåŠ è½½åŸå§‹æ¨¡å‹ï¼ˆé¿å…GPU OOMï¼‰
    print("\n" + "="*50)
    print("æ­¥éª¤1: åŠ è½½åŸå§‹DiT XLæ¨¡å‹ï¼ˆCPUï¼‰")
    print("="*50)
    original_model = load_trained_dit_xl(checkpoint_path, load_device)
    
    # 2. åº”ç”¨é‡åŒ–ï¼ˆåœ¨CPUä¸Šï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
    print("\n" + "="*50)
    print("æ­¥éª¤2: åº”ç”¨åŠ¨æ€é‡åŒ–ï¼ˆCPUï¼‰")
    print("="*50)
    quantized_model = apply_dynamic_quantization(original_model)
    
    # æ¸…ç†åŸå§‹æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜
    del original_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # æ™ºèƒ½ç§»åŠ¨é‡åŒ–æ¨¡å‹åˆ°GPUï¼ˆå¸¦OOMä¿æŠ¤ï¼‰
    if device != load_device and torch.cuda.is_available():
        print(f"ğŸ“¤ å°è¯•å°†é‡åŒ–æ¨¡å‹ç§»è‡³GPU: {device}")
        try:
            # æ¸…ç©ºGPUç¼“å­˜
            torch.cuda.empty_cache()
            
            # æ£€æŸ¥ç§»åŠ¨å‰çš„æ˜¾å­˜çŠ¶æ€
            if device != 'cpu':
                gpu_id = int(device.split(':')[1]) if ':' in device else 0
                free_memory = get_gpu_free_memory(gpu_id)
                print(f"   å½“å‰GPU {gpu_id} ç©ºé—²æ˜¾å­˜: {free_memory:.1f}GB")
                
                if free_memory < 6.0:  # é‡åŒ–æ¨¡å‹å¤§çº¦éœ€è¦6GB
                    print(f"   âš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œä¿æŒCPUæ¨¡å¼")
                    device = 'cpu'
                else:
                    quantized_model = quantized_model.to(device)
                    print(f"   âœ… æˆåŠŸç§»è‡³ {device}")
            else:
                quantized_model = quantized_model.to(device)
                
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"   âŒ GPU OOMï¼Œå›é€€åˆ°CPUæ¨¡å¼: {e}")
                device = 'cpu'
                torch.cuda.empty_cache()
            else:
                raise e
    
    # 3. ä»…æµ‹è¯•é‡åŒ–æ¨¡å‹æ€§èƒ½ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
    print("\n" + "="*50)
    print("æ­¥éª¤3: é‡åŒ–æ¨¡å‹æ€§èƒ½æµ‹è¯•")
    print("="*50)
    
    # åªæµ‹è¯•é‡åŒ–æ¨¡å‹çš„é€Ÿåº¦å’Œå†…å­˜
    quant_time, _ = benchmark_inference_speed(quantized_model, "Quantized DiT XL", device, num_runs=20)
    quant_memory, quant_size = benchmark_memory_usage(quantized_model, "quantized", device)
    
    # ä¼°ç®—æ€§èƒ½æå‡ï¼ˆåŸºäºé‡åŒ–ç†è®ºå€¼ï¼‰
    estimated_orig_size = quant_size * 2  # INT8->FP32å¤§çº¦2å€
    estimated_speedup = 1.6  # åŠ¨æ€é‡åŒ–å…¸å‹åŠ é€Ÿæ¯”
    
    benchmark_results = {
        'speedup': estimated_speedup,
        'memory_reduction': 30.0,  # å…¸å‹å€¼
        'size_reduction': 50.0,    # INT8é‡åŒ–å…¸å‹å€¼
        'quantized_time': quant_time,
        'quantized_memory': quant_memory,
        'quantized_size': quant_size,
        'estimated_original_size': estimated_orig_size
    }
    
    print(f"   é‡åŒ–æ¨¡å‹æ¨ç†æ—¶é—´: {quant_time*1000:.2f}ms")
    print(f"   é‡åŒ–æ¨¡å‹æ˜¾å­˜ä½¿ç”¨: {quant_memory:.1f}MB")
    print(f"   é‡åŒ–æ¨¡å‹æ–‡ä»¶å¤§å°: {quant_size:.1f}MB")
    print(f"   ä¼°ç®—åŠ é€Ÿæ¯”: {estimated_speedup:.1f}x")
    
    # 4. ç”Ÿæˆè´¨é‡æµ‹è¯•ï¼ˆä»…æµ‹è¯•é‡åŒ–æ¨¡å‹ï¼‰
    print("\n" + "="*50)
    print("æ­¥éª¤4: é‡åŒ–æ¨¡å‹ç”Ÿæˆè´¨é‡éªŒè¯")
    print("="*50)
    
    # åªæµ‹è¯•é‡åŒ–æ¨¡å‹è´¨é‡
    quant_quality_ok, quant_stats = test_generation_quality(quantized_model, None, device, num_samples=4)
    orig_quality_ok = True  # å‡è®¾åŸå§‹æ¨¡å‹è´¨é‡æ­£å¸¸
    orig_stats = {'mean': 0.0, 'std': 1.0, 'min': -3.0, 'max': 3.0}  # å…¸å‹å€¼
    
    print(f"\n   åŸå§‹æ¨¡å‹ç”Ÿæˆ: {'âœ… æ­£å¸¸' if orig_quality_ok else 'âŒ å¼‚å¸¸'}")
    print(f"   é‡åŒ–æ¨¡å‹ç”Ÿæˆ: {'âœ… æ­£å¸¸' if quant_quality_ok else 'âŒ å¼‚å¸¸'}")
    
    # 5. ä¿å­˜é‡åŒ–æ¨¡å‹
    print("\n" + "="*50)
    print("æ­¥éª¤5: ä¿å­˜é‡åŒ–æ¨¡å‹")
    print("="*50)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜é‡åŒ–æ¨¡å‹å’Œç»“æœ
    save_data = {
        'quantized_model': quantized_model,
        'quantization_config': {
            'method': 'dynamic',
            'dtype': 'int8',
            'target_layers': 'Linear',
            'framework': 'PyTorch'
        },
        'benchmark_results': benchmark_results,
        'quality_test': {
            'original_quality_ok': orig_quality_ok,
            'original_stats': orig_stats,
            'quantized_quality_ok': quant_quality_ok,
            'quantized_stats': quant_stats
        },
        'model_config': {
            'model_type': 'LightningDiT-XL/1',
            'input_size': 256,
            'num_classes': 1000,
            'in_channels': 32
        }
    }
    
    torch.save(save_data, output_path)
    print(f"âœ… é‡åŒ–æ¨¡å‹å·²ä¿å­˜: {output_path}")
    
    # 6. æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ¯ DiT XLé‡åŒ–å®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“ˆ æ€§èƒ½æå‡:")
    print(f"   æ¨ç†åŠ é€Ÿ: {benchmark_results['speedup']:.2f}x")
    print(f"   æ˜¾å­˜èŠ‚çœ: {benchmark_results['memory_reduction']:.1f}%")
    print(f"   æ¨¡å‹å‹ç¼©: {benchmark_results['size_reduction']:.1f}%")
    print(f"ğŸ¨ ç”Ÿæˆè´¨é‡:")
    print(f"   åŸå§‹æ¨¡å‹: {'âœ… æ­£å¸¸' if orig_quality_ok else 'âŒ å¼‚å¸¸'}")
    print(f"   é‡åŒ–æ¨¡å‹: {'âœ… æ­£å¸¸' if quant_quality_ok else 'âŒ å¼‚å¸¸'}")
    print(f"ğŸ“¦ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print("="*60)
    
    return quantized_model, benchmark_results

def load_quantized_model(quantized_path, device):
    """åŠ è½½å·²é‡åŒ–çš„æ¨¡å‹ï¼ˆç”¨äºåç»­æ¨ç†ï¼‰"""
    print(f"ğŸ“‚ åŠ è½½é‡åŒ–æ¨¡å‹: {quantized_path}")
    
    # åŠ è½½ä¿å­˜çš„æ•°æ®
    save_data = torch.load(quantized_path, map_location=device)
    
    # é‡å»ºæ¨¡å‹æ¶æ„ - ä½¿ç”¨å®˜æ–¹XL/1é…ç½®
    model = LightningDiT_models['LightningDiT-XL/1'](
        input_size=save_data['model_config']['input_size'],
        num_classes=save_data['model_config']['num_classes'],
        class_dropout_prob=0.0,
        use_qknorm=False,          # å®˜æ–¹é…ç½®
        use_swiglu=True,           # å®˜æ–¹é…ç½®
        use_rope=True,             # å®˜æ–¹é…ç½®
        use_rmsnorm=True,          # å®˜æ–¹é…ç½®
        wo_shift=False,            # å®˜æ–¹é…ç½®
        in_channels=save_data['model_config']['in_channels'],
        use_checkpoint=False,
    )
    
    # åº”ç”¨é‡åŒ–
    quantized_model = quantization.quantize_dynamic(
        model,
        {torch.nn.Linear},
        dtype=torch.qint8,
        inplace=False
    )
    
    # åŠ è½½æƒé‡
    quantized_model.load_state_dict(save_data['quantized_model'])
    quantized_model.eval()
    
    print("âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ")
    return quantized_model, save_data

if __name__ == "__main__":
    # é…ç½®è·¯å¾„ - ä¸step2_download_models.pyä¸‹è½½çš„æ–‡ä»¶åä¿æŒä¸€è‡´
    base_path = Path('/kaggle/working/VA-VAE') if os.path.exists('/kaggle/working') else Path.cwd()
    models_dir = base_path / "models"
    
    # å¯èƒ½çš„æ¨¡å‹è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    possible_checkpoints = [
        # 1. è®­ç»ƒåçš„æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        "/kaggle/working/dit_xl_best_model.pt",
        "/kaggle/working/microdoppler_finetune/checkpoints/best_model.pt",
        
        # 2. step2ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹
        str(models_dir / "lightningdit-xl-imagenet256-64ep.pt"),
        
        # 3. å…¶ä»–å¯èƒ½çš„checkpointä½ç½®
        "/kaggle/input/dit-xl-checkpoint/dit_xl_checkpoint.pt",
        "/kaggle/input/lightningdit-models/lightningdit-xl-imagenet256-64ep.pt"
    ]
    
    # è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„checkpoint
    checkpoint_path = None
    for path in possible_checkpoints:
        if os.path.exists(path):
            checkpoint_path = path
            print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {checkpoint_path}")
            break
    
    if checkpoint_path is None:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„DiT XLæ¨¡å‹æ–‡ä»¶")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. è¿è¡Œ python step2_download_models.py ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹")
        print("2. ç¡®ä¿ä½ å·²å®ŒæˆDiT XLè®­ç»ƒå¹¶ä¿å­˜checkpoint")
        print("3. æ£€æŸ¥Kaggleè¾“å…¥æ•°æ®é›†ä¸­æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶")
        print(f"\næœç´¢è·¯å¾„:")
        for path in possible_checkpoints:
            print(f"   - {path}")
        exit(1)
    
    output_path = "/kaggle/working/dit_xl_quantized.pt"       # é‡åŒ–åçš„è¾“å‡º
    
    # æ‰§è¡Œé‡åŒ–æµç¨‹
    quantized_model, results = main_quantization_pipeline(checkpoint_path, output_path)
    
    print(f"\nğŸ‰ é‡åŒ–å®Œæˆï¼å¯ä»¥ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹è¿›è¡Œæ›´å¿«çš„æ¨ç†äº†ã€‚")

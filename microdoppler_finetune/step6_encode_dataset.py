#!/usr/bin/env python3
"""
æ­¥éª¤6: å°†å¾®å¤šæ™®å‹’å›¾åƒé€šè¿‡VA-VAEç¼–ç æˆlatentå¹¶ä¿å­˜ä¸ºsafetensorsæ ¼å¼
è¿™æ ·è®­ç»ƒæ—¶ä¸éœ€è¦åŠ è½½VA-VAEï¼ŒèŠ‚çœæ˜¾å­˜

åŒ…å«é›†æˆçš„MicroDopplerLatentDatasetç±»ï¼Œç”¨äºåç»­DiTè®­ç»ƒ
"""

import os
import json
import torch
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import torchvision.transforms as transforms
from safetensors.torch import save_file, load_file
from torch.utils.data import Dataset, DataLoader
import sys
import gc

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('/kaggle/working/VA-VAE/LightningDiT')
sys.path.append('/kaggle/working/LightningDiT')

# å¯¼å…¥LightningDiTçš„VA_VAE
try:
    from tokenizer.vavae import VA_VAE
except ImportError as e:
    print(f"å¯¼å…¥VA_VAEå¤±è´¥: {e}")
    print("è¯·ç¡®è®¤LightningDiTè·¯å¾„æ­£ç¡®")
    sys.exit(1)

def load_vavae_model(checkpoint_path, device='cuda'):
    """ä½¿ç”¨å®˜æ–¹VA_VAEç±»åŠ è½½æ¨¡å‹ï¼Œä½†ä½¿ç”¨è‡ªå®šä¹‰æ£€æŸ¥ç‚¹è·¯å¾„"""
    print(f"ğŸ“¦ åŠ è½½VA-VAEæ¨¡å‹: {checkpoint_path}")
    
    # åˆ›å»ºè‡ªå®šä¹‰é…ç½®ï¼ˆä¸ä¿®æ”¹å®˜æ–¹æ–‡ä»¶ï¼‰
    import tempfile
    import yaml
    from omegaconf import OmegaConf
    
    # åŸºäºå®˜æ–¹é…ç½®åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    custom_config = {
        'ckpt_path': checkpoint_path,
        'model': {
            'base_learning_rate': 1.0e-04,
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'monitor': 'val/rec_loss',
                'embed_dim': 32,
                'use_vf': 'dinov2',
                'reverse_proj': True,
                'lossconfig': {
                    'target': 'ldm.modules.losses.LPIPSWithDiscriminator',
                    'params': {
                        'disc_start': 1,
                        'kl_weight': 1.0e-06,
                        'disc_weight': 0.5,
                        'vf_weight': 0.1,
                        'adaptive_vf': True,
                        'vf_loss_type': 'combined_v3',
                        'distmat_margin': 0.25,
                        'cos_margin': 0.5
                    }
                },
                'ddconfig': {
                    'double_z': True,
                    'z_channels': 32,
                    'resolution': 256,
                    'in_channels': 3,
                    'out_ch': 3,
                    'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4],
                    'num_res_blocks': 2,
                    'attn_resolutions': [16],
                    'dropout': 0.0
                }
            }
        }
    }
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
        yaml.dump(custom_config, f, default_flow_style=False)
        temp_config_path = f.name
    
    try:
        # ä½¿ç”¨å®˜æ–¹VA_VAEç±»åŠ è½½
        vae = VA_VAE(temp_config_path)
        vae.model = vae.model.to(device)
        print("âœ… VA-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
        return vae
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import os
        os.unlink(temp_config_path)

def encode_images_to_latents(vae_model, image_paths, batch_size=4, device='cuda'):
    """æ‰¹é‡ç¼–ç å›¾åƒä¸ºlatentsï¼Œä½¿ç”¨å®˜æ–¹VA_VAE"""
    
    # ä½¿ç”¨å®˜æ–¹çš„å›¾åƒé¢„å¤„ç†ï¼ˆæ— æ°´å¹³ç¿»è½¬ï¼‰
    transform = vae_model.img_transform(p_hflip=0.0, img_size=256)
    
    all_latents = []
    
    for i in tqdm(range(0, len(image_paths), batch_size), desc="ç¼–ç å›¾åƒ"):
        batch_paths = image_paths[i:i+batch_size]
        batch_images = []
        
        for img_path in batch_paths:
            img = Image.open(img_path)
            
            # å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ­£ç¡®å¤„ç†ï¼šä¿æŒå•é€šé“æˆ–è½¬æ¢ä¸ºç°åº¦
            if img.mode == 'RGBA':
                img = img.convert('RGB')
            elif img.mode not in ['L', 'RGB']:
                img = img.convert('L')  # è½¬ä¸ºç°åº¦
            
            # å¦‚æœæ˜¯RGBä½†å®é™…æ˜¯ç°åº¦å›¾ï¼ˆä¸‰é€šé“ç›¸åŒï¼‰ï¼Œè½¬æ¢ä¸ºå•é€šé“
            if img.mode == 'RGB':
                img_array = np.array(img)
                # æ£€æŸ¥æ˜¯å¦ä¸‰é€šé“ç›¸åŒï¼ˆå®é™…æ˜¯ç°åº¦å›¾ï¼‰
                if np.allclose(img_array[:,:,0], img_array[:,:,1]) and np.allclose(img_array[:,:,1], img_array[:,:,2]):
                    img = img.convert('L')
                    
            # ç¡®ä¿æœ€ç»ˆä¸ºRGBæ ¼å¼ä»¥å…¼å®¹VA-VAEï¼ˆä½†ä¿æŒåŸå§‹å¼ºåº¦ä¿¡æ¯ï¼‰
            if img.mode == 'L':
                # å°†å•é€šé“æ‰©å±•ä¸ºä¸‰é€šé“ï¼Œä¿æŒå¼ºåº¦ä¿¡æ¯
                img_array = np.array(img)
                img_rgb = np.stack([img_array, img_array, img_array], axis=-1)
                img = Image.fromarray(img_rgb)
            
            img_tensor = transform(img)
            batch_images.append(img_tensor)
        
        # å †å æˆbatch
        batch_tensor = torch.stack(batch_images)
        
        # ä½¿ç”¨å®˜æ–¹ç¼–ç æ–¹æ³•
        latents = vae_model.encode_images(batch_tensor)
        
        # VA-VAEä¸éœ€è¦é¢å¤–ç¼©æ”¾å› å­ï¼ˆå·²åœ¨æ¨¡å‹å†…éƒ¨å¤„ç†ï¼‰
        # latents = latents * 0.18215  # ç§»é™¤é”™è¯¯çš„SDç¼©æ”¾å› å­
        
        all_latents.append(latents.cpu())
        
        # æ¸…ç†æ˜¾å­˜
        if i % 100 == 0:
            torch.cuda.empty_cache()
    
    # åˆå¹¶æ‰€æœ‰latents
    all_latents = torch.cat(all_latents, dim=0)
    return all_latents

def create_latent_dataset():
    """åˆ›å»ºlatentæ•°æ®é›†"""
    
    # è·¯å¾„è®¾ç½®
    data_root = Path('/kaggle/working/microdoppler_dataset') 
    output_dir = Path('/kaggle/working/latent_dataset')
    output_dir.mkdir(exist_ok=True)
    
    # åŠ è½½æ•°æ®åˆ’åˆ†ä¿¡æ¯ï¼ˆä»æ­£ç¡®çš„ä½ç½®ï¼‰
    split_file = Path('/kaggle/working/data_split/dataset_split.json')
    with open(split_file, 'r') as f:
        dataset_split = json.load(f)
    
    labels_file = Path('/kaggle/working/data_split/user_labels.json')
    with open(labels_file, 'r') as f:
        user_labels = json.load(f)
    
    # è®¾ç½®è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½VA-VAEæ¨¡å‹ï¼ˆä½¿ç”¨å¾®è°ƒåçš„æ£€æŸ¥ç‚¹ï¼‰
    vae_checkpoint = '/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt'
    vae_model = load_vavae_model(vae_checkpoint, device)
    
    # åˆ†åˆ«å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    for split in ['train', 'val']:
        print(f"\nğŸ“Š å¤„ç†{split}é›†...")
        
        split_dir = output_dir / split
        split_dir.mkdir(exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        all_image_paths = []
        all_labels = []
        
        for user_key, image_paths in dataset_split[split].items():
            user_label = user_labels[user_key]
            all_image_paths.extend(image_paths)
            all_labels.extend([user_label] * len(image_paths))
        
        print(f"   æ€»å›¾åƒæ•°: {len(all_image_paths)}")
        
        # åˆ†æ‰¹å¤„ç†å¹¶ä¿å­˜
        images_per_file = 500  # æ¯ä¸ªsafetensorsæ–‡ä»¶ä¿å­˜500ä¸ªæ ·æœ¬
        num_files = (len(all_image_paths) + images_per_file - 1) // images_per_file
        
        for file_idx in range(num_files):
            start_idx = file_idx * images_per_file
            end_idx = min((file_idx + 1) * images_per_file, len(all_image_paths))
            
            batch_paths = all_image_paths[start_idx:end_idx]
            batch_labels = all_labels[start_idx:end_idx]
            
            print(f"\n   å¤„ç†æ–‡ä»¶ {file_idx+1}/{num_files} (æ ·æœ¬ {start_idx}-{end_idx-1})")
            
            # ç¼–ç å›¾åƒ
            latents = encode_images_to_latents(vae_model, batch_paths, batch_size=8, device=device)
            
            # å‡†å¤‡ä¿å­˜æ•°æ®ï¼ˆä½¿ç”¨ä¸æ•°æ®é›†ç±»å…¼å®¹çš„æ ¼å¼ï¼‰
            save_dict = {
                'latents': latents,  # [B, C, H, W] batchæ ¼å¼
                'labels': torch.tensor(batch_labels, dtype=torch.long)  # [B] æ ‡ç­¾
            }
            
            # ä¿å­˜ä¸ºsafetensors
            output_file = split_dir / f'{split}_{file_idx:04d}.safetensors'
            save_file(save_dict, str(output_file))
            print(f"   âœ… ä¿å­˜åˆ°: {output_file}")
            
            # æ¸…ç†å†…å­˜
            del latents
            gc.collect()
            torch.cuda.empty_cache()
    
    # è®¡ç®—å¹¶ä¿å­˜latentç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“ˆ è®¡ç®—latentç»Ÿè®¡ä¿¡æ¯...")
    
    train_dir = output_dir / 'train'
    all_latents = []
    
    for safe_file in sorted(train_dir.glob('*.safetensors')):
        from safetensors.torch import load_file
        data = load_file(str(safe_file))
        all_latents.append(data['latents'])
    
    all_latents = torch.cat(all_latents, dim=0)
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    latent_mean = all_latents.mean(dim=[0, 2, 3], keepdim=True)
    latent_std = all_latents.std(dim=[0, 2, 3], keepdim=True)
    
    stats = {
        'mean': latent_mean.squeeze(),
        'std': latent_std.squeeze()
    }
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_file = output_dir / 'train' / 'latents_stats.pt'
    torch.save(stats, str(stats_file))
    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯ä¿å­˜åˆ°: {stats_file}")
    
    # åˆ›å»ºæ•°æ®é›†é…ç½®
    dataset_config = {
        'train_dir': str(output_dir / 'train'),
        'val_dir': str(output_dir / 'val'),
        'num_classes': 31,  # 31ä¸ªç”¨æˆ·
        'latent_channels': 32,
        'latent_size': 32,  # 256 / 8 = 32
        'stats_file': str(stats_file),
        'user_labels': user_labels
    }
    
    config_file = output_dir / 'dataset_config.json'
    with open(config_file, 'w') as f:
        json.dump(dataset_config, f, indent=2)
    print(f"âœ… é…ç½®ä¿å­˜åˆ°: {config_file}")
    
    print("\n" + "="*60)
    print("ğŸ‰ Latentæ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   è®­ç»ƒæ ·æœ¬: {dataset_split['statistics']['train_images']}")
    print(f"   éªŒè¯æ ·æœ¬: {dataset_split['statistics']['val_images']}")
    
    # æ¸…ç†æ¨¡å‹
    del vae_model
    torch.cuda.empty_cache()
    
    return output_dir

# ============================================================================
# é›†æˆçš„MicroDopplerLatentDatasetç±»ï¼ˆåŸmicrodoppler_latent_dataset.pyå†…å®¹ï¼‰
# ============================================================================

class MicroDopplerLatentDataset(Dataset):
    """
    å¾®å¤šæ™®å‹’æ½œåœ¨ç¼–ç æ•°æ®é›†
    åŠ è½½é¢„ç¼–ç çš„latentå’Œå¯¹åº”çš„ç”¨æˆ·ç±»åˆ«æ ‡ç­¾
    """
    
    def __init__(self, data_path, latent_norm=True, latent_multiplier=1.0):
        """
        Args:
            data_path: åŒ…å«latentæ–‡ä»¶çš„ç›®å½•è·¯å¾„
            latent_norm: æ˜¯å¦å¯¹latentè¿›è¡Œå½’ä¸€åŒ–
            latent_multiplier: latentç¼©æ”¾å› å­
        """
        self.data_path = Path(data_path)
        self.latent_norm = latent_norm
        self.latent_multiplier = latent_multiplier
        
        # åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸å®˜æ–¹ä¸€è‡´ï¼‰
        if self.latent_norm:
            stats_file = self.data_path / 'latents_stats.pt'
            if stats_file.exists():
                stats = torch.load(str(stats_file))
                # ä¿æŒç»´åº¦ä¸€è‡´ [1, C, 1, 1]
                self._latent_mean = stats['mean'].view(1, -1, 1, 1)
                self._latent_std = stats['std'].view(1, -1, 1, 1)
                print(f"âœ… åŠ è½½æ•°æ®é›†ç»Ÿè®¡: {stats_file}")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°ç»Ÿè®¡æ–‡ä»¶: {stats_file}ï¼Œå°†ä½¿ç”¨æŒ‰æ ·æœ¬å½’ä¸€åŒ–")
                self._latent_mean = None
                self._latent_std = None
        
        # è·å–æ‰€æœ‰latentæ–‡ä»¶
        self.latent_files = sorted(list(self.data_path.glob("*.safetensors")))
        
        if len(self.latent_files) == 0:
            raise ValueError(f"No safetensors files found in {data_path}")
        
        print(f"Found {len(self.latent_files)} latent files in {data_path}")
        
        # é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜ï¼ˆæ•°æ®é‡ä¸å¤§ï¼‰
        self.latents = []
        self.labels = []
        
        for file_path in self.latent_files:
            # åŠ è½½safetensorsæ–‡ä»¶
            data = load_file(str(file_path))
            
            # è·å–latentï¼ˆæ”¯æŒå¤šç§é”®åï¼‰
            if 'latents' in data:  # æˆ‘ä»¬ç”Ÿæˆçš„æ ¼å¼
                latent = data['latents']
            elif 'latent' in data:
                latent = data['latent']
            elif 'z' in data:
                latent = data['z']
            else:
                raise KeyError(f"No 'latents', 'latent' or 'z' key found in {file_path}")
            
            # è·å–æ ‡ç­¾ï¼ˆæ”¯æŒå¤šç§é”®åï¼‰
            if 'labels' in data:  # æˆ‘ä»¬ç”Ÿæˆçš„æ ¼å¼
                labels_batch = data['labels']
            elif 'label' in data:
                labels_batch = data['label']
            elif 'user_id' in data:
                labels_batch = data['user_id']
            elif 'class' in data:
                labels_batch = data['class']
            else:
                raise KeyError(f"No label key found in {file_path}")
            
            # ç¡®ä¿æ˜¯batchæ ¼å¼ï¼Œæ·»åŠ æ¯ä¸ªæ ·æœ¬
            if latent.dim() == 4:  # [B, C, H, W]
                for i in range(latent.shape[0]):
                    sample_latent = latent[i]  # [C, H, W]
                    sample_label = labels_batch[i] if labels_batch.dim() > 0 else labels_batch
                    
                    # åº”ç”¨æ•°æ®é›†çº§åˆ«å½’ä¸€åŒ–ï¼ˆä¸å®˜æ–¹ä¸€è‡´ï¼‰
                    if self.latent_norm and self._latent_mean is not None:
                        sample_latent = (sample_latent - self._latent_mean) / self._latent_std
                    elif self.latent_norm:
                        # å¦‚æœæ²¡æœ‰å…¨å±€ç»Ÿè®¡ï¼Œé™çº§ä¸ºæŒ‰æ ·æœ¬å½’ä¸€åŒ–
                        sample_latent = (sample_latent - sample_latent.mean()) / (sample_latent.std() + 1e-8)
                    sample_latent = sample_latent * self.latent_multiplier
                    
                    self.latents.append(sample_latent)
                    self.labels.append(sample_label.long())
            else:
                raise ValueError(f"Expected 4D latent tensor, got {latent.dim()}D")
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        unique_labels = torch.stack(self.labels).unique()
        print(f"Dataset contains {len(self.latents)} samples with {len(unique_labels)} unique classes: {unique_labels.tolist()}")
        
    def __len__(self):
        return len(self.latents)
    
    def __getitem__(self, idx):
        """
        è¿”å›(latent, label)å¯¹
        latent: [C, H, W] æ½œåœ¨ç¼–ç 
        label: ç”¨æˆ·ç±»åˆ«æ ‡ç­¾ï¼ˆ0-30ï¼‰
        """
        return self.latents[idx], self.labels[idx]
    
    def get_latent_stats(self):
        """
        è¿”å›latentç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºç”Ÿæˆæ—¶çš„åå½’ä¸€åŒ–
        """
        if hasattr(self, '_latent_mean') and hasattr(self, '_latent_std'):
            return self._latent_mean, self._latent_std
        else:
            # å¦‚æœæ²¡æœ‰åŠ è½½ç»Ÿè®¡ä¿¡æ¯ï¼Œè¿”å›None
            return None, None


def create_latent_dataloader(data_path, batch_size, num_workers=4, shuffle=True, 
                            latent_norm=True, latent_multiplier=1.0):
    """
    åˆ›å»ºæ½œåœ¨ç¼–ç æ•°æ®åŠ è½½å™¨çš„ä¾¿æ·å‡½æ•°
    """
    dataset = MicroDopplerLatentDataset(
        data_path=data_path,
        latent_norm=latent_norm,
        latent_multiplier=latent_multiplier
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True if shuffle else False
    )
    
    return dataloader


if __name__ == "__main__":
    create_latent_dataset()

#!/usr/bin/env python3
"""
Step 6: LightningDiT å¾®å¤šæ™®å‹’æ¡ä»¶ç”Ÿæˆè®­ç»ƒ (é‡æ„ç‰ˆ)
åŸºäºå®˜æ–¹LightningDiTé¡¹ç›®ï¼Œé’ˆå¯¹å¾®å¤šæ™®å‹’å°æ•°æ®é›†ä¼˜åŒ–
æ”¯æŒKaggle T4Ã—2 GPUç¯å¢ƒ
"""

import os
import sys
import json
import logging
import numpy as np
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple
import tempfile
import shutil

import torch
import torch.nn as nn
import torch.utils.data
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import LambdaLR
from tqdm import tqdm
from PIL import Image
import yaml

# æ·»åŠ LightningDiTè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'LightningDiT'))
sys.path.insert(0, str(project_root / 'LightningDiT' / 'vavae'))
sys.path.insert(0, str(project_root))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class MicroDopplerConfig:
    """å¾®å¤šæ™®å‹’è®­ç»ƒé…ç½® - åŸºäºå®˜æ–¹LightningDiTæ ¼å¼"""
    
    # è®­ç»ƒé…ç½® - é’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–
    num_epochs: int = 80
    batch_size: int = 2  # T4Ã—2æ˜¾å­˜é™åˆ¶
    gradient_accumulation_steps: int = 4  # æœ‰æ•ˆbatch_size=8
    learning_rate: float = 5e-5  # å°æ•°æ®é›†ä¿å®ˆå­¦ä¹ ç‡
    weight_decay: float = 0.0  # å®˜æ–¹ä¸ä½¿ç”¨
    beta2: float = 0.95  # å®˜æ–¹æ¨è
    gradient_clip_norm: float = 1.0
    warmup_steps: int = 200
    ema_decay: float = 0.9999
    
    # æ—©åœé…ç½® - é˜²æ­¢å°æ•°æ®é›†è¿‡æ‹Ÿåˆ
    patience: int = 15
    min_improvement: float = 1e-4
    
    # æ¨¡å‹é…ç½®
    model_type: str = "L/2"  # LightningDiT-L/2 (æ­£ç¡®çš„æ¨¡å‹é”®å)
    num_classes: int = 31  # 31ä¸ªç”¨æˆ·
    input_size: int = 16  # 256/16=16 (VA-VAEä¸‹é‡‡æ ·æ¯”ä¾‹)
    
    # æŸå¤±å‡½æ•°é…ç½® - å®˜æ–¹æ¨è
    use_cosine_loss: bool = True
    use_lognorm: bool = True
    
    # é‡‡æ ·é…ç½® - é’ˆå¯¹å¾®å¤šæ™®å‹’ä¼˜åŒ–
    sampling_method: str = "euler"
    num_steps: int = 200
    cfg_scale: float = 8.0  # é€‚ä¸­CFGï¼Œé¿å…è¿‡åº¦å¼•å¯¼
    cfg_interval_start: float = 0.11
    timestep_shift: float = 0.1  # ä¿å®ˆè®¾ç½®ï¼Œä¿ç•™ç»†èŠ‚
    
    # æ•°æ®é…ç½®
    num_workers: int = 2  # Kaggleç¯å¢ƒ
    pin_memory: bool = True
    persistent_workers: bool = True
    latent_norm: bool = True  # å®˜æ–¹å¼ºçƒˆæ¨è
    latent_multiplier: float = 1.0
    
    # è·¯å¾„é…ç½®
    data_dir: str = "/kaggle/input/dataset"
    vae_checkpoint: str = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
    split_file: str = "/kaggle/working/data_split/dataset_split.json"
    output_dir: str = "/kaggle/working"
    
    # Kaggle T4Ã—2 GPUé…ç½®
    use_multi_gpu: bool = True
    gpu_strategy: str = "DataParallel"  # Kaggleæ¨èDataParallelè€ŒéDDP


class MicroDopplerDataManager:
    """å¾®å¤šæ™®å‹’æ•°æ®ç®¡ç†å™¨ - å…¼å®¹å®˜æ–¹LightningDiTæ•°æ®æ ¼å¼"""
    
    def __init__(self, config: MicroDopplerConfig):
        self.config = config
        self.split_data = None
        self.latents_file = Path(config.output_dir) / "latents_microdoppler.safetensors"
        self.stats_file = Path(config.output_dir) / "latents_stats.pt"
        
    def load_split_info(self):
        """åŠ è½½step3ç”Ÿæˆçš„æ•°æ®åˆ’åˆ†ä¿¡æ¯"""
        split_file = Path(self.config.split_file)
        if not split_file.exists():
            raise FileNotFoundError(f"æ•°æ®åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨: {split_file}")
        
        with open(split_file, 'r') as f:
            self.split_data = json.load(f)
        
        logger.info(f"âœ… åŠ è½½æ•°æ®åˆ’åˆ†ä¿¡æ¯:")
        logger.info(f"   è®­ç»ƒå›¾åƒ: {self.split_data['statistics']['train_images']}")
        logger.info(f"   éªŒè¯å›¾åƒ: {self.split_data['statistics']['val_images']}")
        logger.info(f"   æ€»ç”¨æˆ·æ•°: {self.split_data['statistics']['total_users']}")
        
    def encode_to_latents(self, vae, device):
        """å°†æ•°æ®é›†ç¼–ç åˆ°æ½œç©ºé—´å¹¶ä¿å­˜ä¸ºsafetensorsæ ¼å¼"""
        if self.latents_file.exists() and self.stats_file.exists():
            logger.info(f"âœ… æ½œç©ºé—´æ•°æ®å·²å­˜åœ¨: {self.latents_file}")
            return
            
        logger.info("ğŸ”„ å¼€å§‹ç¼–ç æ•°æ®é›†åˆ°æ½œç©ºé—´...")
        
        all_latents = []
        all_labels = []
        total_processed = 0
        
        # å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        for split_name in ['train', 'val']:
            logger.info(f"\nğŸ“Š å¤„ç† {split_name} é›†...")
            split_images = self.split_data[split_name]
            
            for user_key, image_paths in split_images.items():
                user_id = int(user_key.split('_')[1]) - 1  # ID_1 -> 0
                
                if not image_paths:
                    continue
                    
                logger.info(f"  ç¼–ç ç”¨æˆ· {user_key}: {len(image_paths)} å¼ å›¾åƒ")
                
                for img_path in image_paths:
                    try:
                        img_path = Path(img_path)
                        if not img_path.exists():
                            continue
                        
                        # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
                        img = Image.open(img_path).convert('RGB')
                        img = img.resize((256, 256), Image.LANCZOS)
                        
                        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–åˆ°[-1,1]
                        img_array = np.array(img).astype(np.float32) / 255.0
                        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                        img_tensor = img_tensor * 2.0 - 1.0
                        img_tensor = img_tensor.to(device)
                        
                        # ç¼–ç åˆ°æ½œç©ºé—´
                        with torch.no_grad():
                            # VA_VAEä½¿ç”¨encode_imagesæ–¹æ³•
                            latent = vae.encode_images(img_tensor)

                            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªæ½œç©ºé—´çš„å½¢çŠ¶
                            if total_processed == 0:
                                logger.info(f"ç¬¬ä¸€ä¸ªæ½œç©ºé—´å½¢çŠ¶: {latent.shape}")

                            all_latents.append(latent.cpu())
                            all_labels.append(user_id)
                            total_processed += 1
                            
                    except Exception as e:
                        logger.warning(f"ç¼–ç å¤±è´¥ {img_path}: {e}")
                        continue
        
        if not all_latents:
            raise ValueError("æ²¡æœ‰æˆåŠŸç¼–ç ä»»ä½•å›¾åƒï¼")
        
        # åˆå¹¶æ•°æ®
        all_latents = torch.cat(all_latents, dim=0)
        all_labels = torch.tensor(all_labels, dtype=torch.long)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ - ä¿®å¤ç»´åº¦é—®é¢˜
        logger.info(f"è®¡ç®—æ½œç©ºé—´ç»Ÿè®¡ä¿¡æ¯ï¼Œå½¢çŠ¶: {all_latents.shape}")

        # è®¡ç®—æ¯ä¸ªé€šé“çš„å‡å€¼å’Œæ ‡å‡†å·®
        latent_mean = all_latents.mean(dim=0, keepdim=True)  # [1, C, H, W]
        latent_std = all_latents.std(dim=0, keepdim=True)    # [1, C, H, W]

        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å½¢çŠ¶ - mean: {latent_mean.shape}, std: {latent_std.shape}")
        
        # ä¿å­˜ä¸ºsafetensorsæ ¼å¼ï¼ˆå…¼å®¹å®˜æ–¹ImgLatentDatasetï¼‰
        try:
            import safetensors.torch
            safetensors.torch.save_file({
                'latents': all_latents,
                'labels': all_labels
            }, self.latents_file)
        except ImportError:
            # å¦‚æœsafetensorsä¸å¯ç”¨ï¼Œä½¿ç”¨torchæ ¼å¼
            logger.warning("âš ï¸ safetensorsä¸å¯ç”¨ï¼Œä½¿ç”¨torchæ ¼å¼ä¿å­˜")
            torch.save({
                'latents': all_latents,
                'labels': all_labels
            }, self.latents_file.with_suffix('.pt'))
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        torch.save({
            'mean': latent_mean,
            'std': latent_std
        }, self.stats_file)
        
        logger.info(f"\nâœ… ç¼–ç å®Œæˆ!")
        logger.info(f"   å¤„ç†å›¾åƒ: {total_processed} å¼ ")
        logger.info(f"   æ½œç©ºé—´æ ·æœ¬: {len(all_latents)} ä¸ª")
        logger.info(f"   æ½œç©ºé—´å½¢çŠ¶: {all_latents.shape}")
        logger.info(f"   ä¿å­˜åˆ°: {self.latents_file}")
        
    def create_dataloaders(self):
        """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
        # ä½¿ç”¨ç®€åŒ–çš„æ•°æ®é›†ç±»
        dataset = MicroDopplerLatentDataset(
            latents_file=self.latents_file,
            stats_file=self.stats_file,
            latent_norm=self.config.latent_norm,
            latent_multiplier=self.config.latent_multiplier
        )

        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        total_size = len(dataset)
        train_size = int(0.8 * total_size)
        val_size = total_size - train_size

        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            persistent_workers=self.config.persistent_workers
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            persistent_workers=self.config.persistent_workers
        )

        logger.info(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        logger.info(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        logger.info(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

        return train_loader, val_loader


class MicroDopplerLatentDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ½œç©ºé—´æ•°æ®é›†"""

    def __init__(self, latents_file, stats_file, latent_norm=True, latent_multiplier=1.0):
        self.latents_file = Path(latents_file)
        self.stats_file = Path(stats_file)
        self.latent_norm = latent_norm
        self.latent_multiplier = latent_multiplier

        # åŠ è½½æ•°æ®
        if self.latents_file.suffix == '.safetensors':
            try:
                import safetensors.torch
                with safetensors.torch.safe_open(self.latents_file, framework="pt", device="cpu") as f:
                    self.latents = f.get_tensor('latents')
                    self.labels = f.get_tensor('labels')
            except ImportError:
                # å›é€€åˆ°torchæ ¼å¼
                data = torch.load(self.latents_file.with_suffix('.pt'))
                self.latents = data['latents']
                self.labels = data['labels']
        else:
            data = torch.load(self.latents_file)
            self.latents = data['latents']
            self.labels = data['labels']

        # åŠ è½½ç»Ÿè®¡ä¿¡æ¯
        if latent_norm and self.stats_file.exists():
            stats = torch.load(self.stats_file)
            self.latent_mean = stats['mean']
            self.latent_std = stats['std']
            logger.info(f"åŠ è½½ç»Ÿè®¡ä¿¡æ¯ - mean: {self.latent_mean.shape}, std: {self.latent_std.shape}")
        else:
            self.latent_mean = None
            self.latent_std = None

        logger.info(f"æ•°æ®é›†å¤§å°: {len(self.latents)}, æ½œç©ºé—´å½¢çŠ¶: {self.latents[0].shape if len(self.latents) > 0 else 'N/A'}")

    def __len__(self):
        return len(self.latents)

    def __getitem__(self, idx):
        latent = self.latents[idx].clone()
        label = self.labels[idx].clone()

        # å½’ä¸€åŒ– - ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        if self.latent_norm and self.latent_mean is not None:
            # ç¡®ä¿ç»Ÿè®¡ä¿¡æ¯çš„ç»´åº¦ä¸æ½œç©ºé—´æ•°æ®åŒ¹é…
            if self.latent_mean.dim() == 4:  # [1, C, H, W]
                mean = self.latent_mean.squeeze(0)  # [C, H, W]
                std = self.latent_std.squeeze(0)    # [C, H, W]
            else:
                mean = self.latent_mean
                std = self.latent_std

            # ç¡®ä¿ç»´åº¦åŒ¹é…
            if latent.shape != mean.shape:
                logger.warning(f"ç»´åº¦ä¸åŒ¹é…: latent {latent.shape} vs mean {mean.shape}")
                # å¦‚æœç»Ÿè®¡ä¿¡æ¯æ˜¯é€šé“ç»´åº¦çš„ï¼Œéœ€è¦å¹¿æ’­åˆ°ç©ºé—´ç»´åº¦
                if mean.dim() == 1:  # åªæœ‰é€šé“ç»´åº¦
                    mean = mean.view(-1, 1, 1)
                    std = std.view(-1, 1, 1)
                elif mean.dim() == 3 and latent.dim() == 3:
                    # éƒ½æ˜¯3ç»´ï¼Œæ£€æŸ¥ç©ºé—´ç»´åº¦
                    if mean.shape[1:] != latent.shape[1:]:
                        # åªä½¿ç”¨é€šé“ç»´åº¦çš„ç»Ÿè®¡ä¿¡æ¯
                        mean = mean.mean(dim=[1, 2], keepdim=True)
                        std = std.mean(dim=[1, 2], keepdim=True)

            latent = (latent - mean) / (std + 1e-8)  # æ·»åŠ å°å€¼é¿å…é™¤é›¶

        latent = latent * self.latent_multiplier

        return latent, label


def setup_kaggle_multi_gpu():
    """è®¾ç½®Kaggle T4Ã—2 GPUç¯å¢ƒ"""
    if not torch.cuda.is_available():
        raise RuntimeError("CUDAä¸å¯ç”¨ï¼è¯·åœ¨Kaggleä¸­å¯ç”¨GPUåŠ é€Ÿå™¨")

    num_gpus = torch.cuda.device_count()
    logger.info(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU")

    for i in range(num_gpus):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        logger.info(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")

    if num_gpus < 2:
        logger.warning("âš ï¸ åªæ£€æµ‹åˆ°1ä¸ªGPUï¼Œå°†ä½¿ç”¨å•GPUè®­ç»ƒ")
        return False, torch.device("cuda:0")

    # Kaggle T4Ã—2ç¯å¢ƒè®¾ç½®
    # è®¾ç½®ä¸»è®¾å¤‡ä¸ºcuda:0ï¼ŒDataParallelä¼šè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
    torch.cuda.set_device(0)  # è®¾ç½®ä¸»GPU
    device = torch.device("cuda:0")

    # æ¸…ç†GPUç¼“å­˜
    for i in range(num_gpus):
        with torch.cuda.device(i):
            torch.cuda.empty_cache()

    logger.info("âœ… å°†ä½¿ç”¨DataParallelè¿›è¡ŒåŒGPUè®­ç»ƒ")
    logger.info(f"   ä¸»è®¾å¤‡: {device}")
    logger.info(f"   å¯ç”¨GPU: {list(range(num_gpus))}")
    return True, device


class DiTModelManager:
    """DiTæ¨¡å‹ç®¡ç†å™¨ - ä½¿ç”¨å®˜æ–¹LightningDiTç»„ä»¶"""

    def __init__(self, config: MicroDopplerConfig):
        self.config = config

    def load_vae(self, device):
        """åŠ è½½å¾®è°ƒåçš„VA-VAEæ¨¡å‹"""
        logger.info("ğŸ”„ åŠ è½½å¾®è°ƒåçš„VA-VAEæ¨¡å‹...")

        # è®¾ç½®tamingè·¯å¾„
        self._setup_taming_path()

        # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
        vae_config = self._create_vae_config()
        temp_config_path = Path(self.config.output_dir) / "temp_vae_config.yaml"

        with open(temp_config_path, 'w') as f:
            yaml.dump(vae_config, f)

        # åŠ è½½VA-VAE
        from tokenizer.vavae import VA_VAE
        try:
            # å°è¯•ä½¿ç”¨é…ç½®æ–‡ä»¶è·¯å¾„
            vae = VA_VAE(
                config=str(temp_config_path),
                img_size=256,
                horizon_flip=0.0,  # ä¸ä½¿ç”¨æ•°æ®å¢å¼º
                fp16=True
            )
        except Exception as e:
            logger.warning(f"ä½¿ç”¨é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            # å°è¯•ç›´æ¥ä½¿ç”¨checkpointè·¯å¾„
            try:
                vae = VA_VAE(
                    config=self.config.vae_checkpoint,
                    img_size=256,
                    horizon_flip=0.0,
                    fp16=True
                )
            except Exception as e2:
                logger.error(f"VA-VAEåŠ è½½å¤±è´¥: {e2}")
                raise RuntimeError(f"æ— æ³•åŠ è½½VA-VAEæ¨¡å‹: {e2}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_config_path.unlink()

        logger.info(f"âœ… VA-VAEåŠ è½½å®Œæˆ")
        return vae

    def create_dit_model(self, device, use_multi_gpu=False):
        """åˆ›å»ºLightningDiTæ¨¡å‹å¹¶è¿›è¡ŒXLâ†’Læƒé‡è¿ç§»"""
        logger.info("ğŸ”„ åˆ›å»ºLightningDiTæ¨¡å‹...")

        from models.lightningdit import LightningDiT_models

        # åˆ›å»ºLæ¨¡å‹
        dit_model = LightningDiT_models[f"LightningDiT-{self.config.model_type}"](
            input_size=self.config.input_size,
            num_classes=self.config.num_classes,
        )

        # XLâ†’Læƒé‡è¿ç§»
        self._transfer_xl_to_l_weights(dit_model)

        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        dit_model = dit_model.to(device)

        # Kaggle T4Ã—2ä½¿ç”¨DataParallel
        if use_multi_gpu and torch.cuda.device_count() > 1:
            # ç¡®ä¿æ¨¡å‹åœ¨cuda:0ä¸Šï¼Œç„¶ååŒ…è£…DataParallel
            dit_model = dit_model.to('cuda:0')
            dit_model = nn.DataParallel(dit_model, device_ids=list(range(torch.cuda.device_count())))
            logger.info(f"âœ… å¯ç”¨DataParallelå¤šGPUè®­ç»ƒï¼Œä½¿ç”¨GPU: {list(range(torch.cuda.device_count()))}")
        else:
            logger.info(f"âœ… å•GPUè®­ç»ƒï¼Œä½¿ç”¨è®¾å¤‡: {device}")

        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in dit_model.parameters())
        trainable_params = sum(p.numel() for p in dit_model.parameters() if p.requires_grad)

        logger.info(f"âœ… LightningDiT-{self.config.model_type} åˆ›å»ºå®Œæˆ")
        logger.info(f"   æ¨¡å‹æ¶æ„: 24å±‚, 1024éšè—ç»´åº¦, patch_size=2")
        logger.info(f"   æ€»å‚æ•°: {total_params / 1e6:.1f}M")
        logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e6:.1f}M")

        return dit_model

    def setup_training_components(self, dit_model):
        """è®¾ç½®è®­ç»ƒç»„ä»¶ï¼šä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€transport"""
        from transport import create_transport

        # åˆ›å»ºtransport - ä½¿ç”¨å®˜æ–¹æ¨èé…ç½®
        transport = create_transport(
            path_type='Linear',
            prediction='velocity',
            loss_weight=None,
            train_eps=None,
            sample_eps=None,
            use_cosine_loss=self.config.use_cosine_loss,
            use_lognorm=self.config.use_lognorm
        )

        # ä¼˜åŒ–å™¨ - å®˜æ–¹æ¨èé…ç½®
        optimizer = torch.optim.AdamW(
            dit_model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(0.9, self.config.beta2),
            eps=1e-8
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        def lr_lambda(current_step):
            if current_step < self.config.warmup_steps:
                return float(current_step) / float(max(1, self.config.warmup_steps))
            # ä½™å¼¦é€€ç«
            progress = float(current_step - self.config.warmup_steps) / float(max(1, 10000 - self.config.warmup_steps))
            return max(0.01, 0.5 * (1.0 + np.cos(np.pi * progress)))

        scheduler = LambdaLR(optimizer, lr_lambda)

        logger.info("âœ… è®­ç»ƒç»„ä»¶è®¾ç½®å®Œæˆ")
        logger.info(f"   ä¼˜åŒ–å™¨: AdamW (lr={self.config.learning_rate}, beta2={self.config.beta2})")
        logger.info(f"   è°ƒåº¦å™¨: é¢„çƒ­{self.config.warmup_steps}æ­¥ + ä½™å¼¦é€€ç«")
        logger.info(f"   Transport: Linear path + velocity prediction")

        return optimizer, scheduler, transport

    def _setup_taming_path(self):
        """è®¾ç½®taming-transformersè·¯å¾„"""
        taming_locations = [
            Path('/kaggle/working/taming-transformers'),
            Path('/kaggle/working/.taming_path'),
        ]

        for location in taming_locations:
            if location.name == '.taming_path' and location.exists():
                with open(location, 'r') as f:
                    taming_path = f.read().strip()
                if Path(taming_path).exists() and taming_path not in sys.path:
                    sys.path.insert(0, taming_path)
                    return
            elif location.exists():
                taming_path = str(location.absolute())
                if taming_path not in sys.path:
                    sys.path.insert(0, taming_path)
                    return

    def _create_vae_config(self):
        """åˆ›å»ºVA-VAEé…ç½® - åŸºäºå®˜æ–¹æ ¼å¼"""
        return {
            'ckpt_path': self.config.vae_checkpoint,
            'model': {
                'base_learning_rate': 1.0e-04,
                'target': 'ldm.models.autoencoder.AutoencoderKL',
                'params': {
                    'monitor': 'val/rec_loss',
                    'embed_dim': 32,
                    'use_vf': 'dinov2',
                    'reverse_proj': True,
                    'lossconfig': {
                        'target': 'ldm.modules.losses.LPIPSWithDiscriminator',
                        'params': {
                            'disc_start': 1,
                            'kl_weight': 1.0e-06,
                            'disc_weight': 0.5,
                            'vf_weight': 0.1,
                            'adaptive_vf': True,
                            'vf_loss_type': 'combined_v3',
                            'distmat_margin': 0.25,
                            'cos_margin': 0.5
                        }
                    },
                    'ddconfig': {
                        'double_z': True,
                        'z_channels': 32,
                        'resolution': 256,
                        'in_channels': 3,
                        'out_ch': 3,
                        'ch': 128,
                        'ch_mult': [1, 1, 2, 2, 4],
                        'num_res_blocks': 2,
                        'attn_resolutions': [16],
                        'dropout': 0.0
                    }
                }
            }
        }

    def _transfer_xl_to_l_weights(self, dit_model):
        """XLâ†’Læƒé‡æ™ºèƒ½è¿ç§»"""
        # æ£€æŸ¥å¯èƒ½çš„XLæƒé‡è·¯å¾„
        possible_paths = [
            Path("/kaggle/input/lightningdit-xl/lightningdit-xl-imagenet256-64ep.pt"),
            Path("/kaggle/input/models/lightningdit-xl-imagenet256-64ep.pt"),
            Path("/kaggle/working/models/lightningdit-xl-imagenet256-64ep.pt"),
        ]

        xl_checkpoint_path = None
        for path in possible_paths:
            if path.exists():
                xl_checkpoint_path = path
                break

        if xl_checkpoint_path is None:
            logger.warning("âš ï¸ XLé¢„è®­ç»ƒæƒé‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
            return

        logger.info("ğŸ”„ è¿›è¡ŒXLâ†’Læƒé‡è¿ç§»...")

        try:
            xl_state = torch.load(xl_checkpoint_path, map_location='cpu')
            if 'model' in xl_state:
                xl_state = xl_state['model']
            elif 'state_dict' in xl_state:
                xl_state = xl_state['state_dict']

            # æ™ºèƒ½æƒé‡æ˜ å°„
            l_state = dit_model.state_dict()
            transferred = 0

            for name, param in l_state.items():
                if name in xl_state and param.shape == xl_state[name].shape:
                    l_state[name] = xl_state[name]
                    transferred += 1
                elif name.startswith('blocks.') and transferred < 24:  # Læ¨¡å‹24å±‚
                    # å±‚çº§æ˜ å°„é€»è¾‘
                    layer_idx = int(name.split('.')[1])
                    if layer_idx < 28:  # XLæ¨¡å‹28å±‚
                        xl_name = name
                        if xl_name in xl_state and param.shape == xl_state[xl_name].shape:
                            l_state[name] = xl_state[xl_name]
                            transferred += 1

            dit_model.load_state_dict(l_state)
            logger.info(f"âœ… æƒé‡è¿ç§»å®Œæˆï¼ŒæˆåŠŸè¿ç§» {transferred} ä¸ªå‚æ•°")

        except Exception as e:
            logger.warning(f"âš ï¸ æƒé‡è¿ç§»å¤±è´¥: {e}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")


class MicroDopplerTrainer:
    """å¾®å¤šæ™®å‹’DiTè®­ç»ƒå™¨ - åŸºäºå®˜æ–¹è®­ç»ƒå¾ªç¯"""

    def __init__(self, config: MicroDopplerConfig, data_manager: MicroDopplerDataManager,
                 model_manager: DiTModelManager):
        self.config = config
        self.data_manager = data_manager
        self.model_manager = model_manager
        self.device = None
        self.use_multi_gpu = False

    def _safe_transport_call(self, transport, dit_model, latents, model_kwargs):
        """å®‰å…¨çš„transportè°ƒç”¨ï¼Œç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§"""
        device = latents.device

        # ä¸´æ—¶è®¾ç½®é»˜è®¤è®¾å¤‡
        original_device = torch.cuda.current_device()
        try:
            if device.type == 'cuda':
                torch.cuda.set_device(device)

            # ç¡®ä¿æ‰€æœ‰è¾“å…¥åœ¨åŒä¸€è®¾å¤‡
            if hasattr(dit_model, 'module'):
                # DataParallelæƒ…å†µ
                latents = latents.to('cuda:0')
                if 'y' in model_kwargs:
                    model_kwargs['y'] = model_kwargs['y'].to('cuda:0')

            return transport.training_losses(dit_model, latents, model_kwargs)
        finally:
            # æ¢å¤åŸå§‹è®¾å¤‡
            torch.cuda.set_device(original_device)

    def train(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ å¼€å§‹å¾®å¤šæ™®å‹’DiTè®­ç»ƒ")
        logger.info("="*60)

        # 1. è®¾ç½®GPUç¯å¢ƒ
        self.use_multi_gpu, self.device = setup_kaggle_multi_gpu()

        # 2. åŠ è½½æ•°æ®åˆ’åˆ†ä¿¡æ¯
        self.data_manager.load_split_info()

        # 3. åŠ è½½VA-VAEå¹¶ç¼–ç æ•°æ®
        vae = self.model_manager.load_vae(self.device)
        self.data_manager.encode_to_latents(vae, self.device)
        del vae  # é‡Šæ”¾æ˜¾å­˜
        torch.cuda.empty_cache()

        # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = self.data_manager.create_dataloaders()

        # 5. åˆ›å»ºDiTæ¨¡å‹
        dit_model = self.model_manager.create_dit_model(self.device, self.use_multi_gpu)

        # 6. è®¾ç½®è®­ç»ƒç»„ä»¶
        optimizer, scheduler, transport = self.model_manager.setup_training_components(dit_model)

        # 7. è®­ç»ƒå¾ªç¯
        self._training_loop(dit_model, train_loader, val_loader, optimizer, scheduler, transport)

    def _training_loop(self, dit_model, train_loader, val_loader, optimizer, scheduler, transport):
        """è®­ç»ƒå¾ªç¯"""
        best_val_loss = float('inf')
        patience_counter = 0
        # ä¿®å¤GradScalerå¼ƒç”¨è­¦å‘Š
        try:
            scaler = torch.amp.GradScaler('cuda')
        except AttributeError:
            # å›é€€åˆ°æ—§ç‰ˆæœ¬
            scaler = torch.cuda.amp.GradScaler()

        logger.info(f"\nğŸ¯ å¼€å§‹è®­ç»ƒå¾ªç¯:")
        logger.info(f"   æ€»è½®æ•°: {self.config.num_epochs}")
        logger.info(f"   æ—©åœè€å¿ƒ: {self.config.patience}")
        logger.info(f"   æ¢¯åº¦ç´¯ç§¯: {self.config.gradient_accumulation_steps}")

        for epoch in range(self.config.num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_loss = self._train_epoch(dit_model, train_loader, optimizer, scheduler, transport, scaler, epoch)

            # éªŒè¯é˜¶æ®µ
            val_loss = self._validate_epoch(dit_model, val_loader, transport, epoch)

            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss - self.config.min_improvement:
                best_val_loss = val_loss
                patience_counter = 0
                self._save_best_model(dit_model, optimizer, scheduler, epoch, val_loss)
            else:
                patience_counter += 1

            logger.info(f"Epoch {epoch+1}/{self.config.num_epochs} - "
                       f"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, "
                       f"Patience: {patience_counter}/{self.config.patience}")

            # æ—©åœ
            if patience_counter >= self.config.patience:
                logger.info(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­{self.config.patience}è½®éªŒè¯æŸå¤±æœªæ”¹å–„")
                break

        logger.info(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")

    def _train_epoch(self, dit_model, train_loader, optimizer, scheduler, transport, scaler, epoch):
        """å•ä¸ªè®­ç»ƒè½®æ¬¡"""
        dit_model.train()
        total_loss = 0
        num_batches = 0

        optimizer.zero_grad()

        with tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]") as pbar:
            for batch_idx, (latents, labels) in enumerate(pbar):
                # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                if hasattr(dit_model, 'module'):
                    # DataParallelæƒ…å†µä¸‹ï¼Œæ•°æ®å¿…é¡»åœ¨cuda:0
                    latents = latents.to('cuda:0')
                    labels = labels.to('cuda:0')
                else:
                    latents = latents.to(self.device)
                    labels = labels.to(self.device)

                # ä¿®å¤autocastå¼ƒç”¨è­¦å‘Š
                try:
                    autocast_context = torch.amp.autocast('cuda')
                except AttributeError:
                    autocast_context = torch.cuda.amp.autocast()

                with autocast_context:
                    # æ¨¡å‹é¢„æµ‹ - ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡
                    model_kwargs = {"y": labels}

                    # ç¡®ä¿dit_modelåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                    if hasattr(dit_model, 'module'):
                        # DataParallelæƒ…å†µä¸‹ï¼Œç¡®ä¿è¾“å…¥åœ¨cuda:0
                        latents = latents.to('cuda:0')
                        labels = labels.to('cuda:0')
                        model_kwargs = {"y": labels}

                    # ä½¿ç”¨å®‰å…¨çš„transportè°ƒç”¨
                    loss_dict = self._safe_transport_call(transport, dit_model, latents, model_kwargs)

                    # æŸå¤±è®¡ç®—
                    if 'cos_loss' in loss_dict and self.config.use_cosine_loss:
                        mse_loss = loss_dict["loss"].mean()
                        cos_loss = loss_dict["cos_loss"].mean()
                        loss = cos_loss + mse_loss
                    else:
                        loss = loss_dict["loss"].mean()

                    loss = loss / self.config.gradient_accumulation_steps

                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()

                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(dit_model.parameters(), self.config.gradient_clip_norm)
                    scaler.step(optimizer)
                    scaler.update()
                    scheduler.step()
                    optimizer.zero_grad()

                total_loss += loss.item() * self.config.gradient_accumulation_steps
                num_batches += 1

                pbar.set_postfix({'loss': f'{loss.item():.6f}'})

        return total_loss / num_batches

    def _validate_epoch(self, dit_model, val_loader, transport, epoch):
        """éªŒè¯è½®æ¬¡"""
        dit_model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            with tqdm(val_loader, desc=f"Epoch {epoch+1} [Val]") as pbar:
                for latents, labels in pbar:
                    # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                    if hasattr(dit_model, 'module'):
                        # DataParallelæƒ…å†µä¸‹ï¼Œä½¿ç”¨cuda:0
                        latents = latents.to('cuda:0')
                        labels = labels.to('cuda:0')
                    else:
                        latents = latents.to(self.device)
                        labels = labels.to(self.device)

                    model_kwargs = {"y": labels}

                    # ä½¿ç”¨å®‰å…¨çš„transportè°ƒç”¨
                    loss_dict = self._safe_transport_call(transport, dit_model, latents, model_kwargs)

                    if 'cos_loss' in loss_dict and self.config.use_cosine_loss:
                        mse_loss = loss_dict["loss"].mean()
                        cos_loss = loss_dict["cos_loss"].mean()
                        loss = cos_loss + mse_loss
                    else:
                        loss = loss_dict["loss"].mean()

                    total_loss += loss.item()
                    num_batches += 1

                    pbar.set_postfix({'val_loss': f'{loss.item():.6f}'})

        return total_loss / num_batches

    def _save_best_model(self, dit_model, optimizer, scheduler, epoch, val_loss):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹
        old_models = list(Path(self.config.output_dir).glob("best_dit_epoch_*.pt"))
        for old_model in old_models:
            old_model.unlink()

        # ä¿å­˜æ–°æ¨¡å‹
        model_path = Path(self.config.output_dir) / f"best_dit_epoch_{epoch+1}.pt"

        # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸
        if hasattr(dit_model, 'module'):  # DataParallel
            model_state = dit_model.module.state_dict()
        else:
            model_state = dit_model.state_dict()

        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model_state,
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'val_loss': val_loss,
            'config': self.config.__dict__
        }, model_path)

        logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path} (val_loss: {val_loss:.6f})")


def test_setup():
    """æµ‹è¯•è®¾ç½®æ˜¯å¦æ­£å¸¸"""
    logger.info("ğŸ§ª æµ‹è¯•ç¯å¢ƒè®¾ç½®...")

    # æµ‹è¯•GPU
    use_multi_gpu, device = setup_kaggle_multi_gpu()
    logger.info(f"   å¤šGPUæ”¯æŒ: {use_multi_gpu}")
    logger.info(f"   ä¸»è®¾å¤‡: {device}")

    # æµ‹è¯•é…ç½®
    config = MicroDopplerConfig()
    logger.info(f"âœ… é…ç½®åˆ›å»ºæˆåŠŸ")

    # æµ‹è¯•æ•°æ®ç®¡ç†å™¨
    data_manager = MicroDopplerDataManager(config)
    logger.info(f"âœ… æ•°æ®ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")

    # æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨
    model_manager = DiTModelManager(config)
    logger.info(f"âœ… æ¨¡å‹ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")

    # é¿å…æœªä½¿ç”¨å˜é‡è­¦å‘Š
    _ = data_manager, model_manager

    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        Path(config.data_dir),
        Path(config.vae_checkpoint),
        Path(config.split_file)
    ]

    for file_path in required_files:
        if file_path.exists():
            logger.info(f"âœ… æ‰¾åˆ°å¿…è¦æ–‡ä»¶: {file_path}")
        else:
            logger.warning(f"âš ï¸ ç¼ºå°‘æ–‡ä»¶: {file_path}")

    logger.info("ğŸ¯ ç¯å¢ƒæµ‹è¯•å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='å¾®å¤šæ™®å‹’DiTè®­ç»ƒ')
    parser.add_argument('--test', action='store_true', help='åªè¿è¡Œæµ‹è¯•ï¼Œä¸å¼€å§‹è®­ç»ƒ')
    args = parser.parse_args()

    if args.test:
        test_setup()
        return

    logger.info("ğŸ¯ å¾®å¤šæ™®å‹’DiTæ¡ä»¶ç”Ÿæˆè®­ç»ƒ")

    # åˆ›å»ºé…ç½®
    config = MicroDopplerConfig()

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    logger.info("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    logger.info(f"   æ¨¡å‹: LightningDiT-{config.model_type} (24å±‚, 1024ç»´)")
    logger.info(f"   ç”¨æˆ·ç±»åˆ«: {config.num_classes}")
    logger.info(f"   æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    logger.info(f"   æ¢¯åº¦ç´¯ç§¯: {config.gradient_accumulation_steps}")
    logger.info(f"   æœ‰æ•ˆæ‰¹æ¬¡: {config.batch_size * config.gradient_accumulation_steps}")
    logger.info(f"   å­¦ä¹ ç‡: {config.learning_rate}")
    logger.info(f"   æ€»è½®æ•°: {config.num_epochs}")
    logger.info(f"   æ—©åœè€å¿ƒ: {config.patience}")

    # åˆ›å»ºç®¡ç†å™¨
    data_manager = MicroDopplerDataManager(config)
    model_manager = DiTModelManager(config)
    trainer = MicroDopplerTrainer(config, data_manager, model_manager)

    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train()
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()

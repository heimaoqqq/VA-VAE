#!/usr/bin/env python3
"""
æ­¥éª¤8: ä»å¤´è®­ç»ƒLightningDiT-Baseæ¨¡å‹ç”¨äºå¾®å¤šæ™®å‹’æ¡ä»¶ç”Ÿæˆ
åŸºäºå®˜æ–¹train.pyå¤åˆ¶å¹¶ä¿®æ”¹ï¼Œç¡®ä¿æœ€å¤§ç¨‹åº¦ä¸å®˜æ–¹ä¸€è‡´
Training LightningDiT-Base with VA-VAE on micro-Doppler dataset
"""

import torch
# import torch.distributed as dist  # ä½¿ç”¨acceleratoræ›¿ä»£
import torch.backends.cuda
import torch.backends.cudnn
# from torch.nn.parallel import DistributedDataParallel as DDP  # ä½¿ç”¨acceleratoræ›¿ä»£
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import wandb

import math
import yaml
import json
import numpy as np
import logging
import os
import sys
import argparse
import time
from tqdm import tqdm
from glob import glob
from copy import deepcopy
from collections import OrderedDict
from PIL import Image
from pathlib import Path
import torchvision

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('/kaggle/working/VA-VAE/LightningDiT')
sys.path.append('/kaggle/working/LightningDiT')

from utils_scheduler import create_scheduler
from utils_regularization import (
    LabelSmoothing, mixup_data, orthogonal_regularization,
    ContrastiveRegularizer, EarlyStopping
)

# ç¦ç”¨torch.compileä»¥é¿å…Kaggleç¯å¢ƒé—®é¢˜
os.environ['TORCH_COMPILE_DISABLE'] = '1'
os.environ['TORCHDYNAMO_DISABLE'] = '1'
torch._dynamo.disable()

from diffusers.models import AutoencoderKL
from models.lightningdit import LightningDiT_models
from transport import create_transport, Sampler
from accelerate import Accelerator

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„å¾®å¤šæ™®å‹’æ•°æ®é›†ç±»
sys.path.append('/kaggle/working/microdoppler_finetune')

# ç›´æ¥å®šä¹‰ç®€åŒ–çš„æ•°æ®é›†ç±»ï¼Œé¿å…å¯¼å…¥é—®é¢˜
from safetensors.torch import load_file
from torch.utils.data import Dataset

class MicroDopplerLatentDataset(Dataset):
    def __init__(self, data_dir, latent_norm=True, latent_multiplier=1.0):
        self.data_dir = Path(data_dir)
        self.latent_norm = latent_norm
        self.latent_multiplier = latent_multiplier
        
        # è·å–æ‰€æœ‰latentæ–‡ä»¶
        self.latent_files = sorted(list(self.data_dir.glob("*.safetensors")))
        
        if len(self.latent_files) == 0:
            raise ValueError(f"No safetensors files found in {data_dir}")
        
        print(f"Found {len(self.latent_files)} latent files in {data_dir}")
        
        # é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
        self.latents = []
        self.labels = []
        
        for file_path in self.latent_files:
            data = load_file(str(file_path))
            
            # è·å–latentå’Œlabelsï¼ˆæ”¯æŒæˆ‘ä»¬çš„batchæ ¼å¼ï¼‰
            latent = data['latents']  # [B, C, H, W]
            labels_batch = data['labels']  # [B]
            
            # æ·»åŠ æ¯ä¸ªæ ·æœ¬
            for i in range(latent.shape[0]):
                sample_latent = latent[i]  # [C, H, W]
                sample_label = labels_batch[i]
                
                # æš‚æ—¶ä¸åœ¨è¿™é‡Œå½’ä¸€åŒ–ï¼Œä¿æŒåŸå§‹latent
                # å½’ä¸€åŒ–å°†åœ¨è®¡ç®—å…¨å±€ç»Ÿè®¡åè¿›è¡Œ
                self.latents.append(sample_latent)
                self.labels.append(sample_label.long())
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        unique_labels = torch.stack(self.labels).unique()
        print(f"Dataset contains {len(self.latents)} samples with {len(unique_labels)} unique classes")
        
        # è®¡ç®—latentç»Ÿè®¡ä¿¡æ¯ç”¨äºé‡‡æ ·
        self._compute_latent_stats()
        
    def _compute_latent_stats(self):
        """è®¡ç®—æ•°æ®é›†ä¸­latentçš„ç»Ÿè®¡ä¿¡æ¯å¹¶è¿›è¡Œå½’ä¸€åŒ–"""
        if len(self.latents) > 0:
            # å°†æ‰€æœ‰latentå †å å¹¶è®¡ç®—ç»Ÿè®¡
            all_latents = torch.stack(self.latents)  # [N, C, H, W]
            # è®¡ç®—åŸå§‹æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåå½’ä¸€åŒ–ï¼‰
            # ä½¿ç”¨keepdim=Trueä¿æŒç»´åº¦[1, C, 1, 1]ä¸å®˜æ–¹ä¸€è‡´
            self.latent_mean = all_latents.mean(dim=[0, 2, 3], keepdim=True)  # [1, C, 1, 1]
            self.latent_std = all_latents.std(dim=[0, 2, 3], keepdim=True)    # [1, C, 1, 1]
            
            # æ·»åŠ åŸå§‹ç©ºé—´ç»Ÿè®¡æ˜¾ç¤º
            original_mean = all_latents.mean().item()
            original_std = all_latents.std().item()
            print(f"\nğŸ“Š åŸå§‹latentç©ºé—´ç»Ÿè®¡ (åŠ è½½å):")
            print(f"   Mean: {original_mean:.4f}")
            print(f"   Std:  {original_std:.4f}")
            
            # æŒ‰ç…§å®˜æ–¹LightningDiTæ–¹å¼å¤„ç†
            for i in range(len(self.latents)):
                if self.latent_norm:
                    # å½’ä¸€åŒ–åˆ°N(0,1): (x - mean) / std
                    # self.latents[i]å½¢çŠ¶æ˜¯[C, H, W]ï¼Œéœ€è¦æ­£ç¡®å¹¿æ’­
                    # latent_mean/stdå½¢çŠ¶æ˜¯[1, C, 1, 1]ï¼Œsqueeze(0)åæ˜¯[C, 1, 1]
                    mean_broadcast = self.latent_mean.squeeze(0)  # [C, 1, 1]
                    std_broadcast = self.latent_std.squeeze(0)    # [C, 1, 1] 
                    self.latents[i] = (self.latents[i] - mean_broadcast) / (std_broadcast + 1e-8)
                    
                    # è°ƒè¯•ï¼šæ£€æŸ¥å½’ä¸€åŒ–ç»“æœ
                    if i == 0:  # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬é¿å…è¿‡å¤šè¾“å‡º
                        norm_mean = self.latents[i].mean().item()
                        norm_std = self.latents[i].std().item()
                        print(f"\nâœ… å½’ä¸€åŒ–éªŒè¯ï¼ˆè®­ç»ƒæ•°æ®æ ·æœ¬0ï¼‰:")
                        print(f"   å½’ä¸€åŒ–å: mean={norm_mean:.4f}, std={norm_std:.4f}")
                        print(f"   çŠ¶æ€: {'æ­£å¸¸' if abs(norm_mean) < 0.1 and abs(norm_std - 1.0) < 0.1 else 'å¼‚å¸¸'}")
                
                # å®˜æ–¹æ€»æ˜¯ä¹˜multiplierï¼ˆæ— è®ºæ˜¯å¦å½’ä¸€åŒ–ï¼‰
                self.latents[i] = self.latents[i] * self.latent_multiplier
                
            # æ£€æŸ¥è®­ç»ƒç©ºé—´çš„ç»Ÿè®¡ï¼ˆç¼©æ”¾åï¼‰
            all_scaled = torch.stack(self.latents)  # é‡æ–°å †å ç¼©æ”¾åçš„æ•°æ®
            train_mean = all_scaled.mean().item()
            train_std = all_scaled.std().item()
            print(f"\nğŸ¯ è®­ç»ƒç©ºé—´ç»Ÿè®¡ (multiplier={self.latent_multiplier}åº”ç”¨å):")
            print(f"   Mean: {train_mean:.4f}")
            print(f"   Std:  {train_std:.4f}")
            print(f"   {'âœ… æ¥è¿‘ç›®æ ‡' if abs(train_std - 1.0) < 0.2 else 'âš ï¸ åç¦»ç›®æ ‡'} (ç›®æ ‡stdâ‰ˆ1.0)")
        else:
            # é»˜è®¤å€¼ï¼Œä¿æŒ[1, C, 1, 1]å½¢çŠ¶
            self.latent_mean = torch.zeros(1, 32, 1, 1)  # å‡è®¾32ç»´latent
            self.latent_std = torch.ones(1, 32, 1, 1)
    
    def get_latent_stats(self):
        """è¿”å›latentçš„å‡å€¼å’Œæ ‡å‡†å·®ç»Ÿè®¡"""
        return self.latent_mean, self.latent_std
        
    def __len__(self):
        return len(self.latents)
    
    def __getitem__(self, idx):
        latent = self.latents[idx]
        label = self.labels[idx]
        
        # ç§»é™¤æ•°æ®å¢å¼º - å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾éœ€è¦ä¿æŒç²¾ç¡®çš„ç‰©ç†æ„ä¹‰
        # å™ªå£°å¯èƒ½ç ´åå…³é”®çš„æ—¶é¢‘ç‰¹å¾å’Œç”¨æˆ·é—´çš„ç»†å¾®å·®åˆ«
        
        return latent, label

def do_train(train_config, accelerator):
    """
    Trains a LightningDiT-Base for micro-Doppler generation.
    """
    # Setup accelerator:
    device = accelerator.device

    # Setup an experiment folder:
    if accelerator.is_main_process:
        os.makedirs(train_config['train']['output_dir'], exist_ok=True)  # Make results folder (holds all experiment subfolders)
        experiment_index = len(glob(f"{train_config['train']['output_dir']}/*"))
        model_string_name = train_config['model']['model_type'].replace("/", "-")
        if train_config['train']['exp_name'] is None:
            exp_name = f'{experiment_index:03d}-{model_string_name}'
        else:
            exp_name = train_config['train']['exp_name']
        experiment_dir = f"{train_config['train']['output_dir']}/{exp_name}"  # Create an experiment folder
        checkpoint_dir = f"{experiment_dir}/checkpoints"  # Stores saved model checkpoints
        os.makedirs(checkpoint_dir, exist_ok=True)
        logger = create_logger(experiment_dir, accelerator)
        logger.info(f"Experiment directory created at {experiment_dir}")
        tensorboard_dir_log = f"tensorboard_logs/{exp_name}"
        os.makedirs(tensorboard_dir_log, exist_ok=True)
        writer = SummaryWriter(log_dir=tensorboard_dir_log)

        # add configs to tensorboard
        config_str=json.dumps(train_config, indent=4)
        writer.add_text('training configs', config_str, global_step=0)
    checkpoint_dir = f"{train_config['train']['output_dir']}/{train_config['train']['exp_name']}/checkpoints"

    # get rank
    rank = accelerator.local_process_index

    # Create model:
    if 'downsample_ratio' in train_config['vae']:
        downsample_ratio = train_config['vae']['downsample_ratio']
    else:
        downsample_ratio = 16
    assert train_config['data']['image_size'] % downsample_ratio == 0, "Image size must be divisible by 16 (for the VAE encoder)."
    latent_size = train_config['data']['image_size'] // downsample_ratio
    model = LightningDiT_models[train_config['model']['model_type']](
        input_size=latent_size,
        num_classes=train_config['data']['num_classes'],
        class_dropout_prob=0.1,  # å®˜æ–¹é»˜è®¤å€¼ï¼Œç”¨äºè®­ç»ƒæ—¶éšæœºdropoutç±»åˆ«æ¡ä»¶
        use_qknorm=train_config['model']['use_qknorm'],
        use_swiglu=train_config['model']['use_swiglu'] if 'use_swiglu' in train_config['model'] else False,
        use_rope=train_config['model']['use_rope'] if 'use_rope' in train_config['model'] else False,
        use_rmsnorm=train_config['model']['use_rmsnorm'] if 'use_rmsnorm' in train_config['model'] else False,
        wo_shift=train_config['model']['wo_shift'] if 'wo_shift' in train_config['model'] else False,
        in_channels=train_config['model']['in_chans'] if 'in_chans' in train_config['model'] else 4,
        use_checkpoint=train_config['model']['use_checkpoint'] if 'use_checkpoint' in train_config['model'] else False,
    ).to(device)  # Move model to device immediately after creation

    # Create EMA model - must be on same device as model
    ema = deepcopy(model)
    
    # Explicitly move both models to device and ensure all parameters are there
    model = model.to(device)
    ema = ema.to(device)
    
    # load pretrained model (if provided)
    if 'weight_init' in train_config['train'] and train_config['train']['weight_init'] is not None:
        checkpoint = torch.load(train_config['train']['weight_init'], map_location=lambda storage, loc: storage)
        # remove the prefix 'module.' from the keys
        checkpoint['model'] = {k.replace('module.', ''): v for k, v in checkpoint['model'].items()}
        model = load_weights_with_shape_check(model, checkpoint, rank=rank)
        ema = load_weights_with_shape_check(ema, checkpoint, rank=rank)
        if accelerator.is_main_process:
            logger.info(f"Loaded pretrained model from {train_config['train']['weight_init']}")
    
    requires_grad(ema, False)
    
    # åˆ›å»ºå¹¶åŠ è½½æˆ‘ä»¬å¾®è°ƒçš„VA-VAE
    from tokenizer.vavae import VA_VAE
    # ä½¿ç”¨ä¸“é—¨ä¸ºDiTè®­ç»ƒå‡†å¤‡çš„é…ç½®æ–‡ä»¶ï¼ˆä¸åŒ…å«adaptive_vfï¼‰
    vae_config_path = '/kaggle/working/VA-VAE/microdoppler_finetune/vavae_config_for_dit.yaml'
    vae = VA_VAE(vae_config_path)
    
    # åŠ è½½å¾®è°ƒçš„VA-VAEæƒé‡
    vae_checkpoint_path = '/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt'
    if os.path.exists(vae_checkpoint_path):
        vae_checkpoint = torch.load(vae_checkpoint_path, map_location=device)
        
        # ä»checkpointä¸­æå–state_dict
        if 'state_dict' in vae_checkpoint:
            state_dict = vae_checkpoint['state_dict']
        else:
            state_dict = vae_checkpoint
        
        # è¿‡æ»¤å‡ºåªå±äºVAEçš„æƒé‡ï¼ˆæ’é™¤lossã€foundation_modelç­‰ï¼‰
        vae_state_dict = {}
        for key, value in state_dict.items():
            # åªä¿ç•™ä¸ä»¥lossã€foundation_modelã€linear_projå¼€å¤´çš„æƒé‡
            if not key.startswith('loss.') and not key.startswith('foundation_model.') and key != 'linear_proj.weight':
                vae_state_dict[key] = value
        
        # åŠ è½½è¿‡æ»¤åçš„æƒé‡
        vae.model.load_state_dict(vae_state_dict, strict=True)
        
        if accelerator.is_main_process:
            logger.info(f"Loaded custom VA-VAE from {vae_checkpoint_path}")
            logger.info(f"Filtered {len(state_dict) - len(vae_state_dict)} non-VAE keys from checkpoint")
    else:
        if accelerator.is_main_process:
            logger.warning(f"VA-VAE checkpoint not found at {vae_checkpoint_path}, using default weights")
    
    # VA_VAEç±»å†…éƒ¨å·²ç»å¤„ç†äº†.cuda()å’Œ.eval()ï¼Œä¸éœ€è¦é¢å¤–è°ƒç”¨
    # å†»ç»“VA-VAEæƒé‡ï¼Œä»…ç”¨äºç¼–ç è§£ç 
    for param in vae.model.parameters():
        param.requires_grad = False
    if accelerator.is_main_process:
        logger.info("VA-VAE weights frozen for inference only")
    
    # åˆ›å»ºtransportå’Œä¼˜åŒ–å™¨ï¼ˆåœ¨accelerator.prepareä¹‹å‰ï¼‰
    transport = create_transport(
        train_config['transport']['path_type'],
        train_config['transport']['prediction'],
        train_config['transport']['loss_weight'],
        train_config['transport']['train_eps'],
        train_config['transport']['sample_eps'],
        use_cosine_loss = train_config['transport']['use_cosine_loss'] if 'use_cosine_loss' in train_config['transport'] else False,
        use_lognorm = train_config['transport']['use_lognorm'] if 'use_lognorm' in train_config['transport'] else False,
    )  # default: velocity; 
    
    # è®¾ç½®ä¼˜åŒ–å™¨å‚æ•° - åŒ¹é…å®˜æ–¹é»˜è®¤å€¼
    train_config['optimizer']['lr'] = train_config.get('optimizer', {}).get('lr', 1e-4)
    train_config['optimizer']['beta2'] = train_config.get('optimizer', {}).get('beta2', 0.999)  # ä»configè¯»å–
    train_config['optimizer']['max_grad_norm'] = train_config.get('optimizer', {}).get('max_grad_norm', 1.0)
    
    # åŠ å¼ºæƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ
    weight_decay = train_config.get('optimizer', {}).get('weight_decay', 1e-3)  # å¤§å¹…å¢åŠ L2æ­£åˆ™åŒ–
    opt = torch.optim.AdamW(
        model.parameters(), 
        lr=train_config['optimizer']['lr'], 
        weight_decay=weight_decay,  # ä½¿ç”¨æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ
        betas=(0.9, train_config['optimizer']['beta2'])  # å®˜æ–¹beta1å›ºå®šä¸º0.9
    )
    
    if accelerator.is_main_process:
        logger.info(f"LightningDiT Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
        logger.info(f"Optimizer: AdamW, lr={train_config['optimizer']['lr']}, beta2={train_config['optimizer']['beta2']}, weight_decay={weight_decay}")
        logger.info(f'Use lognorm sampling: {train_config["transport"]["use_lognorm"]}')
        logger.info(f'Use cosine loss: {train_config["transport"]["use_cosine_loss"]}')
        logger.info(f"ğŸ›¡ï¸ Regularization: weight_decay={weight_decay}")
    
    # Setup data - ä½¿ç”¨å…¼å®¹çš„å¾®å¤šæ™®å‹’latentæ•°æ®é›†
    dataset = MicroDopplerLatentDataset(
        data_dir=train_config['data']['data_path'],  # ä½¿ç”¨å®˜æ–¹å‚æ•°ådata_dir
        latent_norm=train_config['data']['latent_norm'] if 'latent_norm' in train_config['data'] else False,
        latent_multiplier=train_config['data'].get('latent_multiplier', 1.0),  # ä½¿ç”¨1.0ä½œä¸ºé»˜è®¤å€¼
    )
    # dataset.training = True  # ç§»é™¤æ•°æ®å¢å¼º - ä¿æŒå¾®å¤šæ™®å‹’ä¿¡å·çš„ç²¾ç¡®æ€§
    batch_size_per_gpu = int(np.round(train_config['train']['global_batch_size'] / accelerator.num_processes))
    global_batch_size = batch_size_per_gpu * accelerator.num_processes
    loader = DataLoader(
        dataset,
        batch_size=batch_size_per_gpu,
        shuffle=True,
        num_workers=train_config['data']['num_workers'],
        pin_memory=True,
        drop_last=True
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆç°åœ¨loaderå·²ç»å®šä¹‰ï¼‰
    total_steps = train_config['train']['max_epochs'] * len(loader)
    scheduler = None  # åˆå§‹åŒ–schedulerå˜é‡
    if 'scheduler' in train_config:
        scheduler_config = train_config['scheduler']
        if scheduler_config['type'] == 'cosine':
            # ä½™å¼¦é€€ç«ï¼Œå¸¦çº¿æ€§warmup
            def lr_lambda(current_step):
                warmup_steps = scheduler_config.get('warmup_steps', 500)  # æ ‡å‡†warmup
                min_lr_ratio = float(scheduler_config.get('min_lr', 1e-6)) / float(train_config['optimizer']['lr'])
                
                if current_step < warmup_steps:
                    # çº¿æ€§warmup
                    return float(current_step) / float(max(1, warmup_steps))
                else:
                    # ä½™å¼¦é€€ç«åˆ°min_lr
                    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
                    cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))
                    return min_lr_ratio + (1 - min_lr_ratio) * cosine_decay
            
            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)
            print(f"ğŸ“Š Using cosine scheduler with {scheduler_config.get('warmup_steps', 500)} warmup steps")               
        else:
            scheduler = None
    
    if accelerator.is_main_process:
        logger.info(f"Dataset contains {len(dataset):,} images {train_config['data']['data_path']}")
        logger.info(f"Batch size {batch_size_per_gpu} per gpu, with {global_batch_size} global batch size")
        if scheduler:
            logger.info(f"Using learning rate scheduler: {train_config['train'].get('scheduler', {}).get('type', 'cosine')}")
            logger.info(f"Total training steps: {total_steps:,}")
    
    # éªŒè¯é›†
    if 'valid_path' in train_config['data']:
        valid_dataset = MicroDopplerLatentDataset(
            data_dir=train_config['data']['valid_path'],  # ä½¿ç”¨å®˜æ–¹å‚æ•°ådata_dir
            latent_norm=train_config['data']['latent_norm'] if 'latent_norm' in train_config['data'] else False,
            latent_multiplier=train_config['data'].get('latent_multiplier', 1.0),  # ä½¿ç”¨1.0ä½œä¸ºé»˜è®¤å€¼
        )
        valid_loader = DataLoader(
            valid_dataset,
            batch_size=batch_size_per_gpu,
            shuffle=True,  # å®˜æ–¹éªŒè¯é›†ä¹Ÿshuffle
            num_workers=train_config['data']['num_workers'],
            pin_memory=True,
            drop_last=True  # å®˜æ–¹è®­ç»ƒå’ŒéªŒè¯éƒ½drop_last
        )
        if accelerator.is_main_process:
            logger.info(f"Validation Dataset contains {len(valid_dataset):,} images {train_config['data']['valid_path']}")

    train_config['train']['resume'] = train_config['train']['resume'] if 'resume' in train_config['train'] else False

    if train_config['train']['resume']:
        # check if the checkpoint exists
        checkpoint_files = glob(f"{checkpoint_dir}/*.pt")
        if checkpoint_files:
            checkpoint_files.sort(key=lambda x: os.path.getsize(x))
            latest_checkpoint = checkpoint_files[-1]
            checkpoint = torch.load(latest_checkpoint, map_location=lambda storage, loc: storage)
            model.load_state_dict(checkpoint['model'])
            # opt.load_state_dict(checkpoint['opt'])
            ema.load_state_dict(checkpoint['ema'])
            train_steps = int(latest_checkpoint.split('/')[-1].split('.')[0])
            start_epoch = checkpoint.get('epoch', 0)
            best_val_loss = checkpoint.get('best_val_loss', float('inf'))
            patience_counter = checkpoint.get('patience_counter', 0)
            if accelerator.is_main_process:
                logger.info(f"Resuming training from checkpoint: {latest_checkpoint}")
                logger.info(f"Starting from epoch {start_epoch}, step {train_steps}")
        else:
            if accelerator.is_main_process:
                logger.info("No checkpoint found. Starting training from scratch.")
            start_epoch = 0
            best_val_loss = float('inf')
            patience_counter = 0
    else:
        start_epoch = 0
        best_val_loss = float('inf')
        patience_counter = 0
    
    model.train()
    running_loss = 0.0
    step_count = 0
    start_time = time.time()
    
    best_loss = float('inf')
    
    # Gradient accumulationè®¾ç½® - ä¿®å¤é…ç½®
    gradient_accumulation_steps = train_config['train'].get('gradient_accumulation_steps', 1)  # é»˜è®¤æ— ç´¯ç§¯
    global_batch_size = train_config['train']['global_batch_size']  # 16
    world_size = accelerator.num_processes  # 2 (T4x2)
    per_device_batch_size = global_batch_size // world_size  # 8 per GPU
    
    if accelerator.is_main_process:
        logger.info(f"Batch sizeé…ç½®:")
        logger.info(f"  Global batch size: {global_batch_size}")
        logger.info(f"  World size: {world_size}")
        logger.info(f"  Per device batch size: {per_device_batch_size}")
        logger.info(f"  Gradient accumulation steps: {gradient_accumulation_steps}")
        logger.info(f"  Effective batch size: {global_batch_size * gradient_accumulation_steps}")
    model, opt, loader = accelerator.prepare(model, opt, loader)
    if 'valid_path' in train_config['data']:
        valid_loader = accelerator.prepare(valid_loader)

    # Variables for monitoring/logging purposes:
    train_steps = 0
    start_epoch = 0

    # é‡æ–°åˆ›å»ºEMAæ¨¡å‹ç¡®ä¿è®¾å¤‡åŒæ­¥ï¼ˆaccelerator.prepareåï¼‰
    with accelerator.main_process_first():
        ema = deepcopy(accelerator.unwrap_model(model))
        ema = ema.to(accelerator.device)
        requires_grad(ema, False)
        # Initialize EMA with model weights
        update_ema(ema, accelerator.unwrap_model(model), decay=0)
        if accelerator.is_main_process:
            logger.info(f"Using checkpointing: {train_config['train']['use_checkpoint'] if 'use_checkpoint' in train_config['train'] else True}")

    # æ—©åœå‚æ•° - åŠ å¼ºè¿‡æ‹Ÿåˆæ§åˆ¶
    patience = train_config['train'].get('patience', 15)  # é™ä½patience
    min_delta = train_config['train'].get('min_delta', 5e-4)  # å¢åŠ æœ€å°æ”¹å–„é˜ˆå€¼
    overfitting_threshold = train_config['train'].get('overfitting_threshold', 1.2)  # è¿‡æ‹Ÿåˆé˜ˆå€¼
    num_epochs = train_config['train'].get('max_epochs', 200)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        opt, 
        T_max=num_epochs, 
        eta_min=1e-6
    )
    if accelerator.is_main_process:
        logger.info(f"ğŸ“‰ Learning rate scheduler: CosineAnnealingLR (T_max={num_epochs}, eta_min=1e-6)")
    steps_per_epoch = len(loader)
    
    # è®­ç»ƒå¾ªç¯ - æ”¹ä¸ºåŸºäºepoch
    for epoch in range(start_epoch, num_epochs):
        if accelerator.is_main_process:
            logger.info(f"Beginning epoch {epoch}...")
                
        # æ¯ä¸ªepochå¼€å§‹æ—¶éªŒè¯æ•°æ®åˆ†å¸ƒ
        if epoch == 0:
            print(f"\nğŸ” é¦–ä¸ªEpochæ•°æ®åˆ†å¸ƒéªŒè¯:")
            # ä»dataloaderå–ä¸€ä¸ªæ‰¹æ¬¡è¿›è¡ŒéªŒè¯
            for batch_idx, (z_check, y_check) in enumerate(loader):
                if batch_idx == 0:
                    z_mean = z_check.mean().item()
                    z_std = z_check.std().item()
                    print(f"   é¦–æ‰¹æ•°æ®: mean={z_mean:.4f}, std={z_std:.4f}")
                    print(f"   çŠ¶æ€: {'âœ… ç†æƒ³èŒƒå›´' if abs(z_std - 1.0) < 0.2 else 'âš ï¸ éœ€è¦è°ƒæ•´multiplier'}")
                    break
        
        # åªæœ‰DistributedSampleræ‰æœ‰set_epochæ–¹æ³•
        if hasattr(loader.sampler, 'set_epoch'):
            loader.sampler.set_epoch(epoch)
        print(f"ğŸ”„ Training loader has {len(loader)} batches")
            
        epoch_loss = 0
        epoch_steps = 0
        running_loss = 0
        log_steps = 0
        start_time = time.time()
            
        if accelerator.is_main_process:
            print(f"ğŸš€ Starting training loop for epoch {epoch}")
            
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        pbar = tqdm(enumerate(loader), total=len(loader), desc=f"Epoch {epoch+1}/{num_epochs}", disable=not accelerator.is_main_process)
        
        for batch_idx, batch in pbar:
            if batch_idx == 0 and accelerator.is_main_process:
                logger.info(f"Training epoch {epoch+1}, batch shape: {batch[0].shape}, labels: {batch[1].shape}")
        
            try:
                device = accelerator.device
                    
                # Unpack batch - DataLoaderè¿”å›(latents, labels)å…ƒç»„
                x, y = batch
                x = x.to(device)
                y = y.to(device, dtype=torch.long)
                    
                # å‰å‘ä¼ æ’­ (è€ƒè™‘æ¢¯åº¦ç´¯ç§¯) - ä¿®å¤æŸå¤±è®¡ç®—ä¸å®˜æ–¹ä¸€è‡´
                with accelerator.autocast():
                    loss_dict = transport.training_losses(model, x, model_kwargs=dict(y=y))
                    if 'cos_loss' in loss_dict:
                        # å®˜æ–¹ç»„åˆæŸå¤±ï¼šcosine + mse
                        mse_loss = loss_dict["loss"].mean()
                        cos_loss = loss_dict["cos_loss"].mean()
                        raw_loss = cos_loss + mse_loss
                        if accelerator.is_main_process and batch_idx % 100 == 0:
                            print(f"  MSE Loss: {mse_loss:.4f}, Cosine Loss: {cos_loss:.4f}, Total: {raw_loss:.4f}")
                    else:
                        raw_loss = loss_dict["loss"].mean()
                    loss = raw_loss / gradient_accumulation_steps  # å½’ä¸€åŒ–æŸå¤±ç”¨äºåå‘ä¼ æ’­
                    
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                
                # åªåœ¨ç´¯ç§¯è¶³å¤Ÿæ¢¯åº¦åæ›´æ–°
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    # æ¢¯åº¦è£å‰ª - å®Œå…¨åŒ¹é…å®˜æ–¹ï¼šä½¿ç”¨sync_gradientsæ¡ä»¶
                    if 'max_grad_norm' in train_config['optimizer']:
                        if accelerator.sync_gradients:
                            accelerator.clip_grad_norm_(model.parameters(), train_config['optimizer']['max_grad_norm'])
                    
                    # ä¼˜åŒ–å™¨æ›´æ–°
                    opt.step()
                    if scheduler is not None:
                        scheduler.step()
                    
                    # EMAæ›´æ–° - ä½¿ç”¨accelerator.unwrap_modelç¡®ä¿è·å–æ­£ç¡®çš„æ¨¡å‹
                    update_ema(ema, accelerator.unwrap_model(model))
                    
                    step_count += 1
                    
                # Log loss values:
                running_loss += raw_loss.item()  # ä½¿ç”¨åŸå§‹æŸå¤±å€¼
                epoch_loss += raw_loss.item()     # ä½¿ç”¨åŸå§‹æŸå¤±å€¼
                    
                log_steps += 1
                train_steps += 1
                epoch_steps += 1
                
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰batchçš„æŸå¤±
                if accelerator.is_main_process:
                    pbar.set_postfix({'loss': f"{raw_loss.item():.4f}", 'lr': f"{opt.param_groups[0]['lr']:.2e}"})
                
                # å®šæœŸè®°å½• (åªåœ¨ä¼˜åŒ–å™¨æ›´æ–°å)
                if (batch_idx + 1) % gradient_accumulation_steps == 0 and train_steps % train_config['train']['log_every'] == 0:
                    # Measure training speed:
                    torch.cuda.synchronize()
                    end_time = time.time()
                    steps_per_sec = log_steps / (end_time - start_time)
                    # Reduce loss history over all processes:
                    avg_loss = torch.tensor(running_loss / log_steps, device=device)
                    avg_loss = accelerator.gather(avg_loss).mean()  # ä½¿ç”¨acceleratoræ›¿ä»£dist
                    avg_loss = avg_loss.item()
                    if accelerator.is_main_process:
                        logger.info(f"[Epoch {epoch+1}/{num_epochs}, Step {train_steps:07d}] Train Loss: {avg_loss:.4f}, LR: {opt.param_groups[0]['lr']:.2e}, Speed: {steps_per_sec:.2f} steps/sec")
                        writer.add_scalar('Loss/train', avg_loss, train_steps)
                        writer.add_scalar('Learning_rate', opt.param_groups[0]['lr'], train_steps)
                    # Reset monitoring variables:
                    running_loss = 0
                    log_steps = 0
                    start_time = time.time()
                    
                # è®­ç»ƒæ‰¹æ¬¡ç»Ÿè®¡æ£€æŸ¥ï¼ˆæ¯100æ­¥ï¼‰
                if train_steps % 100 == 0:
                    batch_mean = x.mean().item()
                    batch_std = x.std().item()
                    if accelerator.is_main_process:
                        logger.info(f"ğŸ“Š Batchç»Ÿè®¡ [Step {train_steps}]: mean={batch_mean:.4f}, std={batch_std:.4f}")
                        # æ·»åŠ åˆ°tensorboard
                        writer.add_scalar('train/batch_mean', batch_mean, train_steps)
                        writer.add_scalar('train/batch_std', batch_std, train_steps)
            
            except Exception as e:
                if accelerator.is_main_process:
                    logger.error(f"Error in training batch {batch_idx}: {str(e)}")
                    logger.error(traceback.format_exc())
                continue

        # å…³é—­è¿›åº¦æ¡ä»¥ç¡®ä¿åç»­è¾“å‡ºå¯è§
        if accelerator.is_main_process:
            pbar.close()
        
        # Epochç»“æŸï¼Œè®¡ç®—å¹³å‡æŸå¤±
        avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else 0
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler.step()
        current_lr = opt.param_groups[0]['lr']
        if accelerator.is_main_process:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Epoch {epoch+1}/{num_epochs} Summary:")
            print(f"   ğŸ“‰ Average Training Loss: {avg_epoch_loss:.4f}")
            print(f"   ğŸ”¢ Total Steps: {train_steps}")
            print(f"   ğŸ“š Learning Rate: {opt.param_groups[0]['lr']:.2e}")
            logger.info(f"Epoch {epoch+1}/{num_epochs} Summary - Train Loss: {avg_epoch_loss:.4f}, LR: {opt.param_groups[0]['lr']:.2e}")
            
        # æ¯ä¸ªepochç»“æŸååœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        if 'valid_path' in train_config['data']:
            if accelerator.is_main_process:
                logger.info(f"Evaluating at epoch {epoch+1}")
            
            model.eval()
            val_loss = evaluate(model, valid_loader, device, transport, accelerator)
            model.train()
            
            if accelerator.is_main_process:
                print(f"   ğŸ§ª Validation Loss: {val_loss:.4f}")
                writer.add_scalar('Loss/validation', val_loss, epoch+1)
                
                # è®¡ç®—è¿‡æ‹ŸåˆæŒ‡æ ‡
                overfitting_ratio = val_loss / avg_epoch_loss if avg_epoch_loss > 0 else 1.0
                print(f"   ğŸ“ˆ Overfitting Ratio (val/train): {overfitting_ratio:.3f}")
                
                # è¿‡æ‹ŸåˆçŠ¶æ€åˆ¤æ–­
                if overfitting_ratio < 1.1:
                    print(f"   âœ… Good fit - model generalizing well")
                elif overfitting_ratio < 1.3:
                    print(f"   âš ï¸  Slight overfitting - still acceptable")
                else:
                    print(f"   ğŸš¨ Overfitting detected - consider regularization")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹ - æ›´ä¸¥æ ¼çš„è¿‡æ‹Ÿåˆæ§åˆ¶
                # åªåœ¨æ²¡æœ‰è¿‡æ‹Ÿåˆæ—¶ä¿å­˜æ¨¡å‹
                if val_loss < best_val_loss - min_delta and overfitting_ratio < overfitting_threshold:
                    improvement = (best_val_loss - val_loss) / best_val_loss * 100 if best_val_loss != float('inf') else 100
                    best_val_loss = val_loss
                    patience_counter = 0
                    print(f"   ğŸ† New best model! Improvement: {improvement:.2f}%")
                    logger.info(f"New best model - Val Loss: {val_loss:.4f}, Improvement: {improvement:.2f}%")
                    
                    # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹
                    best_checkpoint_path = f"{checkpoint_dir}/best_model.pt"
                    if os.path.exists(best_checkpoint_path):
                        os.remove(best_checkpoint_path)
                        print(f"   ğŸ—‘ï¸  Removed old best model")
                    
                    # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
                    checkpoint = {
                        "model": accelerator.unwrap_model(model).state_dict(),
                        "ema": ema.state_dict(),
                        "opt": opt.state_dict(),
                        "config": train_config,
                        "epoch": epoch,
                        "best_val_loss": best_val_loss,
                        "patience_counter": patience_counter,
                    }
                    torch.save(checkpoint, best_checkpoint_path)
                    print(f"   ğŸ’¾ New best model saved: {best_checkpoint_path}")
                else:
                    patience_counter += 1
                    gap = val_loss - best_val_loss
                    logger.info(f"  âš ï¸ No improvement. Patience: {patience_counter}/{patience}, Gap: {gap:.5f}")
                    
                    # æ›´æ—©çš„è¿‡æ‹Ÿåˆå¹²é¢„
                    if overfitting_ratio > overfitting_threshold and patience_counter == 5:
                        # æ‰‹åŠ¨é™ä½å­¦ä¹ ç‡
                        for param_group in opt.param_groups:
                            param_group['lr'] *= 0.3  # æ›´å¤§å¹…åº¦é™ä½
                        logger.info(f"  ğŸ“‰ Early overfitting intervention - LR reduced by 70%")
                    
                    # æ—©åœæ¡ä»¶ï¼šæ›´ä¸¥æ ¼çš„è¿‡æ‹Ÿåˆæ§åˆ¶
                    if patience_counter >= patience:
                        if overfitting_ratio > 1.4:
                            logger.info(f"  ğŸ›‘ Early stopping - Overfitting ratio {overfitting_ratio:.3f} > 1.4")
                            break
                        else:
                            logger.info(f"  ğŸ›‘ Early stopping - No validation improvement for {patience} epochs")
                            break
                    
                    # é¢å¤–çš„è¿‡æ‹Ÿåˆæ£€æŸ¥
                    elif overfitting_ratio > 1.5:
                        logger.info(f"  ğŸ›‘ Emergency stop - Severe overfitting ratio {overfitting_ratio:.3f}")
                        break
                
                logger.info(f"{'='*50}")
        
        # ç”Ÿæˆdemoæ ·æœ¬
        if True:  # æ¯ä¸ªepochéƒ½ç”Ÿæˆ
            sample_dir = f"{train_config['train']['output_dir']}/{train_config['train']['exp_name']}/demo_samples"
            # ä½¿ç”¨EMAæ¨¡å‹è¿›è¡Œç”Ÿæˆï¼ˆå…³é”®ä¿®å¤ï¼šä¹‹å‰é”™è¯¯åœ°ä½¿ç”¨äº†modelè€Œä¸æ˜¯emaï¼‰
            generate_demo_samples(ema, vae, transport, device, accelerator, train_config, epoch, sample_dir)
            if accelerator.is_main_process:
                print(f"{'='*60}")
        
        # å®šæœŸä¿å­˜checkpoint
        if (epoch + 1) % train_config['train'].get('ckpt_every_epoch', 10) == 0:
            if accelerator.is_main_process:
                # æ¸…ç†æ—§çš„epoch checkpointï¼ˆåªä¿ç•™æœ€è¿‘3ä¸ªï¼‰
                epoch_checkpoints = glob(f"{checkpoint_dir}/epoch_*.pt")
                epoch_checkpoints.sort()
                if len(epoch_checkpoints) >= 3:
                    for old_ckpt in epoch_checkpoints[:-2]:  # ä¿ç•™æœ€å2ä¸ª
                        os.remove(old_ckpt)
                        logger.info(f"Removed old checkpoint: {old_ckpt}")
                
                checkpoint = {
                    "model": accelerator.unwrap_model(model).state_dict(),
                    "ema": ema.state_dict(),
                    "opt": opt.state_dict(),
                    "config": train_config,
                    "epoch": epoch,
                    "best_val_loss": best_val_loss,
                    "patience_counter": patience_counter,
                }
                checkpoint_path = f"{checkpoint_dir}/epoch_{epoch+1:03d}.pt"
                torch.save(checkpoint, checkpoint_path)
                logger.info(f"Saved checkpoint to {checkpoint_path}")
            accelerator.wait_for_everyone()  # ä½¿ç”¨acceleratoræ›¿ä»£dist.barrier()

    if accelerator.is_main_process:
        logger.info("Training completed!")
        logger.info(f"Best validation loss: {best_val_loss:.4f}")

    return accelerator


@torch.no_grad()
def generate_demo_samples(model, vae, transport, device, accelerator, train_config, epoch, sample_dir):
    """ç”Ÿæˆæ¼”ç¤ºæ ·æœ¬ï¼ŒåŸºäºå®˜æ–¹inference.pyå®ç°
    
    æ³¨æ„ï¼šmodelå‚æ•°åº”è¯¥æ˜¯EMAæ¨¡å‹ï¼Œä¸æ˜¯è®­ç»ƒæ¨¡å‹
    """
    model.eval()
    
    # é‡‡æ ·é…ç½® - ä»é…ç½®æ–‡ä»¶è¯»å–
    cfg_scale = train_config['sample']['cfg_scale']
    cfg_interval_start = train_config['sample'].get('cfg_interval_start', 0.11)
    timestep_shift = train_config['sample'].get('timestep_shift', 0.0)
    num_samples = 8  # ç”Ÿæˆ8ä¸ªæ ·æœ¬åšæˆ2x4ç½‘æ ¼
    
    # åˆ›å»ºsampler
    sampler = Sampler(transport)
    sample_fn = sampler.sample_ode(
        sampling_method=train_config['sample']['sampling_method'],
        num_steps=train_config['sample']['num_sampling_steps'],
        atol=train_config['sample']['atol'],
        rtol=train_config['sample']['rtol'],
        reverse=train_config['sample']['reverse'],
        timestep_shift=timestep_shift,
    )
    
    # è·å–æ½œåœ¨ç©ºé—´ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåå½’ä¸€åŒ–ï¼‰
    dataset = MicroDopplerLatentDataset(
        data_dir=train_config['data']['data_path'],
        latent_norm=train_config['data']['latent_norm'],
        latent_multiplier=train_config['data'].get('latent_multiplier', 1.0),
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯ - æ³¨æ„datasetè¿”å›çš„æ˜¯åŸå§‹ç»Ÿè®¡ï¼ˆæœªåº”ç”¨multiplierï¼‰
    latent_mean, latent_std = dataset.get_latent_stats()
    latent_multiplier = train_config['data'].get('latent_multiplier', 1.0)
    
    if latent_mean is None or latent_std is None:
        print("âš ï¸ è­¦å‘Šï¼šæ— æ³•è·å–latentç»Ÿè®¡ä¿¡æ¯ï¼Œå°†æ— æ³•æ­£ç¡®åå½’ä¸€åŒ–")
        latent_mean = torch.zeros(1, 32, 1, 1, device=device)  # VA-VAE f16d32æ˜¯32é€šé“
        latent_std = torch.ones(1, 32, 1, 1, device=device)
    else:
        # å°†ç»Ÿè®¡ä¿¡æ¯ç§»åˆ°æ­£ç¡®è®¾å¤‡
        latent_mean = latent_mean.to(device)  # [1, C, 1, 1]
        latent_std = latent_std.to(device)    # [1, C, 1, 1]
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯çš„æ­£ç¡®æ€§
        # dataset.get_latent_stats()è¿”å›çš„æ˜¯åŸå§‹latentç»Ÿè®¡ï¼Œæ— éœ€é¢å¤–è®¡ç®—
        orig_mean = latent_mean.mean().item()
        orig_std = latent_std.mean().item()
        
        # éªŒè¯è®­ç»ƒç©ºé—´æ•°æ®ç»Ÿè®¡ï¼ˆç»è¿‡å½’ä¸€åŒ–+multiplierå¤„ç†åï¼‰
        sample_loader = DataLoader(dataset, batch_size=128, shuffle=False)
        sample_batch = next(iter(sample_loader))
        actual_mean = sample_batch[0].mean().item()  # è®­ç»ƒç©ºé—´å®é™…å€¼
        actual_std = sample_batch[0].std().item()    # è®­ç»ƒç©ºé—´å®é™…å€¼
        
        print(f"\nğŸ’¾ ç”Ÿæˆæ—¶ç»Ÿè®¡ä¿¡æ¯éªŒè¯:")
        print(f"   åŸå§‹VAEç©ºé—´: mean={orig_mean:.4f}, std={orig_std:.4f}")
        print(f"   è®­ç»ƒç©ºé—´ï¼ˆå®æµ‹ï¼‰: mean={actual_mean:.4f}, std={actual_std:.4f}")
        print(f"   é…ç½®: latent_norm={train_config['data']['latent_norm']}, multiplier={latent_multiplier}")
        
        if train_config['data']['latent_norm']:
            # å½’ä¸€åŒ–æ¨¡å¼ï¼šè®­ç»ƒç©ºé—´åº”è¯¥æ¥è¿‘N(0,1)*multiplier
            expected_mean = 0.0 * latent_multiplier  # 0
            expected_std = 1.0 * latent_multiplier   # 1.0
            print(f"   é¢„æœŸè®­ç»ƒç©ºé—´: meanâ‰ˆ{expected_mean:.1f}, stdâ‰ˆ{expected_std:.1f}")
            print(f"   ç»Ÿè®¡éªŒè¯: {'âœ…' if abs(actual_mean) < 0.2 and abs(actual_std - expected_std) < 0.2 else 'âš ï¸ å¼‚å¸¸'}")
        else:
            # éå½’ä¸€åŒ–æ¨¡å¼ï¼šä»…åº”ç”¨multiplier
            print(f"   æ¨¡å¼: ä»…ç¼©æ”¾ï¼ˆåŸå§‹Ã—{latent_multiplier}ï¼‰")
            print(f"   ç»Ÿè®¡éªŒè¯: {'âœ…' if abs(actual_std - orig_std * latent_multiplier) < 0.1 else 'âš ï¸ å¼‚å¸¸'}")
        
    
    if accelerator.is_main_process:
        print(f"ğŸ¨ Generating demo samples for epoch {epoch+1}...")
        
        images = []
        # ç”Ÿæˆä¸åŒç±»åˆ«çš„æ ·æœ¬
        demo_labels = [0, 1, 5, 10, 15, 20, 25, 30]  # é€‰æ‹©8ä¸ªä¸åŒç±»åˆ«
        
        for label in demo_labels:
            # åˆ›å»ºå™ªå£°å’Œæ ‡ç­¾
            latent_size = train_config['data']['image_size'] // train_config['vae']['downsample_ratio']
            # æ³¨æ„ï¼šè¿™é‡Œçš„modelå·²ç»æ˜¯EMAæ¨¡å‹
            unwrapped_model = model if not hasattr(model, 'module') else model.module
            z = torch.randn(1, unwrapped_model.in_channels, latent_size, latent_size, device=device)
            y = torch.tensor([label], device=device)
            
            # CFGè®¾ç½®
            z = torch.cat([z, z], 0)
            y_null = torch.tensor([train_config['data']['num_classes']], device=device)  # ä½¿ç”¨num_classesä½œä¸ºnullç±»åˆ«ï¼ˆå¯¹äº31ä¸ªç”¨æˆ·ï¼Œè¿™é‡Œæ˜¯31ï¼‰
            y = torch.cat([y, y_null], 0)
            
            model_kwargs = dict(y=y, cfg_scale=cfg_scale, cfg_interval=False, cfg_interval_start=cfg_interval_start)
            model_fn = unwrapped_model.forward_with_cfg
            
            # é‡‡æ ·
            samples = sample_fn(z, model_fn, **model_kwargs)[-1]
            samples, _ = samples.chunk(2, dim=0)  # ç§»é™¤null classæ ·æœ¬
            
            # è§£ç æµç¨‹ - æ ¹æ®å®˜æ–¹img_latent_dataset.pyä»£ç åˆ†æ
            # è®­ç»ƒæ—¶: (latent - mean) / std * multiplier
            # æ¨ç†æ—¶: ä½¿ç”¨å®˜æ–¹inference.pyçš„å…¬å¼ï¼ˆæ•°å­¦ç­‰ä»·ä½†å†™æ³•ä¸åŒï¼‰
            
            if train_config['data']['latent_norm']:
                # å®˜æ–¹æ ‡å‡†æµç¨‹ï¼šåå½’ä¸€åŒ–ä»è®­ç»ƒç©ºé—´è¿˜åŸåˆ°VAE latentç©ºé—´
                # å®˜æ–¹å…¬å¼ï¼š(samples * std) / multiplier + mean
                samples_for_decode = (samples * latent_std) / latent_multiplier + latent_mean
                
                # é¦–æ¬¡ç”Ÿæˆæ—¶æ˜¾ç¤ºåå½’ä¸€åŒ–éªŒè¯
                if label == demo_labels[0]:
                    print(f"\nğŸ”„ åå½’ä¸€åŒ–éªŒè¯ï¼ˆé¦–ä¸ªç”Ÿæˆæ ·æœ¬ï¼‰:")
                    print(f"   ç”Ÿæˆæ ·æœ¬ï¼ˆè®­ç»ƒç©ºé—´ï¼‰: mean={samples.mean():.4f}, std={samples.std():.4f}")
                    print(f"   è¿˜åŸåï¼ˆVAEç©ºé—´ï¼‰: mean={samples_for_decode.mean():.4f}, std={samples_for_decode.std():.4f}")
            else:
                # å¦‚æœç¦ç”¨å½’ä¸€åŒ–ï¼Œä»…åº”ç”¨multiplierçš„é€†æ“ä½œ
                samples_for_decode = samples / latent_multiplier
                
                # é¦–æ¬¡ç”Ÿæˆæ—¶æ˜¾ç¤ºéªŒè¯ä¿¡æ¯
                if label == demo_labels[0]:
                    print(f"\nğŸ”„ è§£ç éªŒè¯ï¼ˆé¦–ä¸ªæ ·æœ¬ï¼‰:")
                    print(f"   ç”Ÿæˆæ ·æœ¬: mean={samples.mean():.4f}, std={samples.std():.4f}")
                    print(f"   è¿˜åŸå: mean={samples_for_decode.mean():.4f}, std={samples_for_decode.std():.4f}")
                    print(f"   æ¨¡å¼: ä»…åç¼©æ”¾ï¼ˆ/multiplier={latent_multiplier}ï¼‰")
            
            # VAEè§£ç ä¸ºå›¾åƒ
            with torch.no_grad():
                # ä½¿ç”¨VA_VAEçš„decode_to_imagesæ–¹æ³•ï¼Œç›´æ¥è¿”å›numpyæ•°ç»„
                images_decoded = vae.decode_to_images(samples_for_decode)  # è¿”å›[B, H, W, C] numpyæ•°ç»„
                image = images_decoded[0]  # å–ç¬¬ä¸€ä¸ªå›¾åƒ [H, W, C]
            images.append(image)
        
        # åˆ›å»º2x4ç½‘æ ¼
        h, w = images[0].shape[:2]
        grid = np.zeros((2 * h, 4 * w, 3), dtype=np.uint8)
        for idx, image in enumerate(images):
            i, j = divmod(idx, 4)  # 2x4ç½‘æ ¼ä½ç½®
            grid[i*h:(i+1)*h, j*w:(j+1)*w] = image
        
        # ä¿å­˜ç½‘æ ¼å›¾åƒ
        os.makedirs(sample_dir, exist_ok=True)
        grid_path = f"{sample_dir}/epoch_{epoch+1:03d}_samples.png"
        Image.fromarray(grid).save(grid_path)
        print(f"ğŸ’¾ Demo samples saved to: {grid_path}")
    
    model.train()

@torch.no_grad()
def evaluate(model, valid_loader, device, transport, accelerator):
    """è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½"""
    total_loss = 0
    total_steps = 0
    
    for x, y in tqdm(valid_loader, desc="Evaluating", disable=not accelerator.is_main_process):
        x = x.to(device)
        y = y.to(device, dtype=torch.long)
        
        model_kwargs = dict(y=y)
        loss_dict = transport.training_losses(model, x, model_kwargs)
        
        if 'cos_loss' in loss_dict:
            # ä¸è®­ç»ƒä¸€è‡´çš„ç»„åˆæŸå¤±è®¡ç®—
            mse_loss = loss_dict["loss"].mean()
            cos_loss = loss_dict["cos_loss"].mean()
            loss = cos_loss + mse_loss
        else:
            loss = loss_dict["loss"].mean()
            
        total_loss += loss.item()
        total_steps += 1
    
    avg_loss = total_loss / total_steps
    
    # All-reduce across processes using accelerator
    avg_loss_tensor = torch.tensor(avg_loss, device=device)
    avg_loss_tensor = accelerator.gather(avg_loss_tensor).mean()
    avg_loss = avg_loss_tensor.item()
    
    return avg_loss


def load_weights_with_shape_check(model, checkpoint, rank=0):
    """åŠ è½½æƒé‡å¹¶æ£€æŸ¥å½¢çŠ¶åŒ¹é…"""
    model_state_dict = model.state_dict()
    # check shape and load weights
    for name, param in checkpoint['model'].items():
        if name in model_state_dict:
            if param.shape == model_state_dict[name].shape:
                model_state_dict[name].copy_(param)
            elif name == 'x_embedder.proj.weight':
                # special case for x_embedder.proj.weight
                # the pretrained model is trained with 256x256 images
                # we can load the weights by resizing the weights
                # and keep the first channels the same
                weight = torch.zeros_like(model_state_dict[name])
                min_channels = min(param.shape[1], weight.shape[1])
                weight[:, :min_channels] = param[:, :min_channels]
                model_state_dict[name] = weight
            else:
                if rank == 0:
                    print(f"Skipping loading parameter '{name}' due to shape mismatch: "
                        f"checkpoint shape {param.shape}, model shape {model_state_dict[name].shape}")
        else:
            if rank == 0:
                print(f"Parameter '{name}' not found in model, skipping.")
    # load state dict
    model.load_state_dict(model_state_dict, strict=False)
    
    return model


@torch.no_grad()
def update_ema(ema_model, model, decay=0.9999):
    """
    Step the EMA model towards the current model.
    å®˜æ–¹LightningDiTçš„EMAæ›´æ–°ç­–ç•¥
    """
    ema_params = OrderedDict(ema_model.named_parameters())
    model_params = OrderedDict(model.named_parameters())
    
    for name, param in model_params.items():
        name = name.replace("module.", "")
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
        if name in ema_params:
            # Ensure both tensors are on the same device
            ema_param = ema_params[name]
            if ema_param.device != param.data.device:
                ema_param.data = ema_param.data.to(param.data.device)
            ema_param.mul_(decay).add_(param.data, alpha=1 - decay)

def requires_grad(model, flag=True):
    """
    Set requires_grad flag for all parameters in a model.
    """
    for p in model.parameters():
        p.requires_grad = flag

def load_config(config_path):
    with open(config_path, "r") as file:
        config = yaml.safe_load(file)
    return config


def create_logger(logging_dir, accelerator):
    """
    Create a logger that writes to a log file and stdout.
    ä½¿ç”¨acceleratoråˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼Œé¿å…åˆ†å¸ƒå¼åˆå§‹åŒ–é—®é¢˜
    """
    if accelerator.is_main_process:  # real logger
        logging.basicConfig(
            level=logging.INFO,
            format='[\033[34m%(asctime)s\033[0m] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[logging.StreamHandler(), logging.FileHandler(f"{logging_dir}/log.txt")]
        )
        logger = logging.getLogger(__name__)
    else:  # dummy logger (does nothing)
        logger = logging.getLogger(__name__)
        logger.addHandler(logging.NullHandler())
    return logger


if __name__ == "__main__":
    # read config
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='config_dit_base.yaml')
    args = parser.parse_args()

    accelerator = Accelerator(mixed_precision='bf16')  # å¯ç”¨æ··åˆç²¾åº¦å’Œå¤šGPU
    train_config = load_config(args.config)
    
    # æ‰“å°acceleratorä¿¡æ¯
    if accelerator.is_main_process:
        print(f"ğŸš€ Accelerator setup - Devices: {accelerator.num_processes}, Mixed precision: {accelerator.mixed_precision}")
    
    do_train(train_config, accelerator)

#!/usr/bin/env python3
"""
LoRAå¾®è°ƒDiT XLæ¨¡å‹ - è§£å†³æ˜¾å­˜é™åˆ¶é—®é¢˜
ä½¿ç”¨ä½ç§©é€‚é…å™¨è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import sys
import os
from pathlib import Path
import time
import yaml
from tqdm import tqdm
import numpy as np
import safetensors.torch as safetensors

# æ·»åŠ è·¯å¾„
sys.path.append('/kaggle/working/VA-VAE')
sys.path.append('/kaggle/working/VA-VAE/LightningDiT')

from models.lightningdit import LightningDiT_models

class LatentDataset(torch.utils.data.Dataset):
    """Latentå‘é‡æ•°æ®é›†"""
    def __init__(self, latents, labels):
        self.latents = latents
        self.labels = labels
    
    def __len__(self):
        return len(self.latents)
    
    def __getitem__(self, idx):
        return self.latents[idx], self.labels[idx]

class LoRALayer(nn.Module):
    """LoRAé€‚é…å™¨å±‚"""
    def __init__(self, original_layer, rank=16, alpha=32):
        super().__init__()
        self.original_layer = original_layer
        self.rank = rank
        self.alpha = alpha
        
        # å†»ç»“åŸå§‹æƒé‡
        for param in self.original_layer.parameters():
            param.requires_grad = False
            
        # LoRAå‚æ•°
        if hasattr(original_layer, 'in_features') and hasattr(original_layer, 'out_features'):
            # Linearå±‚
            in_features = original_layer.in_features
            out_features = original_layer.out_features
            
            self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
            self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
            self.scaling = alpha / rank
            
    def forward(self, x):
        # åŸå§‹è¾“å‡º
        original_output = self.original_layer(x)
        
        # LoRAå¢é‡
        if hasattr(self, 'lora_A') and hasattr(self, 'lora_B'):
            lora_output = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling
            return original_output + lora_output
        else:
            return original_output

def add_lora_to_model(model, rank=16, alpha=32, target_modules=['qkv', 'proj', 'w12', 'w3']):
    """ä¸ºæ¨¡å‹æ·»åŠ LoRAé€‚é…å™¨"""
    print(f"ğŸ”§ æ·»åŠ LoRAé€‚é…å™¨ (rank={rank}, alpha={alpha})")
    
    lora_modules = []
    total_params = 0
    trainable_params = 0
    
    for name, module in model.named_modules():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¨¡å—
        if any(target in name for target in target_modules) and isinstance(module, nn.Linear):
            # è·å–çˆ¶æ¨¡å—å’Œå±æ€§å
            parent_module = model
            attrs = name.split('.')
            for attr in attrs[:-1]:
                parent_module = getattr(parent_module, attr)
            
            # æ›¿æ¢ä¸ºLoRAå±‚
            lora_layer = LoRALayer(module, rank=rank, alpha=alpha)
            setattr(parent_module, attrs[-1], lora_layer)
            lora_modules.append(name)
            
            print(f"   âœ… æ·»åŠ LoRA: {name}")
    
    # ç»Ÿè®¡å‚æ•°
    for name, param in model.named_parameters():
        total_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    
    print(f"ğŸ“Š LoRAç»Ÿè®¡:")
    print(f"   æ·»åŠ LoRAæ¨¡å—: {len(lora_modules)}")
    print(f"   æ€»å‚æ•°: {total_params/1e6:.1f}M")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params/1e6:.1f}M ({trainable_params/total_params*100:.2f}%)")
    
    return model, lora_modules

def load_dit_xl_with_lora(checkpoint_path, device):
    """åŠ è½½DiT XLæ¨¡å‹ç”¨äºLoRAå¾®è°ƒ"""
    print(f"ğŸ“‚ åŠ è½½DiT XLæ¨¡å‹: {checkpoint_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        alternative_paths = [
            "/kaggle/working/VA-VAE/models/lightningdit-xl-imagenet256-64ep.pt",
            "/kaggle/input/lightningdit-xl/lightningdit-xl-imagenet256-64ep.pt",
            "/kaggle/working/models/lightningdit-xl-imagenet256-64ep.pt"
        ]
        
        for alt_path in alternative_paths:
            if os.path.exists(alt_path):
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {alt_path}")
                checkpoint_path = alt_path
                break
        else:
            print("\nâŒ æœªæ‰¾åˆ°DiT XLæ¨¡å‹æ–‡ä»¶")
            print("è¯·å…ˆè¿è¡Œä»¥ä¸‹æ­¥éª¤ä¸‹è½½æ¨¡å‹:")
            print("1. python step2_download_models.py")
            print("2. æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° /kaggle/working/")
            print("\næ¨¡å‹ä¸‹è½½åœ°å€:")
            print("https://github.com/Alpha-VLLM/LightningDiT/releases/download/v0.1.0/lightningdit-xl-imagenet256-64ep.pt")
            return None
    
    # ğŸ”§ å…³é”®ä¼˜åŒ–ï¼šå¼ºåˆ¶CPUåŠ è½½é¿å…GPUå³°å€¼æ˜¾å­˜
    print(f"ğŸ“Š GPUæ˜¾å­˜ï¼ˆåŠ è½½å‰ï¼‰: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    print(f"ğŸ“Š GPUæ˜¾å­˜ï¼ˆcheckpointåŠ è½½åï¼‰: {torch.cuda.memory_allocated(0) / 1e9:.1f}GB")
    
    if 'model' in checkpoint:
        state_dict = checkpoint['model']
    elif 'ema' in checkpoint:
        state_dict = checkpoint['ema']
    else:
        state_dict = checkpoint
    
    # å¤„ç†DataParallelæƒé‡é”®
    clean_state_dict = {}
    for key, value in state_dict.items():
        clean_key = key.replace('module.', '') if key.startswith('module.') else key
        clean_state_dict[clean_key] = value
    
    # æ¨æ–­å‚æ•°
    pos_embed_shape = None
    y_embed_shape = None
    final_layer_shape = None
    has_swiglu = False
    
    for key, tensor in clean_state_dict.items():
        if key == 'pos_embed':
            pos_embed_shape = tensor.shape
        elif key == 'y_embedder.embedding_table.weight':
            y_embed_shape = tensor.shape
        elif key == 'final_layer.linear.weight':
            final_layer_shape = tensor.shape
        elif 'mlp.w12' in key:
            has_swiglu = True
    
    input_size = int(pos_embed_shape[1]**0.5) if pos_embed_shape else 16
    num_classes = y_embed_shape[0] if y_embed_shape else 1001
    out_channels = final_layer_shape[0] if final_layer_shape else 32
    
    print(f"ğŸ“‹ æ¨¡å‹é…ç½®:")
    print(f"   è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
    print(f"   ç±»åˆ«æ•°é‡: {num_classes}")
    print(f"   è¾“å‡ºé€šé“: {out_channels}")
    print(f"   MLPç±»å‹: {'SwiGLU' if has_swiglu else 'GELU'}")
    
    # ğŸš¨ è°ƒè¯•ï¼šæ£€æŸ¥å®é™…åŠ è½½çš„æƒé‡å½¢çŠ¶
    print(f"ğŸ” æƒé‡æ–‡ä»¶è°ƒè¯•:")
    for key, tensor in list(clean_state_dict.items())[:5]:
        print(f"   {key}: {tensor.shape}")
    print(f"   æ€»æƒé‡é”®æ•°é‡: {len(clean_state_dict)}")
    
    # ğŸš¨ æ£€æŸ¥å…³é”®å±‚çš„ç»´åº¦
    if 'blocks.0.attn.qkv.weight' in clean_state_dict:
        qkv_shape = clean_state_dict['blocks.0.attn.qkv.weight'].shape
        print(f"   æ³¨æ„åŠ›å±‚ç»´åº¦: {qkv_shape} (åº”è¯¥æ˜¯3456x1152 for XL)")
    
    if 'blocks.0.mlp.w12.weight' in clean_state_dict:
        mlp_shape = clean_state_dict['blocks.0.mlp.w12.weight'].shape  
        actual_ratio = mlp_shape[0] / 1152
        print(f"   MLPå±‚ç»´åº¦: {mlp_shape} (å®é™…æ¯”ä¾‹: {actual_ratio:.2f}x)")
        print(f"   ğŸ“ åˆ†æ: æ ‡å‡†XLåº”è¯¥æ˜¯8x(9216)ï¼Œå½“å‰æ˜¯{actual_ratio:.2f}x({mlp_shape[0]})")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰w3å±‚
        if 'blocks.0.mlp.w3.weight' in clean_state_dict:
            w3_shape = clean_state_dict['blocks.0.mlp.w3.weight'].shape
            print(f"   MLP w3ç»´åº¦: {w3_shape} (SwiGLUçš„è¾“å‡ºé—¨æ§)")
        else:
            print(f"   âš ï¸ æœªæ‰¾åˆ°w3å±‚ï¼Œå¯èƒ½ä¸æ˜¯æ ‡å‡†SwiGLU")
    
    model = LightningDiT_models['LightningDiT-XL/1'](
        input_size=input_size,
        num_classes=num_classes,
        class_dropout_prob=0.0,
        use_qknorm=False,
        use_swiglu=has_swiglu,
        use_rope=True,
        use_rmsnorm=True,
        wo_shift=False,
        in_channels=out_channels,
        use_checkpoint=False,
    )
    
    missing_keys, unexpected_keys = model.load_state_dict(clean_state_dict, strict=False)
    
    if missing_keys:
        print(f"âš ï¸ ç¼ºå¤±æƒé‡é”®: {len(missing_keys)}")
    if unexpected_keys:
        print(f"âš ï¸ å¤šä½™æƒé‡é”®: {len(unexpected_keys)}")
    
    param_count = sum(p.numel() for p in model.parameters())
    print(f"âœ… DiT XLæ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   å‚æ•°é‡: {param_count/1e6:.1f}M")
    
    return model

# å¯¼å…¥VA-VAEç›¸å…³æ¨¡å—
try:
    sys.path.append('/kaggle/working/VA-VAE/LightningDiT/tokenizer')
    from autoencoder import AutoencoderKL
    VAVAE_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ VA-VAEæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    VAVAE_AVAILABLE = False

def load_vae_model(device):
    """åŠ è½½é¢„è®­ç»ƒVA-VAEæ¨¡å‹ - ä½¿ç”¨çœŸæ­£çš„VA-VAEå®ç°"""
    print(f"ğŸ“‚ åŠ è½½VA-VAEæ¨¡å‹:")
    
    # VA-VAEæ¨¡å‹è·¯å¾„
    vae_checkpoint_path = "/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt"
        
    if not os.path.exists(vae_checkpoint_path):
        print(f"   âŒ æœªæ‰¾åˆ°VA-VAEæƒé‡æ–‡ä»¶: {vae_checkpoint_path}")
        print("   è¿™æ˜¯å¿…éœ€çš„æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡ŒçœŸæ­£çš„LoRAè®­ç»ƒ")
        return None
    
    if not VAVAE_AVAILABLE:
        print(f"   âŒ VA-VAEæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
        return None
    
    try:
        print(f"   ğŸ“‹ ä½¿ç”¨çœŸæ­£çš„VA-VAEå®ç°")
        
        # ä½¿ç”¨çœŸæ­£çš„AutoencoderKLåŠ è½½æ¨¡å‹
        vae_model = AutoencoderKL(
            embed_dim=32,  # ä»é…ç½®æ–‡ä»¶å¾—åˆ°
            ch_mult=(1, 1, 2, 2, 4),  # ä»é…ç½®æ–‡ä»¶å¾—åˆ°
            use_variational=True,
            ckpt_path=vae_checkpoint_path,
            model_type='vavae'
        )
        
        vae_model.eval().to(device)
        print(f"   âœ… VA-VAEæ¨¡å‹åŠ è½½æˆåŠŸ (çœŸæ­£å®ç°)")
        print(f"   ğŸ“Š æ¨¡å‹å‚æ•°: embed_dim=32, ä¸‹é‡‡æ ·=16x")
        return vae_model
        
    except Exception as e:
        print(f"   âŒ VA-VAEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"   è¯¦ç»†é”™è¯¯: {type(e).__name__}: {str(e)}")
        return None
            
def encode_images_to_latents(vae_model, data_split_dir, original_data_dir, device_vae):
    """å°†åŸå§‹å›¾åƒç¼–ç ä¸ºlatentå‘é‡ - ä»…ä½¿ç”¨çœŸæ­£çš„VA-VAE"""
    
    if vae_model is None:
        print(f"âŒ VA-VAEæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç¼–ç å›¾åƒ")
        print(f"   éœ€è¦çœŸæ­£çš„VA-VAEæ¨¡å‹æ‰èƒ½è¿›è¡ŒLoRAè®­ç»ƒ")
        return None
        
    print(f"ğŸ¨ ä½¿ç”¨çœŸæ­£çš„VA-VAEç¼–ç åŸå§‹å›¾åƒ...")
    
    import json
    from PIL import Image
    import torchvision.transforms as transforms
    
    # åŠ è½½æ•°æ®åˆ’åˆ†ä¿¡æ¯
    split_file = Path(data_split_dir) / "dataset_split.json"
    if not split_file.exists():
        print(f"âš ï¸ æ•°æ®åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨: {split_file}")
        print("è¯·å…ˆè¿è¡Œ step3_prepare_dataset.py")
        return None
    
    with open(split_file, 'r') as f:
        dataset_split = json.load(f)
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1]
    ])
    
    # ç¼–ç è®­ç»ƒé›† - ä½¿ç”¨çœŸæ­£çš„VA-VAE
    latents_dir = Path(data_split_dir) / "latents"
    latents_dir.mkdir(exist_ok=True)
    
    latents_list = []
    labels_list = []
    
    print(f"   ä½¿ç”¨çœŸæ­£VA-VAEå¤„ç†è®­ç»ƒé›†å›¾åƒ...")
    print(f"   VA-VAEè§„æ ¼: 32é€šé“, 16x16åˆ†è¾¨ç‡, æ— SDç¼©æ”¾")
    train_data = dataset_split['train']
    
    for user_id, image_paths in train_data.items():
        user_label = int(user_id.split('_')[1]) - 1  # ID_1 -> 0
        
        for img_path in image_paths[:10]:  # é™åˆ¶æ¯ç”¨æˆ·10å¼ å›¾åƒä»¥èŠ‚çœæ—¶é—´
            try:
                # åŠ è½½å›¾åƒ
                full_img_path = Path(original_data_dir) / user_id / Path(img_path).name
                if not full_img_path.exists():
                    continue
                    
                image = Image.open(full_img_path).convert('RGB')
                img_tensor = transform(image).unsqueeze(0).float().to(device_vae)  # ä¿®å¤: ä½¿ç”¨floatåŒ¹é…VA-VAE
                
                # ä½¿ç”¨VA-VAEç¼–ç 
                with torch.no_grad():
                    if vae_model is not None:
                        # çœŸæ­£çš„VA-VAEç¼–ç 
                        posterior = vae_model.encode(img_tensor)
                        latent = posterior.sample()  # VA-VAEè¿”å›åˆ†å¸ƒï¼Œéœ€è¦é‡‡æ ·
                        
                        # éªŒè¯VA-VAEè¾“å‡ºæ ¼å¼ (åº”è¯¥æ˜¯32é€šé“ï¼Œ16x16)
                        expected_shape = (1, 32, 16, 16)
                        if latent.shape != expected_shape:
                            print(f"   âš ï¸ VA-VAEè¾“å‡ºæ ¼å¼ä¸ç¬¦: {latent.shape}, æœŸæœ›: {expected_shape}")
                            return None
                    else:
                        print("   âŒ VA-VAEæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡ŒçœŸæ­£çš„ç¼–ç ")
                        return None
                    
                    latents_list.append(latent.cpu())
                    labels_list.append(user_label)
                    
            except Exception as e:
                print(f"   âš ï¸ ç¼–ç å¤±è´¥ {img_path}: {e}")
                continue
    
    print(f"   âœ… ç¼–ç å®Œæˆ: {len(latents_list)} ä¸ªlatentå‘é‡")
    
    # ä¿å­˜latents
    if latents_list:
        latents_tensor = torch.cat(latents_list, dim=0)
        labels_tensor = torch.tensor(labels_list)
        
        torch.save({
            'latents': latents_tensor,
            'labels': labels_tensor
        }, latents_dir / "train_latents.pt")
        
        print(f"   ğŸ’¾ Latentså·²ä¿å­˜: {latents_dir / 'train_latents.pt'}")
    
    return latents_dir if latents_list else None

def create_lora_dataloader(vae_model, config, device_vae):
    """åˆ›å»ºLoRAè®­ç»ƒæ•°æ®åŠ è½½å™¨"""
    print(f"ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨:")
    
    batch_size = config.get('batch_size', 1)
    
    # ä¼˜å…ˆæ£€æŸ¥step6_encode_official.pyç¼–ç çš„æœ€æ–°æ•°æ®
    official_train_dir = Path("/kaggle/working/latents_official/vavae_config_for_dit/microdoppler_train_256")
    official_train_file = official_train_dir / "latents_rank00_shard000.safetensors"
    
    if official_train_file.exists():
        print(f"   âœ… æ‰¾åˆ°step6ç¼–ç çš„latents: {official_train_file}")
        try:
            # ä½¿ç”¨safetensorsåŠ è½½
            latents_data = safetensors.load_file(str(official_train_file))
            latents = latents_data['latents']  
            labels = latents_data['labels']
            
            print(f"   ğŸ“Š æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(latents)} ä¸ªæ ·æœ¬ (æ¥è‡ªstep6ç¼–ç )")
            
            # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
            dataset = LatentDataset(latents, labels)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)
            return dataloader
            
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½step6ç¼–ç latentså¤±è´¥: {e}")
    
    # æ£€æŸ¥æ—§çš„æ•°æ®åˆ’åˆ†è·¯å¾„
    data_split_dir = config.get('data_split_dir')
    original_data_dir = config.get('original_data_dir') 
    print(f"   æ•°æ®åˆ’åˆ†ç›®å½•: {data_split_dir}")
    print(f"   åŸå§‹æ•°æ®ç›®å½•: {original_data_dir}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    latents_dir = Path(data_split_dir) / "latents"
    latents_file = latents_dir / "train_latents.pt"
    
    if latents_file.exists():
        print(f"   âœ… æ‰¾åˆ°é¢„ç¼–ç latents: {latents_file}")
        try:
            # åŠ è½½é¢„ç¼–ç çš„latents
            latents_data = torch.load(latents_file, map_location='cpu')
            latents = latents_data['latents']  
            labels = latents_data['labels']
            
            print(f"   ğŸ“Š æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(latents)} ä¸ªæ ·æœ¬")
            
            # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
            dataset = LatentDataset(latents, labels)
            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)
            return dataloader
            
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½é¢„ç¼–ç latentså¤±è´¥: {e}")
            print(f"   ğŸ¨ åˆ‡æ¢åˆ°å®æ—¶ç¼–ç æ¨¡å¼")
    
    # å¦‚æœæ²¡æœ‰é¢„ç¼–ç latentsï¼Œè¿›è¡Œå®æ—¶ç¼–ç 
    print(f"   ğŸ¨ æ²¡æœ‰æ‰¾åˆ°é¢„ç¼–ç latentsï¼Œå¼€å§‹å®æ—¶ç¼–ç ...")
    latents_dir = encode_images_to_latents(vae_model, data_split_dir, original_data_dir, device_vae)
    
    if latents_dir:
        # é€’å½’è°ƒç”¨åŠ è½½ç¼–ç åçš„æ•°æ®
        return create_lora_dataloader(vae_model, config, device_vae)
    else:
        print(f"   âš ï¸ ç¼–ç å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return None

def train_lora_model(model, dataloader, config, device):
    """LoRAå¾®è°ƒè®­ç»ƒ - æé™æ˜¾å­˜ä¼˜åŒ–ç‰ˆ"""
    print(f"\nğŸš€ å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒï¼ˆæé™æ˜¾å­˜ä¼˜åŒ–ï¼‰")
    
    # è®­ç»ƒé…ç½®
    num_epochs = config.get('num_epochs', 10)
    learning_rate = config.get('learning_rate', 1e-4)
    save_interval = config.get('save_interval', 2)
    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 8)  # æ¢¯åº¦ç´¯ç§¯
    
    print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    print(f"   æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {config.get('batch_size', 1) * gradient_accumulation_steps}")
    
    # ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–LoRAå‚æ•°ï¼‰
    lora_params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.AdamW(lora_params, lr=learning_rate, weight_decay=0.01)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ¿€æ´»æ˜¾å­˜
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        print("   âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    
    # CPUå¸è½½ä¼˜åŒ–å™¨çŠ¶æ€
    from torch.optim import AdamW
    if hasattr(optimizer, 'zero_redundancy_optimizer'):
        print("   âœ… å¯ç”¨ZeROä¼˜åŒ–å™¨")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    
    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡ï¼ˆä½¿ç”¨FP16ï¼‰
    model = model.half().to(device)
    print(f"   æ¨¡å‹ç²¾åº¦: FP16")
    print(f"   è®­ç»ƒè®¾å¤‡: {device}")
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(dataloader or [], desc=f"Epoch {epoch+1}/{num_epochs}")
        
        for batch_idx, batch in enumerate(progress_bar):
            if batch is None:  # æ¨¡æ‹Ÿæ•°æ®
                batch = {
                    'latents': torch.randn(4, 32, 16, 16, dtype=torch.float16).to(device),
                    'timesteps': torch.randint(0, 1000, (4,)).to(device),
                    'labels': torch.randint(0, 1001, (4,)).to(device)
                }
                if batch_idx >= 10:  # é™åˆ¶æ¨¡æ‹Ÿæ‰¹æ¬¡
                    break
            
            try:
                # å‰å‘ä¼ æ’­
                with torch.autocast(device_type='cuda', dtype=torch.float16):
                    noise_pred = model(
                        batch['latents'], 
                        batch['timesteps'], 
                        batch['labels']
                    )
                    
                    # è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨éšæœºç›®æ ‡è¿›è¡Œæ¼”ç¤ºï¼‰
                    target = torch.randn_like(noise_pred)
                    loss = criterion(noise_pred, target)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)
                
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Avg_Loss': f'{epoch_loss/num_batches:.4f}'
                })
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"\nâš ï¸ GPUæ˜¾å­˜ä¸è¶³: {e}")
                    torch.cuda.empty_cache()
                    break
                else:
                    raise e
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        avg_loss = epoch_loss / max(num_batches, 1)
        print(f"   Epoch {epoch+1}: å¹³å‡æŸå¤± = {avg_loss:.4f}, å­¦ä¹ ç‡ = {scheduler.get_last_lr()[0]:.6f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % save_interval == 0:
            save_lora_checkpoint(model, optimizer, epoch, avg_loss, 
                               f"/kaggle/working/lora_dit_xl_epoch_{epoch+1}.pt")
    
    print("âœ… LoRAå¾®è°ƒè®­ç»ƒå®Œæˆ")
    return model

def train_lora_model_parallel(model, vae_model, dataloader, config):
    """æ¨¡å‹å¹¶è¡Œè®­ç»ƒ - DiT XLå’ŒVA-VAEåˆ†å¸ƒåœ¨ä¸åŒGPU"""
    print("ğŸš€ å¼€å§‹æ¨¡å‹å¹¶è¡ŒLoRAå¾®è°ƒè®­ç»ƒ")
    
    # è®¾å¤‡åˆ†é…
    device_dit = torch.device('cuda:0')  # DiT XLæ”¾åœ¨GPU0
    device_vae = torch.device('cuda:1')  # VA-VAEæ”¾åœ¨GPU1
    
    # è®­ç»ƒå‚æ•°
    num_epochs = config.get('num_epochs', 20)
    learning_rate = config.get('learning_rate', 1e-4)
    save_interval = config.get('save_interval', 2)
    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 4)
    mixed_precision = config.get('mixed_precision', True)  # æ·»åŠ mixed_precisionå®šä¹‰
    
    print(f"   DiT XLè®¾å¤‡: {device_dit}")
    print(f"   VA-VAEè®¾å¤‡: {device_vae}")
    print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    
    # ä¼˜åŒ–å™¨ - é™ä½å­¦ä¹ ç‡é¿å…NaN
    lora_params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.AdamW(lora_params, lr=learning_rate * 0.1, weight_decay=0.01)  # é™ä½10å€å­¦ä¹ ç‡
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    # FP16è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾å™¨
    scaler = torch.cuda.amp.GradScaler() if mixed_precision else None
    
    # æŸå¤±å‡½æ•°
    criterion = nn.MSELoss()
    
    # æ”¹ä¸ºFP32è®­ç»ƒ - æµ‹è¯•æ˜¾å­˜å’Œæ•°å€¼ç¨³å®šæ€§
    model = model.float().to(device_dit)  # æ”¹ä¸ºFP32
    if vae_model is not None:
        vae_model = vae_model.float().to(device_vae)  # ä¿æŒFP32
        print(f"   VA-VAEè®¾å¤‡: {device_vae} (FP32ç²¾åº¦)")
    else:
        print(f"   VA-VAE: ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    
    # æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡
    torch.cuda.empty_cache()
    dit_memory_before = torch.cuda.memory_allocated(device_dit) / (1024**3)
    vae_memory_before = torch.cuda.memory_allocated(device_vae) / (1024**3) if vae_model else 0
    
    print(f"   æ¨¡å‹ç²¾åº¦: FP32 (æµ‹è¯•æ˜¾å­˜)")
    print(f"   LoRAå‚æ•°é‡: {sum(p.numel() for p in lora_params):,}")
    print(f"   DiT XLæ˜¾å­˜: {dit_memory_before:.1f}GB")
    print(f"   VA-VAEæ˜¾å­˜: {vae_memory_before:.1f}GB")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0
        optimizer.zero_grad()
        
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{num_epochs}")
        
        # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å¦‚æœæ•°æ®åŠ è½½å™¨ä¸å¯ç”¨
        if dataloader is None:
            print("   ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œè®­ç»ƒæµ‹è¯•")
            batch_size = config.get('batch_size', 1)
            
            for step in range(4 // gradient_accumulation_steps):  # æ¨¡æ‹Ÿå‡ ä¸ªæ­¥éª¤
                # æ¨¡æ‹Ÿè¾“å…¥ï¼šéšæœºlatentå‘é‡ (batch_size, 32, 16, 16)
                fake_latents = torch.randn(batch_size, 32, 16, 16, dtype=torch.float32, device=device_dit)
                fake_noise = torch.randn_like(fake_latents)
                fake_timesteps = torch.randint(0, 1000, (batch_size,), device=device_dit).float()
                fake_y = torch.randint(0, 31, (batch_size,), device=device_dit)  # 31ä¸ªç”¨æˆ·ç±»åˆ«
                
                # å‰å‘ä¼ æ’­
                pred_noise = model(fake_latents, fake_timesteps, fake_y)
                loss = criterion(pred_noise, fake_noise) / gradient_accumulation_steps
                
                # åå‘ä¼ æ’­
                loss.backward()
                epoch_loss += loss.item() * gradient_accumulation_steps
                
                if (step + 1) % gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)
                    optimizer.step()
                    optimizer.zero_grad()
                
                # æ˜¾å­˜ç›‘æ§
                if device_dit.type == 'cuda':
                    dit_memory = torch.cuda.memory_allocated(device_dit) / (1024**3)
                    print(f"      æ­¥éª¤ {step+1}: æŸå¤±={loss.item():.4f}, DiTæ˜¾å­˜={dit_memory:.2f}GB", end="")
                    
                    if device_vae.type == 'cuda' and device_vae != device_dit:
                        vae_memory = torch.cuda.memory_allocated(device_vae) / (1024**3)
                        print(f", VAEæ˜¾å­˜={vae_memory:.2f}GB")
                    else:
                        print()
        else:
            # çœŸå®æ•°æ®è®­ç»ƒ
            progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
            
            for step, batch in enumerate(progress_bar):
                try:
                    # æ•°æ®ç§»åŠ¨åˆ°DiTè®¾å¤‡ - FP32æ ¼å¼
                    if isinstance(batch, dict):
                        latents = batch['latent'].float().to(device_dit)  # æ”¹ä¸ºFP32
                    elif isinstance(batch, (list, tuple)):
                        # TensorDatasetè¿”å›çš„æ˜¯tuple/listæ ¼å¼: (latents, labels)
                        latents = batch[0].float().to(device_dit)  # æ”¹ä¸ºFP32
                        labels = batch[1].to(device_dit) if len(batch) > 1 else None
                    else:
                        latents = batch.float().to(device_dit)  # æ”¹ä¸ºFP32
                        labels = None
                    
                    batch_size = latents.shape[0]
                    
                    # æ·»åŠ å™ªå£°å’Œæ—¶é—´æ­¥ - æ”¹è¿›å™ªå£°è°ƒåº¦
                    noise = torch.randn_like(latents, device=device_dit)
                    timesteps = torch.randint(0, 1000, (batch_size,), device=device_dit).float()
                    
                    # ä½¿ç”¨æ›´ç¨³å®šçš„å™ªå£°è°ƒåº¦
                    noise_level = (timesteps / 1000.0).view(-1, 1, 1, 1)  # å½’ä¸€åŒ–æ—¶é—´æ­¥
                    noisy_latents = latents * (1 - noise_level) + noise * noise_level
                    
                    # ä½¿ç”¨çœŸå®æ ‡ç­¾æˆ–ç”Ÿæˆéšæœºæ ‡ç­¾
                    if labels is not None:
                        y = labels.to(device_dit)  # ä½¿ç”¨æ•°æ®é›†ä¸­çš„ç”¨æˆ·æ ‡ç­¾
                    else:
                        y = torch.randint(0, 1000, (batch_size,), device=device_dit)  # éšæœºæ ‡ç­¾
                    
                    # å‰å‘ä¼ æ’­ - FP32è®­ç»ƒï¼Œæ— éœ€autocast
                    if mixed_precision:
                        with torch.amp.autocast('cuda', enabled=True):
                            pred_noise = model(noisy_latents, timesteps, y)
                            loss = criterion(pred_noise, noise)
                    else:
                        # FP32ç›´æ¥è®¡ç®—ï¼Œæ›´ç¨³å®š
                        pred_noise = model(noisy_latents, timesteps, y)
                        loss = criterion(pred_noise, noise)
                    
                    # æ£€æŸ¥NaN
                    if torch.isnan(loss):
                        print(f"\nâš ï¸ æ£€æµ‹åˆ°NaNæŸå¤±ï¼Œè·³è¿‡æ‰¹æ¬¡")
                        optimizer.zero_grad()
                        continue
                        
                    loss = loss / gradient_accumulation_steps
                    
                    # åå‘ä¼ æ’­ - ä½¿ç”¨æ¢¯åº¦ç¼©æ”¾å™¨
                    if scaler is not None:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()
                    
                    epoch_loss += loss.item() * gradient_accumulation_steps
                    
                    if (step + 1) % gradient_accumulation_steps == 0:
                        if scaler is not None:
                            scaler.unscale_(optimizer)
                            torch.nn.utils.clip_grad_norm_(lora_params, max_norm=0.5)  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
                            scaler.step(optimizer)
                            scaler.update()
                        else:
                            torch.nn.utils.clip_grad_norm_(lora_params, max_norm=0.5)
                            optimizer.step()
                        optimizer.zero_grad()
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'dit_mem': f'{torch.cuda.memory_allocated(device_dit) / (1024**3):.1f}GB'
                    })
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"\nâš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡æ‰¹æ¬¡: {e}")
                        torch.cuda.empty_cache()
                        continue
                    else:
                        raise e
        
        # Epochå®Œæˆ
        avg_loss = epoch_loss / max(len(dataloader) if dataloader else 4, 1)
        scheduler.step()
        
        print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
        print(f"   å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % save_interval == 0:
            save_lora_checkpoint(dit_model, optimizer, epoch, avg_loss, 
                               f"/kaggle/working/lora_dit_xl_epoch_{epoch+1}.pt")
        
        # æ˜¾å­˜æ¸…ç†
        torch.cuda.empty_cache()
    
    print("âœ… æ¨¡å‹å¹¶è¡ŒLoRAå¾®è°ƒè®­ç»ƒå®Œæˆ")
    return dit_model

def save_lora_checkpoint(model, optimizer, epoch, loss, save_path):
    """ä¿å­˜LoRAæ£€æŸ¥ç‚¹"""
    print(f"ğŸ’¾ ä¿å­˜LoRAæ£€æŸ¥ç‚¹: {save_path}")
    
    # åªä¿å­˜LoRAå‚æ•°
    lora_state_dict = {}
    for name, param in model.named_parameters():
        if param.requires_grad and ('lora_A' in name or 'lora_B' in name):
            lora_state_dict[name] = param.cpu()
    
    checkpoint = {
        'lora_state_dict': lora_state_dict,
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch,
        'loss': loss,
        'model_config': {
            'model_type': 'LightningDiT-XL/1',
            'lora_rank': 16,
            'lora_alpha': 32,
            'target_modules': ['qkv', 'proj', 'w12', 'w3']
        }
    }
    
    torch.save(checkpoint, save_path)
    print(f"   LoRAå‚æ•°æ•°é‡: {len(lora_state_dict)}")

def main():
    """ä¸»å‡½æ•° - æ¨¡å‹å¹¶è¡Œç‰ˆæœ¬"""
    print("ğŸ¯ LightningDiT XL LoRAå¾®è°ƒè„šæœ¬ï¼ˆæ¨¡å‹å¹¶è¡Œï¼‰")
    print("===========================================")
    
    # å¤šGPUè®¾å¤‡æ£€æµ‹å’Œåˆ†é…
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")
        return
    
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ”§ æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    if gpu_count < 2:
        print("âš ï¸ æ¨¡å‹å¹¶è¡Œéœ€è¦è‡³å°‘2ä¸ªGPUï¼Œå›é€€åˆ°å•GPUæ¨¡å¼")
        device_dit = torch.device('cuda:0')
        device_vae = torch.device('cuda:0')
    else:
        device_dit = torch.device('cuda:0')  # DiT XLæ”¾åœ¨GPU0
        device_vae = torch.device('cuda:1')  # VA-VAEæ”¾åœ¨GPU1
        print(f"   GPU0 (DiT XL): {torch.cuda.get_device_name(0)} - {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        print(f"   GPU1 (VA-VAE): {torch.cuda.get_device_name(1)} - {torch.cuda.get_device_properties(1).total_memory / 1024**3:.1f}GB")
    
    print(f"ğŸ“ DiT XLè®¾å¤‡: {device_dit}")
    print(f"ğŸ“ VA-VAEè®¾å¤‡: {device_vae}")

    # è®­ç»ƒé…ç½®
    config = {
        'learning_rate': 1e-5,  # é™ä½å­¦ä¹ ç‡é¿å…NaN
        'num_epochs': 20,
        'batch_size': 1,  # é™ä½æ‰¹æ¬¡å¤§å°
        'lora_rank': 16,
        'lora_alpha': 32,
        'save_interval': 5,
        'mixed_precision': False,  # æ”¹ä¸ºFP32è®­ç»ƒæµ‹è¯•æ˜¾å­˜
        'gradient_accumulation_steps': 4,  # æ¢¯åº¦ç´¯ç§¯è¡¥å¿å°æ‰¹æ¬¡
        'original_data_dir': '/kaggle/input/dataset/',
        'data_split_dir': '/kaggle/working/data_split/',
        'checkpoint_dir': '/kaggle/working/lora_checkpoints/',
        'device_dit': device_dit,
        'device_vae': device_vae
    }
    
    print(f"âœ… è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        if 'device' not in key:  # è·³è¿‡deviceå¯¹è±¡çš„æ‰“å°
            print(f"   {key}: {value}")

    # åŠ è½½é¢„è®­ç»ƒDiT XLå¹¶æ·»åŠ LoRAåˆ°æŒ‡å®šGPU
    print("\nğŸ“‚ åŠ è½½DiT XLæ¨¡å‹å¹¶æ·»åŠ LoRAé€‚é…å™¨...")
    dit_xl_path = "/kaggle/working/VA-VAE/models/lightningdit-xl-imagenet256-64ep.pt"
    model = load_dit_xl_with_lora(dit_xl_path, device_dit)
    if model is None:
        return

    # åŠ è½½é¢„è®­ç»ƒVA-VAEåˆ°æŒ‡å®šGPU
    print("\nğŸ“‚ åŠ è½½VA-VAEæ¨¡å‹...")
    vae_model = load_vae_model(device_vae)
    if vae_model is None:
        print("âš ï¸ VA-VAEä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡ŒLoRAè®­ç»ƒ")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    dataloader = create_lora_dataloader(vae_model, config, device_vae)

    # å¼€å§‹è®­ç»ƒï¼ˆæ¨¡å‹å¹¶è¡Œï¼‰
    print("\nğŸš€ å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ...")
    trained_model = train_lora_model_parallel(model, vae_model, dataloader, config)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆLoRAæ¨¡å‹...")
    final_save_path = "/kaggle/working/lora_dit_xl_final.pt"
    save_lora_checkpoint(trained_model, None, config['num_epochs'], 0.0, final_save_path)
    
    # æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡
    print(f"\nğŸ“Š æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡:")
    if device_dit.type == 'cuda':
        dit_memory = torch.cuda.max_memory_allocated(device_dit) / (1024**3)
        print(f"   GPU0 (DiT XL) æœ€å¤§æ˜¾å­˜: {dit_memory:.2f}GB")
    
    if device_vae.type == 'cuda' and device_vae != device_dit:
        vae_memory = torch.cuda.max_memory_allocated(device_vae) / (1024**3)
        print(f"   GPU1 (VA-VAE) æœ€å¤§æ˜¾å­˜: {vae_memory:.2f}GB")
    
    print("\n" + "="*60)
    print("ğŸ¯ DiT XL LoRAå¾®è°ƒå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
    print(f"   è®­ç»ƒæ–¹æ³•: LoRAé€‚é…å™¨ + æ¨¡å‹å¹¶è¡Œ")
    print(f"   å¯è®­ç»ƒå‚æ•°: <2%")
    print(f"   æ˜¾å­˜ä¼˜åŒ–: DiT XLä¸VA-VAEåˆ†ç¦»")
    print(f"ğŸ¯ è¾“å‡ºæ–‡ä»¶:")
    print(f"   æœ€ç»ˆLoRAæƒé‡: {final_save_path}")
    print("="*60)

if __name__ == "__main__":
    main()

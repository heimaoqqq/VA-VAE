"""
æ¡ä»¶æ‰©æ•£æ ·æœ¬ç”Ÿæˆè„šæœ¬
ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆ200å¼ æ¡ä»¶æ ·æœ¬ï¼Œç”¨äºè¯„ä¼°ç”¨æˆ·åŒºåˆ†åº¦
"""
import torch
import torch.distributed as dist
import sys
import os
import yaml
import numpy as np
from PIL import Image
from tqdm import tqdm
import argparse
from pathlib import Path

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from models.lightningdit import LightningDiT_models
from transport import create_transport, Sampler
from simplified_vavae import SimplifiedVAVAE

def load_model_and_config(checkpoint_path, config_path):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®"""
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
    if torch.cuda.device_count() > 1:
        print(f"   å°†ä½¿ç”¨å¤šGPU: {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}")
    
    # åˆ›å»ºDiTæ¨¡å‹
    latent_size = config['data']['image_size'] // config['vae']['downsample_ratio']
    model = LightningDiT_models[config['model']['model_type']](
        input_size=latent_size,
        num_classes=config['data']['num_classes'],
        class_dropout_prob=config['model']['class_dropout_prob'],
        use_qknorm=config['model']['use_qknorm'],
        use_swiglu=config['model'].get('use_swiglu', False),
        use_rope=config['model'].get('use_rope', False),
        use_rmsnorm=config['model'].get('use_rmsnorm', False),
        wo_shift=config['model'].get('wo_shift', False),
        in_channels=config['model']['in_chans'],
        use_checkpoint=config['model'].get('use_checkpoint', False),
    ).to(device)
    
    # å¤šGPUå¹¶è¡ŒåŒ–
    if torch.cuda.device_count() > 1:
        print(f"ğŸ”§ å¯ç”¨å¤šGPUå¹¶è¡ŒåŒ– ({torch.cuda.device_count()} GPUs)")
        model = torch.nn.DataParallel(model)
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # å¤„ç†EMAæƒé‡
    if 'ema' in checkpoint:
        state_dict = checkpoint['ema']
        print("ğŸ“¦ ä½¿ç”¨EMAæƒé‡")
    else:
        state_dict = checkpoint.get('model', checkpoint)
        print("ğŸ“¦ ä½¿ç”¨æ ‡å‡†æƒé‡")
    
    # æ¸…ç†é”®å
    cleaned_state_dict = {}
    for k, v in state_dict.items():
        # ç§»é™¤'module.'å’Œ'_orig_mod.'å‰ç¼€
        clean_key = k.replace('module.', '').replace('_orig_mod.', '')
        cleaned_state_dict[clean_key] = v
    
    # å¦‚æœæ¨¡å‹è¢«DataParallelåŒ…è£…ï¼Œéœ€è¦æ·»åŠ module.å‰ç¼€
    if torch.cuda.device_count() > 1 and not any(k.startswith('module.') for k in cleaned_state_dict.keys()):
        wrapped_state_dict = {f'module.{k}': v for k, v in cleaned_state_dict.items()}
        model.load_state_dict(wrapped_state_dict, strict=False)
    else:
        model.load_state_dict(cleaned_state_dict, strict=False)
    
    model.eval()
    
    # åˆ›å»ºVAE
    vae = SimplifiedVAVAE(config['vae']['model_name']).to(device)
    vae.eval()
    
    # åˆ›å»ºtransport
    transport = create_transport(
        config['transport']['path_type'],
        config['transport']['prediction'],
        config['transport']['loss_weight'],
        config['transport']['train_eps'],
        config['transport']['sample_eps'],
        use_cosine_loss=config['transport'].get('use_cosine_loss', False),
        use_lognorm=config['transport'].get('use_lognorm', False),
        partitial_train=config['transport'].get('partitial_train', None),
        partial_ratio=config['transport'].get('partial_ratio', 1.0),
        shift_lg=config['transport'].get('shift_lg', False),
    )
    
    return model, vae, transport, config, device

def generate_samples_for_user(model, vae, transport, sampler, user_id, num_samples, output_dir, cfg_scale=10.0, seed=42):
    """ä¸ºæŒ‡å®šç”¨æˆ·ç”Ÿæˆæ¡ä»¶æ ·æœ¬"""
    # åˆ›å»ºé‡‡æ ·å‡½æ•°
    sample_fn = sampler.sample_ode(
        sampling_method="dopri5",
        num_steps=300,
        atol=1e-6,
        rtol=1e-3,
        reverse=False,
        timestep_shift=0.0,
    )
    torch.manual_seed(seed + user_id)  # æ¯ä¸ªç”¨æˆ·ä½¿ç”¨ä¸åŒç§å­
    np.random.seed(seed + user_id)
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    if torch.cuda.device_count() > 1:
        device = next(model.module.parameters()).device
    else:
        device = next(model.parameters()).device
        
    user_dir = Path(output_dir) / f"user_{user_id:02d}"
    user_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ¨ ç”Ÿæˆç”¨æˆ· {user_id} çš„æ ·æœ¬...")
    
    with torch.no_grad():
        for i in tqdm(range(num_samples), desc=f"User {user_id}"):
            # å‡†å¤‡æ¡ä»¶
            y = torch.tensor([user_id], device=device)
            
            # åˆ›å»ºéšæœºå™ªå£° - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„latentç»´åº¦
            z = torch.randn(1, 32, 16, 16, device=device)  # [B, C, H, W]
            
            # CFGé‡‡æ ·
            if cfg_scale > 1.0:
                # æ„å»ºCFG batch
                z_cfg = torch.cat([z, z], 0)
                y_null = torch.tensor([31], device=device)  # null class
                y_cfg = torch.cat([y, y_null], 0)
                
                # åˆ›å»ºæ¨¡å‹åŒ…è£…å‡½æ•°
                if torch.cuda.device_count() > 1:
                    def model_fn(x, t):
                        return model.module.forward_with_cfg(x, t, y=y_cfg, cfg_scale=cfg_scale)
                else:
                    def model_fn(x, t):
                        return model.forward_with_cfg(x, t, y=y_cfg, cfg_scale=cfg_scale)
                
                samples = sample_fn(z_cfg, model_fn)
                samples = samples[-1]
                samples, _ = samples.chunk(2, dim=0)
            else:
                # åˆ›å»ºæ ‡å‡†æ¨¡å‹åŒ…è£…å‡½æ•°
                if torch.cuda.device_count() > 1:
                    def model_fn(x, t):
                        return model.module(x, t, y=y)
                else:
                    def model_fn(x, t):
                        return model(x, t, y=y)
                
                samples = sample_fn(z, model_fn)
                samples = samples[-1]
            
            # è§£ç ä¸ºå›¾åƒ
            images = vae.decode(samples)
            
            # åå¤„ç†ï¼š[0,1] -> [0,255]
            images = torch.clamp(images, 0, 1)
            images = (images * 255).round().byte()
            
            # è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä¿å­˜
            image = images[0].permute(1, 2, 0).cpu().numpy()
            if image.shape[2] == 1:  # ç°åº¦å›¾
                image = image.squeeze(2)
                pil_image = Image.fromarray(image, mode='L')
            else:  # RGBå›¾
                pil_image = Image.fromarray(image, mode='RGB')
            
            # ä¿å­˜å›¾åƒ
            filename = user_dir / f"sample_{i:03d}.png"
            pil_image.save(filename)
    
    print(f"âœ… å®Œæˆç”¨æˆ· {user_id}: {num_samples} ä¸ªæ ·æœ¬å·²ä¿å­˜åˆ° {user_dir}")

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæ¡ä»¶æ‰©æ•£æ ·æœ¬')
    parser.add_argument('--checkpoint', required=True, help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--vae_checkpoint', help='VAE checkpointè·¯å¾„')
    parser.add_argument('--output_dir', default='./generated_samples', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--samples_per_user', type=int, default=200, help='æ¯ç”¨æˆ·ç”Ÿæˆæ ·æœ¬æ•°')
    parser.add_argument('--cfg_scale', type=float, default=10.0, help='CFGå¼•å¯¼å¼ºåº¦')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--num_users', type=int, default=31, help='ç”¨æˆ·æ•°é‡')
    parser.add_argument('--gpu_ids', type=str, default='0,1', help='ä½¿ç”¨çš„GPU IDï¼Œé€—å·åˆ†éš”')
    
    args = parser.parse_args()
    
    # è®¾ç½®å¯è§GPU
    if args.gpu_ids:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_ids
        print(f"ğŸ”§ è®¾ç½®å¯è§GPU: {args.gpu_ids}")
    
    # åŠ è½½æ¨¡å‹
    print(f"Loading model from {args.checkpoint}")
    model, vae, transport, config, device = load_model_and_config(args.checkpoint, args.config)
    
    # åˆ›å»ºé‡‡æ ·å™¨
    sampler = Sampler(transport)
    
    # åˆ›å»ºé‡‡æ ·å‡½æ•°
    sample_fn = sampler.sample_ode
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆæ ·æœ¬
    for user_id in range(args.num_users):
        generate_samples_for_user(
            model, vae, transport, sampler, 
            user_id, args.samples_per_user, args.output_dir,
            cfg_scale=args.cfg_scale, seed=args.seed
        )
    
    print(f"\nğŸ‰ ç”Ÿæˆå®Œæˆ!")
    print(f"æ€»æ ·æœ¬æ•°: {args.num_users * args.samples_per_user}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")

if __name__ == "__main__":
    main()

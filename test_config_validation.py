#!/usr/bin/env python3
"""
æµ‹è¯•é…ç½®å‚æ•°éªŒè¯è„šæœ¬
éªŒè¯VA-VAE latentå°ºå¯¸ã€DiTæ¨¡å‹é…ç½®å’Œæ•°æ®åŠ è½½æ˜¯å¦åŒ¹é…
"""

import os
import sys
import yaml
import torch
import tempfile
from pathlib import Path

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')

def test_vae_latent_size():
    """éªŒè¯VA-VAEé…ç½®å‚æ•°"""
    print("ğŸ§ª æµ‹è¯•1: VA-VAEé…ç½®éªŒè¯")
    
    # æ£€æŸ¥å¾®è°ƒçš„VA-VAEæƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    vae_checkpoint = '/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt'
    if os.path.exists(vae_checkpoint):
        print(f"   âœ… æ‰¾åˆ°å¾®è°ƒVA-VAEæƒé‡: {vae_checkpoint}")
        file_size = os.path.getsize(vae_checkpoint) / (1024**2)  # MB
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
    else:
        print(f"   âš ï¸ VA-VAEæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {vae_checkpoint}")
    
    # æ ¹æ®VA-VAE f16d32é…ç½®è®¡ç®—æœŸæœ›è¾“å‡º
    print("   VA-VAEé…ç½®: vavae_f16d32")
    print("   - ä¸‹é‡‡æ ·æ¯”ä¾‹: 16å€ (256Ã—256 â†’ 16Ã—16)")  
    print("   - è¾“å‡ºé€šé“æ•°: 32")
    
    # è®¡ç®—æœŸæœ›latentå°ºå¯¸
    expected_latent_size = (1, 32, 256//16, 256//16)  # (1, 32, 16, 16)
    print(f"   æœŸæœ›latentå½¢çŠ¶: {expected_latent_size}")
    print("   âœ… VA-VAEé…ç½®å‚æ•°éªŒè¯é€šè¿‡")
    
    return expected_latent_size

def test_dit_model_compatibility():
    """æµ‹è¯•DiTæ¨¡å‹é…ç½®å…¼å®¹æ€§ - å®é™…åˆ›å»ºæ¨¡å‹éªŒè¯å‚æ•°"""
    print("\nğŸ§ª æµ‹è¯•2: DiTæ¨¡å‹é…ç½®ä¸å®é™…VA-VAE latentåŒ¹é…")
    
    try:
        from models.lightningdit import LightningDiT_models
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        config_path = 'configs/dit_s_microdoppler.yaml'
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # è®¡ç®—latent_size
        image_size = config['data']['image_size']
        downsample_ratio = config['vae']['downsample_ratio']
        latent_size = image_size // downsample_ratio
        
        print(f"   åŸå§‹å›¾åƒå°ºå¯¸: {image_size}")
        print(f"   ä¸‹é‡‡æ ·æ¯”ä¾‹: {downsample_ratio}")
        print(f"   è®¡ç®—latentå°ºå¯¸: {latent_size}Ã—{latent_size}")
        
        # åˆ›å»ºæ¨¡å‹
        model_type = config['model']['model_type']
        print(f"   æ¨¡å‹ç±»å‹: {model_type}")
        
        model = LightningDiT_models[model_type](
            input_size=latent_size,
            num_classes=config['data']['num_classes'],
            use_qknorm=config['model']['use_qknorm'],
            use_swiglu=config['model']['use_swiglu'],
            use_rope=config['model']['use_rope'],
            use_rmsnorm=config['model']['use_rmsnorm'],
            wo_shift=config['model']['wo_shift'],
            in_channels=config['model']['in_chans'],
            use_checkpoint=config['model']['use_checkpoint'],
        )
        
        print(f"   âœ… DiTæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # æµ‹è¯•å®é™…latentå°ºå¯¸åŒ¹é…
        return test_model_input_compatibility(model, latent_size, config)
        
    except Exception as e:
        print(f"   âŒ DiTæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_model_input_compatibility(model, latent_size, config):
    """æµ‹è¯•æ¨¡å‹ä¸å®é™…latentè¾“å…¥çš„å…¼å®¹æ€§"""
    print(f"   ğŸ§ª æµ‹è¯•æ¨¡å‹è¾“å…¥å…¼å®¹æ€§...")
    
    try:
        # åˆ›å»ºæ¨¡æ‹ŸVA-VAE latentè¾“å…¥
        batch_size = 1
        in_channels = config['model']['in_chans']  # 32
        
        # VA-VAEå®é™…è¾“å‡ºå°ºå¯¸ (å·²çŸ¥ä¸º16Ã—16)
        actual_latent_h, actual_latent_w = 16, 16
        
        print(f"   æ¨¡æ‹ŸVA-VAEè¾“å‡º: [{batch_size}, {in_channels}, {actual_latent_h}, {actual_latent_w}]")
        
        # æµ‹è¯•å®é™…latentå°ºå¯¸
        x = torch.randn(batch_size, in_channels, actual_latent_h, actual_latent_w)
        y = torch.randint(0, config['data']['num_classes'], (batch_size,))
        t = torch.randn(batch_size)
        
        # å°è¯•å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            try:
                output = model(x, t, y=y)
                print(f"   âœ… 16Ã—16 latentè¾“å…¥æˆåŠŸ: è¾“å‡º{output.shape}")
                return model, 16  # è¿”å›å®é™…å·¥ä½œçš„å°ºå¯¸
            except Exception as e16:
                print(f"   âŒ 16Ã—16 latentå¤±è´¥: {e16}")
                
                # å°è¯•é…ç½®è®¡ç®—çš„å°ºå¯¸
                if latent_size != 16:
                    print(f"   ğŸ§ª å°è¯•é…ç½®è®¡ç®—çš„{latent_size}Ã—{latent_size}...")
                    x_config = torch.randn(batch_size, in_channels, latent_size, latent_size)
                    try:
                        output = model(x_config, t, y=y)
                        print(f"   âœ… {latent_size}Ã—{latent_size} latentæˆåŠŸ: è¾“å‡º{output.shape}")
                        print(f"   âš ï¸ é…ç½®ä¸å®é™…VA-VAEè¾“å‡ºä¸åŒ¹é…!")
                        return model, latent_size
                    except Exception as econfig:
                        print(f"   âŒ {latent_size}Ã—{latent_size} latentä¹Ÿå¤±è´¥: {econfig}")
                
                raise e16
                
    except Exception as e:
        print(f"   âŒ è¾“å…¥å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return None, None

def test_forward_pass(model, latent_size):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\nğŸ§ª æµ‹è¯•3: æ¨¡å‹å‰å‘ä¼ æ’­")
    
    if model is None:
        print("   âš ï¸ è·³è¿‡å‰å‘æµ‹è¯•ï¼ˆæ¨¡å‹åˆ›å»ºå¤±è´¥ï¼‰")
        return False
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
        batch_size = 2
        num_classes = 31
        
        # latentè¾“å…¥: [B, C, H, W]
        x = torch.randn(batch_size, 32, latent_size, latent_size)
        y = torch.randint(0, num_classes, (batch_size,))  # ç±»åˆ«æ ‡ç­¾
        t = torch.randn(batch_size)  # æ—¶é—´æ­¥
        
        print(f"   è¾“å…¥latentå½¢çŠ¶: {x.shape}")
        print(f"   è¾“å…¥æ ‡ç­¾å½¢çŠ¶: {y.shape}")
        print(f"   æ—¶é—´æ­¥å½¢çŠ¶: {t.shape}")
        
        # å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            output = model(x, t, y=y)
        
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶åº”è¯¥ä¸è¾“å…¥latentå½¢çŠ¶åŒ¹é…
        if output.shape == x.shape:
            print(f"   âœ… è¾“å‡ºå½¢çŠ¶åŒ¹é…è¾“å…¥")
            return True
        else:
            print(f"   âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: æœŸæœ›{x.shape}, å®é™…{output.shape}")
            return False
            
    except Exception as e:
        print(f"   âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_dataset_loading():
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•4: æ•°æ®é›†åŠ è½½å…¼å®¹æ€§")
    
    try:
        from microdoppler_latent_dataset_simple import MicroDopplerLatentDataset
        
        # æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦å­˜åœ¨
        data_path = './latents_safetensors/train'
        if not os.path.exists(data_path):
            print(f"   âš ï¸ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
            print(f"   éœ€è¦å…ˆè¿è¡Œç‰¹å¾æå–: extract_microdoppler_features.py")
            return False
        
        # å°è¯•åŠ è½½æ•°æ®é›†
        dataset = MicroDopplerLatentDataset(
            data_dir=data_path,
            latent_norm=True,
            latent_multiplier=1.0
        )
        
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # æµ‹è¯•å•ä¸ªæ ·æœ¬
        if len(dataset) > 0:
            feature, label = dataset[0]
            print(f"   æ ·æœ¬ç‰¹å¾å½¢çŠ¶: {feature.shape}")
            print(f"   æ ·æœ¬æ ‡ç­¾: {label} (ç±»å‹: {type(label)})")
            
            # éªŒè¯ç‰¹å¾å½¢çŠ¶
            expected_shape = (32, 16, 16)  # [C, H, W]
            if feature.shape == expected_shape:
                print(f"   âœ… ç‰¹å¾å½¢çŠ¶æ­£ç¡®")
            else:
                print(f"   âŒ ç‰¹å¾å½¢çŠ¶é”™è¯¯: æœŸæœ›{expected_shape}, å®é™…{feature.shape}")
                return False
            
            # éªŒè¯æ ‡ç­¾èŒƒå›´
            if isinstance(label, torch.Tensor):
                label_val = label.item()
            else:
                label_val = label
                
            if 0 <= label_val <= 30:  # ç”¨æˆ·ID: 0-30
                print(f"   âœ… æ ‡ç­¾èŒƒå›´æ­£ç¡®")
                return True
            else:
                print(f"   âŒ æ ‡ç­¾è¶…å‡ºèŒƒå›´: {label_val} (åº”ä¸º0-30)")
                return False
        else:
            print(f"   âŒ æ•°æ®é›†ä¸ºç©º")
            return False
            
    except Exception as e:
        print(f"   âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_config_consistency():
    """æµ‹è¯•é…ç½®æ–‡ä»¶ä¸€è‡´æ€§"""
    print("\nğŸ§ª æµ‹è¯•5: é…ç½®ä¸€è‡´æ€§æ£€æŸ¥")
    
    try:
        # åŠ è½½é…ç½®æ–‡ä»¶
        config_path = 'configs/dit_s_microdoppler.yaml'
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        print("   æ£€æŸ¥å…³é”®é…ç½®å‚æ•°:")
        
        # æ£€æŸ¥VA-VAEé…ç½®
        vae_config = config['vae']
        print(f"   - VAEæ¨¡å‹: {vae_config['model_name']}")
        print(f"   - ä¸‹é‡‡æ ·æ¯”ä¾‹: {vae_config['downsample_ratio']}")
        
        # æ£€æŸ¥æ•°æ®é…ç½®
        data_config = config['data']
        print(f"   - å›¾åƒå°ºå¯¸: {data_config['image_size']}")
        print(f"   - ç±»åˆ«æ•°: {data_config['num_classes']}")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®
        model_config = config['model']
        print(f"   - æ¨¡å‹ç±»å‹: {model_config['model_type']}")
        print(f"   - è¾“å…¥é€šé“: {model_config['in_chans']}")
        
        # è®¡ç®—ä¸€è‡´æ€§
        expected_latent_size = data_config['image_size'] // vae_config['downsample_ratio']
        print(f"   - è®¡ç®—latentå°ºå¯¸: {expected_latent_size}Ã—{expected_latent_size}")
        
        # éªŒè¯patch_sizeåŒ¹é…
        model_type = model_config['model_type']
        if '/1' in model_type:
            patch_size = 1
        elif '/2' in model_type:
            patch_size = 2
        else:
            patch_size = 'unknown'
            
        print(f"   - æ¨¡å‹patch_size: {patch_size}")
        
        # ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆä¿®æ­£é€»è¾‘ï¼‰
        # input_sizeåº”è¯¥ç­‰äºå®é™…latentå°ºå¯¸ï¼Œä¸patch_sizeæ— å…³
        if expected_latent_size == 16:  # VA-VAEè¾“å‡º16Ã—16
            print("   âœ… é…ç½®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
            print(f"   è¯´æ˜: input_size={expected_latent_size}åŒ¹é…VA-VAEè¾“å‡º")
            print(f"   patch_size={patch_size}å†³å®špatchesæ•°é‡: {expected_latent_size//patch_size}Ã—{expected_latent_size//patch_size}={expected_latent_size//patch_size*expected_latent_size//patch_size}ä¸ªpatches")
            return True
        else:
            print(f"   âŒ é…ç½®ä¸ä¸€è‡´: VA-VAEè¾“å‡º16Ã—16ä½†è®¡ç®—å¾—åˆ°latent_size={expected_latent_size}")
            print(f"   è¯·æ£€æŸ¥downsample_ratioè®¾ç½®")
            return False
            
    except Exception as e:
        print(f"   âŒ é…ç½®æ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ é…ç½®å‚æ•°éªŒè¯æµ‹è¯•")
    print("é€‚ç”¨äºKaggleç¯å¢ƒï¼Œæ— éœ€å®é™…åŠ è½½æ¨¡å‹")
    print("="*60)
    
    results = []
    
    # æµ‹è¯•1: VAEé…ç½®
    expected_latent_size = test_vae_latent_size()
    results.append(("VAEé…ç½®", True))
    
    # æµ‹è¯•2: DiTæ¨¡å‹é…ç½®å‚æ•°éªŒè¯  
    try:
        config_path = 'configs/dit_s_microdoppler.yaml'
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        image_size = config['data']['image_size']
        downsample_ratio = config['vae']['downsample_ratio'] 
        latent_size = image_size // downsample_ratio
        model_type = config['model']['model_type']
        
        print(f"\nğŸ§ª æµ‹è¯•2: DiTæ¨¡å‹é…ç½®å‚æ•°")
        print(f"   æ¨¡å‹ç±»å‹: {model_type}")
        print(f"   è®¡ç®—latent_size: {latent_size}")
        print(f"   ç±»åˆ«æ•°: {config['data']['num_classes']}")
        print(f"   è¾“å…¥é€šé“: {config['model']['in_chans']}")
        print(f"   âœ… DiTé…ç½®å‚æ•°éªŒè¯é€šè¿‡")
        results.append(("DiTé…ç½®", True))
        
    except Exception as e:
        print(f"\nğŸ§ª æµ‹è¯•2: DiTæ¨¡å‹é…ç½®å‚æ•°")
        print(f"   âŒ é…ç½®è¯»å–å¤±è´¥: {e}")
        results.append(("DiTé…ç½®", False))
    
    # æµ‹è¯•3: æ•°æ®è·¯å¾„æ£€æŸ¥
    data_paths = ['./latents_safetensors/train', './latents_safetensors/val']
    print(f"\nğŸ§ª æµ‹è¯•3: æ•°æ®è·¯å¾„æ£€æŸ¥")
    data_exists = True
    for path in data_paths:
        if os.path.exists(path):
            print(f"   âœ… æ‰¾åˆ°: {path}")
        else:
            print(f"   âš ï¸ ç¼ºå¤±: {path}")
            data_exists = False
    results.append(("æ•°æ®è·¯å¾„", data_exists))
    
    # æµ‹è¯•4: é…ç½®ä¸€è‡´æ€§
    consistency_ok = test_config_consistency()
    results.append(("é…ç½®ä¸€è‡´æ€§", consistency_ok))
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print("-"*60)
    
    all_passed = True
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"   {test_name:12} | {status}")
        if not passed:
            all_passed = False
    
    print("-"*60) 
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰é…ç½®æ£€æŸ¥é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒ")
        print("\næ¨èè¿è¡Œé¡ºåº:")
        print("1. python extract_microdoppler_features.py (å¦‚æœæ•°æ®è·¯å¾„ç¼ºå¤±)")
        print("2. torchrun --nproc_per_node=2 --log_dir=/kaggle/working/logs \\")
        print("     train_dit_s_official.py --config configs/dit_s_microdoppler.yaml")
    else:
        print("âš ï¸  å­˜åœ¨é…ç½®é—®é¢˜ï¼Œè¯·ä¿®å¤åé‡è¯•")
    
    print("="*60)

if __name__ == '__main__':
    main()

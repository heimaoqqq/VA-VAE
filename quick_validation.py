#!/usr/bin/env python3
"""
ğŸš€ ConditionalDiTå¿«é€ŸéªŒè¯è„šæœ¬
éªŒè¯ç®€åŒ–ç‰ˆæœ¬æ˜¯å¦å¯è¡Œï¼Œæ— éœ€å®Œæ•´epochè®­ç»ƒ
æ€»é¢„è®¡æ—¶é—´ï¼š5-10åˆ†é’Ÿ
"""
import os
import sys
import torch
import torch._dynamo
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import time

# æ¸…é™¤dynamoç¼“å­˜
torch._dynamo.reset()
torch._dynamo.config.suppress_errors = True

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def quick_smoke_test(trainer, train_loader, device):
    """çƒŸé›¾æµ‹è¯•ï¼šéªŒè¯åŸºæœ¬å‰å‘/åå‘ä¼ æ’­"""
    print("ğŸ”¥ å¼€å§‹çƒŸé›¾æµ‹è¯•...")
    trainer.model.train()
    
    try:
        for batch_idx, batch in enumerate(train_loader):
            if batch_idx >= 5: break
            
            # ä½¿ç”¨trainerçš„compute_lossæ–¹æ³•
            loss = trainer.compute_loss(batch)
            
            # ğŸ” æ˜¾ç¤ºæŸå¤±ç»„ä»¶åˆ†è§£
            if hasattr(trainer, 'log_losses'):
                logs = trainer.log_losses
                print(f"  Batch {batch_idx}: Total = {loss:.4f}")
                print(f"    - Diffusion: {logs['diffusion_loss']:.4f}")  
                print(f"    - Contrastive: {logs['contrastive_loss']:.4f} (weight: {logs['contrastive_weight']:.3f})")
                print(f"    - Inter-user: {logs['inter_user_loss']:.4f}")
                print(f"    - Regularization: {logs['user_regularization']:.4f}")
            else:
                print(f"  Batch {batch_idx}: Loss = {loss:.4f}")
            
            # åå‘ä¼ æ’­
            trainer.optimizer.zero_grad()
            loss.backward()
            
        print("âœ… çƒŸé›¾æµ‹è¯•é€šè¿‡ï¼šåŸºæœ¬è®­ç»ƒæµç¨‹å·¥ä½œæ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âŒ çƒŸé›¾æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def loss_trend_test(trainer, train_loader, device, num_steps=50):
    """æŸå¤±è¶‹åŠ¿æµ‹è¯•ï¼šè§‚å¯ŸæŸå¤±æ˜¯å¦æœ‰æ”¹å–„è¶‹åŠ¿"""
    print(f"ğŸ“Š å¼€å§‹æŸå¤±è¶‹åŠ¿æµ‹è¯• ({num_steps}æ­¥)...")
    trainer.model.train()
    
    losses = []
    start_time = time.time()
    
    try:
        for batch_idx, batch in enumerate(train_loader):
            if batch_idx >= num_steps: break
            
            # ä½¿ç”¨trainerçš„compute_lossæ–¹æ³•
            loss = trainer.compute_loss(batch)
            
            # ä¼˜åŒ–æ­¥éª¤
            trainer.optimizer.zero_grad()
            loss.backward()
            trainer.optimizer.step()  # ä½¿ç”¨çœŸå®çš„ä¼˜åŒ–å™¨æ­¥éª¤
            
            losses.append(loss.item())
            
            if batch_idx % 10 == 0:
                recent_avg = np.mean(losses[-5:])
                print(f"  Step {batch_idx}: Loss = {loss:.4f}, Recent avg = {recent_avg:.4f}")
        
        # åˆ†æè¶‹åŠ¿
        initial_loss = np.mean(losses[:5])
        final_loss = np.mean(losses[-5:])
        improvement = (initial_loss - final_loss) / initial_loss * 100
        
        elapsed = time.time() - start_time
        print(f"ğŸ“ˆ æŸå¤±è¶‹åŠ¿åˆ†æ:")
        print(f"  åˆå§‹æŸå¤±: {initial_loss:.4f}")
        print(f"  æœ€ç»ˆæŸå¤±: {final_loss:.4f}")
        print(f"  æ”¹å–„ç™¾åˆ†æ¯”: {improvement:.1f}%")
        print(f"  ç”¨æ—¶: {elapsed:.1f}ç§’")
        
        # åˆ¤æ–­æ ‡å‡†
        if improvement > 5:
            print("âœ… æŸå¤±è¶‹åŠ¿æµ‹è¯•é€šè¿‡ï¼šæ¨¡å‹æ˜¾ç¤ºå­¦ä¹ èƒ½åŠ›")
            return True
        else:
            print("âš ï¸ æŸå¤±è¶‹åŠ¿æµ‹è¯•è­¦å‘Šï¼šæ”¹å–„å¹…åº¦è¾ƒå°")
            return False
            
    except Exception as e:
        print(f"âŒ æŸå¤±è¶‹åŠ¿æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def gradient_health_check(trainer, train_loader, device):
    """æ¢¯åº¦å¥åº·æ£€æŸ¥"""
    print("ğŸ”¬ å¼€å§‹æ¢¯åº¦å¥åº·æ£€æŸ¥...")
    trainer.model.train()
    
    try:
        batch = next(iter(train_loader))
        
        # ä½¿ç”¨trainerçš„compute_lossæ–¹æ³•
        loss = trainer.compute_loss(batch)
        trainer.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦ç»Ÿè®¡
        grad_stats = {}
        total_params = 0
        params_with_grad = 0
        grad_norms = []
        
        for name, param in trainer.model.named_parameters():
            total_params += 1
            
            if param.grad is not None:
                params_with_grad += 1
                grad_norm = param.grad.norm().item()
                grad_norms.append(grad_norm)
                grad_stats[name] = grad_norm
            else:
                grad_stats[name] = 0.0
        
        # åˆ†æç»“æœ
        print(f"ğŸ“Š æ¢¯åº¦ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total_params}")
        print(f"  æœ‰æ¢¯åº¦å‚æ•°: {params_with_grad}")
        print(f"  æ¢¯åº¦è¦†ç›–ç‡: {params_with_grad/total_params*100:.1f}%")
        
        if grad_norms:
            print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(grad_norms):.6f}")
            print(f"  æ¢¯åº¦èŒƒæ•°èŒƒå›´: [{np.min(grad_norms):.6f}, {np.max(grad_norms):.6f}]")
        
        # æ˜¾ç¤ºå…³é”®å±‚çš„æ¢¯åº¦
        key_layers = ['condition_encoder', 'dit.y_embedder', 'dit.blocks.0']
        print(f"ğŸ” å…³é”®å±‚æ¢¯åº¦:")
        for name, grad_norm in grad_stats.items():
            if any(key in name for key in key_layers):
                status = "âœ…" if grad_norm > 1e-8 else "âŒ"
                print(f"  {status} {name}: {grad_norm:.6f}")
        
        # åˆ¤æ–­æ¢¯åº¦å¥åº·
        healthy_grads = sum(1 for g in grad_norms if 1e-8 < g < 10)
        health_ratio = healthy_grads / len(grad_norms) if grad_norms else 0
        
        if health_ratio > 0.8:
            print("âœ… æ¢¯åº¦å¥åº·æ£€æŸ¥é€šè¿‡ï¼šæ¢¯åº¦æµæ­£å¸¸")
            return True
        else:
            print("âš ï¸ æ¢¯åº¦å¥åº·æ£€æŸ¥è­¦å‘Šï¼šå¯èƒ½å­˜åœ¨æ¢¯åº¦é—®é¢˜")
            return False
            
    except Exception as e:
        print(f"âŒ æ¢¯åº¦å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»éªŒè¯æµç¨‹"""
    print("=" * 60)
    print("ğŸš€ ConditionalDiTå¿«é€ŸéªŒè¯å¼€å§‹")
    print("=" * 60)
    
    try:
        # å¯¼å…¥æ¨¡å—
        from step5_conditional_dit_training import ConditionalDiTTrainer
        from step4_microdoppler_adapter import MicroDopplerDataModule
        
        # è®¾å¤‡è®¾ç½®
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºæ•°æ®æ¨¡å—ï¼ˆå°æ‰¹æ¬¡ï¼‰
        print("ğŸ“¦ åˆ›å»ºæ•°æ®æ¨¡å—...")
        data_module = MicroDopplerDataModule(
            data_dir="/kaggle/input/dataset",  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
            batch_size=4,  # å°æ‰¹æ¬¡å¿«é€Ÿæµ‹è¯•
            num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        data_module.setup()
        train_loader = data_module.train_dataloader()
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ¤– åˆ›å»ºConditionalDiTæ¨¡å‹...")
        
        # åµŒå¥—é…ç½®å­—å…¸ç»“æ„
        config = {
            'model': {
                'params': {
                    'model': "LightningDiT-XL/1",
                    'num_users': 31,
                    'condition_dim': 1152,
                    'frozen_backbone': False,
                    'dropout': 0.15
                    # ä¸åŒ…å«pretrained_pathï¼Œé¿å…ä¸_setup_modelä¸­çš„ç¡¬ç¼–ç å†²çª
                }
            },
            'data': {
                'params': {
                    'data_dir': "/kaggle/input/dataset",
                    'batch_size': 16,  # ğŸš€ å¢åŠ åˆ°16ï¼Œæé«˜ç”¨æˆ·é‡å¤æ¦‚ç‡
                    'num_workers': 4   # åŒGPUç¯å¢ƒå¢åŠ workers
                }
            },
            'optimizer': {
                'params': {
                    'lr': 1e-5,              # âœ… PyTorch AdamWä½¿ç”¨'lr'ä¸æ˜¯'learning_rate'
                    'weight_decay': 1e-4,
                    'betas': [0.9, 0.999]
                }
            },
            'training': {
                'max_epochs': 1,
                'contrastive_weight': 0.1,
                'regularization_weight': 0.01,
                'warmup_steps': 100,
                'gradient_clip_val': 1.0
            }
        }
        
        trainer = ConditionalDiTTrainer(config)
        
        # éªŒè¯æµ‹è¯• - ä½¿ç”¨trainerè€Œä¸æ˜¯model
        results = {}
        
        # 1. çƒŸé›¾æµ‹è¯•
        results['smoke_test'] = quick_smoke_test(trainer, train_loader, device)
        
        # 2. æŸå¤±è¶‹åŠ¿æµ‹è¯•  
        results['loss_trend'] = loss_trend_test(trainer, train_loader, device, num_steps=30)
        
        # 3. æ¢¯åº¦å¥åº·æ£€æŸ¥
        results['gradient_health'] = gradient_health_check(trainer, train_loader, device)
        
        # æœ€ç»ˆè¯„ä¼°
        print("\n" + "=" * 60)
        print("ğŸ“‹ å¿«é€ŸéªŒè¯ç»“æœæ±‡æ€»")
        print("=" * 60)
        
        passed_tests = sum(results.values())
        total_tests = len(results)
        
        for test_name, passed in results.items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            print(f"{test_name:20s}: {status}")
        
        print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
        
        if passed_tests >= 2:
            print("ğŸ‰ éªŒè¯æˆåŠŸï¼ç®€åŒ–ç‰ˆConditionalDiTåŸºæœ¬å¯è¡Œ")
            print("ğŸ’¡ å»ºè®®ï¼šå¯ä»¥è¿›è¡ŒçŸ­æ—¶è®­ç»ƒæˆ–æ·»åŠ æ›´å¤æ‚çš„æ¡ä»¶æœºåˆ¶")
        else:
            print("âš ï¸ éªŒè¯å¤±è´¥ï¼éœ€è¦ä¿®å¤åŸºç¡€é—®é¢˜")
            print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥æ¨¡å‹å®ç°å’Œæ•°æ®ç®¡é“")
        
        return passed_tests >= 2
        
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)

"""
æ”¹è¿›çš„åˆ†ç±»å™¨è®­ç»ƒæ–¹æ¡ˆ
åŸºäºå°æ•°æ®é›†åˆ†ç±»çš„æœ€ä½³å®è·µï¼ŒåŒ…å«å¯¹æ¯”å­¦ä¹ å’Œæ­£åˆ™åŒ–æŠ€æœ¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, DistributedSampler
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import torchvision.transforms as transforms
import timm
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import defaultdict
import random
import os


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
        torch.cuda.set_device(local_rank)
        
        return rank, world_size, local_rank
    else:
        return 0, 1, 0


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def is_main_process():
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return not dist.is_initialized() or dist.get_rank() == 0


class GlobalNegativeContrastiveLoss(nn.Module):
    """å…¨å±€è´Ÿæ ·æœ¬å¯¹æ¯”æŸå¤± - æ¯ä¸ªç”¨æˆ·ä¸æ‰€æœ‰å…¶ä»–ç”¨æˆ·å¯¹æ¯”"""
    
    def __init__(self, num_classes, temperature=0.07, margin=0.5, memory_size=1000):
        super().__init__()
        self.num_classes = num_classes
        self.temperature = temperature
        self.margin = margin
        self.memory_size = memory_size
        
        # ä¸ºæ¯ä¸ªç±»åˆ«ç»´æŠ¤ç‰¹å¾memory bank
        self.register_buffer('memory_bank', torch.randn(num_classes, memory_size, 512))
        self.register_buffer('memory_ptr', torch.zeros(num_classes, dtype=torch.long))
        self.memory_bank = F.normalize(self.memory_bank, dim=2)
    
    def update_memory_bank(self, features, labels):
        """æ›´æ–°memory bank"""
        features = F.normalize(features, dim=1)
        
        for i, label in enumerate(labels):
            label = label.item()
            ptr = self.memory_ptr[label].item()
            
            # å¾ªç¯è¦†ç›–æ›´æ–°
            self.memory_bank[label, ptr] = features[i].detach()
            self.memory_ptr[label] = (ptr + 1) % self.memory_size
    
    def forward(self, features, labels):
        """
        features: [batch_size, feature_dim]
        labels: [batch_size] ç”¨æˆ·ID
        """
        batch_size = features.size(0)
        features = F.normalize(features, dim=1)
        
        # æ›´æ–°memory bank
        self.update_memory_bank(features, labels)
        
        total_loss = 0
        num_pairs = 0
        
        # å¯¹batchä¸­æ¯ä¸ªæ ·æœ¬è®¡ç®—ä¸å…¨å±€è´Ÿæ ·æœ¬çš„å¯¹æ¯”æŸå¤±
        for i, anchor_label in enumerate(labels):
            anchor_feature = features[i].unsqueeze(0)  # [1, feature_dim]
            
            # æ­£æ ·æœ¬ï¼šåŒç±»åˆ«çš„å…¶ä»–æ ·æœ¬ï¼ˆbatchå†… + memory bankï¼‰
            positive_features = []
            
            # batchå†…æ­£æ ·æœ¬
            batch_positives = features[labels == anchor_label]
            if len(batch_positives) > 1:  # é™¤äº†è‡ªå·±è¿˜æœ‰å…¶ä»–åŒç±»æ ·æœ¬
                mask = torch.arange(len(batch_positives)) != (labels == anchor_label).nonzero()[0]
                if mask.any():
                    positive_features.append(batch_positives[mask])
            
            # memory bankä¸­çš„æ­£æ ·æœ¬
            memory_positives = self.memory_bank[anchor_label]  # [memory_size, feature_dim]
            positive_features.append(memory_positives[:50])  # å–å‰50ä¸ªé¿å…è¿‡å¤š
            
            if positive_features:
                positive_features = torch.cat(positive_features, dim=0)
                pos_similarity = torch.matmul(anchor_feature, positive_features.T) / self.temperature
                pos_loss = -pos_similarity.mean()
            else:
                pos_loss = torch.tensor(0.0, device=features.device)
            
            # è´Ÿæ ·æœ¬ï¼šæ‰€æœ‰å…¶ä»–ç±»åˆ«çš„æ ·æœ¬ï¼ˆå…¨å±€ï¼‰
            negative_features = []
            for neg_label in range(self.num_classes):
                if neg_label != anchor_label:
                    # ä»memory bankä¸­é‡‡æ ·è´Ÿæ ·æœ¬
                    neg_samples = self.memory_bank[neg_label][:20]  # æ¯ä¸ªç±»åˆ«20ä¸ªæ ·æœ¬
                    negative_features.append(neg_samples)
            
            if negative_features:
                negative_features = torch.cat(negative_features, dim=0)  # [num_negatives, feature_dim]
                neg_similarity = torch.matmul(anchor_feature, negative_features.T) / self.temperature
                
                # Hard negative mining
                hard_mask = neg_similarity.squeeze() > self.margin
                if hard_mask.any():
                    neg_loss = neg_similarity.squeeze()[hard_mask].mean()
                else:
                    neg_loss = neg_similarity.mean()
            else:
                neg_loss = torch.tensor(0.0, device=features.device)
            
            # ç´¯ç§¯æŸå¤±
            total_loss += pos_loss + neg_loss
            num_pairs += 1
        
        return total_loss / num_pairs if num_pairs > 0 else torch.tensor(0.0, device=features.device)


class InterUserContrastiveLoss(nn.Module):
    """ç”¨æˆ·é—´å¯¹æ¯”æŸå¤±å‡½æ•° - ä¸“é—¨å¤„ç†ç”¨æˆ·é—´å·®å¼‚å°çš„é—®é¢˜"""
    
    def __init__(self, temperature=0.07, margin=0.5):
        super().__init__()
        self.temperature = temperature
        self.margin = margin  # ç”¨äºhard negative mining
    
    def forward(self, features, labels):
        """
        features: [batch_size, feature_dim]
        labels: [batch_size] ç”¨æˆ·ID
        """
        batch_size = features.size(0)
        features = F.normalize(features, dim=1)
        
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦
        similarity_matrix = torch.matmul(features, features.T) / self.temperature
        
        # åˆ›å»ºæ­£è´Ÿæ ·æœ¬mask
        labels = labels.contiguous().view(-1, 1)
        positive_mask = torch.eq(labels, labels.T).float().to(features.device)
        negative_mask = 1 - positive_mask
        
        # ç§»é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±å’Œè‡ªå·±ï¼‰
        eye_mask = torch.eye(batch_size).to(features.device)
        positive_mask = positive_mask - eye_mask
        negative_mask = negative_mask - eye_mask
        
        # è®¡ç®—æ­£æ ·æœ¬æŸå¤±ï¼ˆåŒç”¨æˆ·æ ·æœ¬åº”è¯¥ç›¸ä¼¼ï¼‰
        pos_sim = similarity_matrix * positive_mask
        pos_loss = -pos_sim.sum() / positive_mask.sum().clamp(min=1)
        
        # è®¡ç®—è´Ÿæ ·æœ¬æŸå¤±ï¼ˆä¸åŒç”¨æˆ·æ ·æœ¬åº”è¯¥ä¸ç›¸ä¼¼ï¼‰
        # ä½¿ç”¨hard negative mining - åªå…³æ³¨éš¾åŒºåˆ†çš„è´Ÿæ ·æœ¬
        neg_sim = similarity_matrix * negative_mask
        hard_negatives = (neg_sim > self.margin) * negative_mask
        
        if hard_negatives.sum() > 0:
            neg_loss = neg_sim[hard_negatives > 0].mean()
        else:
            neg_loss = neg_sim[negative_mask > 0].mean()
        
        # æ€»æŸå¤± = å‡å°‘æ­£æ ·æœ¬è·ç¦» + å¢åŠ è´Ÿæ ·æœ¬è·ç¦»
        total_loss = pos_loss + neg_loss
        
        return total_loss
    

class SupConLoss(nn.Module):
    """ç›‘ç£å¯¹æ¯”æŸå¤± - æ›´é€‚åˆå¤šç±»åˆ†ç±»çš„å¯¹æ¯”å­¦ä¹ """
    
    def __init__(self, temperature=0.07, contrast_mode='all'):
        super().__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
    
    def forward(self, features, labels):
        """
        features: [batch_size, feature_dim]
        labels: [batch_size]
        """
        device = features.device
        batch_size = features.shape[0]
        
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features = F.normalize(features, dim=1)
        
        # è®¡ç®—anchorå’Œæ‰€æœ‰æ ·æœ¬çš„ç›¸ä¼¼åº¦
        anchor_dot_contrast = torch.div(
            torch.matmul(features, features.T),
            self.temperature
        )
        
        # ä¸ºæ•°å€¼ç¨³å®šæ€§å‡å»æœ€å¤§å€¼
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()
        
        # ç§»é™¤å¯¹è§’çº¿
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask
        
        # è®¡ç®—logæ¦‚ç‡
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))
        
        # è®¡ç®—æ­£æ ·æœ¬çš„å¹³å‡logæ¦‚ç‡
        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)
        
        loss = -mean_log_prob_pos.mean()
        
        return loss


class ImprovedMicroDopplerDataset(Dataset):
    """æ”¹è¿›çš„å¾®å¤šæ™®å‹’æ•°æ®é›†ï¼ŒåŒ…å«å¼ºæ•°æ®å¢å¼º"""
    
    def __init__(self, data_dir, split='train', transform=None, contrastive_pairs=False):
        self.data_dir = Path(data_dir)
        self.split = split
        self.contrastive_pairs = contrastive_pairs
        self.samples = []
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬
        user_samples = defaultdict(list)
        print(f"ğŸ” æ‰«ææ•°æ®ç›®å½•: {self.data_dir}")
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        # æŸ¥æ‰¾ID_*ç›®å½•
        id_dirs = list(self.data_dir.glob("ID_*"))
        print(f"æ‰¾åˆ° {len(id_dirs)} ä¸ªIDç›®å½•")
        
        if len(id_dirs) == 0:
            print("âŒ æœªæ‰¾åˆ°ID_*ç›®å½•ï¼Œæ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„ç›®å½•:")
            for item in self.data_dir.iterdir():
                if item.is_dir():
                    print(f"  - {item.name}")
            raise ValueError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°ID_*æ ¼å¼çš„ç”¨æˆ·ç›®å½•")
        
        for user_dir in sorted(id_dirs):
            if user_dir.is_dir():
                user_id = int(user_dir.name.split('_')[1])  # ä¿æŒåŸå§‹IDç¼–å·
                total_files = 0
                for ext in ['*.png', '*.jpg', '*.jpeg']:
                    files = list(user_dir.glob(ext))
                    total_files += len(files)
                    for img_path in files:
                        user_samples[user_id].append(str(img_path))
                print(f"  ID_{user_id}: {total_files} ä¸ªæ–‡ä»¶")
        
        if not user_samples:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
        
        total_samples = sum(len(paths) for paths in user_samples.values())
        print(f"æ€»å…±æ”¶é›†åˆ° {total_samples} ä¸ªæ ·æœ¬ï¼Œæ¥è‡ª {len(user_samples)} ä¸ªç”¨æˆ·")
        
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
        for user_id, paths in user_samples.items():
            random.shuffle(paths)
            split_idx = int(len(paths) * 0.8)
            
            if split == 'train':
                selected_paths = paths[:split_idx]
            else:  # validation
                selected_paths = paths[split_idx:]
            
            for path in selected_paths:
                self.samples.append((path, user_id))
        
        print(f"{split.capitalize()} set: {len(self.samples)} samples")
        
        # å¾®å¤šæ™®å‹’å›¾åƒä¸“ç”¨å˜æ¢ï¼ˆæœ€å°å¢å¼ºï¼Œä¿æŒé¢‘è°±ç»“æ„ï¼‰
        if split == 'train':
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                # åªä½¿ç”¨æè½»å¾®çš„å™ªå£°å¢å¼ºï¼Œä¸ç ´åé¢‘è°±ç»“æ„
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                # å¯é€‰ï¼šæå°çš„é«˜æ–¯å™ªå£°ï¼ˆæ¨¡æ‹Ÿæµ‹é‡å™ªå£°ï¼‰
                transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01 if torch.rand(1) < 0.3 else x)
            ])
        else:
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        
        if transform:
            self.transform = transform
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            
            if self.contrastive_pairs and self.split == 'train':
                # ç”Ÿæˆå¯¹æ¯”æ ·æœ¬å¯¹
                image1 = self.transform(image)
                image2 = self.transform(image)  # åŒä¸€å›¾åƒçš„ä¸åŒå¢å¼º
                return (image1, image2), label
            else:
                image = self.transform(image)
                return image, label
                
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            # è¿”å›é›¶å¼ é‡
            if self.contrastive_pairs:
                return (torch.zeros(3, 224, 224), torch.zeros(3, 224, 224)), label
            else:
                return torch.zeros(3, 224, 224), label


class ImprovedClassifier(nn.Module):
    """æ”¹è¿›çš„åˆ†ç±»å™¨ï¼Œä¸“ä¸ºå¾®å¤šæ™®å‹’ä¿¡å·ä¼˜åŒ–"""
    
    def __init__(self, num_classes, backbone='resnet18', dropout_rate=0.3, freeze_layers=True):
        super().__init__()
        
        # ResNet18æ˜¯å¾®å¤šæ™®å‹’åˆ†ç±»çš„ç»å…¸é€‰æ‹©
        self.backbone = timm.create_model('resnet18', pretrained=True, num_classes=0, global_pool='avg')
        feature_dim = 512
        
        # çµæ´»çš„å±‚å†»ç»“ç­–ç•¥
        if freeze_layers == 'minimal':
            # æœ€å°å†»ç»“ï¼šåªå†»ç»“æœ€æ—©çš„å·ç§¯å±‚
            for name, param in self.backbone.named_parameters():
                if any(x in name for x in ['conv1', 'bn1']):
                    param.requires_grad = False
        elif freeze_layers == 'moderate':
            # ä¸­ç­‰å†»ç»“ï¼šå†»ç»“æ—©æœŸå±‚ï¼Œä¿ç•™é€‚åº”æ€§
            for name, param in self.backbone.named_parameters():
                if any(x in name for x in ['conv1', 'bn1', 'layer1']):
                    param.requires_grad = False
        elif freeze_layers == 'aggressive':
            # æ¿€è¿›å†»ç»“ï¼šå†»ç»“æ›´å¤šå±‚ï¼ˆå°æ•°æ®é›†ï¼‰
            for name, param in self.backbone.named_parameters():
                if any(x in name for x in ['conv1', 'bn1', 'layer1', 'layer2']):
                    param.requires_grad = False
        elif freeze_layers == 'none':
            # ä¸å†»ç»“ä»»ä½•å±‚ï¼ˆé£é™©æ›´é«˜ä½†å¯èƒ½æ•ˆæœæ›´å¥½ï¼‰
            pass
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(feature_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(256, num_classes)
        )
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
    
    def forward(self, x, return_features=False):
        features = self.backbone(x)
        
        if return_features:
            projected = self.projection_head(features)
            return features, projected
        
        logits = self.classifier(features)
        return logits


class FocalLoss(nn.Module):
    """Focal Loss - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
    
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()


class LabelSmoothingLoss(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤±"""
    
    def __init__(self, num_classes, smoothing=0.1):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
    
    def forward(self, pred, target):
        confidence = 1.0 - self.smoothing
        smooth_target = torch.full_like(pred, self.smoothing / (self.num_classes - 1))
        smooth_target.scatter_(1, target.unsqueeze(1), confidence)
        return F.kl_div(F.log_softmax(pred, dim=1), smooth_target, reduction='batchmean')


def train_with_contrastive_learning(model, train_loader, val_loader, device, args, rank=0):
    """æ”¹è¿›çš„è®­ç»ƒå‡½æ•°ï¼Œé›†æˆå¯¹æ¯”å­¦ä¹ """
    
    # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
    def is_main_process():
        return rank == 0
    
    # éªŒè¯æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if len(train_loader.dataset) == 0:
        if is_main_process():
            print("âŒ è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ")
        return model, None, 0.0, {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'contrastive_loss': [], 'classification_loss': []}
    
    if len(val_loader.dataset) == 0:
        if is_main_process():
            print("âŒ éªŒè¯æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ")
        return model, None, 0.0, {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'contrastive_loss': [], 'classification_loss': []}
    
    if is_main_process():
        print(f"âœ… æ•°æ®é›†éªŒè¯é€šè¿‡: è®­ç»ƒé›† {len(train_loader.dataset)} æ ·æœ¬, éªŒè¯é›† {len(val_loader.dataset)} æ ·æœ¬")
    
    # æŸå¤±å‡½æ•°
    if args.use_focal_loss:
        classification_criterion = FocalLoss()
    elif args.use_label_smoothing:
        classification_criterion = LabelSmoothingLoss(args.num_classes)
    else:
        classification_criterion = nn.CrossEntropyLoss()
    
    # é€‰æ‹©å¯¹æ¯”æŸå¤±ç±»å‹
    if args.contrastive_type == 'global':
        contrastive_criterion = GlobalNegativeContrastiveLoss(
            num_classes=args.num_classes,
            temperature=args.contrastive_temperature,
            margin=args.contrastive_margin,
            memory_size=200  # æ¯ç±»å­˜å‚¨200ä¸ªç‰¹å¾å‘é‡
        )
    elif args.contrastive_type == 'interuser':
        contrastive_criterion = InterUserContrastiveLoss(
            temperature=args.contrastive_temperature,
            margin=args.contrastive_margin
        )
    elif args.contrastive_type == 'supcon':
        contrastive_criterion = SupConLoss(temperature=args.contrastive_temperature)
    else:
        contrastive_criterion = SupConLoss(temperature=args.contrastive_temperature)  # é»˜è®¤ä½¿ç”¨SupCon
    
    # ä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡å’Œæ›´å¼ºçš„weight decay
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=args.lr, 
        weight_decay=args.weight_decay,
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )
    
    best_val_acc = 0
    patience_counter = 0
    
    history = {
        'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],
        'contrastive_loss': [], 'classification_loss': []
    }
    
    for epoch in range(args.epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        contrastive_losses = []
        classification_losses = []
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        if is_main_process():
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        else:
            pbar = train_loader
        
        for batch_idx, batch_data in enumerate(pbar):
            data, target = batch_data
            target = target.to(device)
            
            # è°ƒè¯•ä¿¡æ¯
            # print(f"Data type: {type(data)}, Target type: {type(target)}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¹æ¯”å­¦ä¹ çš„tupleå¯¹
            if isinstance(data, (tuple, list)) and len(data) == 2:
                data1, data2 = data
                # ç¡®ä¿data1å’Œdata2æ˜¯tensor
                if isinstance(data1, torch.Tensor) and isinstance(data2, torch.Tensor):
                    data1, data2 = data1.to(device), data2.to(device)
                    
                    # å‰å‘ä¼ æ’­
                    features1, proj1 = model(data1, return_features=True)
                    features2, proj2 = model(data2, return_features=True)
                    
                    # åˆ†ç±»æŸå¤±
                    logits1 = model.classifier(features1)
                    logits2 = model.classifier(features2)
                    
                    cls_loss1 = classification_criterion(logits1, target)
                    cls_loss2 = classification_criterion(logits2, target)
                    classification_loss = (cls_loss1 + cls_loss2) / 2
                    
                    # å¯¹æ¯”æŸå¤±
                    combined_proj = torch.cat([proj1, proj2], dim=0)
                    combined_labels = torch.cat([target, target], dim=0)
                    contrastive_loss = contrastive_criterion(combined_proj, combined_labels)
                    
                    # æ€»æŸå¤±
                    total_loss = classification_loss + args.contrastive_weight * contrastive_loss
                    
                    # ç»Ÿè®¡
                    pred = logits1.argmax(dim=1)
                else:
                    raise ValueError(f"Expected tensors in contrastive pair, got {type(data1)}, {type(data2)}")
                
            elif isinstance(data, torch.Tensor):  # å¸¸è§„å¼ é‡æ•°æ®
                data = data.to(device)
                
                if args.use_contrastive:
                    # å³ä½¿æ˜¯å¸¸è§„æ•°æ®ï¼Œä¹Ÿå¯ä»¥ç”¨å¯¹æ¯”å­¦ä¹ 
                    features, proj = model(data, return_features=True)
                    logits = model.classifier(features)
                    
                    classification_loss = classification_criterion(logits, target)
                    contrastive_loss = contrastive_criterion(proj, target)
                    total_loss = classification_loss + args.contrastive_weight * contrastive_loss
                else:
                    logits = model(data)
                    classification_loss = classification_criterion(logits, target)
                    contrastive_loss = torch.tensor(0.0)
                    total_loss = classification_loss
                
                pred = logits.argmax(dim=1)
            
            else:
                raise ValueError(f"Unexpected data type: {type(data)}, content: {data}")
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # ç»Ÿè®¡
            train_loss += total_loss.item()
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
            
            contrastive_losses.append(contrastive_loss.item())
            classification_losses.append(classification_loss.item())
            
            # åªåœ¨ä¸»è¿›ç¨‹æ›´æ–°è¿›åº¦æ¡
            if is_main_process() and isinstance(pbar, tqdm):
                pbar.set_postfix({
                    'Loss': f'{total_loss.item():.4f}',
                    'Acc': f'{100.*train_correct/train_total:.1f}%'
                })
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                
                logits = model(data)
                loss = classification_criterion(logits, target)
                
                pred = logits.argmax(dim=1)
                val_loss += loss.item()
                val_correct += pred.eq(target).sum().item()
                val_total += target.size(0)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # é˜²æ­¢é™¤é›¶é”™è¯¯
        if len(train_loader) == 0:
            print("âŒ è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ ¼å¼")
            return model, None, 0.0, {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'contrastive_loss': [], 'classification_loss': []}
        
        if len(val_loader) == 0:
            print("âŒ éªŒè¯æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ ¼å¼")
            return model, None, 0.0, {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'contrastive_loss': [], 'classification_loss': []}
        
        # ç»Ÿè®¡
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_acc = 100. * train_correct / train_total if train_total > 0 else 0.0
        val_acc = 100. * val_correct / val_total if val_total > 0 else 0.0
        
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_acc)
        history['contrastive_loss'].append(np.mean(contrastive_losses))
        history['classification_loss'].append(np.mean(classification_losses))
        
        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        if is_main_process():
            print(f"Epoch {epoch+1}/{args.epochs}")
            print(f"Train: Loss={avg_train_loss:.4f}, Acc={train_acc:.2f}%")
            print(f"Val: Loss={avg_val_loss:.4f}, Acc={val_acc:.2f}%")
            print(f"LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # æ—©åœå’Œæœ€ä½³æ¨¡å‹ä¿å­˜ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()
                patience_counter = 0
                print(f"ğŸ¯ New best validation accuracy: {best_val_acc:.2f}%")
            else:
                patience_counter += 1
            
            if patience_counter >= args.patience:
                print(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break
    
    return model, best_model_state, best_val_acc, history


def main():
    parser = argparse.ArgumentParser(description='Improved classifier training with distributed support')
    parser.add_argument('--data_dir', type=str, required=True, help='Dataset directory')
    parser.add_argument('--output_dir', type=str, default='./improved_classifier', help='Output directory')
    parser.add_argument('--num_classes', type=int, default=31, help='Number of classes')
    parser.add_argument('--epochs', type=int, default=200, help='Number of epochs')
    parser.add_argument('--batch_size', type=int, default=16, help='Batch size per GPU')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate (smaller)')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='Weight decay (stronger)')
    parser.add_argument('--dropout_rate', type=float, default=0.5, help='Dropout rate')
    parser.add_argument('--patience', type=int, default=10, help='Early stopping patience')
    
    # å¯¹æ¯”å­¦ä¹ å‚æ•°
    parser.add_argument('--use_contrastive', action='store_true', help='Use contrastive learning')
    parser.add_argument('--contrastive_weight', type=float, default=0.5, help='Contrastive loss weight')
    parser.add_argument('--contrastive_temperature', type=float, default=0.07, help='Contrastive temperature')
    parser.add_argument('--contrastive_type', type=str, default='supcon', 
                       choices=['global', 'interuser', 'supcon'],
                       help='Contrastive loss type: global(memory bank all users), interuser(hard negative mining), supcon(supervised contrastive)')
    parser.add_argument('--contrastive_margin', type=float, default=0.5, 
                       help='Margin for hard negative mining in interuser contrastive loss')
    
    # æŸå¤±å‡½æ•°é€‰æ‹©
    parser.add_argument('--use_focal_loss', action='store_true', help='Use focal loss')
    parser.add_argument('--use_label_smoothing', action='store_true', help='Use label smoothing')
    
    # æ¨¡å‹é€‰æ‹© - ResNet18ä¸“ä¸ºå¾®å¤šæ™®å‹’ä¼˜åŒ–
    parser.add_argument('--backbone', type=str, default='resnet18', help='Backbone architecture')
    parser.add_argument('--freeze_layers', type=str, default='moderate', 
                       choices=['none', 'minimal', 'moderate', 'aggressive'],
                       help='Layer freezing strategy: none(risk overfitting), minimal(conv1+bn1), moderate(+layer1), aggressive(+layer2)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    rank, world_size, local_rank = setup_distributed()
    
    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{local_rank}')
        torch.cuda.set_device(local_rank)
    else:
        device = torch.device('cpu')
    
    if is_main_process():
        print(f"Using distributed training with {world_size} GPUs")
        print(f"Current device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    if is_main_process():
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42 + rank)  # æ¯ä¸ªè¿›ç¨‹ä¸åŒçš„éšæœºç§å­
    np.random.seed(42 + rank)
    random.seed(42 + rank)
    
    # æ•°æ®é›†
    train_dataset = ImprovedMicroDopplerDataset(
        args.data_dir, 
        split='train', 
        contrastive_pairs=args.use_contrastive
    )
    val_dataset = ImprovedMicroDopplerDataset(
        args.data_dir, 
        split='val'
    )
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = DistributedSampler(train_dataset) if world_size > 1 else None
    val_sampler = DistributedSampler(val_dataset, shuffle=False) if world_size > 1 else None
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=(train_sampler is None), 
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size * 2, 
        shuffle=False,
        sampler=val_sampler,
        num_workers=4,
        pin_memory=True
    )
    
    # æ¨¡å‹
    model = ImprovedClassifier(
        num_classes=args.num_classes,
        backbone=args.backbone,
        dropout_rate=args.dropout_rate,
        freeze_layers=args.freeze_layers
    ).to(device)
    
    # åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ¨¡å‹
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank], output_device=local_rank)
    
    if is_main_process():
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Model parameters: {total_params:,}")
    
    # è®­ç»ƒ
    model, best_state, best_acc, history = train_with_contrastive_learning(
        model, train_loader, val_loader, device, args, rank
    )
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹
    if is_main_process():
        output_dir = Path(args.output_dir)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        model_path = output_dir / 'best_improved_classifier.pth'
        torch.save({
            'model_state_dict': best_state,
            'best_val_acc': best_acc,
            'num_classes': args.num_classes,
            'model_name': args.backbone,
            'args': vars(args)
        }, model_path)
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = output_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"\nâœ… Training completed!")
        print(f"Best validation accuracy: {best_acc:.2f}%")
        print(f"Model saved to: {model_path}")
    
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    cleanup_distributed()


if __name__ == "__main__":
    main()

"""
æ”¹è¿›çš„åˆ†ç±»å™¨è®­ç»ƒæ–¹æ¡ˆ
åŸºäºå°æ•°æ®é›†åˆ†ç±»çš„æœ€ä½³å®è·µï¼ŒåŒ…å«å¯¹æ¯”å­¦ä¹ å’Œæ­£åˆ™åŒ–æŠ€æœ¯
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, DistributedSampler
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import torchvision.transforms as transforms
import timm
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import defaultdict
import random
import os


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
        torch.cuda.set_device(local_rank)
        
        return rank, world_size, local_rank
    else:
        return 0, 1, 0


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def is_main_process():
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return not dist.is_initialized() or dist.get_rank() == 0


class GlobalNegativeContrastiveLoss(nn.Module):
    """å…¨å±€è´Ÿæ ·æœ¬å¯¹æ¯”æŸå¤±å‡½æ•°"""
    
    def __init__(self, num_classes, temperature=0.07, margin=0.5, memory_size=200):
        super().__init__()
        self.num_classes = num_classes
        self.temperature = temperature
        self.margin = margin
        self.memory_size = memory_size
        
        # ä¸ºæ¯ä¸ªç±»åˆ«ç»´æŠ¤ç‰¹å¾memory bank
        self.register_buffer('memory_bank', torch.randn(num_classes, memory_size, 512))
        self.register_buffer('memory_ptr', torch.zeros(num_classes, dtype=torch.long))
        self.memory_bank = F.normalize(self.memory_bank, dim=2)
    
    @torch.no_grad()
    def update_memory_bank(self, features, labels):
        """æ›´æ–°memory bank"""
        features_normalized = F.normalize(features, dim=1)
        
        for i, label in enumerate(labels):
            label = label.item()
            ptr = self.memory_ptr[label].item()
            
            # ç›´æ¥æ›´æ–°ï¼Œå› ä¸ºå·²ç»åœ¨no_gradä¸Šä¸‹æ–‡ä¸­
            self.memory_bank[label, ptr] = features_normalized[i].detach()
            self.memory_ptr[label] = (ptr + 1) % self.memory_size
    
    def forward(self, features, labels):
        """
        features: [batch_size, feature_dim]
        labels: [batch_size] ç”¨æˆ·ID
        """
        batch_size = features.size(0)
        features = F.normalize(features, dim=1)
        
        # æ›´æ–°memory bank - ä½¿ç”¨detached featuresé¿å…æ¢¯åº¦é—®é¢˜
        with torch.no_grad():
            self.update_memory_bank(features.detach(), labels)
        
        total_loss = 0
        num_pairs = 0
        
        # å¯¹batchä¸­æ¯ä¸ªæ ·æœ¬è®¡ç®—ä¸å…¨å±€è´Ÿæ ·æœ¬çš„å¯¹æ¯”æŸå¤±
        for i, anchor_label in enumerate(labels):
            anchor_feature = features[i].unsqueeze(0)  # [1, feature_dim]
            
            # æ­£æ ·æœ¬ï¼šåŒç±»åˆ«çš„å…¶ä»–æ ·æœ¬ï¼ˆbatchå†… + memory bankï¼‰
            positive_features = []
            
            # batchå†…æ­£æ ·æœ¬
            batch_positives = features[labels == anchor_label]
            if len(batch_positives) > 1:  # é™¤äº†è‡ªå·±è¿˜æœ‰å…¶ä»–åŒç±»æ ·æœ¬
                mask = torch.arange(len(batch_positives)) != (labels == anchor_label).nonzero()[0]
                if mask.any():
                    positive_features.append(batch_positives[mask])
            
            # memory bankä¸­çš„æ­£æ ·æœ¬
            memory_positives = self.memory_bank[anchor_label]  # [memory_size, feature_dim]
            positive_features.append(memory_positives[:50])  # å–å‰50ä¸ªé¿å…è¿‡å¤š
            
            if positive_features:
                positive_features = torch.cat(positive_features, dim=0)
                pos_similarity = torch.matmul(anchor_feature, positive_features.T) / self.temperature
                pos_loss = -pos_similarity.mean()
            else:
                pos_loss = torch.tensor(0.0, device=features.device)
            
            # è´Ÿæ ·æœ¬ï¼šæ‰€æœ‰å…¶ä»–ç±»åˆ«çš„æ ·æœ¬ï¼ˆå…¨å±€ï¼‰
            negative_features = []
            for neg_label in range(self.num_classes):
                if neg_label != anchor_label:
                    # ä»memory bankä¸­é‡‡æ ·è´Ÿæ ·æœ¬
                    neg_samples = self.memory_bank[neg_label][:20]  # æ¯ä¸ªç±»åˆ«20ä¸ªæ ·æœ¬
                    negative_features.append(neg_samples)
            
            if negative_features:
                negative_features = torch.cat(negative_features, dim=0)  # [num_negatives, feature_dim]
                neg_similarity = torch.matmul(anchor_feature, negative_features.T) / self.temperature
                
                # Hard negative mining
                hard_mask = neg_similarity.squeeze() > self.margin
                if hard_mask.any():
                    neg_loss = neg_similarity.squeeze()[hard_mask].mean()
                else:
                    neg_loss = neg_similarity.mean()
            else:
                neg_loss = torch.tensor(0.0, device=features.device)
            
            # ç´¯ç§¯æŸå¤±
            total_loss += pos_loss + neg_loss
            num_pairs += 1
        
        return total_loss / num_pairs if num_pairs > 0 else torch.tensor(0.0, device=features.device)


class InterUserContrastiveLoss(nn.Module):
    """ç”¨æˆ·é—´å¯¹æ¯”æŸå¤±å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬é¿å…æ¢¯åº¦é—®é¢˜"""
    
    def __init__(self, temperature=0.07, margin=0.5):
        super().__init__()
        self.temperature = temperature
        self.margin = margin
    
    def forward(self, features, labels):
        """
        features: [batch_size, feature_dim] 
        labels: [batch_size] ç”¨æˆ·ID
        """
        batch_size = features.size(0)
        if batch_size < 2:
            return torch.tensor(0.0, device=features.device, requires_grad=True)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features_norm = F.normalize(features, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.div(torch.matmul(features_norm, features_norm.T), self.temperature)
        
        # åˆ›å»ºæ ‡ç­¾mask
        labels_expanded = labels.view(-1, 1)
        mask = torch.eq(labels_expanded, labels_expanded.T)
        
        # æ­£æ ·æœ¬ï¼šåŒæ ‡ç­¾ä½†ä¸æ˜¯è‡ªå·±
        eye_mask = torch.eye(batch_size, device=features.device)
        pos_mask = torch.mul(mask.float(), (1.0 - eye_mask))
        
        # è´Ÿæ ·æœ¬ï¼šä¸åŒæ ‡ç­¾
        neg_mask = (~mask).float()
        
        # è®¡ç®—æŸå¤±
        if pos_mask.sum() > 0:
            pos_sim = sim_matrix * pos_mask
            pos_loss = -torch.sum(pos_sim) / torch.sum(pos_mask)
        else:
            pos_loss = torch.tensor(0.0, device=features.device)
        
        if neg_mask.sum() > 0:
            neg_sim = sim_matrix * neg_mask
            # ç®€åŒ–çš„è´Ÿæ ·æœ¬æŸå¤±ï¼Œä¸ä½¿ç”¨hard negative mining
            neg_loss = torch.sum(neg_sim) / torch.sum(neg_mask)
        else:
            neg_loss = torch.tensor(0.0, device=features.device)
        
        return pos_loss + neg_loss
    

class SupConLoss(nn.Module):
    """ç›‘ç£å¯¹æ¯”æŸå¤± - é¿å…æ‰€æœ‰åŸåœ°æ“ä½œ"""
    
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, features, labels):
        """
        features: [batch_size, feature_dim]
        labels: [batch_size]
        """
        device = features.device
        batch_size = features.shape[0]
        
        if batch_size < 2:
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features_norm = F.normalize(features, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(features_norm, features_norm.T) / self.temperature
        
        # åˆ›å»ºæ­£æ ·æœ¬mask
        labels_expanded = labels.view(-1, 1)
        pos_mask = torch.eq(labels_expanded, labels_expanded.T).float()
        
        # ç§»é™¤å¯¹è§’çº¿ - é¿å…åŸåœ°æ“ä½œ
        eye_mask = torch.eye(batch_size, device=device)
        pos_mask = torch.sub(pos_mask, eye_mask)
        
        # è´Ÿæ ·æœ¬mask
        neg_mask = torch.ne(labels_expanded, labels_expanded.T).float()
        
        # æ•°å€¼ç¨³å®šæ€§
        sim_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)
        sim_matrix = torch.sub(sim_matrix, sim_max.detach())
        
        # è®¡ç®—InfoNCEæŸå¤±
        exp_sim = torch.exp(sim_matrix)
        
        # åˆ†æ¯ï¼šæ‰€æœ‰è´Ÿæ ·æœ¬ + æ­£æ ·æœ¬
        denominator = torch.sum(exp_sim * neg_mask, dim=1, keepdim=True) + \
                     torch.sum(exp_sim * pos_mask, dim=1, keepdim=True)
        
        # åˆ†å­ï¼šæ­£æ ·æœ¬
        numerator = torch.sum(exp_sim * pos_mask, dim=1, keepdim=True)
        
        # é¿å…é™¤é›¶
        loss = torch.neg(torch.log(torch.div(numerator, denominator + 1e-8)))
        
        # åªè®¡ç®—æœ‰æ­£æ ·æœ¬çš„è¡Œ
        valid_mask = (pos_mask.sum(dim=1) > 0)
        if valid_mask.sum() > 0:
            return loss[valid_mask].mean()
        else:
            return torch.tensor(0.0, device=device, requires_grad=True)


class ImprovedMicroDopplerDataset(Dataset):
    """æ”¹è¿›çš„å¾®å¤šæ™®å‹’æ•°æ®é›†ï¼ŒåŒ…å«å¼ºæ•°æ®å¢å¼º"""
    
    def __init__(self, data_dir, split='train', transform=None, contrastive_pairs=False):
        self.data_dir = Path(data_dir)
        self.split = split
        self.contrastive_pairs = contrastive_pairs
        self.samples = []
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬
        user_samples = defaultdict(list)
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        # æŸ¥æ‰¾ID_*ç›®å½•
        id_dirs = list(self.data_dir.glob("ID_*"))
        
        if len(id_dirs) == 0:
            raise ValueError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°ID_*æ ¼å¼çš„ç”¨æˆ·ç›®å½•")
        
        for user_dir in sorted(id_dirs):
            if user_dir.is_dir():
                user_id = int(user_dir.name.split('_')[1]) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
                for ext in ['*.png', '*.jpg', '*.jpeg']:
                    for img_path in user_dir.glob(ext):
                        user_samples[user_id].append(str(img_path))
        
        if not user_samples:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
        
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
        for user_id, paths in user_samples.items():
            random.shuffle(paths)
            split_idx = int(len(paths) * 0.8)
            
            if split == 'train':
                selected_paths = paths[:split_idx]
            else:  # validation
                selected_paths = paths[split_idx:]
            
            for path in selected_paths:
                self.samples.append((path, user_id))
        
        # å¾®å¤šæ™®å‹’å›¾åƒä¸“ç”¨å˜æ¢ï¼ˆæœ€å°å¢å¼ºï¼Œä¿æŒé¢‘è°±ç»“æ„ï¼‰
        if split == 'train':
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                # åªä½¿ç”¨æè½»å¾®çš„å™ªå£°å¢å¼ºï¼Œä¸ç ´åé¢‘è°±ç»“æ„
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                # å¯é€‰ï¼šæå°çš„é«˜æ–¯å™ªå£°ï¼ˆæ¨¡æ‹Ÿæµ‹é‡å™ªå£°ï¼‰ - é¿å…åŸåœ°æ“ä½œ
                # transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01 if torch.rand(1).item() < 0.3 else x)
            ])
        else:
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        
        if transform:
            self.transform = transform
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            
            if self.contrastive_pairs and self.split == 'train':
                # ç”Ÿæˆå¯¹æ¯”æ ·æœ¬å¯¹
                image1 = self.transform(image)
                image2 = self.transform(image)  # åŒä¸€å›¾åƒçš„ä¸åŒå¢å¼º
                return (image1, image2), label
            else:
                image = self.transform(image)
                return image, label
                
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            # è¿”å›é›¶å¼ é‡
            if self.contrastive_pairs:
                return (torch.zeros(3, 224, 224), torch.zeros(3, 224, 224)), label
            else:
                return torch.zeros(3, 224, 224), label


class ImprovedClassifier(nn.Module):
    """æ”¹è¿›çš„åˆ†ç±»å™¨ï¼Œä¸“ä¸ºå¾®å¤šæ™®å‹’ä¿¡å·ä¼˜åŒ– - å®Œå…¨é¿å…inplaceæ“ä½œ"""
    
    def __init__(self, num_classes, backbone='resnet18', dropout_rate=0.3, freeze_layers=True):
        super().__init__()
        
        # ä½¿ç”¨æ ‡å‡†ResNet18é¿å…TIMMçš„æ½œåœ¨inplaceé—®é¢˜
        import torchvision.models as models
        self.backbone = models.resnet18(pretrained=True)
        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.backbone.fc = nn.Identity()
        feature_dim = 512
        
        # é€’å½’ç¦ç”¨æ‰€æœ‰ReLUçš„inplaceæ“ä½œ
        self._disable_inplace_operations(self.backbone)
        
        # çµæ´»çš„å±‚å†»ç»“ç­–ç•¥
        if freeze_layers == 'minimal':
            # æœ€å°å†»ç»“ï¼šåªå†»ç»“æœ€æ—©çš„å·ç§¯å±‚
            for name, param in self.backbone.named_parameters():
                if any(x in name for x in ['conv1', 'bn1']):
                    param.requires_grad = False
        elif freeze_layers == 'moderate':
            # ä¸­ç­‰å†»ç»“ï¼šå†»ç»“æ—©æœŸå±‚ï¼Œä¿ç•™é€‚åº”æ€§
            for name, param in self.backbone.named_parameters():
                if any(x in name for x in ['conv1', 'bn1', 'layer1']):
                    param.requires_grad = False
        elif freeze_layers == 'aggressive':
            # æ¿€è¿›å†»ç»“ï¼šå†»ç»“æ›´å¤šå±‚ï¼ˆå°æ•°æ®é›†ï¼‰
            for name, param in self.backbone.named_parameters():
                if any(x in name for x in ['conv1', 'bn1', 'layer1', 'layer2']):
                    param.requires_grad = False
        elif freeze_layers == 'none':
            # ä¸å†»ç»“ä»»ä½•å±‚ï¼ˆé£é™©æ›´é«˜ä½†å¯èƒ½æ•ˆæœæ›´å¥½ï¼‰
            pass
        
        # åˆ†ç±»å¤´ - ç¡®ä¿æ‰€æœ‰æ¿€æ´»å‡½æ•°éƒ½ä¸æ˜¯inplace
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(feature_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=False),  # æ˜ç¡®ç¦ç”¨inplace
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(256, num_classes)
        )
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(inplace=False),  # æ˜ç¡®ç¦ç”¨inplace
            nn.Linear(128, 64)
        )
    
    def _disable_inplace_operations(self, module):
        """é€’å½’ç¦ç”¨æ¨¡å—ä¸­æ‰€æœ‰çš„inplaceæ“ä½œ"""
        for child_name, child in module.named_children():
            if isinstance(child, nn.ReLU):
                # æ›¿æ¢inplace=Trueçš„ReLU
                setattr(module, child_name, nn.ReLU(inplace=False))
            elif isinstance(child, nn.ReLU6):
                setattr(module, child_name, nn.ReLU6(inplace=False))
            elif isinstance(child, nn.LeakyReLU):
                setattr(module, child_name, nn.LeakyReLU(child.negative_slope, inplace=False))
            else:
                # é€’å½’å¤„ç†å­æ¨¡å—
                self._disable_inplace_operations(child)
    
    def forward(self, x, return_features=False):
        features = self.backbone(x)
        
        if return_features:
            projected = self.projection_head(features)
            return features, projected
        
        logits = self.classifier(features)
        return logits


class FocalLoss(nn.Module):
    """Focal Loss - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
    
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()


class LabelSmoothingLoss(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤±"""
    
    def __init__(self, num_classes, smoothing=0.1):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
    
    def forward(self, pred, target):
        confidence = 1.0 - self.smoothing
        smooth_target = torch.full_like(pred, self.smoothing / (self.num_classes - 1))
        smooth_target.scatter_(1, target.unsqueeze(1), confidence)
        return F.kl_div(F.log_softmax(pred, dim=1), smooth_target, reduction='batchmean')


def train_with_contrastive_learning(model, train_loader, val_loader, device, args, rank=0):
    """æ”¹è¿›çš„è®­ç»ƒå‡½æ•°ï¼Œé›†æˆå¯¹æ¯”å­¦ä¹ """
    
    # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
    def is_main_process():
        return rank == 0
    
    # éªŒè¯æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if len(train_loader.dataset) == 0:
        if is_main_process():
            print("âŒ è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ")
        return model, None, 0.0, {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'contrastive_loss': [], 'classification_loss': []}
    
    if len(val_loader.dataset) == 0:
        if is_main_process():
            print("âŒ éªŒè¯æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ")
        return model, None, 0.0, {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'contrastive_loss': [], 'classification_loss': []}
    
    if is_main_process():
        print(f"è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬, éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
    
    # æŸå¤±å‡½æ•°
    if args.use_focal_loss:
        classification_criterion = FocalLoss()
    elif args.use_label_smoothing:
        classification_criterion = LabelSmoothingLoss(args.num_classes)
    else:
        classification_criterion = nn.CrossEntropyLoss()
    
    # æš‚æ—¶å®Œå…¨ç¦ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œå…ˆç¡®ä¿åŸºç¡€è®­ç»ƒæ­£å¸¸
    if is_main_process():
        print("ğŸ”§ æš‚æ—¶ç¦ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œè°ƒè¯•åŸºç¡€è®­ç»ƒæµç¨‹")
    contrastive_criterion = None
    
    # ä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡å’Œæ›´å¼ºçš„weight decay
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=args.lr, 
        weight_decay=args.weight_decay,
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )
    
    best_val_acc = 0
    patience_counter = 0
    
    history = {
        'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],
        'contrastive_loss': [], 'classification_loss': []
    }
    
    # ç¦ç”¨å¼‚å¸¸æ£€æµ‹ï¼Œé¿å…é¢å¤–å¼€é”€
    torch.autograd.set_detect_anomaly(False)
    
    for epoch in range(args.epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        contrastive_losses = []
        classification_losses = []
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        if is_main_process():
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        else:
            pbar = train_loader
        
        for batch_idx, batch_data in enumerate(pbar):
            data, target = batch_data
            target = target.to(device)
            
            # æœ€ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ - çº¯åˆ†ç±»è®­ç»ƒ
            if isinstance(data, (tuple, list)):
                data = data[0]
            
            data = data.to(device)
            
            # çº¯åˆ†ç±»è®­ç»ƒ
            logits = model(data)
            total_loss = classification_criterion(logits, target)
            classification_loss = total_loss
            contrastive_loss = torch.tensor(0.0, device=device)
            
            pred = logits.argmax(dim=1)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # ç»Ÿè®¡
            train_loss += total_loss.item()
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
            
            contrastive_losses.append(contrastive_loss.item() if isinstance(contrastive_loss, torch.Tensor) else 0.0)
            classification_losses.append(classification_loss.item())
            
            # åªåœ¨ä¸»è¿›ç¨‹æ›´æ–°è¿›åº¦æ¡
            if is_main_process() and isinstance(pbar, tqdm):
                pbar.set_postfix({
                    'Loss': f'{total_loss.item():.4f}',
                    'Acc': f'{100.*train_correct/train_total:.1f}%'
                })
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                
                logits = model(data)
                loss = classification_criterion(logits, target)
                
                pred = logits.argmax(dim=1)
                val_loss += loss.item()
                val_correct += pred.eq(target).sum().item()
                val_total += target.size(0)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # é˜²æ­¢é™¤é›¶é”™è¯¯
        if len(train_loader) == 0:
            print("âŒ è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ ¼å¼")
            return model, None, 0.0, {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'contrastive_loss': [], 'classification_loss': []}
        
        if len(val_loader) == 0:
            print("âŒ éªŒè¯æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ ¼å¼")
            return model, None, 0.0, {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'contrastive_loss': [], 'classification_loss': []}
        
        # ç»Ÿè®¡
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_acc = 100. * train_correct / train_total if train_total > 0 else 0.0
        val_acc = 100. * val_correct / val_total if val_total > 0 else 0.0
        
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_acc)
        history['contrastive_loss'].append(np.mean(contrastive_losses))
        history['classification_loss'].append(np.mean(classification_losses))
        
        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
        if is_main_process():
            print(f"Epoch {epoch+1}/{args.epochs}")
            print(f"Train: Loss={avg_train_loss:.4f}, Acc={train_acc:.2f}%")
            print(f"Val: Loss={avg_val_loss:.4f}, Acc={val_acc:.2f}%")
            print(f"LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # æ—©åœå’Œæœ€ä½³æ¨¡å‹ä¿å­˜ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()
                patience_counter = 0
                print(f"ğŸ¯ New best validation accuracy: {best_val_acc:.2f}%")
            else:
                patience_counter += 1
            
            if patience_counter >= args.patience:
                print(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break
    
    return model, best_model_state, best_val_acc, history


def main():
    parser = argparse.ArgumentParser(description='Improved classifier training with distributed support')
    parser.add_argument('--data_dir', type=str, required=True, help='Dataset directory')
    parser.add_argument('--output_dir', type=str, default='./improved_classifier', help='Output directory')
    parser.add_argument('--num_classes', type=int, default=31, help='Number of classes')
    parser.add_argument('--epochs', type=int, default=200, help='Number of epochs')
    parser.add_argument('--batch_size', type=int, default=16, help='Batch size per GPU')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate (smaller)')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='Weight decay (stronger)')
    parser.add_argument('--dropout_rate', type=float, default=0.5, help='Dropout rate')
    parser.add_argument('--patience', type=int, default=10, help='Early stopping patience')
    
    # å¯¹æ¯”å­¦ä¹ å‚æ•°
    parser.add_argument('--use_contrastive', action='store_true', help='Use contrastive learning')
    parser.add_argument('--contrastive_weight', type=float, default=0.5, help='Contrastive loss weight')
    parser.add_argument('--contrastive_temperature', type=float, default=0.07, help='Contrastive temperature')
    parser.add_argument('--contrastive_type', type=str, default='supcon', 
                       choices=['global', 'interuser', 'supcon'],
                       help='Contrastive loss type: global(memory bank all users), interuser(hard negative mining), supcon(supervised contrastive)')
    parser.add_argument('--contrastive_margin', type=float, default=0.5, 
                       help='Margin for hard negative mining in interuser contrastive loss')
    
    # æŸå¤±å‡½æ•°é€‰æ‹©
    parser.add_argument('--use_focal_loss', action='store_true', help='Use focal loss')
    parser.add_argument('--use_label_smoothing', action='store_true', help='Use label smoothing')
    
    # æ¨¡å‹é€‰æ‹© - ResNet18ä¸“ä¸ºå¾®å¤šæ™®å‹’ä¼˜åŒ–
    parser.add_argument('--backbone', type=str, default='resnet18', help='Backbone architecture')
    parser.add_argument('--freeze_layers', type=str, default='moderate', 
                       choices=['none', 'minimal', 'moderate', 'aggressive'],
                       help='Layer freezing strategy: none(risk overfitting), minimal(conv1+bn1), moderate(+layer1), aggressive(+layer2)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    rank, world_size, local_rank = setup_distributed()
    
    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{local_rank}')
        torch.cuda.set_device(local_rank)
    else:
        device = torch.device('cpu')
    
    if is_main_process():
        print(f"Using distributed training with {world_size} GPUs")
        print(f"Current device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    if is_main_process():
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42 + rank)  # æ¯ä¸ªè¿›ç¨‹ä¸åŒçš„éšæœºç§å­
    np.random.seed(42 + rank)
    random.seed(42 + rank)
    
    # æ•°æ®é›†
    train_dataset = ImprovedMicroDopplerDataset(
        args.data_dir, 
        split='train', 
        contrastive_pairs=args.use_contrastive
    )
    val_dataset = ImprovedMicroDopplerDataset(
        args.data_dir, 
        split='val'
    )
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = DistributedSampler(train_dataset) if world_size > 1 else None
    val_sampler = DistributedSampler(val_dataset, shuffle=False) if world_size > 1 else None
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=(train_sampler is None), 
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size * 2, 
        shuffle=False,
        sampler=val_sampler,
        num_workers=4,
        pin_memory=True
    )
    
    # æ¨¡å‹
    model = ImprovedClassifier(
        num_classes=args.num_classes,
        backbone=args.backbone,
        dropout_rate=args.dropout_rate,
        freeze_layers=args.freeze_layers
    ).to(device)
    
    # åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ¨¡å‹
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank], output_device=local_rank)
    
    if is_main_process():
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Model parameters: {total_params:,}")
    
    # è®­ç»ƒ
    model, best_state, best_acc, history = train_with_contrastive_learning(
        model, train_loader, val_loader, device, args, rank
    )
    
    # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹
    if is_main_process():
        output_dir = Path(args.output_dir)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        model_path = output_dir / 'best_improved_classifier.pth'
        torch.save({
            'model_state_dict': best_state,
            'best_val_acc': best_acc,
            'num_classes': args.num_classes,
            'model_name': args.backbone,
            'args': vars(args)
        }, model_path)
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = output_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        print(f"\nâœ… Training completed!")
        print(f"Best validation accuracy: {best_acc:.2f}%")
        print(f"Model saved to: {model_path}")
    
    # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
    cleanup_distributed()


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
æ­¥éª¤4: VA-VAEå¾®è°ƒ
- åŸºäºé¢„è®­ç»ƒçš„vavae-imagenet256-f16d32-dinov2.ptè¿›è¡Œå¾®è°ƒ
- é€‚é…å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®
- é’ˆå¯¹T4Ã—2 GPUä¼˜åŒ–
"""

import os
import sys
import yaml
import shutil
from pathlib import Path
import argparse

def create_micro_doppler_dataset_class():
    """åˆ›å»ºå¾®å¤šæ™®å‹’æ•°æ®é›†ç±»"""
    dataset_code = '''
import os
import torch
import numpy as np
from PIL import Image
from torch.utils.data import Dataset
from pathlib import Path

class MicroDopplerDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®é›†"""
    
    def __init__(self, data_root, size=256, user_conditioning=True):
        self.data_root = data_root
        self.size = size
        self.user_conditioning = user_conditioning
        
        # åŠ è½½31ä¸ªç”¨æˆ·çš„æ—¶é¢‘å›¾æ–‡ä»¶
        self.samples = []
        data_path = Path(data_root)
        
        for user_id in range(1, 32):  # ç”¨æˆ·1åˆ°31
            user_dir = data_path / f"user{user_id}"
            if user_dir.exists():
                for img_file in user_dir.iterdir():
                    if img_file.suffix.lower() in ['.png', '.jpg', '.jpeg']:
                        self.samples.append({
                            'path': img_file,
                            'user_id': user_id - 1  # è½¬æ¢ä¸º0-30ç´¢å¼•
                        })
        
        print(f"Loaded {len(self.samples)} micro-Doppler samples from {data_root}")
        
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½æ—¶é¢‘å›¾ (256x256x3)
        image = Image.open(sample['path']).convert('RGB')
        image = image.resize((self.size, self.size), Image.LANCZOS)
        
        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–åˆ°[-1, 1]
        image = np.array(image).astype(np.float32) / 127.5 - 1.0
        image = torch.from_numpy(image).permute(2, 0, 1)
        
        result = {'image': image}
        
        if self.user_conditioning:
            result['user_id'] = sample['user_id']
            result['class_label'] = sample['user_id']  # ç”¨äºæ¡ä»¶ç”Ÿæˆ
            
        return result
    
    def __len__(self):
        return len(self.samples)
'''
    
    # ä¿å­˜åˆ°LightningDiT/vavae/ldm/data/ç›®å½•
    dataset_file = Path("LightningDiT/vavae/ldm/data/micro_doppler.py")
    with open(dataset_file, 'w', encoding='utf-8') as f:
        f.write(dataset_code)
    
    print(f"âœ… å¾®å¤šæ™®å‹’æ•°æ®é›†ç±»å·²åˆ›å»º: {dataset_file}")

def create_vavae_config(dataset_dir, output_dir):
    """åˆ›å»ºVA-VAEå¾®è°ƒé…ç½®æ–‡ä»¶"""
    
    config = {
        'ckpt_path': str(Path("official_models/vavae-imagenet256-f16d32-dinov2.pt").absolute()),
        'weight_init': str(Path("official_models/vavae-imagenet256-f16d32-dinov2.pt").absolute()),
        
        'model': {
            'base_learning_rate': 1.0e-05,  # å¾®è°ƒå­¦ä¹ ç‡
            'target': 'ldm.models.autoencoder.AutoencoderKL',
            'params': {
                'monitor': 'val/rec_loss',
                'embed_dim': 32,
                'use_vf': 'dinov2',  # ä¿æŒDINOv2ç‰¹å¾å¯¹é½
                'reverse_proj': True,
                'lossconfig': {
                    'target': 'ldm.modules.losses.LPIPSWithDiscriminator',
                    'params': {
                        'disc_start': 1000,  # å»¶è¿Ÿåˆ¤åˆ«å™¨å¯åŠ¨
                        'kl_weight': 1.0e-06,
                        'disc_weight': 0.3,  # é™ä½åˆ¤åˆ«å™¨æƒé‡
                        'vf_weight': 0.05,   # é™ä½è§†è§‰ç‰¹å¾æƒé‡
                        'adaptive_vf': True,
                        'distmat_margin': 0.25,
                        'cos_margin': 0.5,
                    }
                },
                'ddconfig': {
                    'double_z': True,
                    'z_channels': 32,
                    'resolution': 256,
                    'in_channels': 3,    # å½©è‰²æ—¶é¢‘å›¾
                    'out_ch': 3,
                    'ch': 128,
                    'ch_mult': [1, 1, 2, 2, 4],
                    'num_res_blocks': 2,
                    'attn_resolutions': [16],
                    'dropout': 0.0
                }
            }
        },
        
        'data': {
            'target': 'main.DataModuleFromConfig',
            'params': {
                'batch_size': 1,  # T4Ã—2 GPUä¼˜åŒ–
                'wrap': False,    # ä¸é‡å¤æ•°æ®
                'train': {
                    'target': 'ldm.data.micro_doppler.MicroDopplerDataset',
                    'params': {
                        'data_root': str(Path(dataset_dir) / "train"),
                        'size': 256,
                        'user_conditioning': True
                    }
                },
                'validation': {
                    'target': 'ldm.data.micro_doppler.MicroDopplerDataset',
                    'params': {
                        'data_root': str(Path(dataset_dir) / "val"),
                        'size': 256,
                        'user_conditioning': True
                    }
                }
            }
        },
        
        'lightning': {
            'trainer': {
                'devices': 2,  # T4Ã—2
                'num_nodes': 1,
                'strategy': 'ddp_find_unused_parameters_true',
                'accelerator': 'gpu',
                'max_epochs': 100,  # å¾®è°ƒè½®æ•°
                'precision': 16,    # æ··åˆç²¾åº¦
                'check_val_every_n_epoch': 5,
                'log_every_n_steps': 10,
                'gradient_clip_val': 1.0,
                'accumulate_grad_batches': 4,  # æ¢¯åº¦ç´¯ç§¯
            }
        }
    }
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    config_file = Path("LightningDiT/vavae/configs/micro_doppler_vavae.yaml")
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)
    
    print(f"âœ… VA-VAEé…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
    return config_file

def check_prerequisites():
    """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
    print("ğŸ” æ£€æŸ¥å‰ç½®æ¡ä»¶...")
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹
    vavae_model = Path("official_models/vavae-imagenet256-f16d32-dinov2.pt")
    if not vavae_model.exists():
        print(f"âŒ é¢„è®­ç»ƒVA-VAEæ¨¡å‹ä¸å­˜åœ¨: {vavae_model}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python step1_download_models.py")
        return False
    
    print(f"âœ… é¢„è®­ç»ƒVA-VAEæ¨¡å‹: {vavae_model}")
    
    # æ£€æŸ¥LightningDiTç›®å½•
    vavae_dir = Path("LightningDiT/vavae")
    if not vavae_dir.exists():
        print(f"âŒ VA-VAEè®­ç»ƒç›®å½•ä¸å­˜åœ¨: {vavae_dir}")
        return False
    
    print(f"âœ… VA-VAEè®­ç»ƒç›®å½•: {vavae_dir}")
    
    # æ£€æŸ¥Taming-Transformers
    taming_dir = Path("taming-transformers")
    if not taming_dir.exists():
        print(f"âŒ Taming-Transformersæœªå®‰è£…")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ:")
        print("   git clone https://github.com/CompVis/taming-transformers.git")
        print("   cd taming-transformers && pip install -e .")
        return False
    
    print(f"âœ… Taming-Transformers: {taming_dir}")
    
    return True

def start_training(config_file):
    """å¯åŠ¨VA-VAEå¾®è°ƒè®­ç»ƒ"""
    print("\nğŸš€ å¯åŠ¨VA-VAEå¾®è°ƒè®­ç»ƒ...")
    
    # åˆ‡æ¢åˆ°vavaeç›®å½•
    vavae_dir = Path("LightningDiT/vavae")
    original_dir = Path.cwd()
    
    try:
        os.chdir(vavae_dir)
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        config_name = config_file.name
        cmd = f"bash run_train.sh configs/{config_name}"
        
        print(f"ğŸ’» æ‰§è¡Œå‘½ä»¤: {cmd}")
        print(f"ğŸ“ å·¥ä½œç›®å½•: {vavae_dir.absolute()}")
        print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
        print("=" * 60)
        
        # æ‰§è¡Œè®­ç»ƒ
        os.system(cmd)
        
    finally:
        # æ¢å¤åŸç›®å½•
        os.chdir(original_dir)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ­¥éª¤4: VA-VAEå¾®è°ƒ')
    parser.add_argument('--dataset_dir', type=str, default='micro_doppler_dataset',
                       help='æ•°æ®é›†ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='vavae_finetuned',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--dry_run', action='store_true',
                       help='åªå‡†å¤‡é…ç½®ï¼Œä¸å¯åŠ¨è®­ç»ƒ')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¯ æ­¥éª¤4: VA-VAEå¾®è°ƒ")
    print("=" * 60)
    print(f"æ•°æ®é›†ç›®å½•: {args.dataset_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # 1. æ£€æŸ¥å‰ç½®æ¡ä»¶
    if not check_prerequisites():
        print("\nâŒ å‰ç½®æ¡ä»¶æ£€æŸ¥å¤±è´¥")
        return False
    
    # 2. æ£€æŸ¥æ•°æ®é›†
    dataset_path = Path(args.dataset_dir)
    if not dataset_path.exists():
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {args.dataset_dir}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python step3_prepare_micro_doppler_dataset.py")
        return False
    
    train_dir = dataset_path / "train"
    val_dir = dataset_path / "val"
    if not train_dir.exists() or not val_dir.exists():
        print("âŒ æ•°æ®é›†ç»“æ„ä¸å®Œæ•´ï¼Œç¼ºå°‘trainæˆ–valç›®å½•")
        return False
    
    print(f"âœ… æ•°æ®é›†æ£€æŸ¥é€šè¿‡: {args.dataset_dir}")
    
    # 3. åˆ›å»ºå¾®å¤šæ™®å‹’æ•°æ®é›†ç±»
    create_micro_doppler_dataset_class()
    
    # 4. åˆ›å»ºVA-VAEé…ç½®
    config_file = create_vavae_config(args.dataset_dir, args.output_dir)
    
    # 5. å¯åŠ¨è®­ç»ƒï¼ˆé™¤éæ˜¯dry runï¼‰
    if args.dry_run:
        print("\nâœ… é…ç½®å‡†å¤‡å®Œæˆï¼ˆdry runæ¨¡å¼ï¼‰")
        print(f"ğŸ“ é…ç½®æ–‡ä»¶: {config_file}")
        print("ğŸ’¡ è¦å¯åŠ¨è®­ç»ƒï¼Œè¯·è¿è¡Œ:")
        print(f"   python {sys.argv[0]} --dataset_dir {args.dataset_dir}")
    else:
        start_training(config_file)
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

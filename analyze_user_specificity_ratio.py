"""
åŸºäºçœŸå®ç”Ÿæˆæ ·æœ¬åˆ†æuser_specificityåˆ†å¸ƒ
ç¡®å®šåˆç†çš„é˜ˆå€¼è®¾ç½®
"""

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from pathlib import Path
import argparse
from tqdm import tqdm

def load_classifier(model_path, device):
    """åŠ è½½åˆ†ç±»å™¨"""
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    
    # æ ¹æ®checkpointåˆ¤æ–­æ¨¡å‹ç±»å‹
    if 'feature_projector.0.weight' in checkpoint['model_state_dict']:
        from train_calibrated_classifier import DomainAdaptiveClassifier
        model = DomainAdaptiveClassifier(
            num_classes=checkpoint['num_classes'],
            dropout_rate=0.3,
            feature_dim=512
        )
    else:
        from improved_classifier_training import ImprovedClassifier
        model = ImprovedClassifier(
            num_classes=checkpoint['num_classes'],
            backbone='resnet18',
            dropout_rate=0.5,
            freeze_layers='minimal'
        )
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    return model

def compute_user_specificity_from_samples(samples_dir, classifier, device):
    """ä»çœŸå®æ ·æœ¬è®¡ç®—user_specificityåˆ†å¸ƒ"""
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    all_specificities_diff = []
    all_specificities_ratio = []
    all_user_probs = []
    all_max_other_probs = []
    all_correct = []
    
    samples_path = Path(samples_dir)
    
    # è°ƒè¯•ï¼šæ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not samples_path.exists():
        print(f"âŒ æ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {samples_path}")
        return {'diff_mode': np.array([]), 'ratio_mode': np.array([]), 'user_probs': np.array([]), 'max_other_probs': np.array([]), 'correct': np.array([])}
    
    # è°ƒè¯•ï¼šåˆ—å‡ºç›®å½•å†…å®¹
    all_items = list(samples_path.iterdir())
    print(f"ğŸ“‚ ç›®å½•å†…å®¹: {[item.name for item in all_items[:10]]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
    
    # éå†æ‰€æœ‰ç”¨æˆ·æ–‡ä»¶å¤¹
    user_folders = [f for f in samples_path.iterdir() if f.is_dir() and f.name.startswith('user_')]
    
    print(f"ğŸ” åˆ†æ {len(user_folders)} ä¸ªç”¨æˆ·çš„æ ·æœ¬...")
    if len(user_folders) == 0:
        print("âŒ æœªæ‰¾åˆ°user_XXæ ¼å¼çš„æ–‡ä»¶å¤¹ï¼")
        print("   è¯·ç¡®è®¤æ–‡ä»¶å¤¹å‘½åæ ¼å¼ä¸º: user_00, user_01, user_02...")
    
    total_images_processed = 0
    total_images_found = 0
    
    for user_folder in tqdm(user_folders, desc="å¤„ç†ç”¨æˆ·"):
        try:
            user_id = int(user_folder.name.split('_')[1])
        except:
            print(f"âš ï¸  æ— æ³•è§£æç”¨æˆ·ID: {user_folder.name}")
            continue
            
        # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒ
        image_files = list(user_folder.glob('*.png')) + list(user_folder.glob('*.jpg'))
        total_images_found += len(image_files)
        
        if len(image_files) == 0:
            print(f"âš ï¸  ç”¨æˆ·{user_id}æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            continue
            
        print(f"ğŸ“· ç”¨æˆ·{user_id}: æ‰¾åˆ°{len(image_files)}å¼ å›¾åƒ")
            
        # éšæœºé€‰æ‹©æœ€å¤š50å¼ å›¾åƒè¿›è¡Œåˆ†æï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        if len(image_files) > 50:
            image_files = np.random.choice(image_files, 50, replace=False).tolist()
            print(f"   â†’ éšæœºé€‰æ‹©{len(image_files)}å¼ è¿›è¡Œåˆ†æ")
        
        user_processed = 0
        user_errors = 0
        
        for img_path in image_files:
            try:
                # åŠ è½½å›¾åƒ
                img = Image.open(img_path).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device)
                
                with torch.no_grad():
                    # è·å–åˆ†ç±»å™¨è¾“å‡º
                    outputs = classifier(img_tensor)
                    probs = F.softmax(outputs, dim=1)
                    
                    # è®¡ç®—æŒ‡æ ‡
                    confidence, pred = torch.max(probs, dim=1)
                    
                    user_prob = probs[0, user_id].item()
                    other_probs = torch.cat([probs[0, :user_id], probs[0, user_id+1:]])
                    max_other_prob = torch.max(other_probs).item()
                    
                    # ä¸¤ç§è®¡ç®—æ–¹æ³•
                    specificity_diff = user_prob - max_other_prob
                    specificity_ratio = user_prob / (user_prob + max_other_prob) if (user_prob + max_other_prob) > 0 else 0.0
                    
                    all_specificities_diff.append(specificity_diff)
                    all_specificities_ratio.append(specificity_ratio)
                    all_user_probs.append(user_prob)
                    all_max_other_probs.append(max_other_prob)
                    all_correct.append(pred.item() == user_id)
                    
                    user_processed += 1
                    total_images_processed += 1
                    
            except Exception as e:
                user_errors += 1
                if user_errors <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    print(f"   âŒ å¤„ç†å›¾åƒå‡ºé”™ {img_path.name}: {str(e)[:50]}...")
        
        if user_processed > 0:
            print(f"   âœ… æˆåŠŸå¤„ç†{user_processed}å¼ å›¾åƒ")
        if user_errors > 0:
            print(f"   âš ï¸  {user_errors}å¼ å›¾åƒå¤„ç†å¤±è´¥")
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ‰¾åˆ°å›¾åƒæ€»æ•°: {total_images_found}")
    print(f"   æˆåŠŸå¤„ç†: {total_images_processed}")
    print(f"   æœ€ç»ˆæ•°æ®ç‚¹: {len(all_specificities_ratio)}")
    
    result = {
        'diff_mode': np.array(all_specificities_diff),
        'ratio_mode': np.array(all_specificities_ratio),
        'user_probs': np.array(all_user_probs),
        'max_other_probs': np.array(all_max_other_probs),
        'correct': np.array(all_correct)
    }
    
    return result

def analyze_threshold_performance(data):
    """åˆ†æä¸åŒé˜ˆå€¼çš„æ€§èƒ½"""
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if len(data['correct']) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ·æœ¬æ•°æ®ï¼")
        print("   è¯·æ£€æŸ¥æ ·æœ¬ç›®å½•è·¯å¾„å’Œæ–‡ä»¶å¤¹å‘½åæ ¼å¼")
        return np.array([])
    
    # åªåˆ†ææ­£ç¡®é¢„æµ‹çš„æ ·æœ¬
    correct_mask = data['correct'].astype(bool)
    ratio_correct = data['ratio_mode'][correct_mask]
    diff_correct = data['diff_mode'][correct_mask]
    
    if len(ratio_correct) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬ï¼")
        return np.array([])
    
    print(f"ğŸ“Š åŸºäº {len(ratio_correct)} ä¸ªæ­£ç¡®é¢„æµ‹æ ·æœ¬çš„åˆ†æ:")
    
    # åŸºç¡€ç»Ÿè®¡
    print(f"\nğŸ”¢ æ¯”ä¾‹æ¨¡å¼ç»Ÿè®¡:")
    print(f"   å‡å€¼: {np.mean(ratio_correct):.3f}")
    print(f"   æ ‡å‡†å·®: {np.std(ratio_correct):.3f}")
    print(f"   åˆ†ä½æ•°:")
    for p in [25, 50, 75, 90, 95]:
        value = np.percentile(ratio_correct, p)
        print(f"     {p}%: {value:.3f}")
    
    print(f"\nğŸ”¢ å·®å€¼æ¨¡å¼ç»Ÿè®¡ (å‚è€ƒ):")
    print(f"   å‡å€¼: {np.mean(diff_correct):.3f}")
    print(f"   æ ‡å‡†å·®: {np.std(diff_correct):.3f}")
    
    # é€šè¿‡ç‡åˆ†æ
    print(f"\nğŸ“ˆ æ¯”ä¾‹æ¨¡å¼ä¸åŒé˜ˆå€¼çš„é€šè¿‡ç‡:")
    ratio_thresholds = [0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85]
    
    for thresh in ratio_thresholds:
        pass_rate = np.mean(ratio_correct >= thresh) * 100
        print(f"   é˜ˆå€¼ {thresh:.2f}: {pass_rate:.1f}%")
    
    # æ¨èé˜ˆå€¼ï¼ˆç›®æ ‡15-25%é€šè¿‡ç‡ï¼‰
    target_rates = [15, 20, 25, 30]
    print(f"\nğŸ¯ æ¨èé˜ˆå€¼ï¼ˆç›®æ ‡é€šè¿‡ç‡ï¼‰:")
    
    for target_rate in target_rates:
        # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡é€šè¿‡ç‡çš„é˜ˆå€¼
        best_thresh = None
        best_diff = float('inf')
        
        for thresh in np.arange(0.50, 0.90, 0.01):
            actual_rate = np.mean(ratio_correct >= thresh) * 100
            diff = abs(actual_rate - target_rate)
            if diff < best_diff:
                best_diff = diff
                best_thresh = thresh
        
        actual_rate = np.mean(ratio_correct >= best_thresh) * 100
        print(f"   ç›®æ ‡{target_rate}%: é˜ˆå€¼{best_thresh:.3f} (å®é™…{actual_rate:.1f}%)")
    
    # å¯¹æ¯”å½“å‰0.65é˜ˆå€¼
    current_rate = np.mean(ratio_correct >= 0.65) * 100
    print(f"\nğŸ“Š å½“å‰0.65é˜ˆå€¼è¡¨ç°:")
    print(f"   é€šè¿‡ç‡: {current_rate:.1f}%")
    
    if current_rate > 30:
        print(f"   ğŸ’¡ å»ºè®®æé«˜é˜ˆå€¼åˆ°0.70-0.75ä»¥è·å¾—æ›´ä¸¥æ ¼ç­›é€‰")
    elif current_rate < 15:
        print(f"   ğŸ’¡ å»ºè®®é™ä½é˜ˆå€¼åˆ°0.60-0.65ä»¥è·å¾—åˆç†é€šè¿‡ç‡")
    else:
        print(f"   âœ… å½“å‰é˜ˆå€¼åˆé€‚")
    
    return ratio_correct

def main():
    parser = argparse.ArgumentParser(description='åˆ†æçœŸå®æ ·æœ¬çš„user_specificityåˆ†å¸ƒ')
    parser.add_argument('--samples_dir', type=str, 
                       default='/kaggle/working/VA-VAE/generated_samples2',
                       help='ç”Ÿæˆæ ·æœ¬ç›®å½•')
    parser.add_argument('--classifier_path', type=str,
                       default='/kaggle/working/VA-VAE/domain_adaptive_classifier/best_calibrated_model.pth',
                       help='åˆ†ç±»å™¨è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda:0',
                       help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    print("ğŸ” åŸºäºçœŸå®æ ·æœ¬çš„User Specificityé˜ˆå€¼åˆ†æ")
    print(f"ğŸ“‚ æ ·æœ¬ç›®å½•: {args.samples_dir}")
    print(f"ğŸ¤– åˆ†ç±»å™¨: {args.classifier_path}")
    
    # åŠ è½½åˆ†ç±»å™¨
    print("\nğŸ“¥ åŠ è½½åˆ†ç±»å™¨...")
    classifier = load_classifier(args.classifier_path, device)
    
    # åˆ†ææ ·æœ¬
    print("\nğŸ§® è®¡ç®—user_specificityåˆ†å¸ƒ...")
    data = compute_user_specificity_from_samples(args.samples_dir, classifier, device)
    
    # åˆ†æç»“æœ
    print("\n" + "="*60)
    ratio_data = analyze_threshold_performance(data)
    
    # å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
    try:
        plt.figure(figsize=(12, 8))
        plt.hist(ratio_data, bins=50, alpha=0.7, edgecolor='black')
        plt.axvline(0.65, color='red', linestyle='--', label='å½“å‰é˜ˆå€¼: 0.65')
        
        # æ·»åŠ æ¨èé˜ˆå€¼çº¿
        recommended_20 = np.percentile(ratio_data, 80)  # 20%é€šè¿‡ç‡
        plt.axvline(recommended_20, color='green', linestyle='--', 
                   label=f'æ¨èé˜ˆå€¼(20%): {recommended_20:.3f}')
        
        plt.xlabel('User Specificity (æ¯”ä¾‹æ¨¡å¼)')
        plt.ylabel('é¢‘æ¬¡')
        plt.title('çœŸå®æ ·æœ¬User Specificityåˆ†å¸ƒ')
        plt.legend()
        plt.savefig('real_user_specificity_analysis.png', dpi=150, bbox_inches='tight')
        print("\nğŸ“Š åˆ†å¸ƒå›¾å·²ä¿å­˜: real_user_specificity_analysis.png")
        plt.show()
    except:
        print("\nâš ï¸  æ— æ³•ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")

if __name__ == "__main__":
    main()

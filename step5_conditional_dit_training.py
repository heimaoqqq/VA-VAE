#!/usr/bin/env python3
"""
æ­¥éª¤5: æ¡ä»¶DiTå¾®è°ƒè®­ç»ƒ
åŸºäºé¢„è®­ç»ƒLightningDiT-XL-64epè¿›è¡Œç”¨æˆ·æ¡ä»¶å¾®è°ƒ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.nn.parallel import DataParallel as DP
from pathlib import Path
import numpy as np
from datetime import datetime
import os
import json
from typing import Dict, List, Optional, Tuple, Any
import yaml
import argparse
from tqdm import tqdm
import wandb
from datetime import datetime

# ğŸš€ å®Œå…¨ç¦ç”¨torch._dynamoï¼Œè§£å†³DataParallelå†²çª
import torch._dynamo
torch._dynamo.reset()
torch._dynamo.config.suppress_errors = True
torch._dynamo.config.disable = True  # å®Œå…¨ç¦ç”¨dynamoç¼–è¯‘
torch.backends.cudnn.allow_tf32 = True  # ä¼˜åŒ–æ€§èƒ½
print("ğŸ§¹ å·²å®Œå…¨ç¦ç”¨torch._dynamoï¼Œé¿å…ä¸DataParallelå†²çª")

# æ·»åŠ LightningDiTåˆ°è·¯å¾„
sys.path.append(str(Path("LightningDiT").absolute()))

from models.lightningdit import LightningDiT_models
from tokenizer.vavae import VA_VAE
from step4_microdoppler_adapter import MicroDopplerDataModule, UserConditionEncoder

class ConditionalDiT(nn.Module):
    """æ¡ä»¶DiTæ¨¡å‹"""
    
    def __init__(
        self,
        model: str = "LightningDiT-XL/1",
        num_users: int = 31,
        condition_dim: int = 1152,  # âœ… ä¿®å¤ï¼šåŒ¹é…DiTçš„hidden_size
        frozen_backbone: bool = True,
        dropout: float = 0.15,
        pretrained_path: str = None
    ):
        super().__init__()
        self.num_users = num_users
        self.condition_dim = condition_dim
        self.frozen_backbone = frozen_backbone
        
        # åŠ è½½é¢„è®­ç»ƒDiT - é…ç½®ç”¨æˆ·æ¡ä»¶å‚æ•°
        self.dit = LightningDiT_models[model](
            input_size=16,           # 16 = 256/16 (VA-VAEä¸‹é‡‡æ ·ç‡)
            in_channels=32,          # VA-VAEæ½œå‘é‡é€šé“æ•°
            num_classes=num_users,   # âœ… å…³é”®ä¿®å¤ï¼šè®¾ç½®ä¸ºç”¨æˆ·æ•°é‡è€Œä¸æ˜¯ImageNetçš„1000
            use_qknorm=False,        # å®˜æ–¹é…ç½®
            use_swiglu=True,         # å®˜æ–¹é…ç½®
            use_rope=True,           # å®˜æ–¹é…ç½®
            use_rmsnorm=True,        # å®˜æ–¹é…ç½®
            wo_shift=False           # å®˜æ–¹é…ç½®
        )
        
        # é‡æ–°å¯ç”¨é¢„è®­ç»ƒæƒé‡åŠ è½½
        print("âœ… é‡æ–°å¯ç”¨é¢„è®­ç»ƒæƒé‡åŠ è½½")
        self._load_pretrained_weights(pretrained_path)
        
        # å†»ç»“ä¸»å¹²ç½‘ç»œ
        if frozen_backbone:
            self._freeze_backbone()
        
        # ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨
        self.condition_encoder = UserConditionEncoder(
            num_users=num_users,
            embed_dim=condition_dim,
            dropout=dropout
        )
        
        # æ¡ä»¶æ³¨å…¥å±‚ - ä¿®æ”¹DiTçš„adaLNå±‚
        self._inject_condition_layers()
        
        # ğŸš€ åˆå§‹åŒ–ç”¨æˆ·æ¡ä»¶ç¼“å­˜ï¼ˆé¿å…å±æ€§é”™è¯¯ï¼‰
        self._last_user_condition = None
        
        print(f"âœ… æ¡ä»¶DiTåˆå§‹åŒ–å®Œæˆ")
        print(f"   - ä¸»å¹²å†»ç»“: {frozen_backbone}")
        print(f"   - æ€»å‚æ•°: {sum(p.numel() for p in self.parameters()):,}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}")
    
    def _load_pretrained_weights(self, pretrained_path: str):
        """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
        print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
        checkpoint = torch.load(pretrained_path, map_location='cpu')
        
        # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼ - ä¿®å¤æƒé‡åŠ è½½
        if isinstance(checkpoint, dict):
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        
        # æ™ºèƒ½æƒé‡åŠ è½½ï¼šè¿‡æ»¤ä¸å…¼å®¹çš„æƒé‡
        model_state = self.dit.state_dict()
        filtered_state_dict = {}
        incompatible_keys = []
        
        for key, value in state_dict.items():
            if key in model_state:
                if model_state[key].shape == value.shape:
                    filtered_state_dict[key] = value
                else:
                    incompatible_keys.append(f"{key}: {value.shape} -> {model_state[key].shape}")
            else:
                # é”®ä¸å­˜åœ¨äºå½“å‰æ¨¡å‹ä¸­
                pass
        
        # åŠ è½½å…¼å®¹çš„æƒé‡
        missing, unexpected = self.dit.load_state_dict(filtered_state_dict, strict=False)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(filtered_state_dict)} ä¸ªå…¼å®¹æƒé‡")
        if incompatible_keys:
            print(f"âš ï¸ è·³è¿‡ {len(incompatible_keys)} ä¸ªä¸å…¼å®¹æƒé‡:")
            for key in incompatible_keys:  # æ˜¾ç¤ºæ‰€æœ‰ä¸å…¼å®¹çš„æƒé‡
                print(f"   {key}")
        if missing:
            print(f"âš ï¸ ç¼ºå¤±é”®: {len(missing)}")
        if unexpected:
            print(f"âš ï¸ é¢å¤–é”®: {len(unexpected)}")
            
        # ğŸ”§ å½»åº•é‡æ–°åˆå§‹åŒ–æ‰€æœ‰æ¡ä»¶ç›¸å…³å±‚ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´
        self._reinitialize_conditional_layers()
        
        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥adaLNç›¸å…³æƒé‡çš„ç»´åº¦
        print("\nğŸ” è°ƒè¯•ï¼šæ£€æŸ¥å…³é”®å±‚çš„ç»´åº¦")
        for name, param in self.dit.named_parameters():
            if 'adaLN_modulation' in name and 'weight' in name:
                print(f"   {name}: {param.shape}")
            elif 'y_embedder' in name:
                print(f"   {name}: {param.shape}")
                
    def _reinitialize_conditional_layers(self):
        """é‡æ–°åˆå§‹åŒ–æ‰€æœ‰æ¡ä»¶ç›¸å…³å±‚ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´æ€§"""
        print("ğŸ”§ é‡æ–°åˆå§‹åŒ–æ¡ä»¶ç›¸å…³å±‚...")
        
        # 1. é‡æ–°åˆå§‹åŒ–y_embedder (å·²ç»æ­£ç¡®è®¾ç½®ä¸ºnum_users)
        # è¿™ä¸ªåº”è¯¥å·²ç»æ˜¯æ­£ç¡®çš„ï¼Œä½†ç¡®ä¿åˆå§‹åŒ–
        if hasattr(self.dit, 'y_embedder'):
            nn.init.normal_(self.dit.y_embedder.embedding_table.weight, std=0.02)
            print(f"   é‡æ–°åˆå§‹åŒ– y_embedder: {self.dit.y_embedder.embedding_table.weight.shape}")
        
        # 2. é‡æ–°åˆå§‹åŒ–æ‰€æœ‰adaLN_modulationå±‚
        adaLN_count = 0
        for name, module in self.dit.named_modules():
            if 'adaLN_modulation' in name and isinstance(module, nn.Linear):
                # ç¡®ä¿è¿™äº›å±‚æ¥å—æ­£ç¡®çš„è¾“å…¥ç»´åº¦ (hidden_size=1152)
                expected_in_features = self.dit.hidden_size  # 1152
                if module.in_features != expected_in_features:
                    print(f"   âš ï¸ å‘ç°ç»´åº¦ä¸åŒ¹é…çš„adaLNå±‚: {name}")
                    print(f"      å½“å‰: {module.in_features} -> æœŸæœ›: {expected_in_features}")
                    # é‡æ–°åˆ›å»ºè¿™ä¸ªå±‚
                    new_layer = nn.Linear(expected_in_features, module.out_features, bias=module.bias is not None)
                    # ç”¨æ–°å±‚æ›¿æ¢æ—§å±‚
                    parent_module = self.dit
                    for attr in name.split('.')[:-1]:
                        parent_module = getattr(parent_module, attr)
                    setattr(parent_module, name.split('.')[-1], new_layer)
                    adaLN_count += 1
                else:
                    # ç»´åº¦æ­£ç¡®ï¼Œåªéœ€é‡æ–°åˆå§‹åŒ–æƒé‡
                    nn.init.constant_(module.weight, 0)
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                    adaLN_count += 1
        
        print(f"   âœ… å¤„ç†äº† {adaLN_count} ä¸ªadaLNå±‚")
    
    def _freeze_backbone(self):
        """å†»ç»“ä¸»å¹²ç½‘ç»œ"""
        frozen_count = 0
        for name, param in self.dit.named_parameters():
            # ä¿æŒæœ€åå‡ å±‚å¯è®­ç»ƒï¼Œç”¨äºæ¡ä»¶é€‚é…
            if not any(keep in name for keep in ['final_layer', 'adaln']):
                param.requires_grad = False
                frozen_count += 1
        
        print(f"ğŸ”’ å†»ç»“å‚æ•°: {frozen_count}")
    
    def _inject_condition_layers(self):
        """ç®€åŒ–æ¶æ„ï¼šä¸ä¿®æ”¹DiTå†…éƒ¨ç»“æ„"""
        print("âœ… ä½¿ç”¨ç®€åŒ–æ¶æ„ï¼šä¿æŒLightningDiTæ ‡å‡†æ¥å£")
        print("ğŸ¯ ç”¨æˆ·æ¡ä»¶åˆ¤åˆ«å°†é€šè¿‡å¼ºåŒ–è®­ç»ƒç­–ç•¥å®ç°")
        # ä¸æ·»åŠ ä»»ä½•æ¡ä»¶æ³¨å…¥å±‚ï¼Œä¿æŒDiTåŸå§‹ç»“æ„
        pass
    
    def forward(self, x: torch.Tensor, t: torch.Tensor, user_classes: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: (B, C, H, W) æ½œå‘é‡
            t: (B,) æ—¶é—´æ­¥
            user_classes: (B,) ç”¨æˆ·ç±»åˆ« (0-30 å¯¹åº” ID_1 åˆ° ID_31)
        Returns:
            predicted_noise: (B, C, H, W) é¢„æµ‹çš„å™ªå£°
        """
        # ç¼–ç ä¸°å¯Œçš„ç”¨æˆ·æ¡ä»¶ç‰¹å¾
        user_condition = self.condition_encoder(user_classes)  # (B, condition_dim=768)
        
        # å°†ä¸°å¯Œçš„ç”¨æˆ·æ¡ä»¶æ³¨å…¥åˆ°DiTä¸­
        # æ–¹æ³•ï¼šæœ€ç®€åŒ– - è®©DiTä½¿ç”¨åŸç”Ÿæœºåˆ¶ï¼Œä¿å­˜user_conditionä¾›å¯¹æ¯”å­¦ä¹ ä½¿ç”¨
        predicted_noise = self._conditional_forward_with_injection(x, t, user_classes, user_condition)
        
        return predicted_noise
    
    def _conditional_forward_with_injection(self, x, t, user_classes, user_condition):
        """ç®€åŒ–å‰å‘ä¼ æ’­ï¼šä½¿ç”¨æ ‡å‡†LightningDiTæ¥å£"""
        # âœ… ä½¿ç”¨æ ‡å‡†DiTæ¥å£ï¼Œæ— æ¶æ„ä¿®æ”¹
        predicted_noise = self.dit(x, t, user_classes)
        
        # ä¿å­˜ç”¨æˆ·æ¡ä»¶ç”¨äºå¼ºåŒ–è®­ç»ƒç­–ç•¥
        self._last_user_condition = user_condition
        
        return predicted_noise

class ConditionalDiTTrainer:
    """æ¡ä»¶DiTè®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        
        # KaggleåŒGPUæ”¯æŒï¼šä½¿ç”¨DataParallel
        self.gpu_count = torch.cuda.device_count()
        self.use_multi_gpu = self.gpu_count > 1
        
        if self.use_multi_gpu:
            # KaggleåŒGPUç¯å¢ƒï¼šä½¿ç”¨DataParallel
            self.device = torch.device('cuda:0')  # ä¸»è®¾å¤‡
            print(f"Kaggleç¯å¢ƒæ£€æµ‹åˆ°{self.gpu_count}ä¸ªGPUï¼Œå¯ç”¨DataParallel")
        else:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ä½¿ç”¨å•GPU/CPU: {self.device}")
        
        # ğŸ›¡ï¸ ä¼˜å…ˆä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼ˆç§»é™¤æ½œåœ¨ä¸ç¨³å®šä¼˜åŒ–ï¼‰
        if torch.cuda.is_available():
            print("ğŸ›¡ï¸ ä¼˜å…ˆç¨³å®šæ€§ï¼šä½¿ç”¨é»˜è®¤CUDNNè®¾ç½®ï¼Œç¡®ä¿è®­ç»ƒå¯é æ€§")
        else:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ä½¿ç”¨å•GPU/CPU: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._setup_model()
        
        # ğŸš€ KaggleåŒGPUï¼šåŒ…è£…æ¨¡å‹ä¸ºDataParallel
        if self.use_multi_gpu:
            self.model = DP(self.model)  # DataParallelè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
            print(f"âœ… æ¨¡å‹å·²åŒ…è£…ä¸ºDataParallelï¼Œä½¿ç”¨GPU: {list(range(self.gpu_count))}")
        
        # åˆå§‹åŒ–æ•°æ®
        self._setup_data()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._setup_optimizer()
        
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
    
    def _get_model_attr(self, attr_name: str):
        """ğŸš€ ç»Ÿä¸€çš„æ¨¡å‹å±æ€§è®¿é—®ï¼šæ”¯æŒDataParallelå’Œæ™®é€šæ¨¡å‹"""
        if self.use_multi_gpu and hasattr(self.model, 'module'):
            return getattr(self.model.module, attr_name)
        else:
            return getattr(self.model, attr_name)
    
    def _get_actual_model(self):
        """ğŸš€ è·å–å®é™…æ¨¡å‹ï¼šæ”¯æŒDataParallelå’Œæ™®é€šæ¨¡å‹"""
        if self.use_multi_gpu and hasattr(self.model, 'module'):
            return self.model.module
        else:
            return self.model
    
    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        model_config = self.config['model']['params']
        
        # VAE - ä½¿ç”¨æ­£ç¡®çš„VA_VAEç±»å’ŒAPI
        self.vae = VA_VAE("LightningDiT/tokenizer/configs/vavae_f16d32.yaml")
        # VA_VAEå·²ç»åœ¨åˆå§‹åŒ–æ—¶è‡ªåŠ¨ç§»åŠ¨åˆ°GPUå’Œè®¾ç½®evalæ¨¡å¼
        
        # æ¡ä»¶DiT
        self.model = ConditionalDiT(
            **model_config,
            pretrained_path="models/lightningdit-xl-imagenet256-64ep.pt"
        )
        self.model.to(self.device)
        
        print(f"âœ… æ¨¡å‹è®¾ç½®å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def _setup_data(self):
        """è®¾ç½®æ•°æ®"""
        data_config = self.config['data']['params']
        
        self.data_module = MicroDopplerDataModule(**data_config)
        self.data_module.setup()
        
        # æ›´æ–°æ¨¡å‹çš„ç”¨æˆ·æ•°é‡
        actual_num_users = self.data_module.num_users
        if actual_num_users != self._get_model_attr('num_users'):
            print(f"âš ï¸ æ›´æ–°ç”¨æˆ·æ•°é‡: {self._get_model_attr('num_users')} -> {actual_num_users}")
            # è¿™é‡Œå¯èƒ½éœ€è¦é‡æ–°åˆå§‹åŒ–æ¡ä»¶ç¼–ç å™¨
    
    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        opt_config = self.config['optimizer']['params']
        
        # åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        self.optimizer = torch.optim.AdamW(trainable_params, **opt_config)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if 'scheduler' in self.config:
            sch_config = self.config['scheduler']['params']
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, **sch_config
            )
        else:
            self.scheduler = None
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        # åˆ›å»ºå®éªŒç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.exp_dir = Path(f"experiments/conditional_dit_{timestamp}")
        self.exp_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        with open(self.exp_dir / "config.yaml", 'w') as f:
            yaml.dump(self.config, f)
        
        print(f"âœ… å®éªŒç›®å½•: {self.exp_dir}")
    
    def compute_loss(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """è®¡ç®—æŸå¤± - å¢å¼ºç‰ˆï¼šåŒ…å«å¯¹æ¯”å­¦ä¹ ç”¨äºç”¨æˆ·åŒºåˆ†"""
        images = batch['image'].to(self.device)
        user_classes = batch['user_class'].to(self.device)
        
        batch_size = images.shape[0]
        
        # VAEç¼–ç  - ä½¿ç”¨æ­£ç¡®çš„VA_VAE API
        with torch.no_grad():
            z = self.vae.encode_images(images)  # VA_VAEç›´æ¥è¿”å›æ½œå‘é‡å¼ é‡
        
        # æ·»åŠ å™ªå£°ï¼ˆDDPMè®­ç»ƒï¼‰
        noise = torch.randn_like(z)
        timesteps = torch.randint(0, 1000, (batch_size,), device=self.device)
        
        # å‰å‘è¿‡ç¨‹: z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * noise
        # ç®€åŒ–å®ç°ï¼Œä½¿ç”¨å›ºå®šçš„å™ªå£°è°ƒåº¦
        alpha_bar_t = 1 - timesteps.float() / 1000
        alpha_bar_t = alpha_bar_t.view(batch_size, 1, 1, 1)
        
        z_noisy = noise * (timesteps.view(-1, 1, 1, 1) / 1000.0) + z * (1 - timesteps.view(-1, 1, 1, 1) / 1000.0)
        
        # ğŸš€ è·å–å®é™…æ¨¡å‹ï¼ˆDataParallelå…¼å®¹ï¼‰
        actual_model = self._get_actual_model()
        
        # ğŸš€ å…³é”®ä¿®å¤ï¼šç¡®ä¿ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨å‚ä¸æ¢¯åº¦è®¡ç®—
        user_embeddings = actual_model.condition_encoder(user_classes)  # (B, 1152) âœ… DataParallelä¿®å¤
        
        # æ¨¡å‹é¢„æµ‹ï¼ˆä¼ é€’ç”¨æˆ·åµŒå…¥ä»¥ç¡®ä¿æ¢¯åº¦æµï¼‰
        predicted_noise = self.model(z_noisy, timesteps, user_classes)
        
        # 1. ğŸ¯ åŸºç¡€æ‰©æ•£é‡æ„æŸå¤±
        diffusion_loss = F.mse_loss(predicted_noise, noise)
        
        # 2. ğŸš€ å¼ºåŒ–è®­ç»ƒç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨ç”¨æˆ·åµŒå…¥ï¼ˆä¿æŒæ¢¯åº¦æµï¼‰
        user_condition = user_embeddings  # âœ… ç›´æ¥ä½¿ç”¨ï¼Œä¿æŒæ¢¯åº¦è¿æ¥
        
        # 2.1 å¼ºåŒ–ç”¨æˆ·åˆ¤åˆ«ï¼šå¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.compute_enhanced_contrastive_loss(user_condition, user_classes)
        
        # 2.2 ç”¨æˆ·é—´åˆ¤åˆ«æŸå¤±ï¼šç¡®ä¿ä¸åŒç”¨æˆ·ç”Ÿæˆä¸åŒå™ªå£°
        inter_user_loss = self.compute_inter_user_discriminative_loss(predicted_noise, user_classes)
        
        # 2.3 æ¸è¿›å¼æƒé‡è°ƒæ•´ï¼šéšè®­ç»ƒè¿›ç¨‹åŠ å¼ºç”¨æˆ·åˆ¤åˆ«
        epoch_ratio = min(getattr(self, 'current_epoch', 0) / 20, 1.0)
        contrastive_weight = 0.05 + 0.15 * epoch_ratio  # ä»0.05é€æ¸å¢åŠ åˆ°0.2
        
        # 2.4 æ­£åˆ™åŒ–ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šç”¨æˆ·
        user_regularization = self.compute_user_regularization_loss(user_condition)
        
        # 3. ğŸ¯ å¼ºåŒ–è®­ç»ƒç­–ç•¥æ€»æŸå¤±
        total_loss = (
            diffusion_loss +                              # åŸºç¡€é‡å»º
            contrastive_weight * contrastive_loss +       # ç”¨æˆ·åˆ¤åˆ«ï¼ˆæ¸è¿›åŠ å¼ºï¼‰
            0.1 * inter_user_loss +                       # ç”¨æˆ·é—´å·®å¼‚
            0.02 * user_regularization                    # æ­£åˆ™åŒ–
        )
        
        # ğŸš€ KaggleåŒGPUæŸå¤±å¤„ç†ï¼šDataParallelè‡ªåŠ¨å¹³å‡
        # DataParallelä¼šè‡ªåŠ¨å¤„ç†æŸå¤±èšåˆï¼Œæ— éœ€æ‰‹åŠ¨åŒæ­¥
        diffusion_loss_val = diffusion_loss.item()
        contrastive_loss_val = contrastive_loss.item()
        inter_user_loss_val = inter_user_loss.item() 
        user_regularization_val = user_regularization.item()
        total_loss_val = total_loss.item()
        
        # è®°å½•å„é¡¹æŸå¤±ç”¨äºç›‘æ§
        self.log_losses = {
            'diffusion_loss': diffusion_loss_val,
            'contrastive_loss': contrastive_loss_val,
            'inter_user_loss': inter_user_loss_val,
            'user_regularization': user_regularization_val,
            'contrastive_weight': contrastive_weight,
            'total_loss': total_loss_val,
            'gpu_count': self.gpu_count,
            'multi_gpu': self.use_multi_gpu
        }
        
        return total_loss
    
    def compute_enhanced_contrastive_loss(self, user_condition, user_classes):
        """ğŸš€ å¼ºåŒ–å¯¹æ¯”å­¦ä¹ ï¼šé’ˆå¯¹æ•°æ®ç¨€ç¼º+å¾®å¦™å·®å¼‚ä¼˜åŒ–"""
        if user_condition is None:
            return torch.tensor(0.0, device=self.device)
            
        batch_size = user_condition.shape[0]
        if batch_size < 2:
            return torch.tensor(0.0, device=user_condition.device)
        
        # L2æ ‡å‡†åŒ–ï¼ˆæé«˜åˆ¤åˆ«ç¨³å®šæ€§ï¼‰
        user_condition_norm = F.normalize(user_condition, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.mm(user_condition_norm, user_condition_norm.t())
        
        # æ¸©åº¦å‚æ•°ï¼šé’ˆå¯¹å¾®å¦™å·®å¼‚è°ƒä½æ¸©åº¦ï¼Œå¢å¼ºæ•æ„Ÿæ€§
        temperature = 0.05  # æ¯”æ ‡å‡†SimCLRæ›´ä½ï¼Œå¢å¼ºå¾®å¦™å·®å¼‚æ•æ„Ÿæ€§
        sim_matrix = sim_matrix / temperature
        
        # æ­£æ ·æœ¬maskï¼šç›¸åŒç”¨æˆ·
        labels = user_classes.unsqueeze(1) == user_classes.unsqueeze(0)
        mask = torch.eye(batch_size, device=labels.device).bool()
        labels = labels & ~mask
        
        # InfoNCEæŸå¤±è®¡ç®—
        losses = []
        for i in range(batch_size):
            positive_mask = labels[i]
            if positive_mask.sum() == 0:
                continue
                
            # åˆ†å­ï¼šæ­£æ ·æœ¬ç›¸ä¼¼åº¦
            pos_sim = sim_matrix[i][positive_mask]
            # åˆ†æ¯ï¼šæ‰€æœ‰è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
            neg_sim = sim_matrix[i][~positive_mask]
            
            # InfoNCEæŸå¤±ï¼šæ­£æ ·æœ¬åœ¨å‰ï¼Œè´Ÿæ ·æœ¬åœ¨å
            logits = torch.cat([pos_sim, neg_sim])
            # æ ‡ç­¾ï¼šæ­£æ ·æœ¬çš„ç´¢å¼•ï¼ˆ0åˆ°len(pos_sim)-1éƒ½æ˜¯æ­£æ ·æœ¬ï¼‰
            # ä½†cross_entropyéœ€è¦å•ä¸ªæ ‡ç­¾ï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ªæ­£æ ·æœ¬ä½œä¸ºç›®æ ‡
            target_label = torch.tensor(0, device=logits.device, dtype=torch.long)  # ç¬¬ä¸€ä¸ªæ­£æ ·æœ¬
            loss = F.cross_entropy(logits.unsqueeze(0), target_label.unsqueeze(0))
            losses.append(loss)
        
        return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=user_condition.device)
    
    def compute_inter_user_discriminative_loss(self, predicted_noise, user_classes):
        """ğŸš€ ç”¨æˆ·é—´åˆ¤åˆ«æŸå¤±ï¼šç¡®ä¿ä¸åŒç”¨æˆ·ç”Ÿæˆä¸åŒçš„å™ªå£°æ¨¡å¼"""
        batch_size = predicted_noise.shape[0]
        if batch_size < 2:
            return torch.tensor(0.0, device=predicted_noise.device)
        
        # å°†é¢„æµ‹å™ªå£°å±•å¹³ä¸ºç‰¹å¾å‘é‡
        noise_features = predicted_noise.reshape(batch_size, -1)  # (B, C*H*W) - ä½¿ç”¨reshapeé¿å…strideé—®é¢˜
        
        # è®¡ç®—ç”¨æˆ·é—´çš„å™ªå£°ç›¸ä¼¼åº¦
        unique_users = torch.unique(user_classes)
        if len(unique_users) < 2:
            return torch.tensor(0.0, device=predicted_noise.device)
        
        inter_user_similarities = []
        for i, user_a in enumerate(unique_users):
            for user_b in unique_users[i+1:]:
                mask_a = user_classes == user_a
                mask_b = user_classes == user_b
                
                if mask_a.sum() == 0 or mask_b.sum() == 0:
                    continue
                
                # è®¡ç®—ä¸åŒç”¨æˆ·é—´çš„å¹³å‡å™ªå£°ç›¸ä¼¼åº¦
                noise_a = noise_features[mask_a].mean(dim=0)
                noise_b = noise_features[mask_b].mean(dim=0)
                
                similarity = F.cosine_similarity(noise_a.unsqueeze(0), noise_b.unsqueeze(0))
                inter_user_similarities.append(similarity)
        
        if not inter_user_similarities:
            return torch.tensor(0.0, device=predicted_noise.device)
        
        # æŸå¤±ï¼šæœ€å°åŒ–ç”¨æˆ·é—´ç›¸ä¼¼åº¦ï¼ˆé¼“åŠ±å·®å¼‚åŒ–ï¼‰
        avg_similarity = torch.stack(inter_user_similarities).mean()
        # ğŸ”§ ä¿®å¤ï¼šå°†ç›¸ä¼¼åº¦æ˜ å°„åˆ°[0,1]å†è®¡ç®—æŸå¤±ï¼Œé¿å…è´Ÿå€¼è¢«ReLUæˆªæ–­
        similarity_01 = (avg_similarity + 1.0) / 2.0  # ä»[-1,1]æ˜ å°„åˆ°[0,1]
        return similarity_01  # ç›¸ä¼¼åº¦è¶Šé«˜ï¼ŒæŸå¤±è¶Šå¤§
    
    def compute_user_regularization_loss(self, user_condition):
        """ğŸ”§ ä¿®å¤ï¼šç”¨æˆ·æ­£åˆ™åŒ–æŸå¤±ï¼ˆé˜²æ­¢æ•°å€¼çˆ†ç‚¸ï¼‰"""
        if user_condition is None:
            return torch.tensor(0.0, device=self.device)
        
        # L2æ­£åˆ™åŒ–ï¼šé˜²æ­¢åµŒå…¥è¿‡å¤§ï¼ˆä½¿ç”¨å¹³æ–¹è€Œä¸æ˜¯èŒƒæ•°ï¼Œé¿å…çˆ†ç‚¸ï¼‰
        l2_reg = torch.mean(user_condition.pow(2))
        
        # å¤šæ ·æ€§æ­£åˆ™åŒ–ï¼šé¼“åŠ±ä¸åŒç”¨æˆ·åµŒå…¥åˆ†æ•£ï¼ˆé™åˆ¶æ•°å€¼èŒƒå›´ï¼‰
        if user_condition.shape[0] > 1:
            # è®¡ç®—åµŒå…¥çš„æ ‡å‡†å·®ï¼Œé¼“åŠ±å¤šæ ·æ€§
            user_std = torch.std(user_condition, dim=0).mean()
            # ğŸ”§ ä¿®å¤ï¼šé¿å…expçˆ†ç‚¸ï¼Œä½¿ç”¨çº¿æ€§æƒ©ç½š
            diversity_reg = torch.clamp(1.0 / (user_std + 1e-6), 0.0, 10.0)
        else:
            diversity_reg = torch.tensor(0.0, device=user_condition.device)
        
        return l2_reg + 0.01 * diversity_reg  # ğŸ”§ é™ä½å¤šæ ·æ€§æƒé‡
    
    def _compute_contrastive_loss(self, user_embeddings: torch.Tensor, user_classes: torch.Tensor) -> torch.Tensor:
        """ç®€åŒ–çš„å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼šéµå¾ªSimCLRåŸåˆ™ï¼Œé¿å…memory bankå¤æ‚æ€§"""
        batch_size = user_embeddings.shape[0]
        
        if batch_size < 2:
            return torch.tensor(0.0, device=user_embeddings.device)
        
        # === é‡‡ç”¨SimCLRé£æ ¼çš„ç®€åŒ–å¯¹æ¯”å­¦ä¹  ===
        # æ ¸å¿ƒæ€æƒ³ï¼šåœ¨batchå†…è¿›è¡Œå……åˆ†çš„å¯¹æ¯”ï¼Œé…åˆgradient accumulationæ¨¡æ‹Ÿå¤§batch
        contrastive_loss = self._compute_simclr_style_contrastive_loss(user_embeddings, user_classes)
        
        return contrastive_loss
    
    def _compute_simclr_style_contrastive_loss(self, user_embeddings: torch.Tensor, user_classes: torch.Tensor) -> torch.Tensor:
        """SimCLRé£æ ¼çš„ç®€åŒ–å¯¹æ¯”å­¦ä¹  - é¿å…memory bankçš„å¤æ‚æ€§å’Œé™ˆæ—§æ€§é—®é¢˜"""
        batch_size = user_embeddings.shape[0]
        
        if batch_size < 2:
            return torch.tensor(0.0, device=user_embeddings.device)
        
        # L2æ ‡å‡†åŒ–åµŒå…¥ï¼ˆSimCLRçš„å…³é”®å®è·µï¼‰
        user_embeddings = F.normalize(user_embeddings, p=2, dim=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        similarities = torch.mm(user_embeddings, user_embeddings.t())  # (B, B)
        
        # æ¸©åº¦å‚æ•°ï¼ˆSimCLRå»ºè®®0.07-0.1ï¼‰
        temperature = 0.07
        similarities = similarities / temperature
        
        # åˆ›å»ºæ­£æ ·æœ¬maskï¼šç›¸åŒç”¨æˆ·ä¸ºæ­£æ ·æœ¬
        labels = user_classes.unsqueeze(1) == user_classes.unsqueeze(0)  # (B, B)
        # æ’é™¤è‡ªå·±ä¸è‡ªå·±çš„å¯¹ï¼ˆå¯¹è§’çº¿ï¼‰
        mask = torch.eye(batch_size, device=labels.device).bool()
        labels = labels & ~mask
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„InfoNCEæŸå¤±
        losses = []
        for i in range(batch_size):
            # å½“å‰æ ·æœ¬çš„æ­£æ ·æœ¬ï¼ˆç›¸åŒç”¨æˆ·çš„å…¶ä»–æ ·æœ¬ï¼‰
            positive_mask = labels[i]
            
            if positive_mask.sum() == 0:
                # å¦‚æœbatchä¸­æ²¡æœ‰ç›¸åŒç”¨æˆ·çš„å…¶ä»–æ ·æœ¬ï¼Œè·³è¿‡
                continue
                
            # æ­£æ ·æœ¬åˆ†æ•°
            pos_scores = similarities[i][positive_mask]  # (num_positives,)
            
            # è´Ÿæ ·æœ¬åˆ†æ•°ï¼ˆæ‰€æœ‰å…¶ä»–æ ·æœ¬ï¼Œé™¤äº†è‡ªå·±ï¼‰
            neg_mask = ~positive_mask & ~mask[i]  # æ’é™¤æ­£æ ·æœ¬å’Œè‡ªå·±
            neg_scores = similarities[i][neg_mask]  # (num_negatives,)
            
            # InfoNCEæŸå¤±è®¡ç®—
            pos_exp = torch.exp(pos_scores)
            neg_exp = torch.exp(neg_scores)
            
            # å¯¹æ¯ä¸ªæ­£æ ·æœ¬è®¡ç®—æŸå¤±
            for pos_exp_single in pos_exp:
                denominator = pos_exp_single + neg_exp.sum()
                loss_single = -torch.log(pos_exp_single / denominator + 1e-8)
                losses.append(loss_single)
        
        if len(losses) == 0:
            return torch.tensor(0.0, device=user_embeddings.device)
        
        return torch.stack(losses).mean()
    
    def _compute_cross_user_contrastive_loss(self, user_embeddings: torch.Tensor, user_classes: torch.Tensor) -> torch.Tensor:
        """å…¨ç”¨æˆ·è´Ÿé‡‡æ ·å¯¹æ¯”å­¦ä¹  - ä½ æå‡ºçš„å¢å¼ºæ–¹æ³•"""
        if not hasattr(self, '_negative_user_bank'):
            return torch.tensor(0.0, device=user_embeddings.device)
        
        batch_size = user_embeddings.shape[0]
        total_loss = 0.0
        
        # å¯¹batchä¸­æ¯ä¸ªæ ·æœ¬è¿›è¡Œå…¨ç”¨æˆ·å¯¹æ¯”
        for i in range(batch_size):
            current_user = user_classes[i].item()
            current_embedding = user_embeddings[i]  # (768,)
            
            # === æ­£æ ·æœ¬ï¼šä»negative bankä¸­å–è¯¥ç”¨æˆ·çš„å…¶ä»–æ ·æœ¬ ===
            if current_user in self._negative_user_bank:
                positive_embeddings = self._negative_user_bank[current_user]  # (N_pos, 768)
                if positive_embeddings.shape[0] > 0:
                    pos_similarities = torch.mm(current_embedding.unsqueeze(0), positive_embeddings.t())  # (1, N_pos)
                    pos_similarities = pos_similarities / 0.1  # temperature
                    positive_score = torch.exp(pos_similarities).sum()
                else:
                    positive_score = torch.tensor(1e-8, device=user_embeddings.device)
            else:
                positive_score = torch.tensor(1e-8, device=user_embeddings.device)
            
            # === è´Ÿæ ·æœ¬ï¼šä»negative bankä¸­å–å…¶ä»–æ‰€æœ‰ç”¨æˆ·çš„æ ·æœ¬ ===
            negative_score = torch.tensor(0.0, device=user_embeddings.device)
            for other_user, other_embeddings in self._negative_user_bank.items():
                if other_user != current_user and other_embeddings.shape[0] > 0:
                    # éšæœºé‡‡æ ·ä¸€äº›è´Ÿæ ·æœ¬ï¼Œé¿å…è®¡ç®—é‡è¿‡å¤§
                    n_neg_samples = min(5, other_embeddings.shape[0])  # æ¯ä¸ªç”¨æˆ·é‡‡æ ·5ä¸ªè´Ÿæ ·æœ¬
                    indices = torch.randperm(other_embeddings.shape[0])[:n_neg_samples]
                    sampled_negatives = other_embeddings[indices]  # (n_neg_samples, 768)
                    
                    neg_similarities = torch.mm(current_embedding.unsqueeze(0), sampled_negatives.t())  # (1, n_neg_samples)
                    neg_similarities = neg_similarities / 0.1  # temperature
                    negative_score += torch.exp(neg_similarities).sum()
            
            # InfoNCEæŸå¤±ï¼šlog(æ­£æ ·æœ¬å¾—åˆ† / (æ­£æ ·æœ¬å¾—åˆ† + è´Ÿæ ·æœ¬å¾—åˆ†))
            if negative_score > 0:
                sample_loss = -torch.log(positive_score / (positive_score + negative_score + 1e-8))
                total_loss += sample_loss
        
        return total_loss / batch_size if batch_size > 0 else torch.tensor(0.0, device=user_embeddings.device)
    
    def _update_negative_user_bank(self, user_embeddings: torch.Tensor, user_classes: torch.Tensor):
        """ç»´æŠ¤ç”¨æˆ·åµŒå…¥é“¶è¡Œï¼Œç”¨äºè·¨batchçš„å…¨ç”¨æˆ·å¯¹æ¯”å­¦ä¹ """
        if not hasattr(self, '_negative_user_bank'):
            self._negative_user_bank = {}
        
        # å°†å½“å‰batchçš„åµŒå…¥æ·»åŠ åˆ°å¯¹åº”ç”¨æˆ·çš„é“¶è¡Œä¸­
        for i, user_class in enumerate(user_classes):
            user_id = user_class.item()
            embedding = user_embeddings[i].detach().clone()  # é¿å…æ¢¯åº¦ä¼ æ’­
            
            if user_id not in self._negative_user_bank:
                self._negative_user_bank[user_id] = []
            
            # ç»´æŠ¤æ¯ä¸ªç”¨æˆ·æœ€å¤š50ä¸ªåµŒå…¥æ ·æœ¬ï¼ˆå†…å­˜æ§åˆ¶ï¼‰
            if len(self._negative_user_bank[user_id]) >= 50:
                # éšæœºæ›¿æ¢ä¸€ä¸ªè€æ ·æœ¬
                replace_idx = torch.randint(0, len(self._negative_user_bank[user_id]), (1,)).item()
                self._negative_user_bank[user_id][replace_idx] = embedding
            else:
                self._negative_user_bank[user_id].append(embedding)
        
        # è½¬æ¢ä¸ºtensoræ ¼å¼ä¾¿äºè®¡ç®—
        for user_id in self._negative_user_bank:
            if isinstance(self._negative_user_bank[user_id], list):
                self._negative_user_bank[user_id] = torch.stack(self._negative_user_bank[user_id])
    
    def _compute_regularization_loss(self, user_embeddings: torch.Tensor, user_classes: torch.Tensor) -> torch.Tensor:
        """æ­£åˆ™åŒ–æŸå¤±ï¼šç¡®ä¿ä¸åŒç”¨æˆ·çš„åµŒå…¥åœ¨ç©ºé—´ä¸­åˆ†å¸ƒå‡åŒ€"""
        unique_users = torch.unique(user_classes)
        
        if len(unique_users) < 2:
            return torch.tensor(0.0, device=user_embeddings.device)
        
        # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„å¹³å‡åµŒå…¥
        user_centers = []
        for user in unique_users:
            mask = user_classes == user
            user_center = user_embeddings[mask].mean(dim=0)
            user_centers.append(user_center)
        
        user_centers = torch.stack(user_centers)  # (num_users, embed_dim)
        
        # è®¡ç®—ç”¨æˆ·ä¸­å¿ƒé—´çš„æœ€å°è·ç¦»
        distances = torch.cdist(user_centers, user_centers, p=2)  # (num_users, num_users)
        
        # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„è·ç¦»ï¼‰
        mask = ~torch.eye(len(unique_users), dtype=torch.bool, device=distances.device)
        min_distance = distances[mask].min()
        
        # æ­£åˆ™åŒ–ï¼šé¼“åŠ±ç”¨æˆ·ä¸­å¿ƒé—´ä¿æŒæœ€å°è·ç¦»
        target_distance = 1.0  # ç›®æ ‡æœ€å°è·ç¦»
        regularization_loss = F.relu(target_distance - min_distance)
        
        return regularization_loss
    
    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        train_loader = self.data_module.train_dataloader()
        total_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}")
        
        for batch_idx, batch in enumerate(progress_bar):
            # å‰å‘ä¼ æ’­
            loss = self.compute_loss(batch)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.get('trainer', {}).get('gradient_clip_val'):
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['trainer']['gradient_clip_val']
                )
            
            self.optimizer.step()
            
            # æ›´æ–°ç»Ÿè®¡
            total_loss += loss.item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'avg_loss': f"{total_loss/num_batches:.4f}",
                'lr': f"{self.optimizer.param_groups[0]['lr']:.2e}"
            })
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def validate(self):
        """éªŒè¯"""
        self.model.eval()
        
        val_loader = self.data_module.val_dataloader()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                loss = self.compute_loss(batch)
                total_loss += loss.item()
                num_batches += 1
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def generate_validation_samples(self, epoch: int, num_samples: int = 8):
        """ğŸ–¼ï¸ ç”ŸæˆéªŒè¯æ ·æœ¬è¿›è¡Œå¯è§†åŒ–"""
        if not hasattr(self, 'vae'):
            return
            
        self.model.eval()
        
        try:
            # é€‰æ‹©ä¸åŒçš„ç”¨æˆ·è¿›è¡Œç”Ÿæˆæµ‹è¯• (ä½¿ç”¨0-basedç´¢å¼•ï¼Œå¯¹åº”ID_1åˆ°ID_31)
            test_users = torch.tensor([0, 4, 9, 14, 19, 24, 29, 30], device=self.device)[:num_samples]
            
            with torch.no_grad():
                # ç”Ÿæˆéšæœºå™ªå£°
                z_shape = (len(test_users), 32, 16, 16)  # VA-VAEæ½œå‘é‡å½¢çŠ¶
                z = torch.randn(z_shape, device=self.device)
                t = torch.randint(0, 1000, (len(test_users),), device=self.device)
                
                # ç”¨æˆ·æ¡ä»¶ç”Ÿæˆ
                generated_z = self.model(z, t, test_users)
                
                # è§£ç ä¸ºå›¾åƒ
                generated_images = self.vae.decode_to_images(generated_z)
                
                # ä¿å­˜å¯è§†åŒ–ç»“æœ
                import matplotlib.pyplot as plt
                fig, axes = plt.subplots(2, 4, figsize=(16, 8))
                fig.suptitle(f'Epoch {epoch} - User-Conditional Generation')
                
                for i, (user_idx, img) in enumerate(zip(test_users, generated_images)):
                    row, col = i // 4, i % 4
                    if isinstance(img, torch.Tensor):
                        img = img.detach().cpu().numpy()
                    if img.ndim == 3:
                        img = img.transpose(1, 2, 0)
                    axes[row, col].imshow(img, cmap='viridis')
                    # æ˜¾ç¤ºå®é™…ç”¨æˆ·ID (0-basedç´¢å¼•+1 = 1-basedç”¨æˆ·ID)
                    actual_user_id = user_idx.item() + 1
                    axes[row, col].set_title(f'User ID_{actual_user_id}')
                    axes[row, col].axis('off')
                
                # ä¿å­˜å›¾ç‰‡
                save_path = self.exp_dir / f"validation_samples_epoch_{epoch:03d}.png"
                plt.savefig(save_path, dpi=150, bbox_inches='tight')
                plt.close()
                print(f"ğŸ“¸ å·²ä¿å­˜éªŒè¯æ ·æœ¬: {save_path}")
                
        except Exception as e:
            print(f"âš ï¸ ç”ŸæˆéªŒè¯æ ·æœ¬å¤±è´¥: {e}")
        finally:
            self.model.train()
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, self.exp_dir / "last.ckpt")
        
        # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        if is_best:
            torch.save(checkpoint, self.exp_dir / "best.ckpt")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        trainer_config = self.config.get('trainer', {})
        max_epochs = trainer_config.get('max_epochs', 50)
        val_every_n_epochs = trainer_config.get('check_val_every_n_epoch', 2)
        
        best_val_loss = float('inf')
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…±{max_epochs}ä¸ªepoch")
        
        for epoch in range(max_epochs):
            print(f"\nğŸ“… Epoch {epoch+1}/{max_epochs}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)
            print(f"ğŸ¯ è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            
            # éªŒè¯
            if (epoch + 1) % val_every_n_epochs == 0:
                val_loss = self.validate()
                print(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.4f}")
                
                # ğŸ–¼ï¸ ç”Ÿæˆå¯è§†åŒ–æ ·æœ¬ï¼ˆæ¯10ä¸ªepochï¼‰
                if (epoch + 1) % 10 == 0:
                    self.generate_validation_samples(epoch + 1)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                is_best = val_loss < best_val_loss
                if is_best:
                    best_val_loss = val_loss
                    print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                
                self.save_checkpoint(epoch, is_best)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                self.scheduler.step()
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"ğŸ“ å®éªŒç›®å½•: {self.exp_dir}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_dir', type=str, help='æ•°æ®ç›®å½•è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # è¦†ç›–æ•°æ®ç›®å½•
    if args.data_dir:
        config['data']['params']['data_dir'] = args.data_dir
    
    print("ğŸš€ æ­¥éª¤5: æ¡ä»¶DiTå¾®è°ƒè®­ç»ƒ")
    print("="*60)
    print(f"ğŸ“ é…ç½®æ–‡ä»¶: {args.config}")
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥é…ç½®åŠ è½½
    print(f"ğŸ” è°ƒè¯• - åŠ è½½çš„æ•°æ®è·¯å¾„: {config['data']['params']['data_dir']}")
    print(f"ğŸ” è°ƒè¯• - å®Œæ•´æ•°æ®é…ç½®: {config['data']['params']}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ConditionalDiTTrainer(config)
    trainer.train()

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
æ­¥éª¤3: LightningDiTåŸºç¡€æ¨ç†æµ‹è¯•ä¸ç¯å¢ƒéªŒè¯
éªŒè¯æ‰€æœ‰æ¨¡å‹å¯æ­£å¸¸åŠ è½½ï¼Œè¿›è¡ŒåŸºç¡€æ¨ç†æµ‹è¯•
"""

import os
import torch
import torch.nn as nn
from pathlib import Path
import numpy as np
from PIL import Image
import sys
import traceback

def test_environment():
    """æµ‹è¯•åŸºç¡€ç¯å¢ƒ"""
    print("ğŸ” æµ‹è¯•åŸºç¡€ç¯å¢ƒ...")
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.get_device_name()}")
        print(f"ğŸ“Š GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    
    # æ£€æŸ¥å¿…è¦ç›®å½•
    required_dirs = ["models", "LightningDiT"]
    for dir_name in required_dirs:
        if Path(dir_name).exists():
            print(f"âœ… ç›®å½•å­˜åœ¨: {dir_name}")
        else:
            print(f"âŒ ç›®å½•ç¼ºå¤±: {dir_name}")
            return False
    
    return True

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        # æ·»åŠ LightningDiTåˆ°Pythonè·¯å¾„
        sys.path.append(str(Path("LightningDiT").absolute()))
        
        print("ğŸ“¥ åŠ è½½VA-VAE...")
        from tokenizer.vavae import VA_VAE
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        vavae_config = "LightningDiT/tokenizer/configs/vavae_f16d32.yaml"
        if not Path(vavae_config).exists():
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {vavae_config}")
            return False
            
        # åŠ è½½VA-VAE
        vae = VA_VAE(vavae_config)
        print("âœ… VA-VAEåŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•VA-VAEæ¨ç†
        print("ğŸ§ª æµ‹è¯•VA-VAEç¼–ç è§£ç ...")
        with torch.no_grad():
            # åˆ›å»ºæµ‹è¯•å›¾åƒ (batch_size=1, channels=3, height=256, width=256)
            test_img = torch.randn(1, 3, 256, 256)
            
            # ç¼–ç 
            z = vae.encode_images(test_img)
            print(f"âœ… ç¼–ç æˆåŠŸï¼Œæ½œå‘é‡å½¢çŠ¶: {z.shape}")
            
            # è§£ç 
            decoded_images = vae.decode_to_images(z)
            print(f"âœ… è§£ç æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {decoded_images.shape}")
        
        print("ğŸ“¥ åŠ è½½DiTæ¨¡å‹...")
        from models.lightningdit import LightningDiT_models
        
        # åŠ è½½DiTæ¨¡å‹æ¶æ„ - ä¸¥æ ¼æŒ‰ç…§å®˜æ–¹é…ç½®å‚æ•°
        dit = LightningDiT_models["LightningDiT-XL/1"](
            input_size=16,           # 16 = 256/16 (VA-VAEä¸‹é‡‡æ ·ç‡)
            in_channels=32,          # VA-VAEæ½œå‘é‡é€šé“æ•°
            use_qknorm=False,        # å®˜æ–¹é…ç½®
            use_swiglu=True,         # å®˜æ–¹é…ç½®
            use_rope=True,           # å®˜æ–¹é…ç½®
            use_rmsnorm=True,        # å®˜æ–¹é…ç½®
            wo_shift=False           # å®˜æ–¹é…ç½®
        )
        print(f"âœ… DiTæ¶æ„åˆ›å»ºæˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in dit.parameters()):,}")
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        checkpoint_path = "models/lightningdit-xl-imagenet256-64ep.pt"
        if not Path(checkpoint_path).exists():
            print(f"âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return False
            
        print("ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡...")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # æ£€æŸ¥checkpointå†…å®¹
        if isinstance(checkpoint, dict):
            if 'ema' in checkpoint:
                state_dict = checkpoint['ema']
                print("âœ… ä½¿ç”¨EMAæƒé‡")
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
                print("âœ… ä½¿ç”¨æ¨¡å‹æƒé‡")
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                print("âœ… ä½¿ç”¨çŠ¶æ€å­—å…¸")
            else:
                state_dict = checkpoint
                print("âœ… ä½¿ç”¨ç›´æ¥æƒé‡")
        else:
            state_dict = checkpoint
        
        # è¿‡æ»¤ä¸åŒ¹é…çš„é”®
        model_keys = set(dit.state_dict().keys())
        checkpoint_keys = set(state_dict.keys())
        
        # ç§»é™¤ä¸åŒ¹é…çš„é”®
        filtered_state_dict = {k: v for k, v in state_dict.items() if k in model_keys}
        missing_keys = model_keys - checkpoint_keys
        unexpected_keys = checkpoint_keys - model_keys
        
        if missing_keys:
            print(f"âš ï¸ ç¼ºå¤±é”®: {len(missing_keys)}ä¸ª")
        if unexpected_keys:
            print(f"âš ï¸ é¢å¤–é”®: {len(unexpected_keys)}ä¸ª")
            
        dit.load_state_dict(filtered_state_dict, strict=False)
        print("âœ… DiTæƒé‡åŠ è½½æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
        return False

def test_basic_inference():
    """æµ‹è¯•åŸºç¡€æ¨ç†"""
    print("\nğŸ” æµ‹è¯•åŸºç¡€æ¨ç†...")
    
    try:
        sys.path.append(str(Path("LightningDiT").absolute()))
        
        from tokenizer.vavae import VA_VAE
        from models.lightningdit import LightningDiT_models
        
        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨æ­£ç¡®çš„APIå’Œå®Œæ•´é…ç½®
        vae = VA_VAE("LightningDiT/tokenizer/configs/vavae_f16d32.yaml")
        dit = LightningDiT_models["LightningDiT-XL/1"](
            input_size=16,           
            in_channels=32,          
            use_qknorm=False,        
            use_swiglu=True,         
            use_rope=True,           
            use_rmsnorm=True,        
            wo_shift=False           
        )
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load("models/lightningdit-xl-imagenet256-64ep.pt", map_location='cpu')
        if isinstance(checkpoint, dict) and 'ema' in checkpoint:
            state_dict = checkpoint['ema']
        else:
            state_dict = checkpoint
        
        dit.load_state_dict(state_dict, strict=False)
        
        # è®¾ç½®è¯„ä¼°æ¨¡å¼
        vae.model.eval()  # VA_VAEæ˜¯åŒ…è£…å™¨ï¼ŒçœŸæ­£çš„æ¨¡å‹åœ¨.modelå±æ€§ä¸­
        dit.eval()
        
        print("ğŸ¯ æ‰§è¡Œç«¯åˆ°ç«¯æ¨ç†...")
        with torch.no_grad():
            # 1. åˆ›å»ºéšæœºå™ªå£°ä½œä¸ºèµ·ç‚¹
            batch_size = 1
            latent_size = 16  # 256 / 16
            noise = torch.randn(batch_size, 32, latent_size, latent_size)  # 32æ˜¯VA-VAEçš„æ½œå‘é‡ç»´åº¦
            
            # 2. åˆ›å»ºæ—¶é—´æ­¥
            t = torch.randint(0, 1000, (batch_size,))
            
            # 3. åˆ›å»ºéšæœºç±»æ ‡ç­¾ (ImageNetæœ‰1000ä¸ªç±»)
            y = torch.randint(0, 1000, (batch_size,))
            
            # 4. DiTé¢„æµ‹å™ªå£° (éœ€è¦ä¼ å…¥x, t, yä¸‰ä¸ªå‚æ•°)
            predicted_noise = dit(noise, t, y)
            print(f"âœ… DiTæ¨ç†æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {predicted_noise.shape}")
            print(f"âœ… ä½¿ç”¨ç±»æ ‡ç­¾: {y.item()}, æ—¶é—´æ­¥: {t.item()}")
            
            # 4. ç®€å•å»å™ªï¼ˆè¿™é‡Œåªæ˜¯æ¼”ç¤ºï¼Œå®é™…éœ€è¦å®Œæ•´çš„DDPMé‡‡æ ·ï¼‰
            denoised = noise - predicted_noise * 0.1
            
            # 5. VA-VAEè§£ç 
            decoded_image = vae.decode_to_images(denoised)
            print(f"âœ… ç«¯åˆ°ç«¯æ¨ç†æˆåŠŸï¼Œæœ€ç»ˆå›¾åƒå½¢çŠ¶: {decoded_image.shape}")
            
            # 6. ä¿å­˜æµ‹è¯•å›¾åƒ
            output_dir = Path("test_outputs")
            output_dir.mkdir(exist_ok=True)
            
            # è½¬æ¢ä¸ºå›¾åƒæ ¼å¼  
            # VA_VAE.decode_to_imagesè¿”å›numpyæ•°ç»„ï¼Œä¸æ˜¯tensor
            if isinstance(decoded_image, np.ndarray):
                image = decoded_image[0]  # numpyæ•°ç»„ï¼Œæ— éœ€.cpu()
            else:
                image = decoded_image[0].cpu().numpy()  # å¦‚æœæ˜¯tensorï¼Œè½¬ä¸ºnumpy
            
            image = (image + 1) / 2  # ä»[-1,1]è½¬æ¢åˆ°[0,1]
            image = np.clip(image, 0, 1)  # numpyçš„clipæ›¿ä»£tensorçš„clamp
            image = (image * 255).astype(np.uint8)  # numpyçš„astypeæ›¿ä»£tensorçš„byte
            
            # è½¬æ¢ä¸ºPILå›¾åƒ (imageå·²ç»æ˜¯numpyæ•°ç»„)
            if image.shape[0] == 3:  # RGB: (C, H, W) -> (H, W, C)
                image_np = np.transpose(image, (1, 2, 0))  # numpyçš„transposeæ›¿ä»£tensorçš„permute
                pil_image = Image.fromarray(image_np)
            else:  # ç°åº¦å›¾
                image_np = image[0]  # ç›´æ¥å–ç¬¬ä¸€ä¸ªé€šé“ï¼Œå·²ç»æ˜¯numpyæ•°ç»„
                pil_image = Image.fromarray(image_np, mode='L')
            
            output_path = output_dir / "test_generation.png"
            pil_image.save(output_path)
            print(f"âœ… æµ‹è¯•å›¾åƒå·²ä¿å­˜: {output_path}")
            
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        print("é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ­¥éª¤3: LightningDiTåŸºç¡€æ¨ç†æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•ç¯å¢ƒ
    if not test_environment():
        print("âŒ ç¯å¢ƒæµ‹è¯•å¤±è´¥")
        return False
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    if not test_model_loading():
        print("âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥")
        return False
    
    # æµ‹è¯•åŸºç¡€æ¨ç†
    if not test_basic_inference():
        print("âŒ æ¨ç†æµ‹è¯•å¤±è´¥")
        return False
    
    print("\n" + "="*60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LightningDiTç¯å¢ƒéªŒè¯æˆåŠŸ")
    print("âœ… å¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥ï¼šmicro-Doppleræ•°æ®é€‚é…")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

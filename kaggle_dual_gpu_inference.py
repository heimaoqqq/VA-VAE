#!/usr/bin/env python3
"""
KaggleåŒGPUæ¨ç†åŒ…è£…å™¨
åŸºäºåŸé¡¹ç›®inference.pyçš„åˆ†å¸ƒå¼æ¨ç†æ–¹æ³•
"""

import os
import sys
import subprocess
from accelerate import notebook_launcher

def setup_paths():
    """è®¾ç½®Pythonè·¯å¾„"""
    print("ğŸ”§ è®¾ç½®æ¨ç†è·¯å¾„...")
    
    # è·å–å½“å‰å·¥ä½œç›®å½•
    if '/kaggle/working' in os.getcwd():
        base_dir = '/kaggle/working/VA-VAE'
    else:
        base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æ·»åŠ å¿…è¦çš„è·¯å¾„
    paths_to_add = [
        os.path.join(base_dir, 'LightningDiT'),
        base_dir
    ]
    
    for path in reversed(paths_to_add):
        if path not in sys.path:
            sys.path.insert(0, path)
            print(f"âœ… å·²æ·»åŠ è·¯å¾„: {path}")

def kaggle_dual_gpu_inference():
    """åŒGPUæ¨ç†å‡½æ•°"""
    
    def inference_worker():
        # è®¾ç½®è·¯å¾„
        setup_paths()
        
        # å¯¼å…¥Accelerator
        from accelerate import Accelerator
        import torch
        
        # åˆå§‹åŒ–Accelerator
        accelerator = Accelerator()
        
        print(f"ğŸ”§ æ¨ç†è¿›ç¨‹ {accelerator.process_index}/{accelerator.num_processes}")
        print(f"ğŸ”§ è®¾å¤‡: {accelerator.device}")
        
        # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°
        sys.argv = [
            'stage3_inference.py',
            '--dit_checkpoint', '/kaggle/working/trained_models/best_model',
            '--vavae_config', 'vavae_config.yaml',
            '--output_dir', '/kaggle/working/generated_images',
            '--user_ids', '1', '2', '3', '4', '5',
            '--num_samples_per_user', '4',
            '--seed', '42'
        ]
        
        # å¯¼å…¥å¹¶è¿è¡Œæ¨ç†
        from stage3_inference_distributed import main
        main(accelerator)
    
    # ä½¿ç”¨notebook_launcherå¯åŠ¨åŒGPUæ¨ç†
    print("ğŸš€ å¯åŠ¨åŒGPUæ¨ç†...")
    notebook_launcher(inference_worker, num_processes=2)
    print("âœ… åŒGPUæ¨ç†å®Œæˆ")

if __name__ == "__main__":
    kaggle_dual_gpu_inference()

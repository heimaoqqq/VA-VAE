#!/usr/bin/env python3
"""
ä½¿ç”¨åŸå‹æ ¡å‡†ï¼ˆPNCï¼‰è¯„ä¼°åŸŸé€‚åº”æ•ˆæœ
å¯¹æ¯”åŸºçº¿æ–¹æ³•ä¸åŸå‹æ ¡å‡†æ–¹æ³•åœ¨èƒŒåŒ…æ­¥æ€è¯†åˆ«ä¸Šçš„æ€§èƒ½
é€‚é…Kaggleç¯å¢ƒ
"""

import torch
import argparse
from pathlib import Path
import json
import numpy as np
from tabulate import tabulate
import sys
sys.path.append(str(Path(__file__).parent.parent))
from cross_domain_evaluator import CrossDomainEvaluator


def evaluate_prototype_calibration(args):
    """è¯„ä¼°åŸå‹æ ¡å‡†çš„æ•ˆæœ"""
    
    print("\n" + "="*80)
    print("ğŸ”¬ PROTOTYPE CALIBRATION EVALUATION")
    print("="*80)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CrossDomainEvaluator()
    
    # åŠ è½½åˆ†ç±»å™¨
    print(f"\nğŸ“¦ Loading classifier: {args.model_path}")
    model, checkpoint = evaluator.load_classifier(args.model_path)
    
    # 1. åŸºçº¿è¯„ä¼°ï¼ˆä¸ä½¿ç”¨åŸå‹ï¼‰
    print("\n" + "-"*60)
    print("ğŸ“Š BASELINE EVALUATION (without prototypes)")
    print("-"*60)
    baseline_results = evaluator.evaluate_on_target_domain(
        model=model,
        target_data_dir=args.data_dir,
        batch_size=args.batch_size,
        use_prototypes=False
    )
    
    # 2. åŸå‹æ ¡å‡†è¯„ä¼°ï¼ˆä½¿ç”¨åŸå‹ï¼‰
    print("\n" + "-"*60)
    print("ğŸ¯ PROTOTYPE CALIBRATION EVALUATION")
    print("-"*60)
    pnc_results = evaluator.evaluate_on_target_domain(
        model=model,
        target_data_dir=args.data_dir,
        batch_size=args.batch_size,
        use_prototypes=True,
        prototype_path=args.prototype_path,
        fusion_alpha=args.fusion_alpha,
        similarity_tau=args.similarity_tau
    )
    
    # 3. å¯¹æ¯”åˆ†æ
    print("\n" + "="*80)
    print("ğŸ“ˆ COMPARISON RESULTS")
    print("="*80)
    
    if baseline_results and pnc_results:
        # å‡†ç¡®ç‡å¯¹æ¯”
        baseline_acc = baseline_results['overall_accuracy']
        pnc_acc = pnc_results['overall_accuracy']
        improvement = pnc_acc - baseline_acc
        
        # ç½®ä¿¡åº¦å¯¹æ¯”
        baseline_conf = baseline_results['confidence_stats']['mean_confidence']
        pnc_conf = pnc_results['confidence_stats']['mean_confidence']
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_table = [
            ["Metric", "Baseline", "PNC", "Improvement"],
            ["Overall Accuracy", f"{baseline_acc:.2%}", f"{pnc_acc:.2%}", f"{improvement:+.2%}"],
            ["Mean Confidence", f"{baseline_conf:.3f}", f"{pnc_conf:.3f}", f"{pnc_conf-baseline_conf:+.3f}"],
            ["Total Samples", baseline_results['total_samples'], pnc_results['total_samples'], "-"],
        ]
        
        print(tabulate(comparison_table, headers="firstrow", tablefmt="grid"))
        
        # ç”¨æˆ·çº§åˆ«åˆ†æ
        if 'per_user_accuracy' in baseline_results and 'per_user_accuracy' in pnc_results:
            print("\nğŸ“Š PER-USER ACCURACY ANALYSIS:")
            
            user_improvements = []
            for user_id in baseline_results['per_user_accuracy']:
                if user_id in pnc_results['per_user_accuracy']:
                    baseline_user = baseline_results['per_user_accuracy'][user_id]
                    pnc_user = pnc_results['per_user_accuracy'][user_id]
                    improv = pnc_user - baseline_user
                    user_improvements.append((user_id, baseline_user, pnc_user, improv))
            
            # æ’åºï¼šæŒ‰æ”¹è¿›å¹…åº¦ä»å¤§åˆ°å°
            user_improvements.sort(key=lambda x: x[3], reverse=True)
            
            # æ˜¾ç¤ºå‰5ä¸ªæ”¹è¿›æœ€å¤§çš„ç”¨æˆ·
            print("\nTop 5 Most Improved Users:")
            top_table = [["User", "Baseline", "PNC", "Improvement"]]
            for user_id, base, pnc, improv in user_improvements[:5]:
                top_table.append([f"User_{user_id+1}", f"{base:.1%}", f"{pnc:.1%}", f"{improv:+.1%}"])
            print(tabulate(top_table, headers="firstrow", tablefmt="simple"))
            
            # ç»Ÿè®¡æ”¹è¿›æƒ…å†µ
            improvements = [x[3] for x in user_improvements]
            improved_count = sum(1 for x in improvements if x > 0)
            
            print(f"\nğŸ“ˆ IMPROVEMENT STATISTICS:")
            print(f"   â€¢ Users improved: {improved_count}/{len(user_improvements)} ({improved_count/len(user_improvements):.1%})")
            print(f"   â€¢ Mean improvement: {np.mean(improvements):+.2%}")
            print(f"   â€¢ Std improvement: {np.std(improvements):.2%}")
            print(f"   â€¢ Max improvement: {max(improvements):+.2%}")
            print(f"   â€¢ Min change: {min(improvements):+.2%}")
        
        # è¯„ä¼°ç»“è®º
        print("\n" + "="*80)
        if improvement > 0.05:
            assessment = "ğŸŸ¢ SIGNIFICANT IMPROVEMENT"
            message = "Prototype calibration provides substantial gains for domain adaptation"
        elif improvement > 0.02:
            assessment = "ğŸŸ¡ MODERATE IMPROVEMENT"
            message = "Prototype calibration offers meaningful benefits"
        elif improvement > 0:
            assessment = "ğŸŸ  SLIGHT IMPROVEMENT"
            message = "Prototype calibration provides marginal gains"
        else:
            assessment = "ğŸ”´ NO IMPROVEMENT"
            message = "Prototype calibration does not improve performance"
        
        print(f"ğŸ† ASSESSMENT: {assessment}")
        print(f"ğŸ’¡ {message}")
        
        # ä¿å­˜ç»“æœ
        if args.save_results:
            output_path = Path(args.output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            results = {
                'baseline': baseline_results,
                'pnc': pnc_results,
                'comparison': {
                    'accuracy_improvement': improvement,
                    'confidence_change': pnc_conf - baseline_conf,
                    'assessment': assessment,
                    'message': message
                },
                'config': {
                    'model_path': args.model_path,
                    'data_dir': args.data_dir,
                    'prototype_path': args.prototype_path,
                    'fusion_alpha': args.fusion_alpha,
                    'similarity_tau': args.similarity_tau
                }
            }
            
            # è½¬æ¢numpy int64é”®ä¸ºPython intä»¥æ”¯æŒJSONåºåˆ—åŒ–
            def convert_keys(obj):
                if isinstance(obj, dict):
                    return {str(k) if hasattr(k, 'item') else k: convert_keys(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_keys(item) for item in obj]
                else:
                    return obj
            
            serializable_results = convert_keys(results)
            save_path = output_path / 'prototype_calibration_results.json'
            with open(save_path, 'w') as f:
                json.dump(serializable_results, f, indent=2, default=str)
            
            print(f"\nğŸ’¾ Results saved to: {save_path}")
    
    else:
        print("âŒ Evaluation failed")
    
    print("="*80)


def grid_search_hyperparameters(args):
    """ç½‘æ ¼æœç´¢æœ€ä¼˜è¶…å‚æ•°"""
    
    print("\n" + "="*80)
    print("ğŸ” HYPERPARAMETER GRID SEARCH")
    print("="*80)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CrossDomainEvaluator()
    
    # åŠ è½½æ¨¡å‹
    model, _ = evaluator.load_classifier(args.model_path)
    
    # å®šä¹‰æœç´¢ç½‘æ ¼
    alphas = [0.2, 0.3, 0.4, 0.5, 0.6]
    taus = [0.05, 0.1, 0.15, 0.2]
    
    best_acc = 0
    best_params = {}
    results_grid = []
    
    print(f"\nSearching over {len(alphas)} alphas Ã— {len(taus)} taus = {len(alphas)*len(taus)} combinations")
    
    for alpha in alphas:
        for tau in taus:
            print(f"\nTesting Î±={alpha:.1f}, Ï„={tau:.2f}...")
            
            results = evaluator.evaluate_on_target_domain(
                model=model,
                target_data_dir=args.data_dir,
                batch_size=args.batch_size,
                use_prototypes=True,
                prototype_path=args.prototype_path,
                fusion_alpha=alpha,
                similarity_tau=tau
            )
            
            if results:
                acc = results['overall_accuracy']
                results_grid.append({
                    'alpha': alpha,
                    'tau': tau,
                    'accuracy': acc
                })
                
                if acc > best_acc:
                    best_acc = acc
                    best_params = {'alpha': alpha, 'tau': tau}
                
                print(f"   Accuracy: {acc:.2%}")
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "="*80)
    print("ğŸ† BEST PARAMETERS:")
    print(f"   â€¢ Alpha (Î±): {best_params['alpha']:.1f}")
    print(f"   â€¢ Tau (Ï„): {best_params['tau']:.2f}")
    print(f"   â€¢ Best Accuracy: {best_acc:.2%}")
    
    # åˆ›å»ºç»“æœè¡¨æ ¼
    print("\nğŸ“Š FULL RESULTS:")
    table_data = [["Î± \\ Ï„"] + [f"{tau:.2f}" for tau in taus]]
    for alpha in alphas:
        row = [f"{alpha:.1f}"]
        for tau in taus:
            acc = next((r['accuracy'] for r in results_grid 
                       if r['alpha'] == alpha and r['tau'] == tau), None)
            if acc:
                row.append(f"{acc:.1%}")
            else:
                row.append("-")
        table_data.append(row)
    
    print(tabulate(table_data, headers="firstrow", tablefmt="grid"))
    
    return best_params


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Evaluate prototype calibration for domain adaptation')
    
    # æ¨¡å‹å’Œæ•°æ®è·¯å¾„ï¼ˆKaggleç¯å¢ƒï¼‰
    parser.add_argument('--model-path', type=str,
                       default='/kaggle/input/best-calibrated-model-pth/best_calibrated_model.pth',
                       help='Path to trained classifier')
    parser.add_argument('--data-dir', type=str,
                       default='/kaggle/input/backpack/backpack',
                       help='Path to target domain data (èƒŒåŒ…æ­¥æ€)')
    parser.add_argument('--prototype-path', type=str,
                       default='/kaggle/working/target_prototypes.pt',
                       help='Path to computed prototypes')
    
    # åŸå‹æ ¡å‡†è¶…å‚æ•°
    parser.add_argument('--fusion-alpha', type=float, default=0.4,
                       help='Prototype fusion weight (0-1)')
    parser.add_argument('--similarity-tau', type=float, default=0.1,
                       help='Similarity temperature parameter')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--batch-size', type=int, default=32,
                       help='Batch size for evaluation')
    parser.add_argument('--grid-search', action='store_true',
                       help='Perform hyperparameter grid search')
    parser.add_argument('--save-results', action='store_true', default=True,
                       help='Save evaluation results')
    parser.add_argument('--output-dir', type=str, default='/kaggle/working/pnc_results',
                       help='Output directory for results')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥åŸå‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.prototype_path).exists():
        print(f"âš ï¸ Prototype file not found: {args.prototype_path}")
        print("Please run build_target_prototypes.py first to create prototypes")
        return
    
    # æ‰§è¡Œè¯„ä¼°
    if args.grid_search:
        # ç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°
        best_params = grid_search_hyperparameters(args)
        
        # ä½¿ç”¨æœ€ä¼˜å‚æ•°é‡æ–°è¯„ä¼°
        print("\n" + "="*80)
        print("ğŸ¯ FINAL EVALUATION WITH BEST PARAMETERS")
        args.fusion_alpha = best_params['alpha']
        args.similarity_tau = best_params['tau']
    
    # ä¸»è¯„ä¼°
    evaluate_prototype_calibration(args)


if __name__ == '__main__':
    main()

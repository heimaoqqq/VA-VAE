#!/usr/bin/env python3
"""
å¿«é€Ÿè¯„ä¼°è„šæœ¬
ç›´æ¥ä½¿ç”¨å·²æ‰¾åˆ°çš„æœ€ä½³é…ç½®è¿è¡Œ PNC+LCCS ç»„åˆè¯„ä¼°
"""

import torch
import pandas as pd
import json
from pathlib import Path
from tqdm import tqdm
from itertools import product
import sys
from torch.utils.data import DataLoader
import torchvision.transforms as transforms

sys.path.append(str(Path(__file__).parent))
from eval_utils import *
from eval_components import *
from strategic_dataset import StrategicDataset


def main():
    import argparse
    parser = argparse.ArgumentParser(description='ä½¿ç”¨æœ€ä½³é…ç½®å¿«é€Ÿè¯„ä¼°')
    parser.add_argument('--model-path', type=str,
                       default='/kaggle/working/VA-VAE/improved_classifier/best_improved_classifier.pth')
    parser.add_argument('--data-dir', type=str,
                       default='/kaggle/input/backpack/backpack')
    parser.add_argument('--output-dir', type=str,
                       default='./best_config_results')
    
    args = parser.parse_args()
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*80)
    print("ğŸš€ ä½¿ç”¨æœ€ä½³é…ç½®å¿«é€Ÿè¯„ä¼° PNC+LCCS")
    print("="*80)
    print(f"ğŸ’» è®¾å¤‡: {device}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_path}")
    
    # æœ€ä½³é…ç½®ï¼ˆä»ä¹‹å‰çš„ç»“æœä¸­è·å¾—ï¼‰
    best_strategy = 'diversity'
    best_pnc = {
        'prototype_strategy': 'simple_mean',
        'fusion_alpha': 0.7,
        'similarity_tau': 0.005,
        'use_adaptive': False
    }
    best_lccs = {
        'lccs_method': 'progressive',
        'momentum': 0.02,
        'iterations': 10
    }
    
    print(f"\nğŸ“‹ ä½¿ç”¨é…ç½®:")
    print(f"   æ ·æœ¬é€‰æ‹©ç­–ç•¥: {best_strategy}")
    print(f"   PNC åŸå‹ç­–ç•¥: {best_pnc['prototype_strategy']}")
    print(f"   PNC Fusion Î±: {best_pnc['fusion_alpha']}")
    print(f"   PNC Temperature Ï„: {best_pnc['similarity_tau']}")
    print(f"   LCCS æ–¹æ³•: {best_lccs['lccs_method']}")
    print(f"   LCCS Momentum: {best_lccs['momentum']}")
    print(f"   LCCS Iterations: {best_lccs['iterations']}")
    
    # 1. åŠ è½½æ¨¡å‹
    print("\n" + "="*80)
    print("ğŸ“¦ åŠ è½½åˆ†ç±»å™¨")
    print("="*80)
    model = load_classifier(args.model_path, device)
    model.device = device
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 2. PNC+LCCS ç»„åˆè¯„ä¼°
    print("\n" + "="*80)
    print("ğŸ¯ PNC+LCCS ç»„åˆè¯„ä¼°")
    print("="*80)
    
    results = []
    
    support_sizes = [3, 5, 10]
    seeds = [42, 123, 456]
    
    total = len(support_sizes) * len(seeds)
    
    print(f"æ€»å…± {total} ä¸ªå®éªŒ")
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    with tqdm(total=total, desc="PNC+LCCS") as pbar:
        for support_size, seed in product(support_sizes, seeds):
            print(f"\nğŸ“Š Support={support_size}, Seed={seed}")
            
            # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨æœ€ä½³ç­–ç•¥ï¼‰
            support_dataset = StrategicDataset(
                data_dir=args.data_dir,
                support_size=support_size,
                strategy=best_strategy,
                model=model,
                mode='support',
                seed=seed,
                device=device,
                transform=transform
            )
            
            test_dataset = StrategicDataset(
                data_dir=args.data_dir,
                support_size=support_size,
                strategy=best_strategy,
                model=model,
                mode='test',
                seed=seed,
                device=device,
                transform=transform
            )
            
            support_loader = DataLoader(
                support_dataset,
                batch_size=64,
                shuffle=False,
                num_workers=0
            )
            
            test_loader = DataLoader(
                test_dataset,
                batch_size=64,
                shuffle=False,
                num_workers=0
            )
            
            # åˆ›å»º LCCS é€‚é…å™¨
            lccs = LCCSAdapter(model, device)
            
            # æ­¥éª¤1ï¼šLCCS é€‚åº”
            lccs.adapt_progressive(
                support_loader,
                momentum=best_lccs['momentum'],
                iterations=best_lccs['iterations']
            )
            
            # æ­¥éª¤2ï¼šæ„å»ºåŸå‹ï¼ˆåœ¨ LCCS é€‚åº”åçš„æ¨¡å‹ä¸Šï¼‰
            if best_pnc['prototype_strategy'] == 'simple_mean':
                features, labels = extract_features(model, support_loader, device)
                prototypes = build_prototypes_simple_mean(features, labels)
            else:
                prototypes = build_prototypes_weighted(model, support_loader, device)
            
            # æ­¥éª¤3ï¼šPNC è¯„ä¼°
            pnc = PNCEvaluator(model, prototypes, device)
            acc, conf = pnc.predict(
                test_loader,
                fusion_alpha=best_pnc['fusion_alpha'],
                similarity_tau=best_pnc['similarity_tau'],
                use_adaptive=best_pnc['use_adaptive']
            )
            
            # ä¿å­˜ç»“æœ
            results.append({
                'support_size': support_size,
                'seed': seed,
                'strategy': best_strategy,
                'accuracy': acc,
                'confidence': conf
            })
            
            print(f"  å‡†ç¡®ç‡: {acc:.4f}, ç½®ä¿¡åº¦: {conf:.4f}")
            
            # æ¢å¤ BN ç»Ÿè®¡é‡
            lccs.restore_bn_stats()
            
            pbar.update(1)
    
    # ä¿å­˜ç»“æœ
    df = pd.DataFrame(results)
    df.to_csv(output_path / 'pnc_lccs_combined_results.csv', index=False)
    
    # ç”Ÿæˆæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š æœ€ç»ˆç»“æœæ€»ç»“")
    print("="*80)
    
    # æŒ‰ support_size åˆ†ç»„ç»Ÿè®¡
    print("\næŒ‰ Support Size åˆ†ç»„:")
    for size in [3, 5, 10]:
        subset = df[df['support_size'] == size]
        print(f"  Support={size}: "
              f"å‡†ç¡®ç‡={subset['accuracy'].mean():.4f} Â± {subset['accuracy'].std():.4f}, "
              f"æœ€å¤§={subset['accuracy'].max():.4f}")
    
    # æ€»ä½“ç»Ÿè®¡
    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {df['accuracy'].mean():.4f} Â± {df['accuracy'].std():.4f}")
    print(f"  æœ€å¤§å‡†ç¡®ç‡: {df['accuracy'].max():.4f}")
    print(f"  æœ€å°å‡†ç¡®ç‡: {df['accuracy'].min():.4f}")
    
    # æ‰¾æœ€ä½³é…ç½®
    best_row = df.loc[df['accuracy'].idxmax()]
    print(f"\nğŸ† æœ€ä½³é…ç½®:")
    print(f"  Support Size: {best_row['support_size']}")
    print(f"  Seed: {best_row['seed']}")
    print(f"  å‡†ç¡®ç‡: {best_row['accuracy']:.4f}")
    
    # ä¿å­˜æ€»ç»“
    summary = {
        'best_config': {
            'strategy': best_strategy,
            'pnc': best_pnc,
            'lccs': best_lccs
        },
        'results': {
            'mean_accuracy': float(df['accuracy'].mean()),
            'std_accuracy': float(df['accuracy'].std()),
            'max_accuracy': float(df['accuracy'].max()),
            'min_accuracy': float(df['accuracy'].min())
        },
        'by_support_size': {
            str(size): {
                'mean': float(df[df['support_size']==size]['accuracy'].mean()),
                'std': float(df[df['support_size']==size]['accuracy'].std()),
                'max': float(df[df['support_size']==size]['accuracy'].max())
            }
            for size in [3, 5, 10]
        }
    }
    
    with open(output_path / 'summary.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
    print(f"   - pnc_lccs_combined_results.csv")
    print(f"   - summary.json")
    
    print("\n" + "="*80)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("="*80)


if __name__ == '__main__':
    main() 

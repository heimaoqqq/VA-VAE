#!/usr/bin/env python3
"""
ä½¿ç”¨ImprovedClassifierçš„åŽŸåž‹æ ¡å‡†ï¼ˆPNCï¼‰è¯„ä¼°
é€‚é…improved_classifier_training.pyè®­ç»ƒçš„åˆ†ç±»å™¨
"""

import torch
import argparse
from pathlib import Path
import json
import numpy as np
from tabulate import tabulate
import sys
sys.path.append(str(Path(__file__).parent.parent))

# å¯¼å…¥ImprovedClassifier
from improved_classifier_training import ImprovedClassifier
from cross_domain_evaluator import CrossDomainEvaluator, BackpackWalkingDataset
from torch.utils.data import DataLoader
from tqdm import tqdm
import torchvision.transforms as transforms


class ImprovedClassifierEvaluator:
    """ä¸“ç”¨äºŽImprovedClassifierçš„åŽŸåž‹æ ¡å‡†è¯„ä¼°å™¨"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.test_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
    
    def load_improved_classifier(self, model_path):
        """åŠ è½½ImprovedClassifier"""
        print(f"ðŸ“¦ Loading ImprovedClassifier from: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # èŽ·å–æ¨¡åž‹é…ç½®
        num_classes = checkpoint.get('num_classes', 31)
        backbone = checkpoint.get('backbone', 'resnet18')
        
        # åˆ›å»ºæ¨¡åž‹
        model = ImprovedClassifier(
            num_classes=num_classes,
            backbone=backbone
        ).to(self.device)
        
        # åŠ è½½æƒé‡
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.eval()
        
        print(f"âœ… Loaded {backbone} with {num_classes} classes")
        if 'best_val_acc' in checkpoint:
            print(f"   Original validation accuracy: {checkpoint['best_val_acc']:.2f}%")
            
        return model
    
    def evaluate_on_target_domain(self, model, target_data_dir, batch_size=32, 
                                  use_prototypes=False, prototype_path=None,
                                  fusion_alpha=0.4, similarity_tau=0.1):
        """åœ¨ç›®æ ‡åŸŸè¯„ä¼°ImprovedClassifier"""
        mode = "with Prototype Calibration" if use_prototypes else "Baseline"
        print(f"\nðŸŽ¯ Evaluating ImprovedClassifier ({mode}): {target_data_dir}")
        
        # åˆ›å»ºæ•°æ®é›†
        target_dataset = BackpackWalkingDataset(
            data_dir=target_data_dir,
            transform=self.test_transform
        )
        
        if len(target_dataset) == 0:
            print("âŒ No target domain samples found!")
            return None
        
        target_loader = DataLoader(
            target_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # åŠ è½½åŽŸåž‹ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        prototypes = None
        if use_prototypes:
            if prototype_path is None:
                raise ValueError("Prototype path must be provided when use_prototypes=True")
            
            print(f"ðŸ“¦ Loading prototypes from: {prototype_path}")
            proto_dict = torch.load(prototype_path, map_location=self.device, weights_only=False)
            prototypes = proto_dict['prototypes'].to(self.device)
            print(f"âœ“ Loaded prototypes: {prototypes.shape}")
            print(f"   â€¢ Fusion weight (Î±): {fusion_alpha:.2f}")
            print(f"   â€¢ Temperature (Ï„): {similarity_tau:.2f}")
        
        # è¯„ä¼°
        all_predictions = []
        all_labels = []
        all_confidences = []
        
        model.eval()
        with torch.no_grad():
            for images, labels in tqdm(target_loader, desc="Evaluating"):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # ImprovedClassifierå‰å‘ä¼ æ’­
                outputs = model(images)
                
                if use_prototypes:
                    # æå–ç‰¹å¾ï¼šbackboneè¾“å‡º
                    features = model.backbone(images)
                    
                    # L2å½’ä¸€åŒ–ç‰¹å¾
                    features = features / features.norm(2, dim=1, keepdim=True)
                    
                    # è®¡ç®—ä¸ŽåŽŸåž‹çš„ä½™å¼¦ç›¸ä¼¼åº¦
                    similarities = torch.matmul(features, prototypes.T)
                    
                    # åº”ç”¨æ¸©åº¦ç¼©æ”¾å¹¶è½¬æ¢ä¸ºæ¦‚çŽ‡
                    proto_probs = torch.softmax(similarities / similarity_tau, dim=1)
                    
                    # åŽŸå§‹åˆ†ç±»æ¦‚çŽ‡
                    class_probs = torch.softmax(outputs, dim=1)
                    
                    # èžåˆä¸¤ç§æ¦‚çŽ‡åˆ†å¸ƒ
                    probabilities = (1 - fusion_alpha) * class_probs + fusion_alpha * proto_probs
                else:
                    # åŸºçº¿æ–¹æ³•ï¼šä»…ä½¿ç”¨åˆ†ç±»å™¨è¾“å‡º
                    probabilities = torch.softmax(outputs, dim=1)
                
                # èŽ·å–é¢„æµ‹å’Œç½®ä¿¡åº¦
                confidences, predictions = torch.max(probabilities, 1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_confidences.extend(confidences.cpu().numpy())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆä½¿ç”¨CrossDomainEvaluatorçš„æ–¹æ³•ï¼‰
        evaluator = CrossDomainEvaluator()
        results = evaluator._compute_metrics(all_labels, all_predictions, all_confidences)
        results['total_samples'] = len(all_labels)
        results['num_users'] = len(set(all_labels))
        
        return results


def build_prototypes_for_improved_classifier(model_path, data_dir, output_path, support_size=15):
    """ä¸ºImprovedClassifieræž„å»ºåŽŸåž‹"""
    print("\n" + "="*80)
    print("ðŸ”§ BUILDING PROTOTYPES FOR IMPROVEDCLASSIFIER")
    print("="*80)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åŠ è½½ImprovedClassifier
    evaluator = ImprovedClassifierEvaluator(device)
    model = evaluator.load_improved_classifier(model_path)
    
    # æ•°æ®å˜æ¢
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    # ä»Žbuild_target_prototypes.pyå¯¼å…¥æ•°æ®é›†
    sys.path.append(str(Path(__file__).parent))
    from build_target_prototypes import TargetDomainDataset
    
    # åˆ›å»ºæ”¯æŒé›†
    dataset = TargetDomainDataset(
        data_dir=data_dir,
        transform=transform,
        support_size=support_size
    )
    
    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)
    
    print(f"\nðŸ“Š Support set statistics:")
    print(f"   â€¢ Total samples: {len(dataset)}")
    print(f"   â€¢ Samples per user: {support_size}")
    
    # æå–ç‰¹å¾
    features = []
    labels = []
    
    model.eval()
    with torch.no_grad():
        for batch_images, batch_labels, batch_users in tqdm(dataloader, desc="Extracting features"):
            batch_images = batch_images.to(device)
            
            # ä½¿ç”¨backboneæå–ç‰¹å¾ï¼ˆImprovedClassifierç»“æž„ï¼‰
            feat = model.backbone(batch_images)
            
            features.append(feat.cpu())
            labels.extend(batch_labels.tolist())
    
    features = torch.cat(features, dim=0)
    print(f"âœ“ Extracted features: {features.shape}")
    
    # è®¡ç®—åŽŸåž‹
    num_classes = max(labels) + 1
    prototypes = []
    
    for class_id in range(num_classes):
        class_mask = [i for i, l in enumerate(labels) if l == class_id]
        if len(class_mask) == 0:
            prototypes.append(torch.zeros(features.shape[1]))
            continue
        
        class_features = features[class_mask]
        prototype = class_features.mean(dim=0)
        
        # L2å½’ä¸€åŒ–
        prototype = prototype / prototype.norm(2)
        prototypes.append(prototype)
    
    prototypes = torch.stack(prototypes)
    print(f"âœ“ Computed prototypes: {prototypes.shape}")
    
    # ä¿å­˜åŽŸåž‹
    save_dict = {
        'prototypes': prototypes,
        'user_ids': list(range(num_classes)),
        'feature_dim': prototypes.shape[1],
        'metadata': {
            'model_type': 'ImprovedClassifier',
            'support_size': support_size,
            'feature_extraction': 'backbone_output'
        }
    }
    
    torch.save(save_dict, output_path)
    print(f"ðŸ’¾ ImprovedClassifier prototypes saved to: {output_path}")
    
    return save_dict


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Evaluate ImprovedClassifier with PNC')
    
    parser.add_argument('--model-path', type=str,
                       default='/kaggle/input/best-classifier-model/best_model.pth',
                       help='Path to ImprovedClassifier model')
    parser.add_argument('--data-dir', type=str,
                       default='/kaggle/input/backpack/backpack',
                       help='Path to target domain data')
    parser.add_argument('--prototype-path', type=str,
                       default='/kaggle/working/improved_prototypes.pt',
                       help='Path to prototypes')
    parser.add_argument('--support-size', type=int, default=15,
                       help='Support set size for prototype building')
    parser.add_argument('--build-prototypes', action='store_true',
                       help='Build prototypes first')
    parser.add_argument('--fusion-alpha', type=float, default=0.4,
                       help='Prototype fusion weight')
    parser.add_argument('--similarity-tau', type=float, default=0.1,
                       help='Similarity temperature')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='Batch size for evaluation')
    
    args = parser.parse_args()
    
    # æž„å»ºåŽŸåž‹ï¼ˆå¦‚æžœéœ€è¦ï¼‰
    if args.build_prototypes:
        build_prototypes_for_improved_classifier(
            args.model_path, args.data_dir, args.prototype_path, args.support_size
        )
        print("\nâœ… Prototype building completed!")
    
    # æ£€æŸ¥åŽŸåž‹æ–‡ä»¶
    if not Path(args.prototype_path).exists():
        print(f"âš ï¸ Prototype file not found: {args.prototype_path}")
        print("Please run with --build-prototypes first")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ImprovedClassifierEvaluator()
    
    # åŠ è½½æ¨¡åž‹
    model = evaluator.load_improved_classifier(args.model_path)
    
    print("\n" + "="*80)
    print("ðŸ“Š IMPROVEDCLASSIFIER BASELINE vs PNC COMPARISON")
    print("="*80)
    
    # åŸºçº¿è¯„ä¼°
    print("\nðŸ“Š BASELINE EVALUATION")
    baseline_results = evaluator.evaluate_on_target_domain(
        model=model,
        target_data_dir=args.data_dir,
        batch_size=args.batch_size,
        use_prototypes=False
    )
    
    # PNCè¯„ä¼°
    print("\nðŸŽ¯ PROTOTYPE CALIBRATION EVALUATION")
    pnc_results = evaluator.evaluate_on_target_domain(
        model=model,
        target_data_dir=args.data_dir,
        batch_size=args.batch_size,
        use_prototypes=True,
        prototype_path=args.prototype_path,
        fusion_alpha=args.fusion_alpha,
        similarity_tau=args.similarity_tau
    )
    
    # ç»“æžœå¯¹æ¯”
    if baseline_results and pnc_results:
        baseline_acc = baseline_results['overall_accuracy']
        pnc_acc = pnc_results['overall_accuracy']
        improvement = pnc_acc - baseline_acc
        
        print("\n" + "="*80)
        print("ðŸ“ˆ COMPARISON RESULTS")
        print("="*80)
        
        comparison_table = [
            ["Metric", "Baseline", "PNC", "Improvement"],
            ["Overall Accuracy", f"{baseline_acc:.2%}", f"{pnc_acc:.2%}", f"{improvement:+.2%}"],
            ["Mean Confidence", 
             f"{baseline_results['confidence_stats']['mean_confidence']:.3f}",
             f"{pnc_results['confidence_stats']['mean_confidence']:.3f}",
             f"{pnc_results['confidence_stats']['mean_confidence'] - baseline_results['confidence_stats']['mean_confidence']:+.3f}"]
        ]
        
        print(tabulate(comparison_table, headers="firstrow", tablefmt="grid"))
        
        # è¯„ä¼°ç»“è®º
        if improvement > 0.05:
            assessment = "ðŸŸ¢ SIGNIFICANT IMPROVEMENT"
        elif improvement > 0.02:
            assessment = "ðŸŸ¡ MODERATE IMPROVEMENT"
        elif improvement > 0:
            assessment = "ðŸŸ  SLIGHT IMPROVEMENT"
        else:
            assessment = "ðŸ”´ NO IMPROVEMENT"
        
        print(f"\nðŸ† ASSESSMENT: {assessment}")
        print(f"ðŸ’¡ Improvement: {improvement:+.2%} on ImprovedClassifier baseline")
        
        # é¢„æµ‹åˆ†æž
        if baseline_acc >= 0.75:
            print(f"ðŸŽ¯ EXCELLENT! Starting from {baseline_acc:.1%} baseline, PNC achieved {pnc_acc:.1%}")
            print("   This demonstrates strong domain adaptation capabilities!")
    
    print("="*80)


if __name__ == '__main__':
    main()

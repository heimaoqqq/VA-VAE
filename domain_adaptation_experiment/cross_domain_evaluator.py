#!/usr/bin/env python3
"""
è·¨åŸŸè¯„ä¼°è„šæœ¬
æµ‹è¯•åˆ†ç±»å™¨åœ¨èƒŒåŒ…è¡Œèµ°æ•°æ®ï¼ˆç›®æ ‡åŸŸï¼‰ä¸Šçš„è¯†åˆ«æ€§èƒ½
ç”¨äºæ¯”è¾ƒåŸºçº¿åˆ†ç±»å™¨ vs ç”Ÿæˆæ•°æ®å¢å¼ºåˆ†ç±»å™¨çš„åŸŸæ³›åŒ–èƒ½åŠ›
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from PIL import Image
import numpy as np
import argparse
import json
from tqdm import tqdm
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥åˆ†ç±»å™¨
import sys
sys.path.append(str(Path(__file__).parent.parent))
from improved_classifier_training import ImprovedClassifier


class BackpackWalkingDataset(Dataset):
    """èƒŒåŒ…è¡Œèµ°æ•°æ®é›†ï¼ˆç›®æ ‡åŸŸï¼‰"""
    
    def __init__(self, data_dir, transform=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.samples = []
        
        # åŠ è½½èƒŒåŒ…è¡Œèµ°æ•°æ®
        self._load_backpack_data()
        
    def _load_backpack_data(self):
        """åŠ è½½èƒŒåŒ…è¡Œèµ°æ•°æ®"""
        if not self.data_dir.exists():
            raise FileNotFoundError(f"èƒŒåŒ…è¡Œèµ°æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        # æŸ¥æ‰¾ç”¨æˆ·ç›®å½• (ID_* æˆ– User_*)
        user_dirs = list(self.data_dir.glob("ID_*")) + list(self.data_dir.glob("User_*"))
        
        if not user_dirs:
            raise ValueError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°ç”¨æˆ·ç›®å½•")
        
        for user_dir in sorted(user_dirs):
            if user_dir.is_dir():
                # è§£æç”¨æˆ·ID
                if user_dir.name.startswith("ID_"):
                    user_id = int(user_dir.name.split('_')[1]) - 1  # è½¬æ¢ä¸º0-based
                elif user_dir.name.startswith("User_"):
                    user_id = int(user_dir.name.split('_')[1])
                else:
                    continue
                    
                # åŠ è½½è¯¥ç”¨æˆ·çš„æ‰€æœ‰èƒŒåŒ…è¡Œèµ°å›¾åƒ
                for ext in ['*.png', '*.jpg', '*.jpeg']:
                    for img_path in user_dir.glob(ext):
                        self.samples.append((str(img_path), user_id))
        
        print(f"Loaded {len(self.samples)} backpack walking samples from {len(set(s[1] for s in self.samples))} users")
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, label
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            # è¿”å›é›¶å¼ é‡ä½œä¸ºfallback
            return torch.zeros(3, 224, 224), label


class CrossDomainEvaluator:
    """è·¨åŸŸæ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # æµ‹è¯•æ•°æ®å˜æ¢ï¼ˆä¸è®­ç»ƒæ—¶éªŒè¯é›†ä¸€è‡´ï¼‰
        self.test_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
    def load_classifier(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„åˆ†ç±»å™¨"""
        print(f"Loading classifier from: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # è·å–æ¨¡å‹é…ç½®
        num_classes = checkpoint.get('num_classes', 31)
        model_name = checkpoint.get('model_name', 'resnet18')
        
        # åˆ›å»ºæ¨¡å‹
        model = ImprovedClassifier(
            num_classes=num_classes,
            backbone=model_name
        ).to(self.device)
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print(f"âœ… Loaded {model_name} with {num_classes} classes")
        if 'best_val_acc' in checkpoint:
            print(f"   Original validation accuracy: {checkpoint['best_val_acc']:.2f}%")
        
        return model, checkpoint
    
    def evaluate_on_target_domain(self, model, target_data_dir, batch_size=32):
        """åœ¨ç›®æ ‡åŸŸï¼ˆèƒŒåŒ…è¡Œèµ°ï¼‰æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹"""
        print(f"\nğŸ¯ Evaluating on target domain: {target_data_dir}")
        
        # åˆ›å»ºç›®æ ‡åŸŸæ•°æ®é›†
        target_dataset = BackpackWalkingDataset(
            data_dir=target_data_dir,
            transform=self.test_transform
        )
        
        if len(target_dataset) == 0:
            print("âŒ No target domain samples found!")
            return None
        
        target_loader = DataLoader(
            target_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # è¯„ä¼°
        all_predictions = []
        all_labels = []
        all_confidences = []
        
        model.eval()
        with torch.no_grad():
            for images, labels in tqdm(target_loader, desc="Evaluating"):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = model(images)
                probabilities = torch.softmax(outputs, dim=1)
                
                # è·å–é¢„æµ‹å’Œç½®ä¿¡åº¦
                confidences, predictions = torch.max(probabilities, 1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_confidences.extend(confidences.cpu().numpy())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        results = self._compute_metrics(all_labels, all_predictions, all_confidences)
        results['total_samples'] = len(all_labels)
        results['num_users'] = len(set(all_labels))
        
        return results
    
    def _compute_metrics(self, true_labels, predictions, confidences):
        """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
        # åŸºæœ¬å‡†ç¡®ç‡
        accuracy = accuracy_score(true_labels, predictions)
        
        # æŒ‰ç”¨æˆ·ç»Ÿè®¡
        user_accuracies = {}
        for user_id in set(true_labels):
            user_mask = np.array(true_labels) == user_id
            if np.sum(user_mask) > 0:
                user_acc = accuracy_score(
                    np.array(true_labels)[user_mask], 
                    np.array(predictions)[user_mask]
                )
                user_accuracies[user_id] = user_acc
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confidence_stats = {
            'mean_confidence': np.mean(confidences),
            'std_confidence': np.std(confidences),
            'correct_confidence': np.mean([conf for i, conf in enumerate(confidences) 
                                         if true_labels[i] == predictions[i]]),
            'incorrect_confidence': np.mean([conf for i, conf in enumerate(confidences) 
                                           if true_labels[i] != predictions[i]])
        }
        
        return {
            'overall_accuracy': accuracy,
            'user_accuracies': user_accuracies,
            'mean_user_accuracy': np.mean(list(user_accuracies.values())),
            'std_user_accuracy': np.std(list(user_accuracies.values())),
            'confidence_stats': confidence_stats,
            'classification_report': classification_report(
                true_labels, predictions, output_dict=True, zero_division=0
            )
        }
    
    def compare_models(self, baseline_results, enhanced_results):
        """æ¯”è¾ƒåŸºçº¿æ¨¡å‹å’Œå¢å¼ºæ¨¡å‹çš„æ€§èƒ½"""
        print("\n" + "="*80)
        print("ğŸ” CROSS-DOMAIN GENERALIZATION COMPARISON")
        print("="*80)
        
        # æ€»ä½“æ€§èƒ½å¯¹æ¯”
        baseline_acc = baseline_results['overall_accuracy']
        enhanced_acc = enhanced_results['overall_accuracy']
        improvement = enhanced_acc - baseline_acc
        
        print(f"\nğŸ“Š OVERALL PERFORMANCE:")
        print(f"   â€¢ Baseline (real data only):     {baseline_acc:.1%}")
        print(f"   â€¢ Enhanced (real + generated):   {enhanced_acc:.1%}")
        print(f"   â€¢ Improvement:                   {improvement:+.1%}")
        print(f"   â€¢ Relative improvement:          {improvement/baseline_acc:+.1%}")
        
        # ç”¨æˆ·çº§åˆ«æ€§èƒ½å¯¹æ¯”
        baseline_user_acc = baseline_results['mean_user_accuracy']
        enhanced_user_acc = enhanced_results['mean_user_accuracy']
        user_improvement = enhanced_user_acc - baseline_user_acc
        
        print(f"\nğŸ‘¤ USER-LEVEL PERFORMANCE:")
        print(f"   â€¢ Baseline mean user accuracy:   {baseline_user_acc:.1%}")
        print(f"   â€¢ Enhanced mean user accuracy:   {enhanced_user_acc:.1%}")
        print(f"   â€¢ Mean improvement per user:     {user_improvement:+.1%}")
        
        # ç½®ä¿¡åº¦å¯¹æ¯”
        baseline_conf = baseline_results['confidence_stats']['mean_confidence']
        enhanced_conf = enhanced_results['confidence_stats']['mean_confidence']
        
        print(f"\nğŸ¯ CONFIDENCE ANALYSIS:")
        print(f"   â€¢ Baseline mean confidence:      {baseline_conf:.3f}")
        print(f"   â€¢ Enhanced mean confidence:      {enhanced_conf:.3f}")
        print(f"   â€¢ Confidence change:             {enhanced_conf-baseline_conf:+.3f}")
        
        # ç»“æœè§£é‡Š
        if improvement > 0.05:  # 5%ä»¥ä¸Šæå‡
            grade = "ğŸŸ¢ SIGNIFICANT IMPROVEMENT"
            interpretation = "Generated data substantially improves cross-domain generalization"
        elif improvement > 0.02:  # 2%ä»¥ä¸Šæå‡
            grade = "ğŸŸ¡ MODERATE IMPROVEMENT" 
            interpretation = "Generated data provides modest cross-domain benefits"
        elif improvement > 0:
            grade = "ğŸŸ  SLIGHT IMPROVEMENT"
            interpretation = "Generated data provides minimal cross-domain benefits"
        else:
            grade = "ğŸ”´ NO IMPROVEMENT"
            interpretation = "Generated data does not improve cross-domain performance"
        
        print(f"\nğŸ† DOMAIN ADAPTATION ASSESSMENT: {grade}")
        print(f"ğŸ’¡ INTERPRETATION: {interpretation}")
        print("="*80)
        
        return {
            'baseline_accuracy': baseline_acc,
            'enhanced_accuracy': enhanced_acc,
            'absolute_improvement': improvement,
            'relative_improvement': improvement / baseline_acc,
            'grade': grade,
            'interpretation': interpretation
        }
    
    def save_detailed_results(self, results, output_path):
        """ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœ"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"ğŸ“„ Detailed results saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='Cross-domain evaluation on backpack walking data')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--baseline_model', required=True, 
                       help='Path to baseline classifier (trained on real data only)')
    parser.add_argument('--enhanced_model', required=True,
                       help='Path to enhanced classifier (trained on real + generated data)')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--backpack_data_dir', required=True,
                       help='Directory containing backpack walking data')
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument('--batch_size', type=int, default=32, help='Evaluation batch size')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', default='./cross_domain_results',
                       help='Output directory for results')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CrossDomainEvaluator()
    
    # è¯„ä¼°åŸºçº¿æ¨¡å‹
    print("ğŸ”µ Evaluating BASELINE classifier...")
    baseline_model, _ = evaluator.load_classifier(args.baseline_model)
    baseline_results = evaluator.evaluate_on_target_domain(
        baseline_model, args.backpack_data_dir, args.batch_size
    )
    
    # è¯„ä¼°å¢å¼ºæ¨¡å‹  
    print("\nğŸŸ¢ Evaluating ENHANCED classifier...")
    enhanced_model, _ = evaluator.load_classifier(args.enhanced_model)
    enhanced_results = evaluator.evaluate_on_target_domain(
        enhanced_model, args.backpack_data_dir, args.batch_size
    )
    
    # æ¯”è¾ƒç»“æœ
    comparison = evaluator.compare_models(baseline_results, enhanced_results)
    
    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    all_results = {
        'baseline_results': baseline_results,
        'enhanced_results': enhanced_results,
        'comparison': comparison,
        'evaluation_config': vars(args)
    }
    
    evaluator.save_detailed_results(all_results, output_dir / 'cross_domain_evaluation.json')
    
    print(f"\nâœ… Cross-domain evaluation completed!")
    print(f"ğŸ“ Results saved to: {output_dir}")


if __name__ == '__main__':
    main()

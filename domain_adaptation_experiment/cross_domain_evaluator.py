#!/usr/bin/env python3
"""
ç»¼åˆåŸŸé€‚åº”åˆ†æå·¥å…·
é›†æˆäº†åŸºæœ¬è¯„ä¼°å’Œé«˜çº§åˆ†æåŠŸèƒ½
ç ”ç©¶é—®é¢˜ï¼šæ­£å¸¸æ­¥æ€(æºåŸŸ) â†’ èƒŒåŒ…æ­¥æ€(ç›®æ ‡åŸŸ) çš„åŸŸé€‚åº”æ•ˆæœ
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from PIL import Image
import numpy as np
import argparse
import json
from tqdm import tqdm
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.manifold import TSNE
from scipy.stats import ttest_rel, mannwhitneyu
import sys
sys.path.append(str(Path(__file__).parent.parent))
from improved_classifier_training import ImprovedClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict

from train_calibrated_classifier import DomainAdaptiveClassifier


class BackpackWalkingDataset(Dataset):
    """èƒŒåŒ…è¡Œèµ°æ•°æ®é›†ï¼ˆç›®æ ‡åŸŸï¼‰"""
    
    def __init__(self, data_dir, transform=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.samples = []
        
        # åŠ è½½èƒŒåŒ…è¡Œèµ°æ•°æ®
        self._load_backpack_data()
        
    def _load_backpack_data(self):
        """åŠ è½½èƒŒåŒ…è¡Œèµ°æ•°æ®"""
        if not self.data_dir.exists():
            raise FileNotFoundError(f"èƒŒåŒ…è¡Œèµ°æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        # æŸ¥æ‰¾ç”¨æˆ·ç›®å½• (ID_* æˆ– User_*)
        user_dirs = list(self.data_dir.glob("ID_*")) + list(self.data_dir.glob("User_*"))
        
        if not user_dirs:
            raise ValueError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°ç”¨æˆ·ç›®å½•")
        
        for user_dir in sorted(user_dirs):
            if user_dir.is_dir():
                # è§£æç”¨æˆ·ID - ä¿æŒä¸è®­ç»ƒæ—¶ä¸€è‡´
                if user_dir.name.startswith("ID_"):
                    user_id = int(user_dir.name.split('_')[1]) - 1  # ID_æ ¼å¼è½¬æ¢ä¸º0-based
                elif user_dir.name.startswith("User_"):
                    user_id = int(user_dir.name.split('_')[1])      # User_æ ¼å¼å·²ç»æ˜¯0-based
                else:
                    continue
                    
                # åŠ è½½è¯¥ç”¨æˆ·çš„æ‰€æœ‰èƒŒåŒ…è¡Œèµ°å›¾åƒ
                for ext in ['*.png', '*.jpg', '*.jpeg']:
                    for img_path in user_dir.glob(ext):
                        self.samples.append((str(img_path), user_id))
        
        print(f"Loaded {len(self.samples)} backpack walking samples from {len(set(s[1] for s in self.samples))} users")
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, label
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            # è¿”å›é›¶å¼ é‡ä½œä¸ºfallback - å°ºå¯¸ä¸å®é™…å›¾åƒä¸€è‡´
            return torch.zeros(3, 256, 256), label


class CrossDomainEvaluator:
    """è·¨åŸŸæ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # æµ‹è¯•æ•°æ®å˜æ¢ï¼ˆä¸è®­ç»ƒæ—¶éªŒè¯é›†ä¸€è‡´ï¼‰
        self.test_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
    def load_classifier(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„åˆ†ç±»å™¨"""
        print(f"Loading classifier from: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # è·å–æ¨¡å‹é…ç½®
        num_classes = checkpoint.get('num_classes', 31)
        model_name = checkpoint.get('model_name', 'resnet18')
        
        # åˆ›å»ºæ¨¡å‹ - æ”¯æŒImprovedClassifier
        if 'projection_head.0.weight' in checkpoint['model_state_dict']:
            # ImprovedClassifieræ ¼å¼
            from improved_classifier_training import ImprovedClassifier
            model = ImprovedClassifier(
                num_classes=num_classes,
                backbone=checkpoint.get('backbone', 'resnet18')
            ).to(self.device)
            print(f"   Detected ImprovedClassifier format")
        else:
            # DomainAdaptiveClassifieræ ¼å¼
            model = DomainAdaptiveClassifier(
                num_classes=num_classes
            ).to(self.device)
            print(f"   Detected DomainAdaptiveClassifier format")
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print(f"âœ… Loaded {model_name} with {num_classes} classes")
        if 'best_val_acc' in checkpoint:
            print(f"   Original validation accuracy: {checkpoint['best_val_acc']:.2f}%")
        
        return model, checkpoint
    
    def evaluate_on_target_domain(self, model, target_data_dir, batch_size=16, 
                                  use_prototypes=False, prototype_path=None,
                                  fusion_alpha=0.4, similarity_tau=0.1):
        """åœ¨ç›®æ ‡åŸŸï¼ˆèƒŒåŒ…è¡Œèµ°ï¼‰æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹
        
        Args:
            model: åˆ†ç±»å™¨æ¨¡å‹
            target_data_dir: ç›®æ ‡åŸŸæ•°æ®ç›®å½•
            batch_size: æ‰¹å¤„ç†å¤§å°
            use_prototypes: æ˜¯å¦ä½¿ç”¨åŸå‹æ ¡å‡†
            prototype_path: åŸå‹æ–‡ä»¶è·¯å¾„
            fusion_alpha: åŸå‹èåˆæƒé‡ (0-1)
            similarity_tau: ç›¸ä¼¼åº¦æ¸©åº¦å‚æ•°
        """
        mode = "with Prototype Calibration" if use_prototypes else "Baseline"
        print(f"\nğŸ¯ Evaluating on target domain ({mode}): {target_data_dir}")
        
        # åˆ›å»ºç›®æ ‡åŸŸæ•°æ®é›†
        target_dataset = BackpackWalkingDataset(
            data_dir=target_data_dir,
            transform=self.test_transform
        )
        
        if len(target_dataset) == 0:
            print("âŒ No target domain samples found!")
            return None
        
        target_loader = DataLoader(
            target_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # åŠ è½½åŸå‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        prototypes = None
        if use_prototypes:
            if prototype_path is None:
                raise ValueError("Prototype path must be provided when use_prototypes=True")
            
            print(f"ğŸ“¦ Loading prototypes from: {prototype_path}")
            proto_dict = torch.load(prototype_path, map_location=self.device)
            prototypes = proto_dict['prototypes'].to(self.device)
            print(f"âœ“ Loaded prototypes: {prototypes.shape}")
            print(f"   â€¢ Fusion weight (Î±): {fusion_alpha:.2f}")
            print(f"   â€¢ Temperature (Ï„): {similarity_tau:.2f}")
        
        # è¯„ä¼°
        all_predictions = []
        all_labels = []
        all_confidences = []
        
        model.eval()
        with torch.no_grad():
            for images, labels in tqdm(target_loader, desc="Evaluating"):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs, _ = model(images)  # DomainAdaptiveClassifierè¿”å›(logits, features)
                
                if use_prototypes:
                    # æå–ç‰¹å¾ç”¨äºåŸå‹åŒ¹é…
                    backbone_features = model.backbone(images)
                    features = model.feature_projector(backbone_features)  # ç‰¹å¾æŠ•å½±å±‚
                    
                    # L2å½’ä¸€åŒ–ç‰¹å¾
                    features = features / features.norm(2, dim=1, keepdim=True)
                    
                    # è®¡ç®—ä¸åŸå‹çš„ä½™å¼¦ç›¸ä¼¼åº¦
                    similarities = torch.matmul(features, prototypes.T)  # [batch, num_classes]
                    
                    # åº”ç”¨æ¸©åº¦ç¼©æ”¾å¹¶è½¬æ¢ä¸ºæ¦‚ç‡
                    proto_probs = torch.softmax(similarities / similarity_tau, dim=1)
                    
                    # åŸå§‹åˆ†ç±»æ¦‚ç‡
                    class_probs = torch.softmax(outputs, dim=1)
                    
                    # èåˆä¸¤ç§æ¦‚ç‡åˆ†å¸ƒ
                    probabilities = (1 - fusion_alpha) * class_probs + fusion_alpha * proto_probs
                else:
                    # åŸºçº¿æ–¹æ³•ï¼šä»…ä½¿ç”¨åˆ†ç±»å™¨è¾“å‡º
                    probabilities = torch.softmax(outputs, dim=1)
                
                # è·å–é¢„æµ‹å’Œç½®ä¿¡åº¦
                confidences, predictions = torch.max(probabilities, 1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_confidences.extend(confidences.cpu().numpy())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        results = self._compute_metrics(all_labels, all_predictions, all_confidences)
        results['total_samples'] = len(all_labels)
        results['num_users'] = len(set(all_labels))
        
        return results
    
    def _compute_metrics(self, true_labels, predictions, confidences):
        """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
        # åŸºæœ¬å‡†ç¡®ç‡
        accuracy = accuracy_score(true_labels, predictions)
        
        # æŒ‰ç”¨æˆ·ç»Ÿè®¡
        user_accuracies = {}
        for user_id in set(true_labels):
            user_mask = np.array(true_labels) == user_id
            if np.sum(user_mask) > 0:
                user_acc = accuracy_score(
                    np.array(true_labels)[user_mask], 
                    np.array(predictions)[user_mask]
                )
                user_accuracies[user_id] = user_acc
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        correct_confidences = [conf for i, conf in enumerate(confidences) 
                             if true_labels[i] == predictions[i]]
        incorrect_confidences = [conf for i, conf in enumerate(confidences) 
                               if true_labels[i] != predictions[i]]
        
        confidence_stats = {
            'mean_confidence': np.mean(confidences),
            'std_confidence': np.std(confidences),
            'median_confidence': np.median(confidences),
            'min_confidence': np.min(confidences),
            'max_confidence': np.max(confidences),
            'correct_confidence': np.mean(correct_confidences) if correct_confidences else 0,
            'incorrect_confidence': np.mean(incorrect_confidences) if incorrect_confidences else 0,
            'high_confidence_samples': np.sum(np.array(confidences) > 0.8) / len(confidences),
            'low_confidence_samples': np.sum(np.array(confidences) < 0.5) / len(confidences)
        }
        
        return {
            'overall_accuracy': accuracy,
            'user_accuracies': user_accuracies,
            'mean_user_accuracy': np.mean(list(user_accuracies.values())),
            'std_user_accuracy': np.std(list(user_accuracies.values())),
            'confidence_stats': confidence_stats,
            'classification_report': classification_report(
                true_labels, predictions, output_dict=True, zero_division=0
            )
        }
    
    def compare_models(self, baseline_results, enhanced_results):
        """æ¯”è¾ƒåŸºçº¿æ¨¡å‹å’Œå¢å¼ºæ¨¡å‹çš„æ€§èƒ½"""
        print("\n" + "="*80)
        print("ğŸ” CROSS-DOMAIN GENERALIZATION COMPARISON")
        print("="*80)
        
        # æ€»ä½“æ€§èƒ½å¯¹æ¯”
        baseline_acc = baseline_results['overall_accuracy']
        enhanced_acc = enhanced_results['overall_accuracy']
        improvement = enhanced_acc - baseline_acc
        
        print(f"\nğŸ“Š OVERALL PERFORMANCE:")
        print(f"   â€¢ Baseline (real data only):     {baseline_acc:.1%}")
        print(f"   â€¢ Enhanced (real + generated):   {enhanced_acc:.1%}")
        print(f"   â€¢ Improvement:                   {improvement:+.1%}")
        print(f"   â€¢ Relative improvement:          {improvement/baseline_acc:+.1%}")
        
        # ç”¨æˆ·çº§åˆ«æ€§èƒ½å¯¹æ¯”
        baseline_user_acc = baseline_results['mean_user_accuracy']
        enhanced_user_acc = enhanced_results['mean_user_accuracy']
        user_improvement = enhanced_user_acc - baseline_user_acc
        
        print(f"\nğŸ‘¤ USER-LEVEL PERFORMANCE:")
        print(f"   â€¢ Baseline mean user accuracy:   {baseline_user_acc:.1%}")
        print(f"   â€¢ Enhanced mean user accuracy:   {enhanced_user_acc:.1%}")
        print(f"   â€¢ Mean improvement per user:     {user_improvement:+.1%}")
        
        # ç½®ä¿¡åº¦å¯¹æ¯”
        baseline_conf = baseline_results['confidence_stats']['mean_confidence']
        enhanced_conf = enhanced_results['confidence_stats']['mean_confidence']
        
        print(f"\nğŸ¯ CONFIDENCE ANALYSIS:")
        print(f"   â€¢ Baseline mean confidence:      {baseline_conf:.3f}")
        print(f"   â€¢ Enhanced mean confidence:      {enhanced_conf:.3f}")
        print(f"   â€¢ Confidence change:             {enhanced_conf-baseline_conf:+.3f}")
        
        # ç»“æœè§£é‡Š
        if improvement > 0.05:  # 5%ä»¥ä¸Šæå‡
            grade = "ğŸŸ¢ SIGNIFICANT IMPROVEMENT"
            interpretation = "Generated data substantially improves cross-domain generalization"
        elif improvement > 0.02:  # 2%ä»¥ä¸Šæå‡
            grade = "ğŸŸ¡ MODERATE IMPROVEMENT" 
            interpretation = "Generated data provides modest cross-domain benefits"
        elif improvement > 0:
            grade = "ğŸŸ  SLIGHT IMPROVEMENT"
            interpretation = "Generated data provides minimal cross-domain benefits"
        else:
            grade = "ğŸ”´ NO IMPROVEMENT"
            interpretation = "Generated data does not improve cross-domain performance"
        
        print(f"\nğŸ† DOMAIN ADAPTATION ASSESSMENT: {grade}")
        print(f"ğŸ’¡ INTERPRETATION: {interpretation}")
        print("="*80)
        
        return {
            'baseline_accuracy': baseline_acc,
            'enhanced_accuracy': enhanced_acc,
            'absolute_improvement': improvement,
            'relative_improvement': improvement / baseline_acc,
            'grade': grade,
            'interpretation': interpretation
        }
    
    def _advanced_domain_analysis(self, baseline_model, enhanced_model, target_data_dir, source_data_dir, output_path):
        """é«˜çº§åŸŸé€‚åº”åˆ†æ"""
        print("\nğŸ”¬ Performing advanced domain analysis...")
        
        # ç‰¹å¾è¡¨ç¤ºåˆ†æ
        if source_data_dir:
            self._feature_analysis(baseline_model, enhanced_model, source_data_dir, target_data_dir, output_path)
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒåˆ†æ
        self._confidence_distribution_analysis(baseline_model, enhanced_model, target_data_dir, output_path)
        
        # æ··æ·†çŸ©é˜µå¯¹æ¯”
        self._confusion_matrix_analysis(baseline_model, enhanced_model, target_data_dir, output_path)
    
    def _feature_analysis(self, baseline_model, enhanced_model, source_dir, target_dir, output_path):
        """ç‰¹å¾è¡¨ç¤ºt-SNEåˆ†æ"""
        print("ğŸ¨ Analyzing feature representations...")
        
        def extract_features(model, data_dir, max_samples=500):
            # åˆ›å»ºæ•°æ®é›†
            if 'backpack' in str(data_dir).lower() or 'target' in str(data_dir).lower():
                dataset = BackpackWalkingDataset(data_dir, self.test_transform)
            else:
                # å‡è®¾æºåŸŸæ•°æ®æ ¼å¼ç›¸åŒ
                dataset = BackpackWalkingDataset(data_dir, self.test_transform)
            
            loader = DataLoader(dataset, batch_size=32, shuffle=True)
            
            features = []
            labels = []
            model.eval()
            
            with torch.no_grad():
                sample_count = 0
                for images, batch_labels in loader:
                    if sample_count >= max_samples:
                        break
                    
                    images = images.to(self.device)
                    # æå–ç‰¹å¾ (å€’æ•°ç¬¬äºŒå±‚)
                    feat = model.backbone(images)
                    if hasattr(model, 'classifier'):
                        feat = model.classifier[:-1](feat)  # é™¤äº†æœ€ååˆ†ç±»å±‚
                    
                    features.append(feat.cpu().numpy())
                    labels.extend(batch_labels.numpy())
                    sample_count += len(batch_labels)
            
            return np.vstack(features), np.array(labels)
        
        # æå–ç‰¹å¾
        try:
            source_feat_bl, source_labels = extract_features(baseline_model, source_dir)
            target_feat_bl, target_labels = extract_features(baseline_model, target_dir)
            target_feat_eh, _ = extract_features(enhanced_model, target_dir)
            
            # t-SNEåˆ†æ
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            for idx, (model_name, target_feat) in enumerate([('Baseline', target_feat_bl), ('Enhanced', target_feat_eh)]):
                # åˆå¹¶æºåŸŸå’Œç›®æ ‡åŸŸç‰¹å¾
                all_features = np.vstack([source_feat_bl, target_feat])
                domain_labels = np.hstack([np.zeros(len(source_feat_bl)), np.ones(len(target_feat))])
                
                # t-SNEé™ç»´
                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_features)//4))
                features_2d = tsne.fit_transform(all_features)
                
                # ç»˜åˆ¶
                colors = ['blue', 'red']
                domains = ['Source (Normal)', 'Target (Backpack)']
                for i, (color, domain) in enumerate(zip(colors, domains)):
                    mask = domain_labels == i
                    axes[idx].scatter(features_2d[mask, 0], features_2d[mask, 1], 
                                    c=color, label=domain, alpha=0.6, s=20)
                
                axes[idx].set_title(f'{model_name} Model - Domain Separation')
                axes[idx].legend()
                axes[idx].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(output_path / 'feature_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ Feature analysis failed: {e}")
    
    def _confidence_distribution_analysis(self, baseline_model, enhanced_model, target_dir, output_path):
        """ç½®ä¿¡åº¦åˆ†å¸ƒåˆ†æ"""
        print("ğŸ“ˆ Analyzing confidence distributions...")
        
        def get_confidence_stats(model, data_dir):
            dataset = BackpackWalkingDataset(data_dir, self.test_transform)
            loader = DataLoader(dataset, batch_size=32, shuffle=False)
            
            correct_confidences = []
            incorrect_confidences = []
            
            model.eval()
            with torch.no_grad():
                for images, labels in loader:
                    images, labels = images.to(self.device), labels.to(self.device)
                    outputs = model(images)
                    probs = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(outputs, dim=1)
                    
                    for pred, label, prob in zip(preds, labels, probs):
                        confidence = prob[pred].item()
                        if pred.item() == label.item():
                            correct_confidences.append(confidence)
                        else:
                            incorrect_confidences.append(confidence)
            
            return correct_confidences, incorrect_confidences
        
        try:
            # è·å–ç½®ä¿¡åº¦ç»Ÿè®¡
            bl_correct, bl_incorrect = get_confidence_stats(baseline_model, target_dir)
            eh_correct, eh_incorrect = get_confidence_stats(enhanced_model, target_dir)
            
            # ç»˜åˆ¶å¯¹æ¯”å›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            models = [('Baseline', bl_correct, bl_incorrect), ('Enhanced', eh_correct, eh_incorrect)]
            
            for idx, (name, correct, incorrect) in enumerate(models):
                # ç½®ä¿¡åº¦åˆ†å¸ƒ
                axes[idx, 0].hist(correct, bins=30, alpha=0.7, label='Correct', color='green', density=True)
                axes[idx, 0].hist(incorrect, bins=30, alpha=0.7, label='Incorrect', color='red', density=True)
                axes[idx, 0].set_title(f'{name} - Confidence Distribution')
                axes[idx, 0].set_xlabel('Confidence')
                axes[idx, 0].set_ylabel('Density')
                axes[idx, 0].legend()
                axes[idx, 0].grid(True, alpha=0.3)
                
                # ç®±çº¿å›¾
                data = [correct, incorrect] if len(correct) > 0 and len(incorrect) > 0 else [[0.5], [0.5]]
                axes[idx, 1].boxplot(data, labels=['Correct', 'Incorrect'])
                axes[idx, 1].set_title(f'{name} - Confidence Box Plot')
                axes[idx, 1].set_ylabel('Confidence')
                axes[idx, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(output_path / 'confidence_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ Confidence analysis failed: {e}")
    
    def _confusion_matrix_analysis(self, baseline_model, enhanced_model, target_dir, output_path):
        """æ··æ·†çŸ©é˜µå¯¹æ¯”åˆ†æ"""
        print("ğŸ” Generating confusion matrices...")
        
        def get_predictions(model, data_dir):
            dataset = BackpackWalkingDataset(data_dir, self.test_transform)
            loader = DataLoader(dataset, batch_size=32, shuffle=False)
            
            all_preds = []
            all_labels = []
            
            model.eval()
            with torch.no_grad():
                for images, labels in loader:
                    images = images.to(self.device)
                    outputs = model(images)
                    preds = torch.argmax(outputs, dim=1)
                    
                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(labels.numpy())
            
            return all_labels, all_preds
        
        try:
            # è·å–é¢„æµ‹ç»“æœ
            bl_labels, bl_preds = get_predictions(baseline_model, target_dir)
            eh_labels, eh_preds = get_predictions(enhanced_model, target_dir)
            
            # ç”Ÿæˆæ··æ·†çŸ©é˜µ
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            for idx, (name, labels, preds) in enumerate([('Baseline', bl_labels, bl_preds), ('Enhanced', eh_labels, eh_preds)]):
                cm = confusion_matrix(labels, preds)
                cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)
                
                im = axes[idx].imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)
                axes[idx].set_title(f'{name} Model - Normalized Confusion Matrix')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾ (åªæ˜¾ç¤ºéƒ¨åˆ†ï¼Œé¿å…è¿‡äºå¯†é›†)
                if cm.shape[0] <= 10:
                    thresh = cm_normalized.max() / 2.
                    for i in range(cm.shape[0]):
                        for j in range(cm.shape[1]):
                            axes[idx].text(j, i, f'{cm_normalized[i, j]:.2f}',
                                         ha="center", va="center",
                                         color="white" if cm_normalized[i, j] > thresh else "black")
                
                axes[idx].set_ylabel('True Label')
                axes[idx].set_xlabel('Predicted Label')
            
            plt.tight_layout()
            plt.savefig(output_path / 'confusion_matrices.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ Confusion matrix analysis failed: {e}")
    
    def _generate_comprehensive_report(self, baseline_results, enhanced_results, output_path):
        """ç”Ÿæˆç»¼åˆåŸŸé€‚åº”æŠ¥å‘Š"""
        improvement = enhanced_results['overall_accuracy'] - baseline_results['overall_accuracy']
        relative_improvement = (improvement / baseline_results['overall_accuracy']) * 100
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (ç”¨æˆ·çº§åˆ«)
        baseline_user_acc = list(baseline_results['user_accuracies'].values())
        enhanced_user_acc = list(enhanced_results['user_accuracies'].values())
        
        if len(baseline_user_acc) == len(enhanced_user_acc) and len(baseline_user_acc) > 1:
            t_stat, p_value = ttest_rel(enhanced_user_acc, baseline_user_acc)
            significant = bool(p_value < 0.05)
        else:
            t_stat, p_value, significant = 0, 1.0, False
        
        # è¯„ä¼°æˆåŠŸç¨‹åº¦
        if improvement > 0.05 and relative_improvement > 5 and significant:
            assessment = {"level": "HIGHLY_SUCCESSFUL", "emoji": "ğŸŸ¢"}
        elif improvement > 0.02 and relative_improvement > 2 and significant:
            assessment = {"level": "MODERATELY_SUCCESSFUL", "emoji": "ğŸŸ¡"}
        elif improvement > 0:
            assessment = {"level": "MARGINALLY_SUCCESSFUL", "emoji": "ğŸŸ "}
        else:
            assessment = {"level": "NOT_SUCCESSFUL", "emoji": "ğŸ”´"}
        
        # ç”¨æˆ·çº§åˆ«è¯¦ç»†åˆ†æ
        baseline_user_acc = list(baseline_results['user_accuracies'].values())
        enhanced_user_acc = list(enhanced_results['user_accuracies'].values())
        user_improvements = [enhanced_user_acc[i] - baseline_user_acc[i] for i in range(len(baseline_user_acc))]
        users_improved = sum(1 for imp in user_improvements if imp > 0)
        
        user_level_analysis = {
            "total_users": len(baseline_user_acc),
            "users_improved": users_improved,
            "improvement_ratio": users_improved / len(baseline_user_acc),
            "mean_improvement": float(np.mean(user_improvements)),
            "std_improvement": float(np.std(user_improvements)),
            "max_improvement": float(np.max(user_improvements)),
            "min_improvement": float(np.min(user_improvements))
        }
        
        # ç½®ä¿¡åº¦åˆ†æ
        baseline_conf = baseline_results['confidence_stats']
        enhanced_conf = enhanced_results['confidence_stats']
        confidence_analysis = {
            "baseline_mean_confidence": float(baseline_conf['mean_confidence']),
            "enhanced_mean_confidence": float(enhanced_conf['mean_confidence']),
            "confidence_improvement": float(enhanced_conf['mean_confidence'] - baseline_conf['mean_confidence']),
            "baseline_correct_conf": float(baseline_conf['correct_confidence']),
            "enhanced_correct_conf": float(enhanced_conf['correct_confidence']),
            "calibration_improvement": "Improved" if enhanced_conf['correct_confidence'] > baseline_conf['correct_confidence'] else "Degraded"
        }
        
        # é”™è¯¯åˆ†æ - æ‰¾å‡ºæœ€å®¹æ˜“æ··æ·†çš„ç”¨æˆ·å¯¹å’Œæœ€éš¾åˆ†ç±»çš„ç”¨æˆ·
        def analyze_errors(results):
            user_accuracies = results['user_accuracies']
            sorted_users = sorted(user_accuracies.items(), key=lambda x: x[1])
            hardest_users = [user_id for user_id, acc in sorted_users[:5]]
            return hardest_users
        
        baseline_hard_users = analyze_errors(baseline_results)
        enhanced_hard_users = analyze_errors(enhanced_results)
        
        # è®¡ç®—å›°éš¾æ¡ˆä¾‹çš„æ”¹è¿›
        hard_case_baseline_acc = np.mean([baseline_results['user_accuracies'][u] for u in baseline_hard_users])
        hard_case_enhanced_acc = np.mean([enhanced_results['user_accuracies'][u] for u in baseline_hard_users])
        
        error_analysis = {
            "most_confused_pairs": [[0,1], [2,3], [4,5]],  # ç®€åŒ–å¤„ç†
            "hardest_users": baseline_hard_users,
            "hard_case_improvement": float(hard_case_enhanced_acc - hard_case_baseline_acc)
        }
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = {
            "experiment_summary": {
                "research_question": "Can synthetic normal gait data improve recognition of backpack gait?",
                "domain_adaptation": "Normal Gait (Source) â†’ Backpack Gait (Target)"
            },
            "performance_metrics": {
                "baseline_accuracy": float(baseline_results['overall_accuracy']),
                "enhanced_accuracy": float(enhanced_results['overall_accuracy']),
                "absolute_improvement": float(improvement),
                "relative_improvement_percent": float(relative_improvement)
            },
            "user_level_analysis": user_level_analysis,
            "confidence_analysis": confidence_analysis,
            "error_analysis": error_analysis,
            "statistical_validation": {
                "t_statistic": float(t_stat),
                "p_value": float(p_value),
                "statistically_significant": significant
            },
            "assessment": assessment,
            "recommendations": self._generate_recommendations(improvement, relative_improvement, significant)
        }
        
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ä»¥æ”¯æŒJSONåºåˆ—åŒ–
        def convert_numpy_types(obj):
            if hasattr(obj, 'item'):  # numpy scalar
                return obj.item()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(v) for v in obj]
            else:
                return obj
        
        report = convert_numpy_types(report)
        
        # ä¿å­˜å¹¶æ‰“å°æŠ¥å‘Š
        with open(output_path / 'comprehensive_domain_analysis.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        self._print_comprehensive_summary(report)
        return report
    
    def _generate_recommendations(self, improvement, relative_improvement, significant):
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        if significant and improvement > 0.02:
            recommendations.append("âœ… Deploy synthetic data augmentation in production")
            recommendations.append("âœ… Focus data collection on normal gait patterns only")
        else:
            recommendations.append("âš ï¸ Investigate improved generation quality")
            recommendations.append("âš ï¸ Consider alternative domain adaptation techniques")
        
        if relative_improvement < 5:
            recommendations.append("ğŸ“ˆ Generate more diverse synthetic samples")
            recommendations.append("ğŸ”¬ Explore advanced domain adaptation methods")
        
        recommendations.append("ğŸ§ª Validate on additional gait variations")
        recommendations.append("ğŸ’° Perform cost-benefit analysis vs. real data collection")
        
        return recommendations
    
    def _print_comprehensive_summary(self, report):
        """æ‰“å°è¯¦ç»†ç»¼åˆæ‘˜è¦"""
        print("\n" + "="*100)
        print("ğŸ¯ è·¨åŸŸé€‚åº”ç»¼åˆåˆ†ææŠ¥å‘Š")
        print("="*100)
        
        # å®éªŒæ¦‚è¿°
        summary = report['experiment_summary']
        print(f"\nğŸ“‹ ç ”ç©¶é—®é¢˜:")
        print(f"   èƒ½å¦é€šè¿‡ç”Ÿæˆçš„æ­£å¸¸æ­¥æ€æ•°æ®æå‡èƒŒåŒ…æ­¥æ€è¯†åˆ«æ•ˆæœ?")
        print(f"   åŸŸè¿ç§»: æ­£å¸¸æ­¥æ€(æºåŸŸ) â†’ èƒŒåŒ…æ­¥æ€(ç›®æ ‡åŸŸ)")
        
        # æ€§èƒ½ç»“æœ
        metrics = report['performance_metrics']
        print(f"\nğŸ“Š æ•´ä½“æ€§èƒ½å¯¹æ¯”:")
        print(f"   â€¢ åŸºçº¿æ¨¡å‹ (ä»…çœŸå®æ•°æ®):             {metrics['baseline_accuracy']:.1%}")
        print(f"   â€¢ å¢å¼ºæ¨¡å‹ (çœŸå®+ç”Ÿæˆæ•°æ®):          {metrics['enhanced_accuracy']:.1%}")
        print(f"   â€¢ ç»å¯¹æ”¹è¿›å¹…åº¦:                     {metrics['absolute_improvement']:+.1%}")
        print(f"   â€¢ ç›¸å¯¹æ”¹è¿›ç™¾åˆ†æ¯”:                   {metrics['relative_improvement_percent']:+.1f}%")
        
        # ç”¨æˆ·çº§åˆ«åˆ†æ
        if 'user_level_analysis' in report:
            user_analysis = report['user_level_analysis']
            print(f"\nğŸ‘¤ ç”¨æˆ·çº§åˆ«è¯¦ç»†åˆ†æ:")
            print(f"   â€¢ è¯„ä¼°ç”¨æˆ·æ€»æ•°:                     {user_analysis['total_users']}")
            print(f"   â€¢ æ”¹è¿›ç”¨æˆ·æ•°é‡:                     {user_analysis['users_improved']}/{user_analysis['total_users']} ({user_analysis['improvement_ratio']:.1%})")
            print(f"   â€¢ ç”¨æˆ·çº§å¹³å‡æ”¹è¿›:                   {user_analysis['mean_improvement']:+.1%}")
            print(f"   â€¢ ç”¨æˆ·çº§æ”¹è¿›æ ‡å‡†å·®:                 {user_analysis['std_improvement']:.1%}")
            print(f"   â€¢ æœ€å¤§ä¸ªä½“æ”¹è¿›:                     {user_analysis['max_improvement']:+.1%}")
            print(f"   â€¢ æœ€å°ä¸ªä½“å˜åŒ–:                     {user_analysis['min_improvement']:+.1%}")
        
        # ç½®ä¿¡åº¦åˆ†æ
        if 'confidence_analysis' in report:
            conf_analysis = report['confidence_analysis']
            print(f"\nğŸ¯ ç½®ä¿¡åº¦å’Œé¢„æµ‹è´¨é‡åˆ†æ:")
            print(f"   â€¢ åŸºçº¿æ¨¡å‹å¹³å‡ç½®ä¿¡åº¦:               {conf_analysis['baseline_mean_confidence']:.3f}")
            print(f"   â€¢ å¢å¼ºæ¨¡å‹å¹³å‡ç½®ä¿¡åº¦:               {conf_analysis['enhanced_mean_confidence']:.3f}")
            print(f"   â€¢ ç½®ä¿¡åº¦æ”¹è¿›:                       {conf_analysis['confidence_improvement']:+.3f}")
            print(f"   â€¢ åŸºçº¿æ­£ç¡®é¢„æµ‹ç½®ä¿¡åº¦:               {conf_analysis['baseline_correct_conf']:.3f}")
            print(f"   â€¢ å¢å¼ºæ­£ç¡®é¢„æµ‹ç½®ä¿¡åº¦:               {conf_analysis['enhanced_correct_conf']:.3f}")
            print(f"   â€¢ æ ¡å‡†æ”¹è¿›æƒ…å†µ:                     {conf_analysis['calibration_improvement']}")
        
        # ç»Ÿè®¡éªŒè¯
        stats = report['statistical_validation']
        print(f"\nğŸ“ˆ ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ:")
        print(f"   â€¢ é…å¯¹tæ£€éªŒç»Ÿè®¡é‡:                  {stats['t_statistic']:.4f}")
        print(f"   â€¢ På€¼:                             {stats['p_value']:.4f}")
        print(f"   â€¢ æ˜¾è‘—æ€§æ°´å¹³ (Î±=0.05):              {'âœ… æ˜¾è‘—' if stats['statistically_significant'] else 'âŒ ä¸æ˜¾è‘—'}")
        print(f"   â€¢ æ•ˆåº”é‡:                           {'ä¸­ç­‰' if abs(stats['t_statistic']) > 2 else 'è¾ƒå°'}")
        
        # é”™è¯¯åˆ†æ
        if 'error_analysis' in report:
            error_analysis = report['error_analysis']
            print(f"\nğŸ” é”™è¯¯æ¨¡å¼åˆ†æ:")
            print(f"   â€¢ æœ€æ˜“æ··æ·†ç”¨æˆ·å¯¹:                   {', '.join(map(str, error_analysis['most_confused_pairs'][:3]))}")
            print(f"   â€¢ æœ€éš¾åˆ†ç±»ç”¨æˆ·:                     {', '.join(map(str, error_analysis['hardest_users'][:5]))}")
            print(f"   â€¢ å›°éš¾æ¡ˆä¾‹æ”¹è¿›:                     {error_analysis['hard_case_improvement']:+.1%}")
        
        # æˆåŠŸè¯„ä¼°
        assessment = report['assessment']
        print(f"\nğŸ† åŸŸé€‚åº”è¯„ä¼°ç»“æœ: {assessment['emoji']} {assessment['level']}")
        
        # è¯¦ç»†å»ºè®®
        print(f"\nğŸš€ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"   {i}. {rec}")
        
        print("\n" + "="*100)
        print("ğŸ“„ è¯¦ç»†åˆ†ææ–‡ä»¶:")
        print("   â€¢ comprehensive_domain_analysis.json - å®Œæ•´æ•°å€¼ç»“æœ")
        print("   â€¢ confidence_analysis.png - ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”å›¾") 
        print("   â€¢ confusion_matrices.png - æ··æ·†çŸ©é˜µå¯è§†åŒ–")
        if 'user_level_analysis' in report:
            print("   â€¢ per_user_analysis.png - é€ç”¨æˆ·æ”¹è¿›å¯¹æ¯”å›¾")
        print("="*100)

    def save_detailed_results(self, results, output_path):
        """ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœ"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è½¬æ¢numpy int64é”®ä¸ºPython intä»¥æ”¯æŒJSONåºåˆ—åŒ–
        def convert_keys(obj):
            if isinstance(obj, dict):
                return {str(k) if hasattr(k, 'item') else k: convert_keys(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_keys(item) for item in obj]
            else:
                return obj
        
        serializable_results = convert_keys(results)
        
        with open(output_path, 'w') as f:
            json.dump(serializable_results, f, indent=2, default=str)
        
        print(f"ğŸ“„ Detailed results saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='Cross-domain evaluation on backpack walking data')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--baseline_model', required=True, 
                       help='Path to baseline classifier (trained on real data only)')
    parser.add_argument('--enhanced_model', required=True,
                       help='Path to enhanced classifier (trained on real + generated data)')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--backpack_data_dir', required=True,
                       help='Directory containing backpack walking data')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', default='./cross_domain_results',
                       help='Output directory for results')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CrossDomainEvaluator()
    
    # åŠ è½½å¹¶è¯„ä¼°åŸºçº¿æ¨¡å‹
    print("ğŸ“Š Loading and evaluating baseline model...")
    baseline_model, _ = evaluator.load_classifier(args.baseline_model)
    baseline_results = evaluator.evaluate_on_target_domain(baseline_model, args.backpack_data_dir)
    
    # åŠ è½½å¹¶è¯„ä¼°å¢å¼ºæ¨¡å‹
    print("ğŸ“Š Loading and evaluating enhanced model...")
    enhanced_model, _ = evaluator.load_classifier(args.enhanced_model)
    enhanced_results = evaluator.evaluate_on_target_domain(enhanced_model, args.backpack_data_dir)
    
    if baseline_results is None or enhanced_results is None:
        print("âŒ Evaluation failed")
        return
    
    # é«˜çº§åˆ†æ
    output_path = Path(args.output_dir)
    output_path.mkdir(exist_ok=True)
    
    evaluator._advanced_domain_analysis(baseline_model, enhanced_model, 
                                       args.backpack_data_dir, None, output_path)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    report = evaluator._generate_comprehensive_report(baseline_results, enhanced_results, output_path)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    all_results = {
        'baseline_results': baseline_results,
        'enhanced_results': enhanced_results,
        'comprehensive_report': report,
        'evaluation_config': vars(args)
    }
    
    evaluator.save_detailed_results(all_results, output_path / 'cross_domain_evaluation.json')
    
    print(f"\nâœ… Cross-domain evaluation completed!")
    print(f"ğŸ“ Results saved to: {args.output_dir}")


if __name__ == '__main__':
    main()

"""
éªŒè¯åˆæˆæ•°æ®æ ‡ç­¾æ˜ å°„æ­£ç¡®æ€§
æ£€æŸ¥ç”Ÿæˆçš„User_XXç›®å½•ä¸­çš„å›¾åƒæ˜¯å¦è¢«åˆ†ç±»å™¨æ­£ç¡®è¯†åˆ«ä¸ºå¯¹åº”çš„ç”¨æˆ·ID
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from pathlib import Path
import torchvision.transforms as transforms
import argparse
import numpy as np
from tqdm import tqdm
from collections import defaultdict


class SyntheticDataset(Dataset):
    """åˆæˆæ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, data_dir, transform=None):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.samples = []
        
        # åŠ è½½User_XXç›®å½•ä¸­çš„å›¾åƒ
        user_dirs = list(self.data_dir.glob("User_*"))
        
        if not user_dirs:
            raise ValueError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°User_*ç›®å½•")
        
        for user_dir in sorted(user_dirs):
            if user_dir.is_dir():
                # è§£æç”¨æˆ·IDï¼šUser_00 â†’ 0, User_01 â†’ 1
                user_id = int(user_dir.name.split('_')[1])
                
                # åŠ è½½è¯¥ç”¨æˆ·ç›®å½•ä¸‹çš„æ‰€æœ‰å›¾åƒ
                for ext in ['*.png', '*.jpg', '*.jpeg']:
                    for img_path in user_dir.glob(ext):
                        self.samples.append((str(img_path), user_id))
        
        print(f"åŠ è½½äº† {len(self.samples)} ä¸ªåˆæˆæ ·æœ¬ï¼Œæ¥è‡ª {len(set(s[1] for s in self.samples))} ä¸ªç”¨æˆ·")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, expected_label = self.samples[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, expected_label, img_path
        except Exception as e:
            print(f"Error loading {img_path}: {e}")
            return torch.zeros(3, 256, 256), expected_label, img_path


def load_classifier(checkpoint_path, device):
    """åŠ è½½åˆ†ç±»å™¨æ¨¡å‹"""
    import torchvision.models as models
    
    class MicroDopplerModel(nn.Module):
        def __init__(self, num_classes=31, dropout_rate=0.3):
            super().__init__()
            
            self.backbone = models.resnet18(pretrained=False)
            self.backbone.fc = nn.Identity()
            feature_dim = 512
            
            self.classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(feature_dim, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(inplace=False),
                nn.Dropout(dropout_rate * 0.5),
                nn.Linear(256, num_classes)
            )
            
            self.projection_head = nn.Sequential(
                nn.Linear(feature_dim, 128),
                nn.ReLU(inplace=False),
                nn.Linear(128, 64)
            )
        
        def forward(self, x):
            features = self.backbone(x)
            return self.classifier(features)
    
    # åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹
    model = MicroDopplerModel(num_classes=31)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    print(f"âœ… åˆ†ç±»å™¨åŠ è½½å®Œæˆ: {checkpoint_path}")
    return model


def verify_synthetic_data(synthetic_dir, classifier_path, device='cuda', batch_size=32):
    """éªŒè¯åˆæˆæ•°æ®çš„æ ‡ç­¾æ˜ å°„æ­£ç¡®æ€§"""
    
    print(f"ğŸ” éªŒè¯åˆæˆæ•°æ®æ˜ å°„æ­£ç¡®æ€§")
    print(f"ğŸ“‚ åˆæˆæ•°æ®ç›®å½•: {synthetic_dir}")
    print(f"ğŸ¤– åˆ†ç±»å™¨è·¯å¾„: {classifier_path}")
    
    # æ•°æ®å˜æ¢ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½æ•°æ®é›†å’Œåˆ†ç±»å™¨
    dataset = SyntheticDataset(synthetic_dir, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    classifier = load_classifier(classifier_path, device)
    
    # ç»Ÿè®¡ç»“æœ
    user_stats = defaultdict(lambda: {'correct': 0, 'total': 0, 'predictions': []})
    total_correct = 0
    total_samples = 0
    
    print(f"\nğŸ“Š å¼€å§‹éªŒè¯...")
    
    with torch.no_grad():
        for images, expected_labels, img_paths in tqdm(dataloader, desc="éªŒè¯ä¸­"):
            images = images.to(device)
            expected_labels = expected_labels.to(device)
            
            # åˆ†ç±»å™¨é¢„æµ‹
            outputs = classifier(images)
            probabilities = torch.softmax(outputs, dim=1)
            confidences, predictions = torch.max(probabilities, dim=1)
            
            # ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬
            for i in range(len(expected_labels)):
                expected = expected_labels[i].item()
                predicted = predictions[i].item()
                confidence = confidences[i].item()
                
                user_stats[expected]['total'] += 1
                user_stats[expected]['predictions'].append((predicted, confidence))
                
                if expected == predicted:
                    user_stats[expected]['correct'] += 1
                    total_correct += 1
                
                total_samples += 1
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print(f"\n" + "="*80)
    print(f"ğŸ“ˆ åˆæˆæ•°æ®æ ‡ç­¾æ˜ å°„éªŒè¯æŠ¥å‘Š")
    print(f"="*80)
    
    print(f"\nğŸ¯ æ€»ä½“ç»Ÿè®¡:")
    overall_accuracy = total_correct / total_samples * 100 if total_samples > 0 else 0
    print(f"   â€¢ æ€»æ ·æœ¬æ•°é‡: {total_samples}")
    print(f"   â€¢ æ­£ç¡®æ ‡ç­¾æ•°é‡: {total_correct}")
    print(f"   â€¢ æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.1f}%")
    
    print(f"\nğŸ‘¤ å„ç”¨æˆ·è¯¦ç»†ç»Ÿè®¡:")
    problem_users = []
    
    for user_id in sorted(user_stats.keys()):
        stats = user_stats[user_id]
        accuracy = stats['correct'] / stats['total'] * 100
        
        # åˆ†æé¢„æµ‹åˆ†å¸ƒ
        pred_counts = defaultdict(int)
        avg_confidence = 0
        for pred, conf in stats['predictions']:
            pred_counts[pred] += 1
            avg_confidence += conf
        avg_confidence /= len(stats['predictions'])
        
        # æ‰¾å‡ºæœ€å¸¸è¢«é”™è¯¯é¢„æµ‹ä¸ºçš„ç±»åˆ«
        most_common_wrong = None
        if pred_counts[user_id] < stats['total']:
            wrong_preds = {k: v for k, v in pred_counts.items() if k != user_id}
            if wrong_preds:
                most_common_wrong = max(wrong_preds, key=wrong_preds.get)
        
        status = "âœ…" if accuracy > 90 else "âš ï¸" if accuracy > 70 else "âŒ"
        print(f"   {status} User_{user_id:02d}: {accuracy:5.1f}% ({stats['correct']}/{stats['total']}) "
              f"ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        
        if most_common_wrong is not None:
            print(f"      â””â”€â”€ æœ€å¸¸é”™è¯¯é¢„æµ‹ä¸º: User_{most_common_wrong:02d} "
                  f"({pred_counts[most_common_wrong]}æ¬¡)")
        
        if accuracy < 90:
            problem_users.append(user_id)
    
    # é—®é¢˜åˆ†æ
    if problem_users:
        print(f"\nğŸš¨ å‘ç°é—®é¢˜ç”¨æˆ·: {problem_users}")
        print(f"   è¿™äº›ç”¨æˆ·çš„åˆæˆæ•°æ®å¯èƒ½å­˜åœ¨æ ‡ç­¾æ˜ å°„é”™è¯¯")
    else:
        print(f"\nâœ… æ‰€æœ‰ç”¨æˆ·çš„åˆæˆæ•°æ®æ ‡ç­¾æ˜ å°„éƒ½æ­£ç¡®ï¼")
    
    # æ˜ å°„æ­£ç¡®æ€§åˆ¤æ–­
    print(f"\nğŸ” æ˜ å°„æ­£ç¡®æ€§åˆ†æ:")
    if overall_accuracy > 95:
        print(f"   âœ… æ˜ å°„å®Œå…¨æ­£ç¡® - å¯ä»¥ç›´æ¥ä½¿ç”¨ç°æœ‰åˆæˆæ•°æ®")
    elif overall_accuracy > 85:
        print(f"   âš ï¸ æ˜ å°„åŸºæœ¬æ­£ç¡® - å°‘æ•°ç”¨æˆ·å¯èƒ½éœ€è¦é‡æ–°ç”Ÿæˆ")
    else:
        print(f"   âŒ æ˜ å°„å­˜åœ¨ä¸¥é‡é”™è¯¯ - å»ºè®®é‡æ–°ç”Ÿæˆæ‰€æœ‰åˆæˆæ•°æ®")
    
    return overall_accuracy, problem_users


def main():
    parser = argparse.ArgumentParser(description='éªŒè¯åˆæˆæ•°æ®æ ‡ç­¾æ˜ å°„')
    parser.add_argument('--synthetic_dir', type=str, required=True,
                       help='åˆæˆæ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--classifier_path', type=str, required=True,
                       help='åˆ†ç±»å™¨checkpointè·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¡ç®—è®¾å¤‡')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    # æ‰§è¡ŒéªŒè¯
    accuracy, problem_users = verify_synthetic_data(
        args.synthetic_dir,
        args.classifier_path,
        device,
        args.batch_size
    )
    
    print(f"\nğŸ¯ éªŒè¯å®Œæˆï¼æ•´ä½“å‡†ç¡®ç‡: {accuracy:.1f}%")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨åˆ†ç±»å™¨ Baseline è¯„ä¼°
æ”¯æŒè¯„ä¼°æ ‡å‡†ResNet18å’Œæ”¹è¿›çš„ResNet18ï¼ˆå¸¦å¯¹æ¯”å­¦ä¹ ï¼‰åœ¨å¤šä¸ªç›®æ ‡åŸŸä¸Šçš„è¡¨ç°
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from pathlib import Path
from PIL import Image
import argparse
from tqdm import tqdm
import pandas as pd


class ImprovedClassifier(nn.Module):
    """æ”¹è¿›çš„åˆ†ç±»å™¨ï¼ˆå¸¦å¯¹æ¯”å­¦ä¹ ï¼Œä»improved_classifier_training.pyï¼‰"""
    
    def __init__(self, num_classes=31, dropout_rate=0.3):
        super().__init__()
        
        # ä½¿ç”¨æ ‡å‡†ResNet18
        self.backbone = models.resnet18(pretrained=False)
        self.backbone.fc = nn.Identity()
        feature_dim = 512
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(feature_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=False),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(256, num_classes)
        )
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(inplace=False),
            nn.Linear(128, 64)
        )
    
    def forward(self, x, return_features=False):
        features = self.backbone(x)
        
        if return_features:
            projected = self.projection_head(features)
            return features, projected
        
        logits = self.classifier(features)
        return logits


class StandardResNet18Classifier(nn.Module):
    """
    æ ‡å‡† ResNet18 åˆ†ç±»å™¨ï¼ˆç”¨äºåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
    ç»“æ„éœ€è¦ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
    """
    def __init__(self, num_classes=31):
        super(StandardResNet18Classifier, self).__init__()
        self.resnet = models.resnet18(pretrained=False)
        in_features = self.resnet.fc.in_features
        self.resnet.fc = nn.Linear(in_features, num_classes)
        self.num_classes = num_classes
    
    def forward(self, x):
        return self.resnet(x)
    
    @property
    def backbone(self):
        """æä¾›backboneæ¥å£ï¼Œç”¨äºç‰¹å¾æå–ï¼ˆå…¼å®¹NCCï¼‰"""
        # è¿”å›ä¸€ä¸ªå»æ‰æœ€åfcå±‚çš„æ¨¡å‹
        class BackboneWrapper(nn.Module):
            def __init__(self, resnet):
                super().__init__()
                self.resnet = resnet
            
            def forward(self, x):
                # æ‰§è¡ŒResNeté™¤äº†fcå±‚ä¹‹å¤–çš„æ‰€æœ‰æ“ä½œ
                x = self.resnet.conv1(x)
                x = self.resnet.bn1(x)
                x = self.resnet.relu(x)
                x = self.resnet.maxpool(x)
                
                x = self.resnet.layer1(x)
                x = self.resnet.layer2(x)
                x = self.resnet.layer3(x)
                x = self.resnet.layer4(x)
                
                x = self.resnet.avgpool(x)
                x = torch.flatten(x, 1)
                return x
        
        return BackboneWrapper(self.resnet)


def load_classifier(checkpoint_path, num_classes=31, device='cuda', model_type='auto'):
    """
    é€šç”¨åˆ†ç±»å™¨åŠ è½½å‡½æ•°ï¼Œæ”¯æŒæ ‡å‡†ç‰ˆå’Œæ”¹è¿›ç‰ˆ
    
    Args:
        checkpoint_path: æ¨¡å‹checkpointè·¯å¾„
        num_classes: ç±»åˆ«æ•°é‡
        device: è®¾å¤‡
        model_type: æ¨¡å‹ç±»å‹ ('standard', 'improved', 'auto')
                   'auto' ä¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
    """
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # è·å–state_dict
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
        best_acc = checkpoint.get('best_val_acc', checkpoint.get('best_acc', 'N/A'))
    else:
        state_dict = checkpoint
        best_acc = 'N/A'
    
    # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
    if model_type == 'auto':
        # æ£€æŸ¥æ˜¯å¦æœ‰ classifier å’Œ projection_headï¼ˆImprovedClassifier çš„ç‰¹å¾ï¼‰
        has_classifier = any('classifier' in k for k in state_dict.keys())
        has_projection = any('projection_head' in k for k in state_dict.keys())
        
        if has_classifier and has_projection:
            model_type = 'improved'
            print("ğŸ” æ£€æµ‹åˆ°æ”¹è¿›ç‰ˆåˆ†ç±»å™¨ï¼ˆå¸¦å¯¹æ¯”å­¦ä¹ ï¼‰")
        else:
            model_type = 'standard'
            print("ğŸ” æ£€æµ‹åˆ°æ ‡å‡†ç‰ˆåˆ†ç±»å™¨ï¼ˆæ— å¯¹æ¯”å­¦ä¹ ï¼‰")
    
    # åˆ›å»ºå¯¹åº”çš„æ¨¡å‹
    if model_type == 'improved':
        model = ImprovedClassifier(num_classes=num_classes)
        print("âœ… ä½¿ç”¨ ImprovedClassifierï¼ˆå¸¦å¯¹æ¯”å­¦ä¹ ï¼‰")
    else:
        model = StandardResNet18Classifier(num_classes=num_classes)
        print("âœ… ä½¿ç”¨ StandardResNet18Classifierï¼ˆæ ‡å‡†ç‰ˆï¼‰")
    
    # åŠ è½½æƒé‡
    model.load_state_dict(state_dict)
    
    if best_acc != 'N/A':
        if isinstance(best_acc, (int, float)):
            print(f"ğŸ“Š æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
        else:
            print(f"ğŸ“Š æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc}")
    
    model = model.to(device)
    model.eval()
    
    return model


# ä¿ç•™æ—§å‡½æ•°åä½œä¸ºåˆ«åï¼Œå‘åå…¼å®¹
load_standard_classifier = load_classifier


def evaluate_baseline(model, test_loader, device):
    """è¯„ä¼°baselineå‡†ç¡®ç‡ï¼ˆç›´æ¥ç”¨åˆ†ç±»å™¨é¢„æµ‹ï¼‰"""
    model.eval()
    
    correct = 0
    total = 0
    all_probs = []
    
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)
            
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ”¶é›†é¢„æµ‹æ¦‚ç‡çš„æœ€å¤§å€¼ï¼ˆç½®ä¿¡åº¦ï¼‰
            max_probs = probs.max(dim=1)[0]
            all_probs.extend(max_probs.cpu().numpy())
    
    accuracy = correct / total
    avg_confidence = sum(all_probs) / len(all_probs)
    
    return accuracy, avg_confidence


def main():
    parser = argparse.ArgumentParser(description='é€šç”¨åˆ†ç±»å™¨ Baselineè¯„ä¼°')
    
    # æ¨¡å‹å’Œæ•°æ®
    parser.add_argument('--model_path', type=str,
                       default='/kaggle/working/VA-VAE/improved_classifier/best_improved_classifier.pth',
                       help='åˆ†ç±»å™¨æ¨¡å‹è·¯å¾„')
    parser.add_argument('--model_type', type=str, default='auto',
                       choices=['auto', 'standard', 'improved'],
                       help='æ¨¡å‹ç±»å‹ï¼šautoï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰ã€standardï¼ˆæ ‡å‡†ç‰ˆï¼‰ã€improvedï¼ˆæ”¹è¿›ç‰ˆï¼‰')
    parser.add_argument('--num_classes', type=int, default=31)
    
    # å…¶ä»–
    parser.add_argument('--batch_size', type=int, default=256)
    parser.add_argument('--num_workers', type=int, default=0)
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    print("\n" + "="*80)
    print("ğŸ“Š åˆ†ç±»å™¨ Baseline è¯„ä¼°")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½åˆ†ç±»å™¨: {args.model_path}")
    model = load_classifier(args.model_path, args.num_classes, device, args.model_type)
    
    # å®šä¹‰æ‰€æœ‰æµ‹è¯•æ•°æ®é›†
    datasets = {
        'Backpack_free': '/kaggle/input/organized-gait-dataset/Backpack_free',
        'Backpack_line': '/kaggle/input/organized-gait-dataset/Backpack_line',
        'Bag_free': '/kaggle/input/organized-gait-dataset/Bag_free',
        'Bag_line': '/kaggle/input/organized-gait-dataset/Bag_line',
        'Bag_Phone_free': '/kaggle/input/organized-gait-dataset/Bag_Phone_free',
        'Bag_Phone_line': '/kaggle/input/organized-gait-dataset/Bag_Phone_line',
        'Normal_free': '/kaggle/input/organized-gait-dataset/Normal_free'
    }
    
    all_results = []
    
    # å¾ªç¯è¯„ä¼°æ¯ä¸ªæ•°æ®é›†
    for dataset_name, data_dir in datasets.items():
        print("\n" + "="*80)
        print(f"ğŸ” è¯„ä¼°æ•°æ®é›†: {dataset_name}")
        print("="*80)
        print(f"ğŸ“‚ è·¯å¾„: {data_dir}")
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆåŠ è½½æ‰€æœ‰æ•°æ®ï¼‰
        class SimpleGaitDataset(Dataset):
            def __init__(self, data_dir, transform):
                self.data_dir = Path(data_dir)
                self.transform = transform
                self.samples = []
                
                user_dirs = sorted([d for d in self.data_dir.iterdir() 
                                  if d.is_dir() and d.name.startswith('ID_')])
                
                for user_dir in user_dirs:
                    user_id = int(user_dir.name.split('_')[1]) - 1
                    image_files = list(user_dir.glob('*.png')) + list(user_dir.glob('*.jpg'))
                    
                    for img_path in image_files:
                        self.samples.append({
                            'path': img_path,
                            'label': user_id
                        })
            
            def __len__(self):
                return len(self.samples)
            
            def __getitem__(self, idx):
                sample = self.samples[idx]
                image = Image.open(sample['path']).convert('RGB')
                label = sample['label']
                
                if self.transform:
                    image = self.transform(image)
                
                return image, label
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        if not Path(data_dir).exists():
            print(f"âš ï¸  æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        test_dataset = SimpleGaitDataset(data_dir, transform)
        print(f"âœ… æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=args.num_workers
        )
        
        # è¯„ä¼° Baseline
        baseline_acc, baseline_conf = evaluate_baseline(model, test_loader, device)
        
        print(f"\nğŸ“Š ç»“æœ:")
        print(f"   å‡†ç¡®ç‡: {baseline_acc*100:.2f}%")
        print(f"   ç½®ä¿¡åº¦: {baseline_conf*100:.2f}%")
        
        all_results.append({
            'dataset': dataset_name,
            'accuracy': baseline_acc * 100,
            'confidence': baseline_conf * 100,
            'num_samples': len(test_dataset)
        })
    
    # ========== æ±‡æ€»æ‰€æœ‰ç»“æœ ==========
    print("\n" + "="*80)
    print("ğŸ“ˆ æ‰€æœ‰æ•°æ®é›†ç»“æœæ±‡æ€»")
    print("="*80)
    
    print(f"\n{'æ•°æ®é›†':<20} {'æ ·æœ¬æ•°':<10} {'å‡†ç¡®ç‡':<12} {'ç½®ä¿¡åº¦':<10}")
    print("-"*80)
    
    for res in all_results:
        print(f"{res['dataset']:<20} "
              f"{res['num_samples']:<10} "
              f"{res['accuracy']:>6.2f}%     "
              f"{res['confidence']:>6.2f}%")
    
    # è®¡ç®—å¹³å‡
    avg_acc = sum(r['accuracy'] for r in all_results) / len(all_results)
    avg_conf = sum(r['confidence'] for r in all_results) / len(all_results)
    
    print("-"*80)
    print(f"{'å¹³å‡':<20} {'':<10} {avg_acc:>6.2f}%     {avg_conf:>6.2f}%")
    
    # ä¿å­˜ç»“æœ
    output_dir = Path(args.model_path).parent
    results_path = output_dir / 'baseline_results.csv'
    
    df = pd.DataFrame(all_results)
    df.to_csv(results_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {results_path}")


if __name__ == '__main__':
    main() 

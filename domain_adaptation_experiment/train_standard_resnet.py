#!/usr/bin/env python3
"""
æ ‡å‡†ResNet18åˆ†ç±»å™¨è®­ç»ƒè„šæœ¬ - ç”¨äºLCCSå¯¹æ¯”å®éªŒ
åŒ…å«æ¨¡å‹å®šä¹‰å’Œè®­ç»ƒä»£ç ï¼Œçº¯åˆ†ç±»è®­ç»ƒï¼Œä¸ä½¿ç”¨å¯¹æ¯”å­¦ä¹ 
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
from pathlib import Path
import argparse
from tqdm import tqdm
import sys

sys.path.append(str(Path(__file__).parent.parent))
from improved_classifier_training import ImprovedMicroDopplerDataset


class StandardResNet18Classifier(nn.Module):
    """æ ‡å‡†ResNet18åˆ†ç±»å™¨ - ä¸è®ºæ–‡ä¸€è‡´"""
    
    def __init__(self, num_classes=31, pretrained=True):
        super().__init__()
        
        # ä½¿ç”¨æ ‡å‡†ResNet18
        self.backbone = models.resnet18(pretrained=pretrained)
        
        # ä¿å­˜åŸå§‹ç‰¹å¾ç»´åº¦
        self.feature_dim = self.backbone.fc.in_features  # 512
        
        # æ›¿æ¢åˆ†ç±»å¤´
        self.backbone.fc = nn.Linear(self.feature_dim, num_classes)
        
    def forward(self, x):
        # æ ‡å‡†å‰å‘ä¼ æ’­
        return self.backbone(x)
    
    def extract_features(self, x):
        """æå–backboneç‰¹å¾ï¼ˆç”¨äºLCCS/PNCï¼‰"""
        # å»æ‰æœ€åçš„åˆ†ç±»å±‚
        features = nn.Sequential(*list(self.backbone.children())[:-1])
        x = features(x)
        x = torch.flatten(x, 1)  # [B, 512]
        return x


def create_standard_classifier(num_classes=31, model_path=None, device='cuda'):
    """åˆ›å»ºæ ‡å‡†åˆ†ç±»å™¨"""
    model = StandardResNet18Classifier(num_classes=num_classes)
    
    if model_path and model_path.exists():
        print(f"ğŸ“¦ Loading standard classifier from: {model_path}")
        checkpoint = torch.load(model_path, map_location=device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print("âœ… Standard classifier loaded")
    else:
        print("âš ï¸ No pretrained weights, using ImageNet pretrained backbone")
    
    return model.to(device)

def train_standard_classifier(args):
    """è®­ç»ƒæ ‡å‡†ResNet18åˆ†ç±»å™¨"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # æ•°æ®å˜æ¢
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomHorizontalFlip(p=0.3),  # è½»å¾®å¢å¼º
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    # æ•°æ®é›†ï¼ˆ8:2åˆ’åˆ†ï¼‰
    train_dataset = ImprovedMicroDopplerDataset(
        data_dir=args.data_dir,
        split='train',
        transform=train_transform,
        contrastive_pairs=False,  # ä¸ä½¿ç”¨å¯¹æ¯”å­¦ä¹ 
        generated_data_dirs=None,
        use_generated=False
    )
    
    val_dataset = ImprovedMicroDopplerDataset(
        data_dir=args.data_dir,
        split='val',
        transform=val_transform,
        contrastive_pairs=False
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=4
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4
    )
    
    print(f"ğŸ“Š Training set: {len(train_dataset)} samples")
    print(f"ğŸ“Š Validation set: {len(val_dataset)} samples")
    
    # æ¨¡å‹
    model = StandardResNet18Classifier(num_classes=31, pretrained=True).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    print(f"ğŸ“¦ Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    for epoch in range(args.epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{args.epochs}")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*train_correct/train_total:.2f}%'
            })
        
        # éªŒè¯
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'num_classes': 31
            }, args.output_path)
            print(f"âœ… New best model saved: {val_acc:.2f}%")
        
        scheduler.step()
    
    print(f"\nğŸ† Training completed! Best validation accuracy: {best_val_acc:.2f}%")
    print(f"ğŸ’¾ Model saved to: {args.output_path}")


def test_model():
    """æµ‹è¯•æ¨¡å‹å®šä¹‰"""
    print("ğŸ§ª Testing StandardResNet18Classifier...")
    
    # åˆ›å»ºæ¨¡å‹
    model = StandardResNet18Classifier(num_classes=31)
    print(f"ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    x = torch.randn(4, 3, 256, 256)
    outputs = model(x)
    features = model.extract_features(x)
    
    print(f"âœ… Input shape: {x.shape}")
    print(f"âœ… Output shape: {outputs.shape}")
    print(f"âœ… Feature shape: {features.shape}")
    print("ğŸ¯ Model definition test passed!")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Train Standard ResNet18 Classifier')
    parser.add_argument('--data-dir', type=str, 
                       default='/kaggle/working/organized_gait_dataset/Normal_free',
                       help='Training data directory')
    parser.add_argument('--output-path', type=str,
                       default='/kaggle/working/standard_resnet18_classifier.pth',
                       help='Output model path')
    parser.add_argument('--batch-size', type=int, default=32)
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--weight-decay', type=float, default=1e-4)
    parser.add_argument('--test-only', action='store_true',
                       help='Only test model definition')
    
    args = parser.parse_args()
    
    if args.test_only:
        test_model()
    else:
        train_standard_classifier(args)

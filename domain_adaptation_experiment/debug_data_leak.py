#!/usr/bin/env python3
"""
æ£€æŸ¥æ•°æ®æ³„æ¼å’Œå¼‚å¸¸æƒ…å†µ
"""

import torch
from pathlib import Path
from cross_domain_evaluator import BackpackWalkingDataset
from build_improved_prototypes import TargetDomainDataset
import torchvision.transforms as transforms

def check_data_leak():
    """æ£€æŸ¥åŸå‹æ„å»ºæ•°æ®ä¸æµ‹è¯•æ•°æ®æ˜¯å¦æœ‰é‡å """
    
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
    ])
    
    # æ„å»ºæ”¯æŒé›†ï¼ˆåŸå‹ç”¨ï¼‰
    support_dataset = TargetDomainDataset(
        data_dir='/kaggle/input/backpack/backpack',
        transform=transform,
        support_size=10,
        seed=42
    )
    
    # å®Œæ•´æµ‹è¯•é›†
    test_dataset = BackpackWalkingDataset(
        data_dir='/kaggle/input/backpack/backpack',
        transform=transform
    )
    
    # è·å–æ–‡ä»¶è·¯å¾„
    support_paths = set()
    for sample in support_dataset.samples:
        support_paths.add(str(sample['path']))
    
    test_paths = set()
    # BackpackWalkingDatasetçš„æ•°æ®æ ¼å¼ä¸åŒï¼Œç›´æ¥è®¿é—®å†…éƒ¨å±æ€§
    if hasattr(test_dataset, 'samples') and len(test_dataset.samples) > 0:
        for sample in test_dataset.samples:
            if isinstance(sample, dict):
                test_paths.add(str(sample['path']))
            else:
                # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œå°è¯•é€šè¿‡æ•°æ®åŠ è½½å™¨è·å–
                print(f"âš ï¸ BackpackWalkingDatasetæ ¼å¼ä¸åŒï¼Œå°è¯•æ‰‹åŠ¨è·å–è·¯å¾„...")
                break
    
    # å¦‚æœä¸Šé¢çš„æ–¹æ³•ä¸è¡Œï¼Œæ‰‹åŠ¨æ‰«æç›®å½•
    if len(test_paths) == 0:
        data_dir = Path('/kaggle/input/backpack/backpack')
        for user_dir in data_dir.iterdir():
            if user_dir.is_dir():
                for img_file in user_dir.glob('*.png'):
                    test_paths.add(str(img_file))
                for img_file in user_dir.glob('*.jpg'):
                    test_paths.add(str(img_file))
    
    # æ£€æŸ¥é‡å 
    overlap = support_paths & test_paths
    
    print(f"ğŸ“Š æ•°æ®é›†åˆ†æ:")
    print(f"   æ”¯æŒé›†æ ·æœ¬: {len(support_paths)}")
    print(f"   æµ‹è¯•é›†æ ·æœ¬: {len(test_paths)}")
    print(f"   é‡å æ ·æœ¬: {len(overlap)}")
    print(f"   é‡å æ¯”ä¾‹: {len(overlap)/len(support_paths)*100:.1f}%")
    
    if len(overlap) > 0:
        print(f"âš ï¸ å‘ç°æ•°æ®æ³„æ¼ï¼")
        print(f"   å‰5ä¸ªé‡å æ–‡ä»¶:")
        for i, path in enumerate(list(overlap)[:5]):
            print(f"   {i+1}. {path}")
        return True
    else:
        print(f"âœ… æ— æ•°æ®æ³„æ¼")
        return False

def analyze_baseline_performance():
    """åˆ†æåŸºçº¿æ€§èƒ½æ˜¯å¦æ­£å¸¸"""
    
    print(f"\nğŸ“ˆ åŸºçº¿æ€§èƒ½åˆ†æ:")
    print(f"   å½“å‰åŸºçº¿: 75.5%")
    print(f"   é¢„æœŸåŸºçº¿ï¼ˆç›¸ä¼¼åŸŸï¼‰: >80%")
    
    if 75.5 < 80:
        print(f"âš ï¸ åŸºçº¿åä½ï¼Œå¯èƒ½åŸå› :")
        print(f"   1. åŸŸå·®å¼‚è¢«ä½ä¼°")
        print(f"   2. æ¨¡å‹åœ¨ç›®æ ‡åŸŸæ³›åŒ–èƒ½åŠ›å¼±")
        print(f"   3. æ•°æ®è´¨é‡é—®é¢˜")

def check_extreme_improvement():
    """æ£€æŸ¥æç«¯æå‡æ˜¯å¦åˆç†"""
    
    improvement = 11.37
    print(f"\nğŸ¯ æå‡å¹…åº¦åˆ†æ:")
    print(f"   å½“å‰æå‡: +{improvement}%")
    print(f"   å…¸å‹PNCæå‡: 2-5%")
    print(f"   å¼‚å¸¸é˜ˆå€¼: >8%")
    
    if improvement > 8:
        print(f"ğŸš¨ æå‡å¼‚å¸¸å¤§ï¼å¯èƒ½åŸå› :")
        print(f"   1. æ•°æ®æ³„æ¼")
        print(f"   2. è¿‡æ‹Ÿåˆåˆ°æ”¯æŒé›†")
        print(f"   3. è¯„ä¼°æ–¹æ³•é”™è¯¯")
        print(f"   4. Ï„=0.01è¿‡äºæç«¯")

if __name__ == '__main__':
    print("ğŸ” PNCå¼‚å¸¸ç»“æœè¯Šæ–­")
    print("="*50)
    
    # æ£€æŸ¥æ•°æ®æ³„æ¼
    has_leak = check_data_leak()
    
    # åˆ†æåŸºçº¿æ€§èƒ½
    analyze_baseline_performance()
    
    # æ£€æŸ¥æå‡å¹…åº¦
    check_extreme_improvement()
    
    print(f"\nğŸ è¯Šæ–­ç»“è®º:")
    if has_leak:
        print(f"âŒ å­˜åœ¨æ•°æ®æ³„æ¼ï¼Œç»“æœä¸å¯ä¿¡")
    else:
        print(f"âœ… æ— æ˜æ˜¾æ•°æ®æ³„æ¼")
        print(f"âš ï¸ ä½†æå‡å¹…åº¦ä»ç„¶å¼‚å¸¸ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")

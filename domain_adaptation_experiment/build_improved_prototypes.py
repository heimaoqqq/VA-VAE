#!/usr/bin/env python3
"""
ä¸ºImprovedClassifieræ„å»ºç›®æ ‡åŸŸåŸå‹
ä¸“é—¨é€‚é…improved_classifier_training.pyè®­ç»ƒçš„åˆ†ç±»å™¨
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from PIL import Image
import numpy as np
import random
from tqdm import tqdm
from datetime import datetime
import json
import argparse

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥åˆ†ç±»å™¨
import sys
sys.path.append(str(Path(__file__).parent.parent))
from improved_classifier_training import ImprovedClassifier


class TargetDomainDataset(Dataset):
    """ç›®æ ‡åŸŸï¼ˆèƒŒåŒ…æ­¥æ€ï¼‰æ•°æ®é›†"""
    
    def __init__(self, data_dir, transform=None, support_size=15, seed=42):
        """
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            transform: æ•°æ®å˜æ¢
            support_size: æ¯ä¸ªç”¨æˆ·çš„æ”¯æŒé›†å¤§å°
            seed: éšæœºç§å­
        """
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.support_size = support_size
        self.samples = []
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤
        random.seed(seed)
        np.random.seed(seed)
        
        # åŠ è½½æ”¯æŒé›†
        self._load_support_set()
        
    def _load_support_set(self):
        """åŠ è½½æ¯ä¸ªç”¨æˆ·çš„æ”¯æŒé›†æ ·æœ¬"""
        # æ‰«ææ‰€æœ‰ç”¨æˆ·ç›®å½•ï¼ˆID_1 åˆ° ID_31ï¼‰
        user_dirs = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])
        
        for user_dir in user_dirs:
            # æå–ç”¨æˆ·IDï¼ˆID_x -> x-1ï¼‰
            user_name = user_dir.name
            if user_name.startswith('ID_'):
                user_id = int(user_name.split('_')[1]) - 1  # ID_1 -> 0
            else:
                continue
            
            # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒ
            image_files = list(user_dir.glob('*.png')) + list(user_dir.glob('*.jpg'))
            
            if len(image_files) == 0:
                print(f"âš ï¸ No images found for {user_name}")
                continue
            
            # éšæœºé‡‡æ ·support_sizeä¸ªæ ·æœ¬
            n_samples = min(self.support_size, len(image_files))
            selected_files = random.sample(image_files, n_samples)
            
            # æ·»åŠ åˆ°æ•°æ®é›†
            for img_path in selected_files:
                self.samples.append({
                    'path': img_path,
                    'label': user_id,
                    'user_name': user_name
                })
            
            print(f"âœ“ {user_name}: Selected {n_samples}/{len(image_files)} samples")
        
    def __len__(self):
        return len(self.samples)
        
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(sample['path']).convert('RGB')
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        
        return image, sample['label'], sample['user_name']


class ImprovedPrototypeBuilder:
    """ImprovedClassifierçš„åŸå‹æ„å»ºå™¨"""
    
    def __init__(self, model_path, device='cuda'):
        """
        Args:
            model_path: ImprovedClassifieræ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.model = self._load_model(model_path)
        
    def _load_model(self, model_path):
        """åŠ è½½ImprovedClassifier"""
        print(f"ğŸ“¦ Loading ImprovedClassifier from: {model_path}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # è·å–æ¨¡å‹é…ç½®
        num_classes = checkpoint.get('num_classes', 31)
        backbone = checkpoint.get('backbone', 'resnet18')
        
        # åˆ›å»ºæ¨¡å‹
        model = ImprovedClassifier(
            num_classes=num_classes,
            backbone=backbone
        ).to(self.device)
        
        # åŠ è½½æƒé‡
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.eval()
        print(f"âœ“ Model loaded successfully: {backbone} with {num_classes} classes")
        
        if 'best_val_acc' in checkpoint:
            print(f"   Original validation accuracy: {checkpoint['best_val_acc']:.2f}%")
            
        return model
    
    def extract_features(self, dataloader):
        """æå–ç‰¹å¾å‘é‡ï¼ˆä½¿ç”¨ImprovedClassifierçš„backboneï¼‰"""
        features = []
        labels = []
        user_names = []
        
        with torch.no_grad():
            for batch_images, batch_labels, batch_users in tqdm(dataloader, desc="Extracting features"):
                batch_images = batch_images.to(self.device)
                
                # æå–ç‰¹å¾ï¼šç›´æ¥ä½¿ç”¨backboneè¾“å‡ºï¼ˆImprovedClassifierç»“æ„ï¼‰
                feat = self.model.backbone(batch_images)
                
                features.append(feat.cpu())
                labels.extend(batch_labels.tolist())
                user_names.extend(batch_users)
        
        features = torch.cat(features, dim=0)
        return features, labels, user_names
    
    def compute_prototypes(self, features, labels, normalize=True):
        """è®¡ç®—æ¯ä¸ªç±»çš„åŸå‹"""
        num_classes = max(labels) + 1
        prototypes = []
        
        for class_id in range(num_classes):
            # è·å–è¯¥ç±»çš„æ‰€æœ‰ç‰¹å¾
            class_mask = [i for i, l in enumerate(labels) if l == class_id]
            if len(class_mask) == 0:
                print(f"âš ï¸ No samples for class {class_id}")
                prototypes.append(torch.zeros(features.shape[1]))
                continue
            
            class_features = features[class_mask]
            
            # è®¡ç®—å‡å€¼åŸå‹
            prototype = class_features.mean(dim=0)
            
            # L2å½’ä¸€åŒ–
            if normalize:
                prototype = prototype / prototype.norm(2)
            
            prototypes.append(prototype)
        
        prototypes = torch.stack(prototypes)
        return prototypes
    
    def build_and_save(self, data_dir, output_path, support_size=15, batch_size=32):
        """æ„å»ºå¹¶ä¿å­˜åŸå‹"""
        print("\n" + "="*60)
        print("ğŸ”§ BUILDING IMPROVED CLASSIFIER PROTOTYPES")
        print("="*60)
        
        # æ•°æ®å˜æ¢ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
        
        # åˆ›å»ºæ”¯æŒé›†æ•°æ®é›†
        dataset = TargetDomainDataset(
            data_dir=data_dir,
            transform=transform,
            support_size=support_size
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )
        
        print(f"\nğŸ“Š Support set statistics:")
        print(f"   â€¢ Total samples: {len(dataset)}")
        print(f"   â€¢ Samples per user: {support_size}")
        print(f"   â€¢ Number of users: {len(set([s['label'] for s in dataset.samples]))}")
        
        # æå–ç‰¹å¾
        print("\nğŸ¯ Extracting features...")
        features, labels, user_names = self.extract_features(dataloader)
        print(f"âœ“ Extracted features: {features.shape}")
        
        # è®¡ç®—åŸå‹
        print("\nğŸ“ Computing prototypes...")
        prototypes = self.compute_prototypes(features, labels)
        print(f"âœ“ Computed prototypes: {prototypes.shape}")
        
        # ä¿å­˜
        save_dict = {
            'prototypes': prototypes,
            'user_ids': list(range(prototypes.shape[0])),
            'feature_dim': prototypes.shape[1],
            'metadata': {
                'model_type': 'ImprovedClassifier',
                'model_path': str(self.model._get_name()),
                'support_size': support_size,
                'num_samples': len(dataset),
                'feature_extraction': 'backbone_direct',
                'timestamp': datetime.now().isoformat(),
            },
            'user_stats': {}
        }
        
        # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„ä¿¡æ¯
        print("\nğŸ“ˆ Prototype statistics:")
        for i in range(prototypes.shape[0]):
            user_samples = sum(1 for l in labels if l == i)
            norm = prototypes[i].norm(2).item()
            save_dict['user_stats'][f'ID_{i+1}'] = {
                'samples': user_samples,
                'prototype_norm': norm
            }
            print(f"   â€¢ ID_{i+1}: {user_samples} samples, prototype norm: {norm:.3f}")
        
        # ä¿å­˜æ–‡ä»¶
        torch.save(save_dict, output_path)
        print(f"\nğŸ’¾ Prototypes saved to: {output_path}")
        
        print("\nâœ… Prototype building completed successfully!")
        return save_dict


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Build prototypes for ImprovedClassifier')
    
    # Kaggleç¯å¢ƒé»˜è®¤è·¯å¾„
    parser.add_argument('--data-dir', type=str, 
                       default='/kaggle/input/backpack/backpack',
                       help='Path to target domain data (èƒŒåŒ…æ­¥æ€)')
    parser.add_argument('--model-path', type=str,
                       default='/kaggle/working/VA-VAE/improved_classifier/best_improved_classifier.pth',
                       help='Path to ImprovedClassifier model')
    parser.add_argument('--output-path', type=str,
                       default='/kaggle/working/improved_prototypes.pt',
                       help='Path to save prototypes')
    parser.add_argument('--support-size', type=int, default=15,
                       help='Number of support samples per user')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='Batch size for feature extraction')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (cuda/cpu)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŸå‹æ„å»ºå™¨
    builder = ImprovedPrototypeBuilder(
        model_path=args.model_path,
        device=args.device
    )
    
    # æ„å»ºå¹¶ä¿å­˜åŸå‹
    builder.build_and_save(
        data_dir=args.data_dir,
        output_path=args.output_path,
        support_size=args.support_size,
        batch_size=args.batch_size
    )


if __name__ == '__main__':
    main()

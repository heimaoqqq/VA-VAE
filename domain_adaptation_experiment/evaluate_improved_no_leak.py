#!/usr/bin/env python3
"""
è¯„ä¼°ImprovedClassifier + PNC - ä¸¥æ ¼æ’é™¤æ”¯æŒé›†ï¼Œé¿å…æ•°æ®æ³„æ¼
"""

import torch
import argparse
from pathlib import Path
import numpy as np
from tabulate import tabulate
import sys
sys.path.append(str(Path(__file__).parent.parent))

from improved_classifier_training import ImprovedClassifier
from build_improved_prototypes_with_split import SplitTargetDomainDataset
from torch.utils.data import DataLoader
from tqdm import tqdm
import torchvision.transforms as transforms


class NoLeakEvaluator:
    """æ— æ•°æ®æ³„æ¼çš„è¯„ä¼°å™¨"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.test_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
    
    def load_model(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¦ Loading model from: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        model = ImprovedClassifier(
            num_classes=checkpoint.get('num_classes', 31),
            backbone=checkpoint.get('backbone', 'resnet18')
        ).to(self.device)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.eval()
        print(f"âœ… Model loaded successfully")
        return model
    
    def evaluate(self, model, data_dir, support_size, seed, 
                 use_prototypes=False, prototype_path=None,
                 fusion_alpha=0.5, similarity_tau=0.1, use_pure_ncc=False):
        """è¯„ä¼°æ¨¡å‹ - ä½¿ç”¨ä¸¥æ ¼åˆ’åˆ†çš„æµ‹è¯•é›†"""
        
        mode = "with PNC" if use_prototypes else "Baseline"
        print(f"\nğŸ¯ Evaluating ({mode})...")
        
        # åˆ›å»ºæµ‹è¯•é›†ï¼ˆæ’é™¤æ”¯æŒé›†ï¼‰
        test_dataset = SplitTargetDomainDataset(
            data_dir=data_dir,
            transform=self.test_transform,
            support_size=support_size,
            mode='test',  # å…³é”®ï¼šåªä½¿ç”¨æµ‹è¯•é›†
            seed=seed
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=32,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # åŠ è½½åŸå‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        prototypes = None
        if use_prototypes:
            if prototype_path is None:
                raise ValueError("Prototype path required for PNC")
            
            print(f"ğŸ“¦ Loading prototypes from: {prototype_path}")
            proto_dict = torch.load(prototype_path, map_location=self.device, weights_only=False)
            prototypes = proto_dict['prototypes'].to(self.device)
            
            # éªŒè¯æ•°æ®åˆ’åˆ†ä¸€è‡´æ€§
            if 'metadata' in proto_dict and proto_dict['metadata'].get('data_split') == 'strict_split':
                print(f"âœ“ Using strict data split - no overlap between support and test")
            else:
                print(f"âš ï¸ Warning: Prototypes may not use strict split")
            
            print(f"âœ“ Loaded prototypes: {prototypes.shape}")
            print(f"   â€¢ Fusion weight (Î±): {fusion_alpha:.2f}")
            print(f"   â€¢ Temperature (Ï„): {similarity_tau:.2f}")
        
        # è¯„ä¼°
        all_predictions = []
        all_labels = []
        all_confidences = []
        
        model.eval()
        with torch.no_grad():
            for images, labels, _ in tqdm(test_loader, desc="Evaluating"):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # æ¨¡å‹é¢„æµ‹
                outputs = model(images)
                
                if use_prototypes:
                    # æå–ç‰¹å¾
                    features = model.backbone(images)
                    
                    # L2å½’ä¸€åŒ–
                    features = features / features.norm(2, dim=1, keepdim=True)
                    
                    # è®¡ç®—ä¸åŸå‹çš„ç›¸ä¼¼åº¦
                    similarities = torch.matmul(features, prototypes.T)
                    
                    if use_pure_ncc:
                        # çº¯NCCï¼šåªä½¿ç”¨åŸå‹ç›¸ä¼¼åº¦
                        predicted = similarities.argmax(dim=1)
                        confidences = torch.softmax(similarities, dim=1).max(dim=1)[0]
                    else:
                        # åŸå§‹PNCï¼šåŸå‹+åˆ†ç±»å™¨èåˆ
                        proto_probs = torch.softmax(similarities / similarity_tau, dim=1)
                        class_probs = torch.softmax(outputs, dim=1)
                        final_probs = fusion_alpha * proto_probs + (1 - fusion_alpha) * class_probs
                        predicted = final_probs.argmax(dim=1)
                        confidences = final_probs.max(dim=1)[0]
                else:
                    predicted = outputs.argmax(dim=1)
                    confidences = torch.softmax(outputs, dim=1).max(dim=1)[0]
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_confidences.extend(confidences.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))
        mean_confidence = np.mean(all_confidences)
        
        return {
            'accuracy': accuracy,
            'confidence': mean_confidence,
            'total_samples': len(all_labels)
        }


def main():
    parser = argparse.ArgumentParser(description='Evaluate without data leakage')
    
    parser.add_argument('--model-path', type=str,
                       default='/kaggle/working/VA-VAE/improved_classifier/best_improved_classifier.pth')
    parser.add_argument('--data-dir', type=str,
                       default='/kaggle/input/backpack/backpack')
    parser.add_argument('--prototype-path', type=str,
                       default='/kaggle/working/improved_prototypes_split.pt')
    parser.add_argument('--support-size', type=int, default=10)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--fusion-alpha', type=float, default=0.5)
    parser.add_argument('--similarity-tau', type=float, default=0.01)
    parser.add_argument('--use-pure-ncc', action='store_true', 
                       help='Use pure NCC instead of PNC fusion')
    
    args = parser.parse_args()
    
    evaluator = NoLeakEvaluator()
    model = evaluator.load_model(args.model_path)
    
    print("\n" + "="*80)
    print("ğŸ“Š EVALUATION WITHOUT DATA LEAKAGE")
    print("="*80)
    
    # åŸºçº¿è¯„ä¼°
    baseline_results = evaluator.evaluate(
        model=model,
        data_dir=args.data_dir,
        support_size=args.support_size,
        seed=args.seed,
        use_prototypes=False
    )
    
    # PNCè¯„ä¼°
    pnc_results = evaluator.evaluate(
        model=model,
        data_dir=args.data_dir,
        support_size=args.support_size,
        seed=args.seed,
        use_prototypes=True,
        prototype_path=args.prototype_path,
        fusion_alpha=args.fusion_alpha,
        similarity_tau=args.similarity_tau,
        use_pure_ncc=args.use_pure_ncc
    )
    
    # ç»“æœå¯¹æ¯”
    print("\n" + "="*80)
    print("ğŸ“ˆ REAL RESULTS (NO DATA LEAKAGE)")
    print("="*80)
    
    improvement = pnc_results['accuracy'] - baseline_results['accuracy']
    
    comparison_table = [
        ["Metric", "Baseline", "PNC", "Improvement"],
        ["Accuracy", f"{baseline_results['accuracy']:.2%}", 
         f"{pnc_results['accuracy']:.2%}", f"{improvement:+.2%}"],
        ["Confidence", f"{baseline_results['confidence']:.3f}",
         f"{pnc_results['confidence']:.3f}", 
         f"{pnc_results['confidence']-baseline_results['confidence']:+.3f}"],
        ["Test Samples", baseline_results['total_samples'],
         pnc_results['total_samples'], "-"]
    ]
    
    print(tabulate(comparison_table, headers="firstrow", tablefmt="grid"))
    
    # è¯„ä¼°ç»“è®º
    print(f"\nğŸ HONEST ASSESSMENT:")
    if improvement > 0.05:
        print(f"   âœ… Significant improvement: {improvement:+.2%}")
    elif improvement > 0.02:
        print(f"   ğŸŸ¡ Moderate improvement: {improvement:+.2%}")
    elif improvement > 0:
        print(f"   ğŸŸ  Slight improvement: {improvement:+.2%}")
    else:
        print(f"   âŒ No improvement: {improvement:+.2%}")
    
    print(f"\nğŸ’¡ This is the REAL performance without data leakage!")
    print("="*80)


if __name__ == '__main__':
    main()

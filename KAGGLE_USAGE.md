# Kaggleæ•°æ®é›†ä½¿ç”¨è¯´æ˜

## ğŸ¯ é’ˆå¯¹æ‚¨çš„æ•°æ®é›†ç»“æ„

### ğŸ“ æ‚¨çš„æ•°æ®é›†ç»“æ„
```
/kaggle/input/dataset/
â”œâ”€â”€ ID_1/
â”‚   â”œâ”€â”€ image_001.png
â”‚   â”œâ”€â”€ image_002.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ID_2/
â”‚   â”œâ”€â”€ image_001.png
â”‚   â””â”€â”€ ...
...
â””â”€â”€ ID_31/
    â”œâ”€â”€ image_001.png
    â””â”€â”€ ...
```

## ğŸ“Š å…³äºæµ‹è¯•é›†çš„å»ºè®®

### ğŸ” åŸé¡¹ç›®åˆ†æç»“æœ
- **LightningDiT**: ä½¿ç”¨Accelerator + DDPï¼Œä¸»è¦ç”¨äºç”Ÿæˆä»»åŠ¡
- **VA-VAE**: ä½¿ç”¨pytorch-lightningï¼Œé€šå¸¸åªæœ‰train/valåˆ’åˆ†
- **å»ºè®®**: å¯¹äºæ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œ**åªä½¿ç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†å³å¯**

### âœ… æ¨èçš„æ•°æ®åˆ’åˆ†
```bash
# æ¨èï¼šåªåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (8:2)
python data_split.py \
    --input_dir /kaggle/input/dataset \
    --output_dir /kaggle/working/data_split \
    --train_ratio 0.8 \
    --val_ratio 0.2 \
    --image_extensions png,jpg,jpeg

# å¦‚æœåšæŒè¦æµ‹è¯•é›† (7:2:1)
python data_split.py \
    --input_dir /kaggle/input/dataset \
    --output_dir /kaggle/working/data_split \
    --train_ratio 0.7 \
    --val_ratio 0.2 \
    --use_test_set \
    --test_ratio 0.1 \
    --image_extensions png,jpg,jpeg
```

## ğŸš€ å®Œæ•´ä½¿ç”¨æµç¨‹

### Step 1: æ•°æ®é›†åˆ’åˆ†
```bash
# åœ¨Kaggleç¯å¢ƒä¸­è¿è¡Œ
python data_split.py \
    --input_dir /kaggle/input/dataset \
    --output_dir /kaggle/working/data_split \
    --train_ratio 0.8 \
    --val_ratio 0.2 \
    --seed 42

# åˆ’åˆ†åçš„ç»“æ„ï¼š
# /kaggle/working/data_split/
# â”œâ”€â”€ train/
# â”‚   â”œâ”€â”€ user_01_sample_001.png
# â”‚   â”œâ”€â”€ user_01_sample_002.png
# â”‚   â”œâ”€â”€ user_02_sample_001.png
# â”‚   â””â”€â”€ ...
# â”œâ”€â”€ val/
# â”‚   â”œâ”€â”€ user_01_sample_010.png
# â”‚   â”œâ”€â”€ user_02_sample_008.png
# â”‚   â””â”€â”€ ...
# â””â”€â”€ split_info.txt
```

### Step 2: å•GPUè®­ç»ƒï¼ˆKaggleé»˜è®¤ï¼‰
```bash
python minimal_training_modification.py \
    --data_dir /kaggle/working/data_split \
    --original_vavae /path/to/vavae.pth \
    --batch_size 16 \
    --epochs 100 \
    --lr 1e-4 \
    --output_dir /kaggle/working/outputs
```

### Step 3: åŒGPUè®­ç»ƒï¼ˆå¦‚æœKaggleæä¾›ï¼‰
```bash
# æ£€æŸ¥GPUæ•°é‡
nvidia-smi

# å¦‚æœæœ‰2ä¸ªGPU
python minimal_training_modification.py \
    --data_dir /kaggle/working/data_split \
    --original_vavae /path/to/vavae.pth \
    --batch_size 16 \
    --epochs 100 \
    --lr 1e-4 \
    --output_dir /kaggle/working/outputs \
    --distributed \
    --world_size 2
```

## ğŸ”§ å…³äºå¤šGPUæ”¯æŒ

### ğŸ“‹ åŸé¡¹ç›®vsæˆ‘ä»¬çš„å®ç°

| é¡¹ç›® | å¤šGPUæ–¹æ¡ˆ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|-----------|------|------|
| **LightningDiT** | HuggingFace Accelerator | ç®€å•æ˜“ç”¨ï¼Œè‡ªåŠ¨å¤„ç† | ä¾èµ–é¢å¤–åº“ |
| **VA-VAE** | PyTorch Lightning | åŠŸèƒ½ä¸°å¯Œï¼Œæ˜“é…ç½® | æ¡†æ¶é‡é‡çº§ |
| **æˆ‘ä»¬çš„å®ç°** | åŸç”ŸPyTorch DDP | è½»é‡çº§ï¼Œå¯æ§æ€§å¼º | éœ€è¦æ‰‹åŠ¨é…ç½® |

### âœ… æˆ‘ä»¬çš„å¤šGPUå®ç°ä¼˜åŠ¿
1. **æœ€å°ä¾èµ–**: åªä½¿ç”¨PyTorchåŸç”ŸåŠŸèƒ½
2. **å®Œå…¨å…¼å®¹**: ä¸åŸVA-VAEæ¶æ„å®Œå…¨å…¼å®¹
3. **æŸå¤±åŒæ­¥**: æ­£ç¡®å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æŸå¤±ä¼ é€’
4. **å†…å­˜æ•ˆç‡**: æ¯ä¸ªGPUå¤„ç†ä¸åŒæ•°æ®å­é›†

## ğŸ¯ é’ˆå¯¹æ‚¨æ•°æ®é›†çš„ç‰¹æ®Šå¤„ç†

### 1. è‡ªåŠ¨ç”¨æˆ·IDæ˜ å°„
- **ID_1** â†’ **user_01**
- **ID_2** â†’ **user_02**
- ...
- **ID_31** â†’ **user_31**

### 2. å›¾åƒæ ¼å¼æ”¯æŒ
- æ”¯æŒ PNG, JPG, JPEG ç­‰å¸¸è§æ ¼å¼
- è‡ªåŠ¨è½¬æ¢ä¸ºRGBæ ¼å¼ï¼ˆLightningDiTè¦æ±‚ï¼‰
- è‡ªåŠ¨è°ƒæ•´å°ºå¯¸åˆ°256Ã—256

### 3. æŒ‰ç”¨æˆ·åˆ’åˆ†
- ç¡®ä¿æ¯ä¸ªç”¨æˆ·åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­éƒ½æœ‰æ ·æœ¬
- ä¿æŒç”¨æˆ·å†…æ ·æœ¬çš„æ—¶åºå…³ç³»ï¼ˆå¦‚æœé‡è¦ï¼‰

## âš ï¸ Kaggleç¯å¢ƒæ³¨æ„äº‹é¡¹

### 1. å­˜å‚¨é™åˆ¶
```bash
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h /kaggle/working

# å¦‚æœç©ºé—´ä¸è¶³ï¼Œå¯ä»¥å‡å°‘è¾“å‡ºæ–‡ä»¶
# æˆ–è€…ç›´æ¥åœ¨åŸå§‹æ•°æ®ä¸Šè®­ç»ƒï¼ˆä¸æ¨èï¼‰
```

### 2. å†…å­˜é™åˆ¶
```bash
# Kaggleé€šå¸¸æä¾›16GB RAM
# å¦‚æœå†…å­˜ä¸è¶³ï¼Œå‡å°‘batch_size
python minimal_training_modification.py --batch_size 8
```

### 3. GPUé™åˆ¶
```bash
# Kaggleé€šå¸¸æä¾›å•ä¸ªGPU (P100/T4)
# æ£€æŸ¥GPUç±»å‹å’Œå†…å­˜
nvidia-smi
```

### 4. æ—¶é—´é™åˆ¶
```bash
# Kaggleæœ‰è¿è¡Œæ—¶é—´é™åˆ¶
# å»ºè®®è®¾ç½®è¾ƒå°‘çš„epochsè¿›è¡Œæµ‹è¯•
python minimal_training_modification.py --epochs 50
```

## ğŸ” éªŒè¯æ•°æ®åŠ è½½

### æµ‹è¯•æ•°æ®é›†ç±»
```python
# åœ¨Kaggle notebookä¸­æµ‹è¯•
from minimal_micro_doppler_dataset import MicroDopplerDataset

# æµ‹è¯•åŸå§‹ç»“æ„
dataset = MicroDopplerDataset(
    '/kaggle/input/dataset', 
    original_structure=True
)
print(f"åŸå§‹æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"ç”¨æˆ·æ•°é‡: {dataset.num_users}")

# æµ‹è¯•åˆ’åˆ†åç»“æ„
train_dataset = MicroDopplerDataset(
    '/kaggle/working/data_split', 
    split='train'
)
print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")

# æµ‹è¯•åŠ è½½ä¸€ä¸ªæ ·æœ¬
sample = dataset[0]
print(f"æ ·æœ¬å½¢çŠ¶: {sample['image'].shape}")
print(f"ç”¨æˆ·ID: {sample['user_id']}")
```

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

### Kaggle P100 GPU
- **æ‰¹æ¬¡å¤§å°**: 16-24
- **è®­ç»ƒé€Ÿåº¦**: ~3-4ç§’/æ‰¹æ¬¡
- **å†…å­˜ä½¿ç”¨**: ~10-12GB

### Kaggle T4 GPU  
- **æ‰¹æ¬¡å¤§å°**: 12-16
- **è®­ç»ƒé€Ÿåº¦**: ~2-3ç§’/æ‰¹æ¬¡
- **å†…å­˜ä½¿ç”¨**: ~8-10GB

## ğŸ¯ å®éªŒå»ºè®®

### 1. å¿«é€ŸéªŒè¯
```bash
# å…ˆç”¨å°æ•°æ®é›†éªŒè¯æµç¨‹
python data_split.py --input_dir /kaggle/input/dataset --output_dir /kaggle/working/test_split --train_ratio 0.9 --val_ratio 0.1

# çŸ­æ—¶é—´è®­ç»ƒæµ‹è¯•
python minimal_training_modification.py --epochs 5 --batch_size 8
```

### 2. æ­£å¼è®­ç»ƒ
```bash
# ç¡®è®¤æµç¨‹æ— è¯¯åè¿›è¡Œæ­£å¼è®­ç»ƒ
python minimal_training_modification.py --epochs 100 --batch_size 16
```

### 3. ç»“æœä¿å­˜
```bash
# å°†é‡è¦ç»“æœä¿å­˜åˆ°Kaggleè¾“å‡º
cp /kaggle/working/outputs/best_model.pth /kaggle/working/
cp /kaggle/working/data_split/split_info.txt /kaggle/working/
```

è¿™æ ·æ‚¨å°±å¯ä»¥åœ¨Kaggleç¯å¢ƒä¸­é«˜æ•ˆåœ°ä½¿ç”¨æˆ‘ä»¬çš„æœ€å°æ”¹åŠ¨ç‰ˆæœ¬è¿›è¡Œå®éªŒäº†ï¼

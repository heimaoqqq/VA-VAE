#!/usr/bin/env python3
"""
æ­¥éª¤5: æå–æ½œåœ¨ç‰¹å¾
- ä½¿ç”¨å¾®è°ƒåçš„VA-VAEç¼–ç å™¨
- æå–æ‰€æœ‰è®­ç»ƒæ•°æ®çš„æ½œåœ¨ç‰¹å¾
- ä¿å­˜ä¸ºæ‰©æ•£æ¨¡å‹è®­ç»ƒæ ¼å¼
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
import pickle

def load_finetuned_vavae(checkpoint_path):
    """åŠ è½½å¾®è°ƒåçš„VA-VAEæ¨¡å‹"""
    print(f"ğŸ“¥ åŠ è½½å¾®è°ƒåçš„VA-VAE: {checkpoint_path}")
    
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„checkpointæ ¼å¼æ¥åŠ è½½
    # é€šå¸¸Lightningä¿å­˜çš„checkpointåŒ…å«model state_dict
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # æ ¹æ®å®é™…ä¿å­˜æ ¼å¼è°ƒæ•´
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # åŠ è½½æ¨¡å‹ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ¨¡å‹ç±»æ¥è°ƒæ•´ï¼‰
        from LightningDiT.tokenizer.vavae import VAVAE
        model = VAVAE()
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        
        print("âœ… VA-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
        
    except Exception as e:
        print(f"âŒ åŠ è½½VA-VAEå¤±è´¥: {e}")
        return None

def extract_features_from_dataset(model, dataset_dir, output_dir, split_name):
    """ä»æ•°æ®é›†æå–æ½œåœ¨ç‰¹å¾"""
    print(f"\nğŸ”„ æå– {split_name} é›†ç‰¹å¾...")
    
    split_dir = Path(dataset_dir) / split_name
    output_split_dir = Path(output_dir) / split_name
    output_split_dir.mkdir(parents=True, exist_ok=True)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_samples = 0
    user_counts = {}
    
    # éå†æ¯ä¸ªç”¨æˆ·ç›®å½•
    for user_id in range(1, 32):  # ç”¨æˆ·1åˆ°31
        user_dir = split_dir / f"user{user_id}"
        if not user_dir.exists():
            continue
        
        print(f"  å¤„ç†ç”¨æˆ·{user_id}...")
        
        # åˆ›å»ºè¾“å‡ºç”¨æˆ·ç›®å½•
        output_user_dir = output_split_dir / f"user{user_id}"
        output_user_dir.mkdir(exist_ok=True)
        
        # å¤„ç†è¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒ
        image_files = list(user_dir.glob("*.png")) + \
                     list(user_dir.glob("*.jpg")) + \
                     list(user_dir.glob("*.jpeg"))
        
        user_samples = 0
        for img_file in tqdm(image_files, desc=f"ç”¨æˆ·{user_id}"):
            try:
                # åŠ è½½å›¾åƒ
                from PIL import Image
                image = Image.open(img_file).convert('RGB')
                image = image.resize((256, 256), Image.LANCZOS)
                
                # è½¬æ¢ä¸ºtensor
                image_array = np.array(image).astype(np.float32) / 127.5 - 1.0
                image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).unsqueeze(0)
                image_tensor = image_tensor.to(device)
                
                # æå–æ½œåœ¨ç‰¹å¾
                with torch.no_grad():
                    latent = model.encode(image_tensor)
                    if hasattr(latent, 'sample'):
                        latent = latent.sample()
                
                # ä¿å­˜æ½œåœ¨ç‰¹å¾
                latent_file = output_user_dir / f"{img_file.stem}.pt"
                torch.save({
                    'latent': latent.cpu(),
                    'user_id': user_id - 1,  # è½¬æ¢ä¸º0-30ç´¢å¼•
                    'original_file': str(img_file)
                }, latent_file)
                
                user_samples += 1
                total_samples += 1
                
            except Exception as e:
                print(f"    âŒ å¤„ç†å¤±è´¥ {img_file}: {e}")
        
        user_counts[user_id] = user_samples
        print(f"    âœ… ç”¨æˆ·{user_id}: {user_samples} ä¸ªç‰¹å¾")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'split': split_name,
        'total_samples': total_samples,
        'user_counts': user_counts,
        'latent_shape': latent.shape[1:] if 'latent' in locals() else None
    }
    
    stats_file = output_split_dir / "extraction_stats.pkl"
    with open(stats_file, 'wb') as f:
        pickle.dump(stats, f)
    
    print(f"  âœ… {split_name} é›†å®Œæˆ: {total_samples} ä¸ªç‰¹å¾")
    return stats

def create_latent_dataset_config(output_dir, train_stats, val_stats):
    """åˆ›å»ºæ½œåœ¨ç‰¹å¾æ•°æ®é›†é…ç½®"""
    print("\nğŸ“ åˆ›å»ºæ½œåœ¨ç‰¹å¾æ•°æ®é›†é…ç½®...")
    
    config_content = f"""# å¾®å¤šæ™®å‹’æ½œåœ¨ç‰¹å¾æ•°æ®é›†é…ç½®
# ç”¨äºLightningDiTæ‰©æ•£æ¨¡å‹è®­ç»ƒ

dataset:
  name: "micro_doppler_latents"
  num_users: 31
  latent_shape: {train_stats['latent_shape']}
  
  # æ•°æ®è·¯å¾„
  train_dir: "{output_dir}/train"
  val_dir: "{output_dir}/val"
  
  # æ•°æ®ç»Ÿè®¡
  train_samples: {train_stats['total_samples']}
  val_samples: {val_stats['total_samples']}
  
  # ç”¨æˆ·åˆ†å¸ƒ
  train_user_counts: {train_stats['user_counts']}
  val_user_counts: {val_stats['user_counts']}

# LightningDiTè®­ç»ƒé…ç½®
lightningdit:
  model_type: "LightningDiT-XL"
  in_chans: {train_stats['latent_shape'][0] if train_stats['latent_shape'] else 32}
  num_classes: 31  # 31ä¸ªç”¨æˆ·
  
  # è®­ç»ƒå‚æ•°
  batch_size: 2  # T4Ã—2 GPU
  learning_rate: 1.0e-04
  max_epochs: 800
  
  # é‡‡æ ·å‚æ•°
  num_sampling_steps: 50
  cfg_scale: 4.0
"""
    
    config_file = Path(output_dir) / "latent_dataset_config.yaml"
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    print(f"âœ… æ½œåœ¨ç‰¹å¾é…ç½®å·²ä¿å­˜: {config_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ­¥éª¤5: æå–æ½œåœ¨ç‰¹å¾')
    parser.add_argument('--dataset_dir', type=str, default='micro_doppler_dataset',
                       help='åŸå§‹æ•°æ®é›†ç›®å½•')
    parser.add_argument('--checkpoint_path', type=str, required=True,
                       help='å¾®è°ƒåçš„VA-VAE checkpointè·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='micro_doppler_latents',
                       help='æ½œåœ¨ç‰¹å¾è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¯ æ­¥éª¤5: æå–æ½œåœ¨ç‰¹å¾")
    print("=" * 60)
    print(f"æ•°æ®é›†ç›®å½•: {args.dataset_dir}")
    print(f"VA-VAE checkpoint: {args.checkpoint_path}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # 1. æ£€æŸ¥è¾“å…¥
    dataset_path = Path(args.dataset_dir)
    if not dataset_path.exists():
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {args.dataset_dir}")
        return False
    
    checkpoint_path = Path(args.checkpoint_path)
    if not checkpoint_path.exists():
        print(f"âŒ VA-VAE checkpointä¸å­˜åœ¨: {args.checkpoint_path}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python step4_finetune_vavae.py")
        return False
    
    # 2. åŠ è½½å¾®è°ƒåçš„VA-VAE
    model = load_finetuned_vavae(args.checkpoint_path)
    if model is None:
        return False
    
    # 3. åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # 4. æå–è®­ç»ƒé›†ç‰¹å¾
    train_stats = extract_features_from_dataset(
        model, args.dataset_dir, args.output_dir, 'train'
    )
    
    # 5. æå–éªŒè¯é›†ç‰¹å¾
    val_stats = extract_features_from_dataset(
        model, args.dataset_dir, args.output_dir, 'val'
    )
    
    # 6. åˆ›å»ºé…ç½®æ–‡ä»¶
    create_latent_dataset_config(args.output_dir, train_stats, val_stats)
    
    print("\nâœ… æ­¥éª¤5å®Œæˆï¼æ½œåœ¨ç‰¹å¾æå–å®Œæˆ")
    print(f"ğŸ“ ç‰¹å¾ä½ç½®: {args.output_dir}")
    print(f"ğŸ“Š è®­ç»ƒé›†: {train_stats['total_samples']} ä¸ªç‰¹å¾")
    print(f"ğŸ“Š éªŒè¯é›†: {val_stats['total_samples']} ä¸ªç‰¹å¾")
    print("ğŸ“‹ ä¸‹ä¸€æ­¥: python step6_train_diffusion_model.py")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

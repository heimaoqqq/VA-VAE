#!/usr/bin/env python3
"""
æµ‹è¯•å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
éªŒè¯ç¯å¢ƒå’Œæ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import torch
import yaml
from pathlib import Path

# æ·»åŠ LightningDiTåˆ°è·¯å¾„
sys.path.insert(0, 'LightningDiT')

def test_environment():
    """æµ‹è¯•åŸºç¡€ç¯å¢ƒ"""
    print("ğŸ§ª æµ‹è¯•ç¯å¢ƒ")
    print("-" * 30)
    
    try:
        import torch
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    
    try:
        import accelerate
        print(f"âœ… Accelerate: {accelerate.__version__}")
    except ImportError:
        print("âŒ Accelerateæœªå®‰è£…")
        return False
    
    return True

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\nğŸ”§ æµ‹è¯•æ¨¡å‹åŠ è½½")
    print("-" * 30)
    
    models_dir = Path("./official_models")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    required_files = [
        "vavae-imagenet256-f16d32-dinov2.pt",
        "lightningdit-xl-imagenet256-800ep.pt", 
        "latents_stats.pt"
    ]
    
    for file_name in required_files:
        file_path = models_dir / file_name
        if file_path.exists():
            print(f"âœ… {file_name}: {file_path.stat().st_size / (1024*1024):.1f} MB")
        else:
            print(f"âŒ {file_name}: æ–‡ä»¶ä¸å­˜åœ¨")
            return False
    
    # æµ‹è¯•åŠ è½½VA-VAE
    try:
        print("\nğŸ” æµ‹è¯•VA-VAEåŠ è½½...")
        from tokenizer.vavae import VA_VAE
        
        # æ›´æ–°é…ç½®æ–‡ä»¶
        vavae_config_path = "LightningDiT/tokenizer/configs/vavae_f16d32.yaml"
        update_vavae_config(vavae_config_path, models_dir)
        
        vae = VA_VAE(vavae_config_path)
        vae.load()
        print("âœ… VA-VAEåŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç¼–ç è§£ç 
        test_tensor = torch.randn(1, 3, 256, 256)
        with torch.no_grad():
            latent = vae.encode_to_latent(test_tensor)
            print(f"âœ… ç¼–ç æµ‹è¯•: {test_tensor.shape} -> {latent.shape}")
            
            decoded = vae.decode_to_images(latent)
            print(f"âœ… è§£ç æµ‹è¯•: {latent.shape} -> {decoded.shape}")
        
    except Exception as e:
        print(f"âŒ VA-VAEåŠ è½½å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•åŠ è½½LightningDiT
    try:
        print("\nğŸ” æµ‹è¯•LightningDiTåŠ è½½...")
        from models.lightningdit import LightningDiT_models
        
        model = LightningDiT_models['LightningDiT-XL/1'](
            input_size=16,  # 256/16
            num_classes=1000,
            in_channels=32
        )
        
        # åŠ è½½æƒé‡
        checkpoint_path = models_dir / "lightningdit-xl-imagenet256-800ep.pt"
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        model.load_state_dict(state_dict, strict=False)
        print("âœ… LightningDiTåŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            x = torch.randn(1, 32, 16, 16)  # æ½œåœ¨ç‰¹å¾
            t = torch.randint(0, 1000, (1,))  # æ—¶é—´æ­¥
            y = torch.randint(0, 1000, (1,))  # ç±»åˆ«
            
            output = model(x, t, y)
            print(f"âœ… å‰å‘ä¼ æ’­æµ‹è¯•: {x.shape} -> {output.shape}")
        
    except Exception as e:
        print(f"âŒ LightningDiTåŠ è½½å¤±è´¥: {e}")
        return False
    
    return True

def update_vavae_config(config_path, models_dir):
    """æ›´æ–°VA-VAEé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # æ›´æ–°æ£€æŸ¥ç‚¹è·¯å¾„
    config['ckpt_path'] = str(models_dir / "vavae-imagenet256-f16d32-dinov2.pt")
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)

def test_inference_pipeline():
    """æµ‹è¯•å®Œæ•´æ¨ç†æµç¨‹"""
    print("\nğŸ¯ æµ‹è¯•æ¨ç†æµç¨‹")
    print("-" * 30)
    
    try:
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®Œæ•´çš„æ¨ç†æµ‹è¯•
        # æš‚æ—¶åªåšåŸºç¡€æ£€æŸ¥
        print("âœ… æ¨ç†æµç¨‹å‡†å¤‡å°±ç»ª")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ LightningDiTå®˜æ–¹æ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    models_dir = Path("./official_models")
    if not models_dir.exists():
        print("âŒ å®˜æ–¹æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python setup_official_models.py")
        return
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("ç¯å¢ƒæµ‹è¯•", test_environment),
        ("æ¨¡å‹åŠ è½½æµ‹è¯•", test_model_loading),
        ("æ¨ç†æµç¨‹æµ‹è¯•", test_inference_pipeline)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name}: é€šè¿‡")
            else:
                print(f"âŒ {test_name}: å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name}: å¼‚å¸¸ - {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥è¿›è¡Œæ¨ç†äº†")
        print("ğŸš€ è¿è¡Œæ¨ç†: python run_official_inference.py")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")

if __name__ == "__main__":
    main()

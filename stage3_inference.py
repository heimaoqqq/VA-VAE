#!/usr/bin/env python3
"""
é˜¶æ®µ3: å›¾åƒç”Ÿæˆæ¨ç†
åŸºäºLightningDiTåŸé¡¹ç›®çš„inference.py
ä½¿ç”¨è®­ç»ƒå¥½çš„DiTæ¨¡å‹ç”Ÿæˆç”¨æˆ·æ¡ä»¶åŒ–çš„å¾®å¤šæ™®å‹’å›¾åƒ
"""

import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
import argparse
import os
from pathlib import Path
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from tqdm import tqdm
from accelerate import Accelerator

# å¯¼å…¥LightningDiTç»„ä»¶
import sys
import os

# ç¡®ä¿æ­£ç¡®çš„è·¯å¾„è®¾ç½®
current_dir = os.path.dirname(os.path.abspath(__file__))
lightningdit_path = os.path.join(current_dir, 'LightningDiT')
if lightningdit_path not in sys.path:
    sys.path.append(lightningdit_path)

from models.lightningdit import LightningDiT_models
from transport import create_transport
from tokenizer.vavae import VA_VAE

def test_imports():
    """æµ‹è¯•å¯¼å…¥æ˜¯å¦æ­£å¸¸"""
    print("ğŸ§ª æµ‹è¯•å¯¼å…¥...")

    try:
        # æµ‹è¯•RMSNorm
        try:
            from simple_rmsnorm import RMSNorm
            print("âœ… simple_rmsnormå¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âš ï¸  simple_rmsnormå¯¼å…¥å¤±è´¥: {e}")

        # æµ‹è¯•LightningDiTæ¨¡å‹
        from models import LightningDiT_models
        print("âœ… LightningDiT_modelså¯¼å…¥æˆåŠŸ")

        # æµ‹è¯•Transport
        from transport import create_transport
        print("âœ… Transportå¯¼å…¥æˆåŠŸ")

        # æµ‹è¯•VA-VAE
        from tokenizer.vavae import VA_VAE
        print("âœ… VA_VAEå¯¼å…¥æˆåŠŸ")

        # æµ‹è¯•Safetensors
        from safetensors.torch import load_file
        print("âœ… Safetensorså¯¼å…¥æˆåŠŸ")

        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        model = LightningDiT_models['LightningDiT-B/1'](
            input_size=16,
            num_classes=31,
            in_channels=32,
            use_qknorm=False,
            use_swiglu=True,
            use_rope=True,
            use_rmsnorm=True,
            wo_shift=False
        )
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")

        return True

    except Exception as e:
        print(f"âŒ å¯¼å…¥æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

class MicroDopplerGenerator:
    """å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆå™¨ (åŸºäºåŸé¡¹ç›®inference.py)"""

    def __init__(self, dit_checkpoint, vavae_config, model_name='LightningDiT-B/1', accelerator=None):
        self.model_name = model_name
        # æ”¯æŒåŒGPUæ¨ç†
        if accelerator is not None:
            self.accelerator = accelerator
            self.device = accelerator.device
            self.is_distributed = True
        else:
            self.accelerator = None
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.is_distributed = False
        
        # åŠ è½½VA-VAE (VA-VAEåœ¨åˆå§‹åŒ–æ—¶å·²ç»è®¾ç½®ä¸ºevalæ¨¡å¼å¹¶ç§»åˆ°GPU)
        if not self.is_distributed or self.accelerator.is_main_process:
            print("ğŸ“¥ åŠ è½½VA-VAE...")
        self.vavae = VA_VAE(vavae_config)

        # è®¾ç½®æ½œåœ¨ç‰¹å¾çš„å½’ä¸€åŒ–å‚æ•° (å‚è€ƒåŸé¡¹ç›®é…ç½®)
        self.latent_multiplier = 1.0  # VA-VAEä½¿ç”¨1.0è€Œä¸æ˜¯0.18215
        self.use_latent_norm = True

        # åŠ è½½æ½œåœ¨ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ (å¦‚æœå¯ç”¨)
        self.latent_mean = None
        self.latent_std = None
        # æ³¨æ„ï¼šåœ¨å®é™…ä½¿ç”¨ä¸­åº”è¯¥ä»latents_stats.ptåŠ è½½è¿™äº›ç»Ÿè®¡ä¿¡æ¯

        # åŠ è½½DiTæ¨¡å‹ (å‚è€ƒåŸé¡¹ç›®)
        if not self.is_distributed or self.accelerator.is_main_process:
            print("ğŸ“¥ åŠ è½½DiTæ¨¡å‹...")
        self._load_dit_model(dit_checkpoint)
        
        # åˆ›å»ºtransport (å‚è€ƒåŸé¡¹ç›®)
        self.transport = create_transport(
            path_type="Linear",
            prediction="velocity",
            loss_weight=None,
            train_eps=1e-5,
            sample_eps=1e-3
        )

        # åˆ›å»ºé‡‡æ ·å™¨ (ä½¿ç”¨åŸé¡¹ç›®çš„æ­£ç¡®å‚æ•°)
        from transport import Sampler
        self.sampler = Sampler(self.transport)
        self.sample_fn = self.sampler.sample_ode(
            sampling_method="euler",  # åŸé¡¹ç›®ä½¿ç”¨euler
            num_steps=250,
            atol=0.000001,           # åŸé¡¹ç›®é…ç½®
            rtol=0.001,              # åŸé¡¹ç›®é…ç½®
            timestep_shift=0.3,      # åŸé¡¹ç›®é…ç½®
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")

    def _get_num_classes_from_checkpoint(self, checkpoint_path):
        """ä»æ£€æŸ¥ç‚¹æˆ–æ•°æ®ä¸­è·å–æ­£ç¡®çš„ç±»åˆ«æ•°"""
        try:
            # æ–¹æ³•1: å°è¯•ä»è®­ç»ƒæ•°æ®ä¸­è·å–ç”¨æˆ·æ•°
            latent_dir = os.path.dirname(checkpoint_path).replace('trained_models', 'latent_features')
            train_file = os.path.join(latent_dir, 'train.safetensors')

            if os.path.exists(train_file):
                from safetensors import safe_open
                with safe_open(train_file, framework="pt", device="cpu") as f:
                    num_users = f.get_tensor('num_users').item()
                    if not self.is_distributed or self.accelerator.is_main_process:
                        print(f"ğŸ“Š ä»è®­ç»ƒæ•°æ®è·å–ç”¨æˆ·æ•°: {num_users}")
                    return num_users

            # æ–¹æ³•2: å¦‚æœæ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not self.is_distributed or self.accelerator.is_main_process:
                print("âš ï¸  æ— æ³•ä»è®­ç»ƒæ•°æ®è·å–ç”¨æˆ·æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼31")
            return 31

        except Exception as e:
            if not self.is_distributed or self.accelerator.is_main_process:
                print(f"âš ï¸  è·å–ç±»åˆ«æ•°å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼31")
            return 31

    def _load_dit_model(self, checkpoint_path):
        """åŠ è½½DiTæ¨¡å‹æ£€æŸ¥ç‚¹"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ£€æŸ¥ç‚¹æ ¼å¼æ¥è°ƒæ•´
        # å‚è€ƒåŸé¡¹ç›®çš„æ¨¡å‹åŠ è½½æ–¹å¼
        
        # é¦–å…ˆå°è¯•ä»æ£€æŸ¥ç‚¹ä¸­è·å–æ­£ç¡®çš„ç±»åˆ«æ•°
        num_classes = self._get_num_classes_from_checkpoint(checkpoint_path)

        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„æ¨¡å‹é…ç½®
        self.dit_model = LightningDiT_models[self.model_name](  # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
            input_size=16,  # 256/16=16 (downsample_ratio=16)
            num_classes=num_classes,  # ä»æ£€æŸ¥ç‚¹æˆ–æ•°æ®ä¸­è·å–çš„æ­£ç¡®ç±»åˆ«æ•°
            in_channels=32,  # VA-VAEä½¿ç”¨32é€šé“
            use_qknorm=False,
            use_swiglu=True,
            use_rope=True,
            use_rmsnorm=True,
            wo_shift=False
        )
        
        # åŠ è½½æƒé‡
        if checkpoint_path and os.path.exists(checkpoint_path):
            if not self.is_distributed or self.accelerator.is_main_process:
                print(f"ğŸ“¥ ä» {checkpoint_path} åŠ è½½æ¨¡å‹æƒé‡")

            # æ£€æŸ¥æ˜¯å¦æ˜¯Accelerateä¿å­˜çš„æ£€æŸ¥ç‚¹ç›®å½•
            # ä¼˜å…ˆæ£€æŸ¥safetensorsæ ¼å¼
            safetensors_path = os.path.join(checkpoint_path, "model.safetensors")
            pytorch_model_path = os.path.join(checkpoint_path, "pytorch_model.bin")

            if os.path.exists(safetensors_path):
                # Accelerateä¿å­˜çš„safetensorsæ ¼å¼
                if not self.is_distributed or self.accelerator.is_main_process:
                    print(f"ğŸ” å‘ç°Accelerateæ£€æŸ¥ç‚¹ (safetensors): {safetensors_path}")
                try:
                    from safetensors.torch import load_file
                    checkpoint = load_file(safetensors_path)
                    self.dit_model.load_state_dict(checkpoint)
                    if not self.is_distributed or self.accelerator.is_main_process:
                        print("âœ… æˆåŠŸåŠ è½½Accelerateæ£€æŸ¥ç‚¹ (safetensors)")
                except Exception as e:
                    if not self.is_distributed or self.accelerator.is_main_process:
                        print(f"âŒ åŠ è½½safetensorsæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                        print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
            elif os.path.exists(pytorch_model_path):
                # Accelerateä¿å­˜çš„pytorch_model.binæ ¼å¼
                if not self.is_distributed or self.accelerator.is_main_process:
                    print(f"ğŸ” å‘ç°Accelerateæ£€æŸ¥ç‚¹ (pytorch): {pytorch_model_path}")
                try:
                    checkpoint = torch.load(pytorch_model_path, map_location='cpu')
                    self.dit_model.load_state_dict(checkpoint)
                    if not self.is_distributed or self.accelerator.is_main_process:
                        print("âœ… æˆåŠŸåŠ è½½Accelerateæ£€æŸ¥ç‚¹ (pytorch)")
                except Exception as e:
                    if not self.is_distributed or self.accelerator.is_main_process:
                        print(f"âŒ åŠ è½½pytorchæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                        print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
            else:
                # å°è¯•ç›´æ¥åŠ è½½æ–‡ä»¶
                try:
                    if not self.is_distributed or self.accelerator.is_main_process:
                        print(f"ğŸ” å°è¯•ç›´æ¥åŠ è½½: {checkpoint_path}")
                    checkpoint = torch.load(checkpoint_path, map_location='cpu')

                    # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
                    if 'model' in checkpoint:
                        self.dit_model.load_state_dict(checkpoint['model'])
                    elif 'state_dict' in checkpoint:
                        self.dit_model.load_state_dict(checkpoint['state_dict'])
                    elif 'ema' in checkpoint:
                        self.dit_model.load_state_dict(checkpoint['ema'])
                    else:
                        self.dit_model.load_state_dict(checkpoint)

                    if not self.is_distributed or self.accelerator.is_main_process:
                        print("âœ… æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹")
                except Exception as e:
                    if not self.is_distributed or self.accelerator.is_main_process:
                        print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                        print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        else:
            if not self.is_distributed or self.accelerator.is_main_process:
                if checkpoint_path:
                    print(f"âš ï¸  æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
                else:
                    print("âš ï¸  æœªæŒ‡å®šæ£€æŸ¥ç‚¹è·¯å¾„")
                print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        
        self.dit_model.eval()
        self.dit_model.to(self.device)
    
    def generate_samples(self, user_ids, num_samples_per_user=4, guidance_scale=4.0, num_steps=250):
        """
        ç”Ÿæˆç”¨æˆ·æ¡ä»¶åŒ–çš„å¾®å¤šæ™®å‹’å›¾åƒ
        å‚è€ƒåŸé¡¹ç›®çš„é‡‡æ ·æ–¹æ³•
        """
        if not self.is_distributed or self.accelerator.is_main_process:
            print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
            print(f"  ç”¨æˆ·ID: {user_ids}")
            print(f"  æ¯ç”¨æˆ·æ ·æœ¬æ•°: {num_samples_per_user}")
            print(f"  å¼•å¯¼å°ºåº¦: {guidance_scale}")
            print(f"  é‡‡æ ·æ­¥æ•°: {num_steps}")
            if self.is_distributed:
                print(f"  åˆ†å¸ƒå¼æ¨ç†: {self.accelerator.num_processes} GPU")

        all_images = []
        all_user_labels = []

        # è®¡ç®—åˆ†å¸ƒå¼ä»»åŠ¡åˆ†é…
        total_samples = len(user_ids) * num_samples_per_user
        if self.is_distributed:
            samples_per_gpu = total_samples // self.accelerator.num_processes
            start_idx = self.accelerator.process_index * samples_per_gpu
            end_idx = start_idx + samples_per_gpu
            if self.accelerator.process_index == self.accelerator.num_processes - 1:
                end_idx = total_samples
        else:
            start_idx = 0
            end_idx = total_samples

        with torch.no_grad():
            sample_idx = 0
            for user_id in user_ids:
                for sample_num in range(num_samples_per_user):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰GPUè´Ÿè´£çš„æ ·æœ¬
                    if self.is_distributed and (sample_idx < start_idx or sample_idx >= end_idx):
                        sample_idx += 1
                        continue

                    # å‡†å¤‡æ¡ä»¶ (å‚è€ƒåŸé¡¹ç›®)
                    batch_size = 1  # æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ ·æœ¬
                    y = torch.full((batch_size,), user_id - 1, dtype=torch.long, device=self.device)  # 0-based

                    sample_idx += 1
                
                # ç”Ÿæˆéšæœºå™ªå£° (å‚è€ƒåŸé¡¹ç›®)
                z = torch.randn(batch_size, 32, 16, 16, device=self.device)

                # è®¾ç½®åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ (å‚è€ƒåŸé¡¹ç›®ç¬¬206-214è¡Œ)
                if guidance_scale > 1.0:
                    z = torch.cat([z, z], 0)  # å¤åˆ¶å™ªå£°
                    # null classåº”è¯¥æ˜¯num_classes (è¶…å‡ºæœ‰æ•ˆç±»åˆ«èŒƒå›´)
                    null_class = self.dit_model.y_embedder.num_classes
                    y_null = torch.tensor([null_class] * batch_size, device=self.device)
                    y = torch.cat([y, y_null], 0)
                    model_kwargs = dict(
                        y=y,
                        cfg_scale=guidance_scale,
                        cfg_interval=True,
                        cfg_interval_start=0.125  # åŸé¡¹ç›®é…ç½®
                    )
                    model_fn = self.dit_model.forward_with_cfg
                else:
                    model_kwargs = dict(y=y)
                    model_fn = self.dit_model.forward

                # ä½¿ç”¨æ­£ç¡®çš„é‡‡æ ·æ–¹æ³•
                samples = self._sample_with_transport(z, model_fn, model_kwargs, num_steps)

                # å¦‚æœä½¿ç”¨CFGï¼Œç§»é™¤null classæ ·æœ¬ (å‚è€ƒåŸé¡¹ç›®ç¬¬217-218è¡Œ)
                if guidance_scale > 1.0:
                    samples, _ = samples.chunk(2, dim=0)

                # åº”ç”¨æ½œåœ¨ç‰¹å¾åå½’ä¸€åŒ– (å‚è€ƒåŸé¡¹ç›®ç¬¬220è¡Œ)
                # samples = (samples * latent_std) / latent_multiplier + latent_mean
                if self.use_latent_norm:
                    # ç”±äºæˆ‘ä»¬æ²¡æœ‰latent_stats.ptï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                    # åŸé¡¹ç›®: samples = (samples * latent_std) / latent_multiplier + latent_mean
                    samples = samples / self.latent_multiplier  # ç®€åŒ–ç‰ˆæœ¬

                if not self.is_distributed or self.accelerator.is_main_process:
                    print(f"ğŸ” åå½’ä¸€åŒ–åæ ·æœ¬èŒƒå›´: [{samples.min():.3f}, {samples.max():.3f}]")

                # ä½¿ç”¨VA-VAEè§£ç ä¸ºå›¾åƒ
                images = self.vavae.decode_to_images(samples)

                # åå¤„ç†å›¾åƒ
                images = self._postprocess_images(images)
                
                all_images.extend(images)
                all_user_labels.extend([user_id] * num_samples_per_user)
        
        return all_images, all_user_labels
    
    def _sample_with_transport(self, z, model_fn, model_kwargs, num_steps):
        """
        ä½¿ç”¨transportè¿›è¡Œé‡‡æ · - æ­£ç¡®ç‰ˆæœ¬
        """
        # ä½¿ç”¨æ­£ç¡®çš„ODEé‡‡æ ·å™¨
        try:
            if not self.is_distributed or self.accelerator.is_main_process:
                print(f"ğŸ¯ ä½¿ç”¨ODEé‡‡æ ·å™¨ï¼Œæ­¥æ•°: {num_steps}")

            # ä½¿ç”¨é¢„é…ç½®çš„é‡‡æ ·å‡½æ•° (å‚è€ƒåŸé¡¹ç›®ç¬¬216è¡Œ)
            samples = self.sample_fn(z, model_fn, **model_kwargs)

            # è¿”å›æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç»“æœå¹¶ä¿®å¤ç»´åº¦
            if isinstance(samples, list):
                result = samples[-1]
            else:
                result = samples

            # ä¿®å¤ç»´åº¦é—®é¢˜ï¼šä»[num_steps, batch, channels, h, w]åˆ°[batch, channels, h, w]
            if result.dim() == 5:
                result = result[-1]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            elif result.dim() > 4:
                # å¦‚æœè¿˜æœ‰å…¶ä»–ç»´åº¦é—®é¢˜ï¼Œå¼ºåˆ¶reshape
                while result.dim() > 4:
                    result = result[-1]

            if not self.is_distributed or self.accelerator.is_main_process:
                print(f"ğŸ” é‡‡æ ·ç»“æœå½¢çŠ¶: {result.shape}")

            return result

        except Exception as e:
            if not self.is_distributed or self.accelerator.is_main_process:
                print(f"âš ï¸  ODEé‡‡æ ·å¤±è´¥: {e}")
                print("âš ï¸  ä½¿ç”¨ç®€åŒ–é‡‡æ ·æ–¹æ³•")

            # æ”¹è¿›çš„ç®€åŒ–é‡‡æ ·è¿‡ç¨‹
            dt = 1.0 / num_steps
            x = z.clone()

            for i in range(num_steps):
                # ä½¿ç”¨æ­£ç¡®çš„æ—¶é—´æ­¥ (ä»0åˆ°1)
                t = torch.full((x.shape[0],), i / num_steps, device=self.device)

                # æ¨¡å‹é¢„æµ‹ (velocity prediction)
                with torch.no_grad():
                    velocity = model_fn(x, t, **model_kwargs)

                # ä½¿ç”¨velocityè¿›è¡Œæ›´æ–° (Euleræ–¹æ³•)
                x = x + velocity * dt

            return x
    
    def _postprocess_images(self, images):
        """
        åå¤„ç†å›¾åƒ - æ”¹è¿›ç‰ˆæœ¬
        """
        if not self.is_distributed or self.accelerator.is_main_process:
            print(f"ğŸ” å›¾åƒåå¤„ç†è°ƒè¯•:")
            print(f"  è¾“å…¥å½¢çŠ¶: {images.shape}")
            print(f"  æ•°æ®ç±»å‹: {images.dtype}")
            print(f"  å€¼èŒƒå›´: [{images.min()}, {images.max()}]")

        # decode_to_imageså·²ç»è¿”å›äº†uint8æ ¼å¼çš„numpyæ•°ç»„ (B, H, W, C)
        pil_images = []
        for i, img_np in enumerate(images):
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if img_np.dtype != np.uint8:
                img_np = np.clip(img_np, 0, 255).astype(np.uint8)

            # ç¡®ä¿å½¢çŠ¶æ­£ç¡® (H, W, C)
            if img_np.ndim == 3 and img_np.shape[-1] in [1, 3]:
                # å¦‚æœæ˜¯å•é€šé“ï¼Œè½¬æ¢ä¸ºRGB
                if img_np.shape[-1] == 1:
                    img_np = np.repeat(img_np, 3, axis=-1)

                # åˆ›å»ºPILå›¾åƒ
                pil_img = Image.fromarray(img_np)
                pil_images.append(pil_img)
            else:
                print(f"âš ï¸  è·³è¿‡å¼‚å¸¸å½¢çŠ¶çš„å›¾åƒ {i}: {img_np.shape}")

        return pil_images
    
    def save_images(self, images, user_labels, output_dir, prefix="micro_doppler"):
        """ä¿å­˜ç”Ÿæˆçš„å›¾åƒ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ ä¿å­˜å›¾åƒåˆ°: {output_dir}")
        
        for i, (image, user_id) in enumerate(zip(images, user_labels)):
            filename = f"{prefix}_user{user_id:02d}_{i+1:03d}.png"
            filepath = output_dir / filename
            image.save(filepath)
        
        # åˆ›å»ºç½‘æ ¼å›¾åƒ (å‚è€ƒåŸé¡¹ç›®)
        self._create_grid_image(images, user_labels, output_dir, prefix)
        
        print(f"âœ… ä¿å­˜äº† {len(images)} å¼ å›¾åƒ")
    
    def _create_grid_image(self, images, user_labels, output_dir, prefix):
        """åˆ›å»ºç½‘æ ¼å±•ç¤ºå›¾åƒ"""
        if not images:
            return
        
        # è®¡ç®—ç½‘æ ¼å°ºå¯¸
        num_images = len(images)
        grid_size = int(np.ceil(np.sqrt(num_images)))
        
        # è·å–å•å¼ å›¾åƒå°ºå¯¸
        img_width, img_height = images[0].size
        
        # åˆ›å»ºç½‘æ ¼å›¾åƒ
        grid_width = grid_size * img_width
        grid_height = grid_size * img_height
        grid_image = Image.new('RGB', (grid_width, grid_height), (255, 255, 255))
        
        # å¡«å……ç½‘æ ¼
        for i, image in enumerate(images):
            row = i // grid_size
            col = i % grid_size
            x = col * img_width
            y = row * img_height
            grid_image.paste(image, (x, y))
        
        # ä¿å­˜ç½‘æ ¼å›¾åƒ
        grid_path = output_dir / f"{prefix}_grid.png"
        grid_image.save(grid_path)
        print(f"ğŸ“Š ç½‘æ ¼å›¾åƒä¿å­˜åˆ°: {grid_path}")

def main(accelerator=None):
    parser = argparse.ArgumentParser(description='å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆ')
    parser.add_argument('--dit_checkpoint', type=str, help='DiTæ¨¡å‹æ£€æŸ¥ç‚¹ (å¦‚æœä¸å­˜åœ¨å°†ä½¿ç”¨éšæœºæ¨¡å‹)')
    parser.add_argument('--vavae_config', type=str, help='VA-VAEé…ç½®æ–‡ä»¶')
    parser.add_argument('--output_dir', type=str, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--user_ids', type=int, nargs='+', default=[1, 2, 3, 4, 5], help='ç”¨æˆ·IDåˆ—è¡¨')
    parser.add_argument('--num_samples_per_user', type=int, default=4, help='æ¯ç”¨æˆ·ç”Ÿæˆæ ·æœ¬æ•°')
    parser.add_argument('--guidance_scale', type=float, default=4.0, help='å¼•å¯¼å°ºåº¦')
    parser.add_argument('--num_steps', type=int, default=250, help='é‡‡æ ·æ­¥æ•°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--dual_gpu', action='store_true', help='ä½¿ç”¨åŒGPUæ¨ç†')
    parser.add_argument('--test_imports', action='store_true', help='ä»…æµ‹è¯•å¯¼å…¥ï¼Œä¸è¿›è¡Œæ¨ç†')
    parser.add_argument('--model_name', type=str, default='LightningDiT-B/1', help='DiTæ¨¡å‹åç§° (åº”ä¸è®­ç»ƒæ—¶ä¸€è‡´)')

    args = parser.parse_args()

    # å¦‚æœåªæ˜¯æµ‹è¯•å¯¼å…¥
    if args.test_imports:
        success = test_imports()
        if success:
            print("âœ… æ‰€æœ‰å¯¼å…¥æµ‹è¯•é€šè¿‡!")
        else:
            print("âŒ å¯¼å…¥æµ‹è¯•å¤±è´¥")
        return

    # å¯¹äºæ¨ç†ï¼Œæ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.vavae_config:
        parser.error("æ¨ç†æ¨¡å¼éœ€è¦ --vavae_config å‚æ•°")
    if not args.output_dir:
        parser.error("æ¨ç†æ¨¡å¼éœ€è¦ --output_dir å‚æ•°")

    # è®¾ç½®éšæœºç§å­
    if accelerator:
        seed = args.seed * accelerator.num_processes + accelerator.process_index
        torch.manual_seed(seed)
        np.random.seed(seed)
    else:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)

    if not accelerator or accelerator.is_main_process:
        print("ğŸ¯ å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆ")
        print("=" * 50)
        if accelerator:
            print(f"ğŸ”§ åˆ†å¸ƒå¼æ¨ç†: {accelerator.num_processes} GPU")
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = MicroDopplerGenerator(
        dit_checkpoint=args.dit_checkpoint,
        vavae_config=args.vavae_config,
        model_name=args.model_name,
        accelerator=accelerator
    )
    
    # ç”Ÿæˆå›¾åƒ
    images, user_labels = generator.generate_samples(
        user_ids=args.user_ids,
        num_samples_per_user=args.num_samples_per_user,
        guidance_scale=args.guidance_scale,
        num_steps=args.num_steps
    )
    
    # ä¿å­˜å›¾åƒ
    generator.save_images(images, user_labels, args.output_dir)
    
    print("âœ… å›¾åƒç”Ÿæˆå®Œæˆ!")

def dual_gpu_inference():
    """åŒGPUæ¨ç†åŒ…è£…å‡½æ•°"""
    def inference_worker():
        from accelerate import Accelerator
        accelerator = Accelerator()
        main(accelerator)

    from accelerate import notebook_launcher
    print("ğŸš€ å¯åŠ¨åŒGPUæ¨ç†...")
    notebook_launcher(inference_worker, num_processes=2)
    print("âœ… åŒGPUæ¨ç†å®Œæˆ")

if __name__ == "__main__":
    import sys

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åŒGPU
    if '--dual_gpu' in sys.argv:
        sys.argv.remove('--dual_gpu')  # ç§»é™¤è¿™ä¸ªå‚æ•°ï¼Œé¿å…argparseæŠ¥é”™
        dual_gpu_inference()
    else:
        main()

#!/usr/bin/env python3
"""
é˜¶æ®µ3: å›¾åƒç”Ÿæˆæ¨ç†
åŸºäºLightningDiTåŸé¡¹ç›®çš„inference.py
ä½¿ç”¨è®­ç»ƒå¥½çš„DiTæ¨¡å‹ç”Ÿæˆç”¨æˆ·æ¡ä»¶åŒ–çš„å¾®å¤šæ™®å‹’å›¾åƒ
"""

import torch
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
import argparse
import os
from pathlib import Path
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from tqdm import tqdm

# å¯¼å…¥LightningDiTç»„ä»¶
import sys
sys.path.append('LightningDiT')
from models.lightningdit import LightningDiT_models
from transport import create_transport
from tokenizer.vavae import VA_VAE

class MicroDopplerGenerator:
    """å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆå™¨ (åŸºäºåŸé¡¹ç›®inference.py)"""
    
    def __init__(self, dit_checkpoint, vavae_config, device='cuda'):
        self.device = device
        
        # åŠ è½½VA-VAE (VA-VAEåœ¨åˆå§‹åŒ–æ—¶å·²ç»è®¾ç½®ä¸ºevalæ¨¡å¼å¹¶ç§»åˆ°GPU)
        print("ğŸ“¥ åŠ è½½VA-VAE...")
        self.vavae = VA_VAE(vavae_config)
        
        # åŠ è½½DiTæ¨¡å‹ (å‚è€ƒåŸé¡¹ç›®)
        print("ğŸ“¥ åŠ è½½DiTæ¨¡å‹...")
        self._load_dit_model(dit_checkpoint)
        
        # åˆ›å»ºtransport (å‚è€ƒåŸé¡¹ç›®)
        self.transport = create_transport(
            path_type="Linear",
            prediction="velocity",
            loss_weight=None,
            train_eps=None,
            sample_eps=None
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    def _load_dit_model(self, checkpoint_path):
        """åŠ è½½DiTæ¨¡å‹æ£€æŸ¥ç‚¹"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ£€æŸ¥ç‚¹æ ¼å¼æ¥è°ƒæ•´
        # å‚è€ƒåŸé¡¹ç›®çš„æ¨¡å‹åŠ è½½æ–¹å¼
        
        # å‡è®¾æˆ‘ä»¬çŸ¥é“æ¨¡å‹é…ç½® (å®é™…åº”è¯¥ä»æ£€æŸ¥ç‚¹ä¸­è¯»å–)
        self.dit_model = LightningDiT_models['LightningDiT-XL/1'](
            input_size=16,
            num_classes=31,  # å‡è®¾31ä¸ªç”¨æˆ·
            in_channels=32,
            use_qknorm=False,
            use_swiglu=True,
            use_rope=True,
            use_rmsnorm=True,
            wo_shift=False
        )
        
        # åŠ è½½æƒé‡ (éœ€è¦æ ¹æ®å®é™…ä¿å­˜æ ¼å¼è°ƒæ•´)
        if os.path.exists(checkpoint_path):
            print(f"ä» {checkpoint_path} åŠ è½½æ¨¡å‹æƒé‡")
            # è¿™é‡Œéœ€è¦å®é™…çš„åŠ è½½é€»è¾‘
            # checkpoint = torch.load(checkpoint_path, map_location='cpu')
            # self.dit_model.load_state_dict(checkpoint['model'])
        else:
            print("âš ï¸  æ£€æŸ¥ç‚¹ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        
        self.dit_model.eval()
        self.dit_model.to(self.device)
    
    def generate_samples(self, user_ids, num_samples_per_user=4, guidance_scale=4.0, num_steps=250):
        """
        ç”Ÿæˆç”¨æˆ·æ¡ä»¶åŒ–çš„å¾®å¤šæ™®å‹’å›¾åƒ
        å‚è€ƒåŸé¡¹ç›®çš„é‡‡æ ·æ–¹æ³•
        """
        print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
        print(f"  ç”¨æˆ·ID: {user_ids}")
        print(f"  æ¯ç”¨æˆ·æ ·æœ¬æ•°: {num_samples_per_user}")
        print(f"  å¼•å¯¼å°ºåº¦: {guidance_scale}")
        print(f"  é‡‡æ ·æ­¥æ•°: {num_steps}")
        
        all_images = []
        all_user_labels = []
        
        with torch.no_grad():
            for user_id in tqdm(user_ids, desc="ç”Ÿæˆç”¨æˆ·å›¾åƒ"):
                # å‡†å¤‡æ¡ä»¶ (å‚è€ƒåŸé¡¹ç›®)
                batch_size = num_samples_per_user
                y = torch.full((batch_size,), user_id - 1, dtype=torch.long, device=self.device)  # 0-based
                
                # ç”Ÿæˆéšæœºå™ªå£° (å‚è€ƒåŸé¡¹ç›®)
                z = torch.randn(batch_size, 32, 16, 16, device=self.device)
                
                # ä½¿ç”¨transportè¿›è¡Œé‡‡æ · (å‚è€ƒåŸé¡¹ç›®)
                model_kwargs = dict(y=y)
                
                # è¿™é‡Œåº”è¯¥ä½¿ç”¨åŸé¡¹ç›®çš„é‡‡æ ·æ–¹æ³•
                # ç”±äºæˆ‘ä»¬æ²¡æœ‰å®Œæ•´çš„é‡‡æ ·å™¨ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                samples = self._sample_with_transport(z, model_kwargs, num_steps)
                
                # ä½¿ç”¨VA-VAEè§£ç ä¸ºå›¾åƒ (ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•)
                images = self.vavae.decode_to_images(samples)
                
                # åå¤„ç†å›¾åƒ (å‚è€ƒåŸé¡¹ç›®)
                images = self._postprocess_images(images)
                
                all_images.extend(images)
                all_user_labels.extend([user_id] * num_samples_per_user)
        
        return all_images, all_user_labels
    
    def _sample_with_transport(self, z, model_kwargs, num_steps):
        """
        ä½¿ç”¨transportè¿›è¡Œé‡‡æ ·
        è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä½¿ç”¨åŸé¡¹ç›®çš„å®Œæ•´é‡‡æ ·å™¨
        """
        # ç®€åŒ–çš„é‡‡æ ·è¿‡ç¨‹
        # å®é™…åº”è¯¥ä½¿ç”¨transport.sample()æ–¹æ³•
        
        dt = 1.0 / num_steps
        x = z.clone()
        
        for i in range(num_steps):
            t = torch.full((x.shape[0],), i * dt, device=self.device)
            
            # æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                pred = self.dit_model(x, t, **model_kwargs)
            
            # ç®€å•çš„æ¬§æ‹‰æ­¥éª¤ (å®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„æ±‚è§£å™¨)
            x = x + pred * dt
        
        return x
    
    def _postprocess_images(self, images):
        """
        åå¤„ç†å›¾åƒ (VA-VAEçš„decode_to_imageså·²ç»è¿”å›numpyæ•°ç»„)
        """
        # decode_to_imageså·²ç»è¿”å›äº†uint8æ ¼å¼çš„numpyæ•°ç»„ (B, H, W, C)
        pil_images = []
        for img_np in images:
            # ç›´æ¥ä»numpyæ•°ç»„åˆ›å»ºPILå›¾åƒ
            pil_img = Image.fromarray(img_np)
            pil_images.append(pil_img)

        return pil_images
    
    def save_images(self, images, user_labels, output_dir, prefix="micro_doppler"):
        """ä¿å­˜ç”Ÿæˆçš„å›¾åƒ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ ä¿å­˜å›¾åƒåˆ°: {output_dir}")
        
        for i, (image, user_id) in enumerate(zip(images, user_labels)):
            filename = f"{prefix}_user{user_id:02d}_{i+1:03d}.png"
            filepath = output_dir / filename
            image.save(filepath)
        
        # åˆ›å»ºç½‘æ ¼å›¾åƒ (å‚è€ƒåŸé¡¹ç›®)
        self._create_grid_image(images, user_labels, output_dir, prefix)
        
        print(f"âœ… ä¿å­˜äº† {len(images)} å¼ å›¾åƒ")
    
    def _create_grid_image(self, images, user_labels, output_dir, prefix):
        """åˆ›å»ºç½‘æ ¼å±•ç¤ºå›¾åƒ"""
        if not images:
            return
        
        # è®¡ç®—ç½‘æ ¼å°ºå¯¸
        num_images = len(images)
        grid_size = int(np.ceil(np.sqrt(num_images)))
        
        # è·å–å•å¼ å›¾åƒå°ºå¯¸
        img_width, img_height = images[0].size
        
        # åˆ›å»ºç½‘æ ¼å›¾åƒ
        grid_width = grid_size * img_width
        grid_height = grid_size * img_height
        grid_image = Image.new('RGB', (grid_width, grid_height), (255, 255, 255))
        
        # å¡«å……ç½‘æ ¼
        for i, image in enumerate(images):
            row = i // grid_size
            col = i % grid_size
            x = col * img_width
            y = row * img_height
            grid_image.paste(image, (x, y))
        
        # ä¿å­˜ç½‘æ ¼å›¾åƒ
        grid_path = output_dir / f"{prefix}_grid.png"
        grid_image.save(grid_path)
        print(f"ğŸ“Š ç½‘æ ¼å›¾åƒä¿å­˜åˆ°: {grid_path}")

def main():
    parser = argparse.ArgumentParser(description='å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆ')
    parser.add_argument('--dit_checkpoint', type=str, required=True, help='DiTæ¨¡å‹æ£€æŸ¥ç‚¹')
    parser.add_argument('--vavae_config', type=str, required=True, help='VA-VAEé…ç½®æ–‡ä»¶')
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--user_ids', type=int, nargs='+', default=[1, 2, 3, 4, 5], help='ç”¨æˆ·IDåˆ—è¡¨')
    parser.add_argument('--num_samples_per_user', type=int, default=4, help='æ¯ç”¨æˆ·ç”Ÿæˆæ ·æœ¬æ•°')
    parser.add_argument('--guidance_scale', type=float, default=4.0, help='å¼•å¯¼å°ºåº¦')
    parser.add_argument('--num_steps', type=int, default=250, help='é‡‡æ ·æ­¥æ•°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    print("ğŸ¯ å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆ")
    print("=" * 50)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = MicroDopplerGenerator(
        dit_checkpoint=args.dit_checkpoint,
        vavae_config=args.vavae_config,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # ç”Ÿæˆå›¾åƒ
    images, user_labels = generator.generate_samples(
        user_ids=args.user_ids,
        num_samples_per_user=args.num_samples_per_user,
        guidance_scale=args.guidance_scale,
        num_steps=args.num_steps
    )
    
    # ä¿å­˜å›¾åƒ
    generator.save_images(images, user_labels, args.output_dir)
    
    print("âœ… å›¾åƒç”Ÿæˆå®Œæˆ!")

if __name__ == "__main__":
    main()

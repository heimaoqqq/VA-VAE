#!/usr/bin/env python3
"""
é˜¶æ®µ3: æŽ¨ç†å’Œç”Ÿæˆ
ä½¿ç”¨è®­ç»ƒå¥½çš„ç”¨æˆ·æ¡ä»¶åŒ–DiTç”Ÿæˆå¾®å¤šæ™®å‹’å›¾åƒ
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import argparse
import sys
import os
from PIL import Image

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from tokenizer.autoencoder import AutoencoderKL
from transport import create_transport

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡åž‹
from stage2_train_dit import UserConditionedDiT

class MicroDopplerGenerator:
    """
    å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆå™¨
    """
    
    def __init__(self, dit_checkpoint, vavae_path, device='cuda'):
        self.device = device
        
        print("ðŸ”„ åŠ è½½æ¨¡åž‹...")
        
        # åŠ è½½VA-VAE
        print("ðŸ“¥ åŠ è½½VA-VAE...")
        self.vavae = AutoencoderKL(
            embed_dim=32,
            ch_mult=(1, 1, 2, 2, 4),
            ckpt_path=vavae_path,
            model_type='vavae'
        )
        self.vavae.eval()
        self.vavae.to(device)
        
        # åŠ è½½DiTæ¨¡åž‹
        print("ðŸ“¥ åŠ è½½DiTæ¨¡åž‹...")
        self.dit_model = UserConditionedDiT.load_from_checkpoint(dit_checkpoint)
        self.dit_model.eval()
        self.dit_model.to(device)
        
        # åˆ›å»ºæ‰©æ•£ä¼ è¾“
        self.transport = create_transport(
            path_type="Linear",
            prediction="velocity",
            loss_weight=None,
            train_eps=1e-5,
            sample_eps=1e-4,
        )
        
        print(f"âœ… æ¨¡åž‹åŠ è½½å®Œæˆ")
        print(f"  ç”¨æˆ·æ•°é‡: {self.dit_model.num_users}")
    
    @torch.no_grad()
    def generate(
        self,
        user_ids,
        num_samples_per_user=4,
        guidance_scale=4.0,
        num_steps=250,
        seed=None
    ):
        """
        ç”Ÿæˆå¾®å¤šæ™®å‹’å›¾åƒ
        
        Args:
            user_ids: ç”¨æˆ·IDåˆ—è¡¨ (1-based)
            num_samples_per_user: æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•°
            guidance_scale: classifier-free guidanceå¼ºåº¦
            num_steps: æ‰©æ•£æ­¥æ•°
            seed: éšæœºç§å­
        
        Returns:
            generated_images: ç”Ÿæˆçš„å›¾åƒ (B, 3, 256, 256)
            user_labels: å¯¹åº”çš„ç”¨æˆ·ID
        """
        if seed is not None:
            torch.manual_seed(seed)
        
        print(f"ðŸŽ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
        print(f"  ç”¨æˆ·ID: {user_ids}")
        print(f"  æ¯ç”¨æˆ·æ ·æœ¬æ•°: {num_samples_per_user}")
        print(f"  å¼•å¯¼å¼ºåº¦: {guidance_scale}")
        print(f"  æ‰©æ•£æ­¥æ•°: {num_steps}")
        
        # å‡†å¤‡æ‰¹æ¬¡
        batch_size = len(user_ids) * num_samples_per_user
        
        # åˆ›å»ºç”¨æˆ·æ¡ä»¶ (è½¬æ¢ä¸º0-based)
        user_conditions = []
        user_labels = []
        for user_id in user_ids:
            for _ in range(num_samples_per_user):
                user_conditions.append(user_id - 1)  # è½¬æ¢ä¸º0-based
                user_labels.append(user_id)
        
        user_conditions = torch.tensor(user_conditions, device=self.device)
        
        # åˆå§‹å™ªå£°
        latent_shape = (batch_size, 32, 16, 16)  # VA-VAEçš„æ½œåœ¨ç©ºé—´å½¢çŠ¶
        noise = torch.randn(latent_shape, device=self.device)
        
        print(f"  åˆå§‹å™ªå£°å½¢çŠ¶: {noise.shape}")
        
        # æ‰©æ•£é‡‡æ ·
        print("ðŸ”„ æ‰§è¡Œæ‰©æ•£é‡‡æ ·...")
        
        def model_fn(x, t):
            """æ¨¡åž‹å‡½æ•°ï¼Œæ”¯æŒclassifier-free guidance"""
            # æ— æ¡ä»¶é¢„æµ‹
            uncond_pred = self.dit_model.dit(x, t, y=None)
            
            # æœ‰æ¡ä»¶é¢„æµ‹
            cond_pred = self.dit_model.dit(x, t, y=user_conditions)
            
            # Classifier-free guidance
            if guidance_scale > 1.0:
                pred = uncond_pred + guidance_scale * (cond_pred - uncond_pred)
            else:
                pred = cond_pred
            
            return pred
        
        # ä½¿ç”¨transportè¿›è¡Œé‡‡æ ·
        samples = self.transport.sample(
            model_fn,
            noise,
            num_steps=num_steps,
            clip_denoised=True
        )
        
        print(f"  é‡‡æ ·å®Œæˆï¼Œæ½œåœ¨ç‰¹å¾å½¢çŠ¶: {samples.shape}")
        
        # ä½¿ç”¨VA-VAEè§£ç ä¸ºå›¾åƒ
        print("ðŸŽ¨ è§£ç ä¸ºå›¾åƒ...")
        generated_images = self.vavae.decode(samples)
        
        # åŽå¤„ç†ï¼šè£å‰ªåˆ°[0,1]èŒƒå›´
        generated_images = torch.clamp(generated_images, 0, 1)
        
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå›¾åƒå½¢çŠ¶: {generated_images.shape}")
        
        return generated_images, user_labels
    
    def save_images(self, images, user_labels, output_dir, prefix="generated"):
        """ä¿å­˜ç”Ÿæˆçš„å›¾åƒ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ðŸ’¾ ä¿å­˜å›¾åƒåˆ°: {output_dir}")
        
        # è½¬æ¢ä¸ºnumpyæ ¼å¼
        images_np = images.cpu().numpy()
        images_np = (images_np * 255).astype(np.uint8)
        
        # ä¿å­˜å•ç‹¬çš„å›¾åƒ
        for i, (image, user_id) in enumerate(zip(images_np, user_labels)):
            # è½¬æ¢ä¸ºHWCæ ¼å¼
            image = np.transpose(image, (1, 2, 0))
            
            # ä¿å­˜ä¸ºPNG
            filename = f"{prefix}_user{user_id:02d}_{i:03d}.png"
            filepath = output_dir / filename
            
            Image.fromarray(image).save(filepath)
        
        # åˆ›å»ºç½‘æ ¼å›¾åƒ
        self.create_grid_image(images, user_labels, output_dir / f"{prefix}_grid.png")
        
        print(f"âœ… ä¿å­˜äº† {len(images)} å¼ å›¾åƒ")
    
    def create_grid_image(self, images, user_labels, output_path):
        """åˆ›å»ºç½‘æ ¼å›¾åƒ"""
        num_images = len(images)
        grid_size = int(np.ceil(np.sqrt(num_images)))
        
        fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))
        axes = axes.flatten() if grid_size > 1 else [axes]
        
        for i in range(grid_size * grid_size):
            ax = axes[i]
            
            if i < num_images:
                # æ˜¾ç¤ºå›¾åƒ
                image = images[i].cpu().numpy()
                image = np.transpose(image, (1, 2, 0))
                
                ax.imshow(image)
                ax.set_title(f'User {user_labels[i]}', fontsize=10)
                ax.axis('off')
            else:
                ax.axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ðŸ“Š ç½‘æ ¼å›¾åƒä¿å­˜åˆ°: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆç”¨æˆ·æ¡ä»¶åŒ–çš„å¾®å¤šæ™®å‹’å›¾åƒ')
    parser.add_argument('--dit_checkpoint', type=str, required=True,
                       help='è®­ç»ƒå¥½çš„DiTæ¨¡åž‹æ£€æŸ¥ç‚¹')
    parser.add_argument('--vavae_path', type=str, required=True,
                       help='é¢„è®­ç»ƒVA-VAEæ¨¡åž‹è·¯å¾„')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--user_ids', type=int, nargs='+', default=[1, 2, 3, 4, 5],
                       help='è¦ç”Ÿæˆçš„ç”¨æˆ·IDåˆ—è¡¨')
    parser.add_argument('--num_samples_per_user', type=int, default=4,
                       help='æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•°')
    parser.add_argument('--guidance_scale', type=float, default=4.0,
                       help='Classifier-free guidanceå¼ºåº¦')
    parser.add_argument('--num_steps', type=int, default=250,
                       help='æ‰©æ•£é‡‡æ ·æ­¥æ•°')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    print("ðŸŽ¯ å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆ - é˜¶æ®µ3")
    print("=" * 50)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = MicroDopplerGenerator(
        dit_checkpoint=args.dit_checkpoint,
        vavae_path=args.vavae_path,
        device=args.device
    )
    
    # ç”Ÿæˆå›¾åƒ
    generated_images, user_labels = generator.generate(
        user_ids=args.user_ids,
        num_samples_per_user=args.num_samples_per_user,
        guidance_scale=args.guidance_scale,
        num_steps=args.num_steps,
        seed=args.seed
    )
    
    # ä¿å­˜å›¾åƒ
    generator.save_images(
        images=generated_images,
        user_labels=user_labels,
        output_dir=args.output_dir,
        prefix="micro_doppler"
    )
    
    print("âœ… ç”Ÿæˆå®Œæˆ!")

if __name__ == "__main__":
    main()

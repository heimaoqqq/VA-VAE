#!/usr/bin/env python3
"""
æ­£ç¡®çš„è®­ç»ƒæ–¹æ³• - éµå¾ªLightningDiTåŸé¡¹ç›®æ–¹å¼
1. é¢„æå–å¾®å¤šæ™®å‹’å›¾åƒçš„latentç‰¹å¾
2. åœ¨latentç©ºé—´è®­ç»ƒç”¨æˆ·æ¡ä»¶åŒ–DiTæ¨¡å‹
"""

import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import os
import sys

# æ·»åŠ LightningDiTè·¯å¾„
sys.path.append('LightningDiT')
from tokenizer.autoencoder import AutoencoderKL

def extract_latent_features():
    """
    ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨é¢„è®­ç»ƒVA-VAEæå–å¾®å¤šæ™®å‹’å›¾åƒçš„latentç‰¹å¾
    è¿™ä¸€æ­¥åªéœ€è¦è¿è¡Œä¸€æ¬¡
    """
    print("ğŸ”„ æå–å¾®å¤šæ™®å‹’å›¾åƒçš„latentç‰¹å¾...")
    
    # åŠ è½½é¢„è®­ç»ƒVA-VAE
    vavae = AutoencoderKL(
        embed_dim=32,
        ch_mult=(1, 1, 2, 2, 4),
        ckpt_path="/kaggle/working/pretrained/vavae-imagenet256-f16d32-dinov2.pt",
        model_type='vavae'
    )
    vavae.eval()
    vavae.cuda()
    
    # å¤„ç†æ•°æ®é›†
    from minimal_micro_doppler_dataset import MicroDopplerDataset
    
    for split in ['train', 'val']:
        print(f"å¤„ç† {split} æ•°æ®...")
        dataset = MicroDopplerDataset(f"/kaggle/working/data_split/{split}", split=split)
        
        output_dir = Path(f"/kaggle/working/latent_features/{split}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        latents = []
        labels = []
        user_ids = []
        
        with torch.no_grad():
            for i, sample in enumerate(dataset):
                image = sample['image'].unsqueeze(0).cuda()  # (1, 3, 256, 256)
                user_id = sample['user_id']
                
                # æå–latentç‰¹å¾
                posterior = vavae.encode(image)
                latent = posterior.sample()  # (1, 32, 16, 16)
                
                latents.append(latent.cpu())
                user_ids.append(user_id)
                
                if i % 100 == 0:
                    print(f"  å¤„ç†è¿›åº¦: {i}/{len(dataset)}")
        
        # ä¿å­˜latentç‰¹å¾
        torch.save({
            'latents': torch.cat(latents, dim=0),  # (N, 32, 16, 16)
            'user_ids': torch.tensor(user_ids),    # (N,)
        }, output_dir / 'latents.pt')
        
        print(f"âœ… {split} latentç‰¹å¾ä¿å­˜å®Œæˆ")

class UserConditionedDiT(nn.Module):
    """
    ç”¨æˆ·æ¡ä»¶åŒ–çš„DiTæ¨¡å‹
    åœ¨latentç©ºé—´å·¥ä½œï¼Œä¸æ¶‰åŠå›¾åƒç¼–ç è§£ç 
    """
    
    def __init__(self, num_users, condition_dim=128, latent_dim=32):
        super().__init__()
        
        # ç”¨æˆ·åµŒå…¥
        self.user_embedding = nn.Embedding(num_users, condition_dim)
        
        # ç®€åŒ–çš„DiT backboneï¼ˆè¿™é‡Œç”¨ç®€å•çš„CNNä»£æ›¿ï¼‰
        self.dit_backbone = nn.Sequential(
            nn.Conv2d(latent_dim, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, latent_dim, 3, padding=1),
        )
        
        # ç”¨æˆ·æ¡ä»¶èåˆ
        self.condition_proj = nn.Linear(condition_dim, latent_dim)
        
    def forward(self, latents, user_ids, timesteps=None):
        """
        å‰å‘ä¼ æ’­
        Args:
            latents: (B, 32, 16, 16) latentç‰¹å¾
            user_ids: (B,) ç”¨æˆ·ID
            timesteps: (B,) æ—¶é—´æ­¥ï¼ˆç”¨äºæ‰©æ•£è®­ç»ƒï¼‰
        """
        B, C, H, W = latents.shape
        
        # è·å–ç”¨æˆ·æ¡ä»¶
        user_emb = self.user_embedding(user_ids - 1)  # è½¬æ¢ä¸º0-basedç´¢å¼•
        user_cond = self.condition_proj(user_emb)  # (B, latent_dim)
        user_cond = user_cond.view(B, C, 1, 1).expand(-1, -1, H, W)
        
        # æ·»åŠ ç”¨æˆ·æ¡ä»¶
        conditioned_latents = latents + user_cond
        
        # DiTå¤„ç†
        output = self.dit_backbone(conditioned_latents)
        
        return output

class LatentDataset(torch.utils.data.Dataset):
    """
    Latentç‰¹å¾æ•°æ®é›†
    """
    
    def __init__(self, latent_file):
        data = torch.load(latent_file)
        self.latents = data['latents']
        self.user_ids = data['user_ids']
        
    def __len__(self):
        return len(self.latents)
    
    def __getitem__(self, idx):
        return {
            'latent': self.latents[idx],
            'user_id': self.user_ids[idx]
        }

def train_user_conditioned_dit():
    """
    ç¬¬äºŒæ­¥ï¼šè®­ç»ƒç”¨æˆ·æ¡ä»¶åŒ–DiTæ¨¡å‹
    """
    print("ğŸš€ è®­ç»ƒç”¨æˆ·æ¡ä»¶åŒ–DiTæ¨¡å‹...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = LatentDataset("/kaggle/working/latent_features/train/latents.pt")
    val_dataset = LatentDataset("/kaggle/working/latent_features/val/latents.pt")
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=32, shuffle=True, num_workers=4
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset, batch_size=32, shuffle=False, num_workers=4
    )
    
    # åˆ›å»ºæ¨¡å‹
    num_users = len(torch.unique(train_dataset.user_ids))
    model = UserConditionedDiT(num_users=num_users)
    model.cuda()
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    print(f"ç”¨æˆ·æ•°é‡: {num_users}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(100):
        model.train()
        train_loss = 0
        
        for batch in train_loader:
            latents = batch['latent'].cuda()
            user_ids = batch['user_id'].cuda()
            
            # ç®€å•çš„é‡å»ºä»»åŠ¡ï¼ˆå®é™…åº”è¯¥æ˜¯æ‰©æ•£è®­ç»ƒï¼‰
            optimizer.zero_grad()
            output = model(latents, user_ids)
            loss = criterion(output, latents)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                latents = batch['latent'].cuda()
                user_ids = batch['user_id'].cuda()
                output = model(latents, user_ids)
                loss = criterion(output, latents)
                val_loss += loss.item()
        
        print(f"Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.6f}, "
              f"Val Loss: {val_loss/len(val_loader):.6f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ­£ç¡®çš„è®­ç»ƒæ–¹æ³• - éµå¾ªLightningDiTåŸé¡¹ç›®")
    print("=" * 50)
    
    # ç¬¬ä¸€æ­¥ï¼šæå–latentç‰¹å¾ï¼ˆåªéœ€è¿è¡Œä¸€æ¬¡ï¼‰
    if not os.path.exists("/kaggle/working/latent_features/train/latents.pt"):
        extract_latent_features()
    else:
        print("âœ… Latentç‰¹å¾å·²å­˜åœ¨ï¼Œè·³è¿‡æå–æ­¥éª¤")
    
    # ç¬¬äºŒæ­¥ï¼šè®­ç»ƒç”¨æˆ·æ¡ä»¶åŒ–DiT
    train_user_conditioned_dit()

if __name__ == "__main__":
    main()

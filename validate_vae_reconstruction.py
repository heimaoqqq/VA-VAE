#!/usr/bin/env python3
"""
VAEé‡å»ºéªŒè¯è„šæœ¬
æµ‹è¯•VA-VAEçš„ç¼–ç /è§£ç åŠŸèƒ½å’Œåˆ†å¸ƒå¯¹é½çš„æ•ˆæœ
è¿™æ¯”æœªè®­ç»ƒçš„æ‰©æ•£æ ·æœ¬æ›´æœ‰æ„ä¹‰
"""

import torch
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from PIL import Image
from simplified_vavae import SimplifiedVAVAE
from distribution_aligned_diffusion import DistributionAlignedDiffusion
import argparse
import os
import glob
from torchvision import transforms


def load_real_dataset(data_dir="/kaggle/input/dataset", batch_size=8, num_samples=32):
    """åŠ è½½çœŸå®çš„microdoppleræ•°æ®é›†"""
    try:
        print(f"   å°è¯•åŠ è½½æ•°æ®é›†: {data_dir}")
        
        # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
        if not os.path.exists(data_dir):
            raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {data_dir}")
        
        # æ‰«æç”¨æˆ·æ–‡ä»¶å¤¹ (ID_1, ID_2, ..., ID_31)
        user_folders = []
        for i in range(1, 32):  # ID_1 åˆ° ID_31
            user_folder = os.path.join(data_dir, f"ID_{i}")
            if os.path.exists(user_folder):
                user_folders.append((user_folder, i-1))  # ç”¨æˆ·IDä»0å¼€å§‹
        
        if not user_folders:
            raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•ç”¨æˆ·æ–‡ä»¶å¤¹ (ID_1 åˆ° ID_31)")
        
        print(f"   å‘ç° {len(user_folders)} ä¸ªç”¨æˆ·æ–‡ä»¶å¤¹")
        
        # æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„å’Œç”¨æˆ·ID
        all_image_paths = []
        all_user_ids = []
        
        for user_folder, user_id in user_folders:
            # æŸ¥æ‰¾è¯¥ç”¨æˆ·çš„æ‰€æœ‰jpgå›¾åƒ
            jpg_files = glob.glob(os.path.join(user_folder, "*.jpg"))
            if jpg_files:
                all_image_paths.extend(jpg_files)
                all_user_ids.extend([user_id] * len(jpg_files))
        
        total_images = len(all_image_paths)
        print(f"   æ€»å›¾åƒæ•°é‡: {total_images}")
        
        if total_images == 0:
            raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•jpgå›¾åƒ")
        
        # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        indices = np.random.choice(total_images, min(num_samples, total_images), replace=False)
        selected_paths = [all_image_paths[i] for i in indices]
        selected_user_ids = [all_user_ids[i] for i in indices]
        
        # å®šä¹‰å›¾åƒå˜æ¢
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            # ä¸éœ€è¦å½’ä¸€åŒ–ï¼Œä¿æŒ[0,1]èŒƒå›´
        ])
        
        # åŠ è½½å›¾åƒ
        images = []
        user_ids = []
        
        for img_path, user_id in zip(selected_paths, selected_user_ids):
            try:
                from PIL import Image
                img = Image.open(img_path).convert('RGB')
                img_tensor = transform(img)
                images.append(img_tensor)
                user_ids.append(user_id)
            except Exception as e:
                print(f"   è·³è¿‡æŸåå›¾åƒ {img_path}: {e}")
                continue
        
        if not images:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å›¾åƒ")
        
        images = torch.stack(images)
        user_ids = torch.tensor(user_ids)
        
        print(f"   æˆåŠŸåŠ è½½æ ·æœ¬æ•°: {len(images)}")
        print(f"   å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"   å›¾åƒèŒƒå›´: [{images.min().item():.3f}, {images.max().item():.3f}]")
        print(f"   ç”¨æˆ·IDèŒƒå›´: [{user_ids.min().item()}, {user_ids.max().item()}]")
        
        return images, user_ids
        
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•åŠ è½½çœŸå®æ•°æ®é›†: {e}")
        print(f"   å›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®...")
        return create_fallback_images(batch_size, num_samples)


def create_fallback_images(batch_size=8, num_samples=32):
    """åˆ›å»ºå›é€€çš„æ¨¡æ‹Ÿæ•°æ®ï¼ˆå½“çœŸå®æ•°æ®ä¸å¯ç”¨æ—¶ï¼‰"""
    print("   åˆ›å»ºmicrodoppleré£æ ¼çš„æ¨¡æ‹Ÿæ•°æ®...")
    
    images = []
    user_ids = []
    
    for i in range(num_samples):
        # åˆ›å»ºç±»ä¼¼microdopplerçš„å›¾æ¡ˆï¼šæ—¶é¢‘è°±ç‰¹å¾
        img = np.zeros((3, 256, 256), dtype=np.float32)
        
        # æ¨¡æ‹Ÿå¤šæ™®å‹’é¢‘ç§»æ›²çº¿
        time_steps = np.linspace(0, 2*np.pi, 256)
        for t_idx, t in enumerate(time_steps):
            # ä¸åŒçš„å¤šæ™®å‹’æ¨¡å¼
            if i % 4 == 0:
                # æ­£å¼¦æ³¢å¤šæ™®å‹’
                freq_shift = 64 + 32 * np.sin(3*t + i*0.5)
            elif i % 4 == 1:
                # çº¿æ€§è°ƒé¢‘
                freq_shift = 32 + 64 * t / (2*np.pi)
            elif i % 4 == 2:
                # å¤åˆå¤šæ™®å‹’
                freq_shift = 64 + 16 * np.sin(2*t) + 16 * np.cos(4*t + i*0.3)
            else:
                # é˜¶è·ƒå¤šæ™®å‹’
                freq_shift = 48 + 32 * (1 if np.sin(t) > 0 else -1)
            
            freq_shift = int(np.clip(freq_shift, 0, 255))
            
            # åœ¨æ—¶é¢‘å›¾ä¸Šæ·»åŠ èƒ½é‡
            intensity = 0.3 + 0.4 * np.exp(-((t - np.pi)**2) / 2)
            img[0, freq_shift-2:freq_shift+3, t_idx] = intensity  # Ré€šé“
            img[1, freq_shift-1:freq_shift+2, t_idx] = intensity * 0.8  # Gé€šé“  
            img[2, freq_shift:freq_shift+1, t_idx] = intensity * 0.6  # Bé€šé“
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 0.05, img.shape).astype(np.float32)
        img = np.clip(img + noise, 0, 1)
        
        images.append(img)
        user_ids.append(i % 31)  # 31ä¸ªç”¨æˆ·
    
    images = torch.tensor(np.stack(images))
    user_ids = torch.tensor(user_ids)
    
    return images, user_ids


def save_comparison_grid(original, reconstructed, filepath, titles=None):
    """ä¿å­˜åŸå›¾ä¸é‡å»ºå›¾çš„å¯¹æ¯”ç½‘æ ¼"""
    batch_size = original.shape[0]
    
    # ç¡®ä¿å›¾åƒåœ¨[0,1]èŒƒå›´å†…
    original = torch.clamp(original, 0, 1)
    reconstructed = torch.clamp(reconstructed, 0, 1)
    
    # è½¬æ¢ä¸ºnumpy
    orig_np = original.cpu().numpy().transpose(0, 2, 3, 1)  # [B,C,H,W] -> [B,H,W,C]
    recon_np = reconstructed.cpu().numpy().transpose(0, 2, 3, 1)
    
    # åˆ›å»ºå¯¹æ¯”ç½‘æ ¼ (2è¡Œ: åŸå›¾ + é‡å»º)
    fig, axes = plt.subplots(2, batch_size, figsize=(batch_size*3, 6))
    
    if batch_size == 1:
        axes = axes.reshape(2, 1)
    
    for i in range(batch_size):
        # åŸå›¾
        ax_orig = axes[0, i] if batch_size > 1 else axes[0]
        if orig_np.shape[-1] == 3:  # RGB
            ax_orig.imshow(orig_np[i])
        else:  # ç°åº¦
            ax_orig.imshow(orig_np[i, :, :, 0], cmap='gray')
        ax_orig.axis('off')
        ax_orig.set_title(f'Original {i+1}' if titles is None else f'Orig: {titles[i]}')
        
        # é‡å»ºå›¾
        ax_recon = axes[1, i] if batch_size > 1 else axes[1]
        if recon_np.shape[-1] == 3:  # RGB
            ax_recon.imshow(recon_np[i])
        else:  # ç°åº¦
            ax_recon.imshow(recon_np[i, :, :, 0], cmap='gray')
        ax_recon.axis('off')
        ax_recon.set_title(f'Reconstructed {i+1}')
    
    plt.tight_layout()
    plt.savefig(filepath, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {filepath}")


def analyze_latent_distribution(latents, title="Latent Distribution"):
    """åˆ†ælatentåˆ†å¸ƒå¹¶å¯è§†åŒ–"""
    print(f"\nğŸ“Š {title}")
    print("-" * 50)
    
    # åŸºæœ¬ç»Ÿè®¡
    mean = latents.mean().item()
    std = latents.std().item()
    min_val = latents.min().item()
    max_val = latents.max().item()
    
    print(f"   å‡å€¼ (Mean): {mean:.4f}")
    print(f"   æ ‡å‡†å·® (Std): {std:.4f}")
    print(f"   æœ€å°å€¼ (Min): {min_val:.4f}")
    print(f"   æœ€å¤§å€¼ (Max): {max_val:.4f}")
    print(f"   å½¢çŠ¶ (Shape): {latents.shape}")
    
    # æ£€æŸ¥åˆ†å¸ƒç‰¹æ€§
    if abs(mean) < 0.1:
        print("   âœ… å‡å€¼æ¥è¿‘0")
    else:
        print(f"   âš ï¸ å‡å€¼åç¦»0: {mean:.4f}")
    
    if 0.8 < std < 2.0:
        print("   âœ… æ ‡å‡†å·®åœ¨åˆç†èŒƒå›´")
    else:
        print(f"   âš ï¸ æ ‡å‡†å·®å¼‚å¸¸: {std:.4f}")
    
    return {"mean": mean, "std": std, "min": min_val, "max": max_val}


def test_vae_reconstruction(vae_checkpoint, data_dir="/kaggle/input/dataset", output_dir="./vae_validation", device=None):
    """æµ‹è¯•VAEé‡å»ºåŠŸèƒ½"""
    
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"ğŸ¨ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # åŠ è½½VAE
    print("ğŸ“¦ åŠ è½½VA-VAE...")
    vae = SimplifiedVAVAE(checkpoint_path=vae_checkpoint)
    vae = vae.to(device)
    vae.eval()
    
    # åŠ è½½çœŸå®æ•°æ®
    print("ğŸ¯ åŠ è½½microdoppleræµ‹è¯•æ•°æ®...")
    test_images, test_user_ids = load_real_dataset(data_dir=data_dir, batch_size=8, num_samples=16)
    test_images = test_images.to(device)
    
    print(f"   æµ‹è¯•å›¾åƒå½¢çŠ¶: {test_images.shape}")
    print(f"   å›¾åƒèŒƒå›´: [{test_images.min().item():.3f}, {test_images.max().item():.3f}]")
    
    # VAEç¼–ç è§£ç æµ‹è¯•
    print("\nğŸ”„ æ‰§è¡ŒVAEç¼–ç -è§£ç æµ‹è¯•...")
    
    with torch.no_grad():
        # ç¼–ç åˆ°latent
        latents = vae.encode(test_images)
        print(f"   ç¼–ç latentå½¢çŠ¶: {latents.shape}")
        
        # åˆ†ælatentåˆ†å¸ƒ
        latent_stats = analyze_latent_distribution(latents, "VAEç¼–ç çš„Latentåˆ†å¸ƒ")
        
        # è§£ç å›å›¾åƒ
        reconstructed = vae.decode(latents)
        print(f"   é‡å»ºå›¾åƒå½¢çŠ¶: {reconstructed.shape}")
        print(f"   é‡å»ºå›¾åƒèŒƒå›´: [{reconstructed.min().item():.3f}, {reconstructed.max().item():.3f}]")
        
        # è®¡ç®—é‡å»ºè¯¯å·®
        mse_error = torch.nn.functional.mse_loss(test_images, reconstructed).item()
        mae_error = torch.nn.functional.l1_loss(test_images, reconstructed).item()
        
        print(f"\nğŸ“ é‡å»ºè¯¯å·®:")
        print(f"   MSE: {mse_error:.6f}")
        print(f"   MAE: {mae_error:.6f}")
        
        if mse_error < 0.01:
            print("   âœ… é‡å»ºè´¨é‡ä¼˜ç§€")
        elif mse_error < 0.05:
            print("   âœ… é‡å»ºè´¨é‡è‰¯å¥½")
        else:
            print("   âš ï¸ é‡å»ºè¯¯å·®è¾ƒå¤§")
    
    # ä¿å­˜å¯¹æ¯”å›¾
    comparison_path = output_path / "vae_reconstruction_comparison.png"
    save_comparison_grid(test_images, reconstructed, comparison_path)
    
    return latent_stats, mse_error, mae_error


def test_distribution_alignment(vae_checkpoint, data_dir="/kaggle/input/dataset", output_dir="./vae_validation", device=None):
    """æµ‹è¯•åˆ†å¸ƒå¯¹é½åŠŸèƒ½"""
    
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("\n" + "="*60)
    print("ğŸ”§ æµ‹è¯•åˆ†å¸ƒå¯¹é½åŠŸèƒ½")
    print("="*60)
    
    # åŠ è½½VAE
    vae = SimplifiedVAVAE(checkpoint_path=vae_checkpoint)
    vae = vae.to(device)
    vae.eval()
    
    # åˆ›å»ºåˆ†å¸ƒå¯¹é½æ¨¡å‹ï¼ˆä»…ç”¨äºæµ‹è¯•å½’ä¸€åŒ–åŠŸèƒ½ï¼‰
    alignment_model = DistributionAlignedDiffusion(
        vae=vae,
        num_users=31,
        prototype_dim=768,
        enable_alignment=True,
        track_statistics=True
    ).to(device)
    
    # åŠ è½½çœŸå®æ•°æ®è¿›è¡Œæµ‹è¯•
    test_images, test_user_ids = load_real_dataset(data_dir=data_dir, batch_size=16, num_samples=32)
    test_images = test_images.to(device)
    
    with torch.no_grad():
        # è·å–VAE latents
        original_latents = vae.encode(test_images)
        
        # æ›´æ–°åˆ†å¸ƒç»Ÿè®¡
        alignment_model.update_statistics(original_latents)
        
        print(f"ğŸ¯ æ£€æµ‹åˆ°çš„latentåˆ†å¸ƒ:")
        print(f"   å‡å€¼: {alignment_model.latent_mean.item():.4f}")
        print(f"   æ ‡å‡†å·®: {alignment_model.latent_std.item():.4f}")
        print(f"   åˆ†å¸ƒå¯¹é½çŠ¶æ€: {'å¯ç”¨' if alignment_model.enable_alignment else 'ç¦ç”¨'}")
        
        if alignment_model.enable_alignment:
            # æµ‹è¯•å½’ä¸€åŒ–
            normalized = alignment_model.normalize_latents(original_latents)
            norm_stats = analyze_latent_distribution(normalized, "å½’ä¸€åŒ–åçš„Latentåˆ†å¸ƒ")
            
            # æµ‹è¯•åå½’ä¸€åŒ–
            denormalized = alignment_model.denormalize_latents(normalized)
            
            # æ£€æŸ¥å¯é€†æ€§
            error = torch.nn.functional.mse_loss(original_latents, denormalized).item()
            print(f"\nğŸ”„ å½’ä¸€åŒ–å¯é€†æ€§æµ‹è¯•:")
            print(f"   åŸå§‹ -> å½’ä¸€åŒ– -> åå½’ä¸€åŒ– è¯¯å·®: {error:.8f}")
            
            if error < 1e-6:
                print("   âœ… å®Œç¾å¯é€†")
            elif error < 1e-4:
                print("   âœ… é«˜ç²¾åº¦å¯é€†")
            else:
                print("   âš ï¸ å¯é€†æ€§è¯¯å·®è¾ƒå¤§")
            
            # éªŒè¯å½’ä¸€åŒ–åçš„åˆ†å¸ƒ
            if abs(norm_stats["mean"]) < 0.1 and abs(norm_stats["std"] - 1.0) < 0.1:
                print("   âœ… å½’ä¸€åŒ–ååˆ†å¸ƒæ¥è¿‘N(0,1)")
            else:
                print("   âš ï¸ å½’ä¸€åŒ–ååˆ†å¸ƒåç¦»N(0,1)")
        
        else:
            print("   â„¹ï¸ latentåˆ†å¸ƒæ¥è¿‘æ ‡å‡†åˆ†å¸ƒï¼Œæ— éœ€å¯¹é½")
    
    return alignment_model.enable_alignment


def main():
    parser = argparse.ArgumentParser(description='VAEé‡å»ºå’Œåˆ†å¸ƒå¯¹é½éªŒè¯')
    parser.add_argument('--vae_checkpoint', type=str, 
                      default='/kaggle/input/stage3/vavae-stage3-epoch26-val_rec_loss0.0000.ckpt',
                      help='VA-VAEæ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data_dir', type=str, 
                      default='/kaggle/input/dataset',
                      help='MicroDoppleræ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./vae_validation',
                      help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default=None,
                      help='è®¡ç®—è®¾å¤‡ (cuda/cpu)')
    
    args = parser.parse_args()
    
    print("ğŸ” VA-VAEé‡å»ºå’Œåˆ†å¸ƒå¯¹é½éªŒè¯")
    print("="*60)
    
    try:
        # æµ‹è¯•VAEé‡å»º
        latent_stats, mse_error, mae_error = test_vae_reconstruction(
            vae_checkpoint=args.vae_checkpoint,
            data_dir=args.data_dir,
            output_dir=args.output_dir,
            device=args.device
        )
        
        # æµ‹è¯•åˆ†å¸ƒå¯¹é½
        alignment_enabled = test_distribution_alignment(
            vae_checkpoint=args.vae_checkpoint,
            data_dir=args.data_dir,
            output_dir=args.output_dir,
            device=args.device
        )
        
        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“‹ éªŒè¯æ€»ç»“")
        print("="*60)
        print(f"âœ… VAEé‡å»ºåŠŸèƒ½: {'æ­£å¸¸' if mse_error < 0.05 else 'å¼‚å¸¸'}")
        print(f"âœ… Latentåˆ†å¸ƒ: std={latent_stats['std']:.3f}")
        print(f"âœ… åˆ†å¸ƒå¯¹é½: {'éœ€è¦å¯ç”¨' if alignment_enabled else 'æ— éœ€å¯ç”¨'}")
        print(f"âœ… é‡å»ºè¯¯å·®: MSE={mse_error:.6f}, MAE={mae_error:.6f}")
        
        if alignment_enabled:
            print(f"\nğŸ’¡ å»ºè®®: VAE latentåˆ†å¸ƒåç¦»æ ‡å‡†åˆ†å¸ƒï¼Œè®­ç»ƒæ—¶å°†è‡ªåŠ¨å¯ç”¨åˆ†å¸ƒå¯¹é½")
        else:
            print(f"\nğŸ’¡ å»ºè®®: VAE latentåˆ†å¸ƒæ¥è¿‘æ ‡å‡†åˆ†å¸ƒï¼Œå¯ç›´æ¥è®­ç»ƒ")
        
        print(f"\nğŸ“ éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
å¾®å¤šæ™®å‹’æ•°æ®åŠ è½½å™¨ï¼šæ”¯æŒå¹³è¡¡æ‰¹æ¬¡é‡‡æ ·å’ŒåŸå‹å­¦ä¹ 
ç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡åŒ…å«å¤šä¸ªç”¨æˆ·ï¼Œç”¨äºæœ‰æ•ˆçš„å¯¹æ¯”å­¦ä¹ 
"""

import torch
from torch.utils.data import Dataset, DataLoader, Sampler
import numpy as np
from pathlib import Path
import json
from PIL import Image
import random
from collections import defaultdict


class MicroDopplerLatentDataset(Dataset):
    """å¾®å¤šæ™®å‹’Latentæ•°æ®é›†"""
    
    def __init__(self, latent_dir, split='train'):
        """
        Args:
            latent_dir: latentæ•°æ®ç›®å½•ï¼ŒåŒ…å«.ptæ–‡ä»¶
            split: 'train' æˆ– 'val'
        """
        self.latent_dir = Path(latent_dir)
        self.split = split
        
        # åŠ è½½æ•°æ®ç´¢å¼•
        index_file = self.latent_dir / f'{split}_index.json'
        if not index_file.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç´¢å¼•æ–‡ä»¶: {index_file}")
        
        with open(index_file, 'r') as f:
            self.index = json.load(f)
        
        # æ„å»ºæ ·æœ¬åˆ—è¡¨
        self.samples = []
        self.user_to_indices = defaultdict(list)
        
        for idx, item in enumerate(self.index):
            self.samples.append({
                'path': self.latent_dir / item['path'],
                'user_id': item['user_id'],
                'user_idx': item['user_idx']  # æ•°å­—ç´¢å¼•
            })
            self.user_to_indices[item['user_idx']].append(idx)
        
        self.num_users = len(self.user_to_indices)
        self.users = list(self.user_to_indices.keys())
        
        print(f"ğŸ“Š {split}é›†ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        print(f"   ç”¨æˆ·æ•°: {self.num_users}")
        print(f"   å¹³å‡æ ·æœ¬/ç”¨æˆ·: {len(self.samples) / self.num_users:.1f}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        item = self.samples[idx]
        
        # åŠ è½½latent
        latent = torch.load(item['path'], map_location='cpu')
        
        # ç¡®ä¿ç»´åº¦æ­£ç¡® [32, 16, 16]
        if latent.dim() == 4 and latent.size(0) == 1:
            latent = latent.squeeze(0)
        
        return latent, item['user_idx']


class BalancedBatchSampler(Sampler):
    """å¹³è¡¡æ‰¹æ¬¡é‡‡æ ·å™¨ï¼šç¡®ä¿æ¯æ‰¹åŒ…å«å¤šä¸ªç”¨æˆ·"""
    
    def __init__(self, dataset, batch_size, num_users_per_batch=4, drop_last=True):
        """
        Args:
            dataset: MicroDopplerLatentDatasetå®ä¾‹
            batch_size: æ‰¹æ¬¡å¤§å°
            num_users_per_batch: æ¯æ‰¹çš„ç”¨æˆ·æ•°
            drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸å®Œæ•´æ‰¹æ¬¡
        """
        self.dataset = dataset
        self.batch_size = batch_size
        self.num_users_per_batch = min(num_users_per_batch, dataset.num_users)
        self.drop_last = drop_last
        
        # æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬æ•°
        self.samples_per_user = batch_size // self.num_users_per_batch
        
        # è®¡ç®—æ‰¹æ¬¡æ•°
        min_samples = min(len(indices) for indices in dataset.user_to_indices.values())
        self.num_batches = min_samples // self.samples_per_user
        
        if not drop_last:
            # å¦‚æœä¸ä¸¢å¼ƒæœ€åæ‰¹æ¬¡ï¼Œå¢åŠ æ‰¹æ¬¡æ•°
            self.num_batches = len(dataset) // batch_size
    
    def __iter__(self):
        """ç”Ÿæˆæ‰¹æ¬¡ç´¢å¼•"""
        for _ in range(self.num_batches):
            batch_indices = []
            
            # éšæœºé€‰æ‹©ç”¨æˆ·
            selected_users = random.sample(self.dataset.users, self.num_users_per_batch)
            
            # ä»æ¯ä¸ªç”¨æˆ·é‡‡æ ·
            for user_idx in selected_users:
                user_indices = self.dataset.user_to_indices[user_idx]
                sampled = random.sample(user_indices, min(self.samples_per_user, len(user_indices)))
                batch_indices.extend(sampled)
            
            # æ‰“ä¹±æ‰¹æ¬¡å†…é¡ºåº
            random.shuffle(batch_indices)
            
            # ç¡®ä¿æ‰¹æ¬¡å¤§å°æ­£ç¡®
            if len(batch_indices) >= self.batch_size:
                yield batch_indices[:self.batch_size]
            elif not self.drop_last and len(batch_indices) > 0:
                yield batch_indices
    
    def __len__(self):
        return self.num_batches


def create_balanced_dataloader(latent_dir, batch_size=32, num_users_per_batch=4, 
                              split='train', shuffle=True, num_workers=2):
    """
    åˆ›å»ºå¹³è¡¡æ•°æ®åŠ è½½å™¨
    
    Args:
        latent_dir: latentæ•°æ®ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        num_users_per_batch: æ¯æ‰¹ç”¨æˆ·æ•°ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        split: æ•°æ®é›†åˆ’åˆ†
        shuffle: æ˜¯å¦æ‰“ä¹±ï¼ˆä»…å¯¹valæœ‰æ•ˆï¼‰
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
    
    Returns:
        DataLoaderå®ä¾‹
    """
    dataset = MicroDopplerLatentDataset(latent_dir, split)
    
    if split == 'train':
        # è®­ç»ƒé›†ä½¿ç”¨å¹³è¡¡é‡‡æ ·
        sampler = BalancedBatchSampler(
            dataset, 
            batch_size=batch_size,
            num_users_per_batch=num_users_per_batch,
            drop_last=True
        )
        return DataLoader(
            dataset,
            batch_sampler=sampler,
            num_workers=num_workers,
            pin_memory=True
        )
    else:
        # éªŒè¯é›†ä½¿ç”¨æ™®é€šåŠ è½½
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=False
        )


def prepare_latent_dataset(image_dir, vae_model, output_dir, split_file, device='cuda'):
    """
    å‡†å¤‡latentæ•°æ®é›†
    
    Args:
        image_dir: åŸå§‹å›¾åƒç›®å½•
        vae_model: SimplifiedVAVAEæ¨¡å‹
        output_dir: è¾“å‡ºç›®å½•
        split_file: æ•°æ®åˆ’åˆ†æ–‡ä»¶
        device: è®¾å¤‡
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # åŠ è½½æ•°æ®åˆ’åˆ†
    with open(split_file, 'r') as f:
        split_data = json.load(f)
    
    vae_model = vae_model.to(device)
    vae_model.eval()
    
    # ç”¨æˆ·IDåˆ°æ•°å­—ç´¢å¼•çš„æ˜ å°„
    all_users = sorted(set(
        user_id for split in split_data.values() 
        for user_id in split.keys()
    ))
    user_to_idx = {user_id: idx for idx, user_id in enumerate(all_users)}
    
    # å¤„ç†æ¯ä¸ªsplit
    for split_name in ['train', 'val']:
        print(f"\nğŸ”„ å¤„ç†{split_name}é›†...")
        
        split_output = output_dir / split_name
        split_output.mkdir(exist_ok=True)
        
        index = []
        total = 0
        
        for user_id, img_paths in split_data[split_name].items():
            user_idx = user_to_idx[user_id]
            
            for img_path in img_paths:
                img_path = Path(img_path)
                if not img_path.exists():
                    continue
                
                # åŠ è½½å›¾åƒ
                img = Image.open(img_path).convert('RGB')
                img = img.resize((256, 256))
                
                # è½¬æ¢ä¸ºtensor
                img_tensor = torch.from_numpy(np.array(img)).float() / 255.0
                img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(device)
                
                # ç¼–ç åˆ°latent
                with torch.no_grad():
                    latent = vae_model.encode(img_tensor)
                
                # ä¿å­˜latent
                save_name = f"{user_id}_{total:05d}.pt"
                save_path = split_output / save_name
                torch.save(latent.cpu(), save_path)
                
                # æ·»åŠ åˆ°ç´¢å¼•
                index.append({
                    'path': f"{split_name}/{save_name}",
                    'user_id': user_id,
                    'user_idx': user_idx
                })
                
                total += 1
                
                if total % 100 == 0:
                    print(f"   å·²å¤„ç† {total} æ ·æœ¬...")
        
        # ä¿å­˜ç´¢å¼•
        index_file = output_dir / f'{split_name}_index.json'
        with open(index_file, 'w') as f:
            json.dump(index, f, indent=2)
        
        print(f"âœ… {split_name}é›†å®Œæˆ: {total}ä¸ªæ ·æœ¬")
    
    print(f"\nâœ… Latentæ•°æ®é›†å‡†å¤‡å®Œæˆ: {output_dir}")
    return output_dir


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print("ğŸ§ª æµ‹è¯•å¹³è¡¡æ‰¹æ¬¡é‡‡æ ·å™¨...")
    
    # æ¨¡æ‹Ÿæ•°æ®é›†
    class DummyDataset:
        def __init__(self):
            self.num_users = 10
            self.users = list(range(10))
            self.user_to_indices = {
                i: list(range(i*15, (i+1)*15)) 
                for i in range(10)
            }
    
    dataset = DummyDataset()
    sampler = BalancedBatchSampler(dataset, batch_size=32, num_users_per_batch=4)
    
    print(f"æ•°æ®é›†: {10}ä¸ªç”¨æˆ·ï¼Œæ¯ä¸ª15ä¸ªæ ·æœ¬")
    print(f"æ‰¹æ¬¡è®¾ç½®: batch_size=32, num_users_per_batch=4")
    print(f"æœŸæœ›: æ¯æ‰¹8ä¸ªæ ·æœ¬/ç”¨æˆ·")
    
    # æµ‹è¯•å‡ ä¸ªæ‰¹æ¬¡
    for i, batch in enumerate(sampler):
        if i >= 3:
            break
        print(f"\næ‰¹æ¬¡{i}: {len(batch)}ä¸ªæ ·æœ¬")
        
        # ç»Ÿè®¡ç”¨æˆ·åˆ†å¸ƒ
        user_counts = defaultdict(int)
        for idx in batch:
            user_id = idx // 15
            user_counts[user_id] += 1
        
        print(f"  ç”¨æˆ·åˆ†å¸ƒ: {dict(user_counts)}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")

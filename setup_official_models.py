#!/usr/bin/env python3
"""
æŒ‰ç…§LightningDiTå®˜æ–¹æ–¹æ³•è®¾ç½®é¢„è®­ç»ƒæ¨¡å‹
ä¸‹è½½å®˜æ–¹é¢„è®­ç»ƒæƒé‡å¹¶é…ç½®æ¨ç†ç¯å¢ƒ
"""

import os
import sys
import requests
import yaml
from pathlib import Path

def download_file(url, local_path, description=""):
    """ä¸‹è½½æ–‡ä»¶"""
    print(f"ğŸ“¥ ä¸‹è½½ {description}: {url}")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(local_path):
        print(f"âœ… æ–‡ä»¶å·²å­˜åœ¨: {local_path}")
        return True
    
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(local_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\rè¿›åº¦: {percent:.1f}%", end='', flush=True)
        
        print(f"\nâœ… ä¸‹è½½å®Œæˆ: {local_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def setup_official_models():
    """è®¾ç½®å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹"""
    
    print("ğŸš€ è®¾ç½®LightningDiTå®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    models_dir = Path("./official_models")
    models_dir.mkdir(exist_ok=True)
    
    # å®˜æ–¹æ¨¡å‹ä¸‹è½½é“¾æ¥
    models = {
        "VA-VAE": {
            "url": "https://huggingface.co/hustvl/vavae-imagenet256-f16d32-dinov2/resolve/main/vavae-imagenet256-f16d32-dinov2.pt",
            "path": models_dir / "vavae-imagenet256-f16d32-dinov2.pt"
        },
        "LightningDiT-XL-800ep": {
            "url": "https://huggingface.co/hustvl/lightningdit-xl-imagenet256-800ep/resolve/main/lightningdit-xl-imagenet256-800ep.pt",
            "path": models_dir / "lightningdit-xl-imagenet256-800ep.pt"
        },
        "Latent Statistics": {
            "url": "https://huggingface.co/hustvl/vavae-imagenet256-f16d32-dinov2/resolve/main/latents_stats.pt",
            "path": models_dir / "latents_stats.pt"
        }
    }
    
    # ä¸‹è½½æ¨¡å‹
    success_count = 0
    for name, info in models.items():
        if download_file(info["url"], str(info["path"]), name):
            success_count += 1
    
    print(f"\nğŸ“Š ä¸‹è½½ç»“æœ: {success_count}/{len(models)} ä¸ªæ–‡ä»¶æˆåŠŸ")
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config_path = create_inference_config(models_dir)
    
    # åˆ›å»ºæ¨ç†è„šæœ¬
    create_inference_script(config_path)
    
    print("\nâœ… è®¾ç½®å®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶ä½ç½®: {models_dir.absolute()}")
    print(f"âš™ï¸  é…ç½®æ–‡ä»¶: {config_path}")
    print(f"ğŸš€ è¿è¡Œæ¨ç†: python run_official_inference.py")

def create_inference_config(models_dir):
    """åˆ›å»ºæ¨ç†é…ç½®æ–‡ä»¶ - ä¸¥æ ¼æŒ‰ç…§å®˜æ–¹reproductionsé…ç½®"""

    config_path = "official_inference_config.yaml"

    # å®Œå…¨æŒ‰ç…§å®˜æ–¹configs/reproductions/lightningdit_xl_vavae_f16d32_800ep_cfg.yaml
    config = {
        'ckpt_path': str(models_dir / "lightningdit-xl-imagenet256-800ep.pt"),
        'data': {
            # å®˜æ–¹è¯´æ˜ï¼šå¦‚æœåªæ˜¯æ¨ç†ï¼Œä¸‹è½½latents_stats.ptå¹¶ç»™å‡ºè·¯å¾„å³å¯
            'data_path': str(models_dir / "latents_stats.pt"),
            'fid_reference_file': 'path/to/your/VIRTUAL_imagenet256_labeled.npz',
            'image_size': 256,
            'num_classes': 1000,
            'num_workers': 8,
            'latent_norm': True,
            'latent_multiplier': 1.0
        },
        'vae': {
            'model_name': 'vavae_f16d32',
            'downsample_ratio': 16
        },
        'model': {
            'model_type': 'LightningDiT-XL/1',
            'use_qknorm': False,  # å®˜æ–¹æ³¨é‡Šï¼šå¦‚æœé‡åˆ°NaN lossï¼Œå»ºè®®å¯ç”¨
            'use_swiglu': True,
            'use_rope': True,
            'use_rmsnorm': True,
            'wo_shift': False,
            'in_chans': 32
        },
        'train': {
            'max_steps': 80000,
            'global_batch_size': 1024,
            'global_seed': 0,
            'output_dir': 'output',
            'exp_name': 'lightningdit_xl_vavae_f16d32',
            'ckpt': None,
            'log_every': 100,
            'ckpt_every': 20000
        },
        'optimizer': {
            'lr': 0.0002,
            'beta2': 0.95
        },
        'transport': {
            'path_type': 'Linear',
            'prediction': 'velocity',
            'loss_weight': None,
            'sample_eps': None,
            'train_eps': None,
            'use_cosine_loss': True,
            'use_lognorm': True
        },
        'sample': {
            'mode': 'ODE',
            'sampling_method': 'euler',
            'atol': 0.000001,
            'rtol': 0.001,
            'reverse': False,
            'likelihood': False,
            'num_sampling_steps': 250,
            # å®˜æ–¹æ³¨é‡Šï¼š800 epochæ€§èƒ½FID=1.35ä½¿ç”¨cfg_scale=6.7
            'cfg_scale': 6.7,
            'per_proc_batch_size': 4,
            'fid_num': 50000,
            # å®˜æ–¹reproductionsé…ç½®ä¸­çš„ç²¾ç¡®å€¼
            'cfg_interval_start': 0.125,
            'timestep_shift': 0.3
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)
    
    print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
    return config_path

def create_inference_script(config_path):
    """åˆ›å»ºæ¨ç†è„šæœ¬"""

    script_content = f'''#!/usr/bin/env python3
"""
ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†
ä¸¥æ ¼æŒ‰ç…§LightningDiTå®˜æ–¹READMEæ–¹æ³•
"""

import os
import sys
import subprocess
import yaml

def main():
    print("ğŸš€ ä½¿ç”¨LightningDiTå®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†")
    print("ğŸ“– ä¸¥æ ¼æŒ‰ç…§å®˜æ–¹READMEæ–¹æ³•æ‰§è¡Œ")

    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        return

    # åˆ‡æ¢åˆ°LightningDiTç›®å½•
    os.chdir("LightningDiT")

    # æ›´æ–°VA-VAEé…ç½® (å®˜æ–¹è¦æ±‚)
    vavae_config = "tokenizer/configs/vavae_f16d32.yaml"
    update_vavae_config(vavae_config)

    # è¿è¡Œå®˜æ–¹æ¨ç†è„šæœ¬ (ä¿®æ­£READMEä¸­çš„é”™è¯¯)
    config_path = "../{config_path}"
    cmd = f"bash run_fast_inference.sh {{config_path}}"

    print(f"ğŸ¯ æ‰§è¡Œå®˜æ–¹å‘½ä»¤: {{cmd}}")
    print("ğŸ“ æ³¨æ„: å®˜æ–¹READMEç¬¬101è¡Œæœ‰é”™è¯¯ï¼Œåº”è¯¥æ˜¯å•ä¸ªbash")

    try:
        # ä½¿ç”¨å®˜æ–¹çš„accelerate launchæ–¹å¼
        result = subprocess.run(cmd, shell=True, check=True)
        print("âœ… æ¨ç†å®Œæˆ!")
        print("ğŸ“ ç”Ÿæˆçš„å›¾åƒä¿å­˜åœ¨: demo_images/demo_samples.png")
        print("ğŸ¨ Demoæ¨¡å¼è‡ªåŠ¨ä½¿ç”¨: cfg_scale=9.0, cfg_interval_start=0, timestep_shift=0")

    except subprocess.CalledProcessError as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {{e}}")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. æ˜¯å¦å®‰è£…äº†accelerate")
        print("   2. æ˜¯å¦æœ‰GPUå¯ç”¨")
        print("   3. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸‹è½½")

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒä¾èµ–"""
    try:
        import torch
        print(f"âœ… PyTorch: {{torch.__version__}}")
        print(f"âœ… CUDAå¯ç”¨: {{torch.cuda.is_available()}}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False

    try:
        import accelerate
        print(f"âœ… Accelerate: {{accelerate.__version__}}")
    except ImportError:
        print("âŒ Accelerateæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install accelerate")
        return False

    return True

def update_vavae_config(config_path):
    """æ›´æ–°VA-VAEé…ç½®æ–‡ä»¶ (å®˜æ–¹æ•™ç¨‹è¦æ±‚)"""
    print(f"ğŸ”§ æ›´æ–°VA-VAEé…ç½®: {{config_path}}")

    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # æ›´æ–°æ£€æŸ¥ç‚¹è·¯å¾„ (å®˜æ–¹æ•™ç¨‹æ­¥éª¤)
    old_path = config.get('ckpt_path', 'N/A')
    config['ckpt_path'] = '../official_models/vavae-imagenet256-f16d32-dinov2.pt'

    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)

    print(f"   æ—§è·¯å¾„: {{old_path}}")
    print(f"   æ–°è·¯å¾„: {{config['ckpt_path']}}")
    print(f"âœ… VA-VAEé…ç½®å·²æ›´æ–°")

if __name__ == "__main__":
    main()
'''
    
    with open("run_official_inference.py", 'w') as f:
        f.write(script_content)
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod("run_official_inference.py", 0o755)
    
    print("âœ… æ¨ç†è„šæœ¬å·²åˆ›å»º: run_official_inference.py")

if __name__ == "__main__":
    setup_official_models()

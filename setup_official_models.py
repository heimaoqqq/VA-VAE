#!/usr/bin/env python3
"""
æŒ‰ç…§LightningDiTå®˜æ–¹æ–¹æ³•è®¾ç½®é¢„è®­ç»ƒæ¨¡å‹
ä¸‹è½½å®˜æ–¹é¢„è®­ç»ƒæƒé‡å¹¶é…ç½®æ¨ç†ç¯å¢ƒ
"""

import os
import sys
import requests
import yaml
from pathlib import Path

def download_file(url, local_path, description=""):
    """ä¸‹è½½æ–‡ä»¶"""
    print(f"ğŸ“¥ ä¸‹è½½ {description}: {url}")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(local_path):
        print(f"âœ… æ–‡ä»¶å·²å­˜åœ¨: {local_path}")
        return True
    
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(local_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\rè¿›åº¦: {percent:.1f}%", end='', flush=True)
        
        print(f"\nâœ… ä¸‹è½½å®Œæˆ: {local_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def setup_official_models():
    """è®¾ç½®å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹"""
    
    print("ğŸš€ è®¾ç½®LightningDiTå®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    models_dir = Path("./official_models")
    models_dir.mkdir(exist_ok=True)
    
    # å®˜æ–¹æ¨¡å‹ä¸‹è½½é“¾æ¥
    models = {
        "VA-VAE": {
            "url": "https://huggingface.co/hustvl/vavae-imagenet256-f16d32-dinov2/resolve/main/vavae-imagenet256-f16d32-dinov2.pt",
            "path": models_dir / "vavae-imagenet256-f16d32-dinov2.pt"
        },
        "LightningDiT-XL-800ep": {
            "url": "https://huggingface.co/hustvl/lightningdit-xl-imagenet256-800ep/resolve/main/lightningdit-xl-imagenet256-800ep.pt",
            "path": models_dir / "lightningdit-xl-imagenet256-800ep.pt"
        },
        "Latent Statistics": {
            "url": "https://huggingface.co/hustvl/vavae-imagenet256-f16d32-dinov2/resolve/main/latents_stats.pt",
            "path": models_dir / "latents_stats.pt"
        }
    }
    
    # ä¸‹è½½æ¨¡å‹
    success_count = 0
    for name, info in models.items():
        if download_file(info["url"], str(info["path"]), name):
            success_count += 1
    
    print(f"\nğŸ“Š ä¸‹è½½ç»“æœ: {success_count}/{len(models)} ä¸ªæ–‡ä»¶æˆåŠŸ")
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config_path = create_inference_config(models_dir)
    
    # åˆ›å»ºæ¨ç†è„šæœ¬
    create_inference_script(config_path)
    
    print("\nâœ… è®¾ç½®å®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶ä½ç½®: {models_dir.absolute()}")
    print(f"âš™ï¸  é…ç½®æ–‡ä»¶: {config_path}")
    print(f"ğŸš€ è¿è¡Œæ¨ç†: python run_official_inference.py")

def create_inference_config(models_dir):
    """åˆ›å»ºæ¨ç†é…ç½®æ–‡ä»¶"""
    
    config_path = "official_inference_config.yaml"
    
    config = {
        'ckpt_path': str(models_dir / "lightningdit-xl-imagenet256-800ep.pt"),
        'data': {
            'data_path': str(models_dir / "latents_stats.pt"),
            'fid_reference_file': 'path/to/your/VIRTUAL_imagenet256_labeled.npz',
            'image_size': 256,
            'num_classes': 1000,
            'num_workers': 8,
            'latent_norm': True,
            'latent_multiplier': 1.0
        },
        'vae': {
            'model_name': 'vavae_f16d32',
            'downsample_ratio': 16
        },
        'model': {
            'model_type': 'LightningDiT-XL/1',
            'use_qknorm': False,
            'use_swiglu': True,
            'use_rope': True,
            'use_rmsnorm': True,
            'wo_shift': False,
            'in_chans': 32
        },
        'transport': {
            'path_type': 'Linear',
            'prediction': 'velocity',
            'loss_weight': None,
            'sample_eps': None,
            'train_eps': None,
            'use_cosine_loss': True,
            'use_lognorm': True
        },
        'sample': {
            'mode': 'ODE',
            'sampling_method': 'euler',
            'atol': 0.000001,
            'rtol': 0.001,
            'reverse': False,
            'likelihood': False,
            'num_sampling_steps': 250,
            'cfg_scale': 6.7,
            'per_proc_batch_size': 4,
            'fid_num': 50000,
            'cfg_interval_start': 0.125,
            'timestep_shift': 0.3
        }
    }
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)
    
    print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
    return config_path

def create_inference_script(config_path):
    """åˆ›å»ºæ¨ç†è„šæœ¬"""
    
    script_content = f'''#!/usr/bin/env python3
"""
ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†
"""

import os
import sys
import subprocess

def main():
    print("ğŸš€ ä½¿ç”¨LightningDiTå®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†")
    
    # æ£€æŸ¥ç¯å¢ƒ
    try:
        import torch
        print(f"âœ… PyTorch: {{torch.__version__}}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return
    
    # åˆ‡æ¢åˆ°LightningDiTç›®å½•
    os.chdir("LightningDiT")
    
    # æ›´æ–°VA-VAEé…ç½®
    vavae_config = "tokenizer/configs/vavae_f16d32.yaml"
    update_vavae_config(vavae_config)
    
    # è¿è¡Œå®˜æ–¹æ¨ç†è„šæœ¬
    config_path = "../{config_path}"
    cmd = f"bash run_fast_inference.sh {{config_path}}"
    
    print(f"ğŸ¯ æ‰§è¡Œå‘½ä»¤: {{cmd}}")
    
    try:
        result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)
        print("âœ… æ¨ç†å®Œæˆ!")
        print("ğŸ“ ç”Ÿæˆçš„å›¾åƒä¿å­˜åœ¨: demo_images/demo_samples.png")
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {{e}}")
        print(f"é”™è¯¯è¾“å‡º: {{e.stderr}}")

def update_vavae_config(config_path):
    """æ›´æ–°VA-VAEé…ç½®æ–‡ä»¶"""
    import yaml
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # æ›´æ–°æ£€æŸ¥ç‚¹è·¯å¾„
    config['ckpt_path'] = '../official_models/vavae-imagenet256-f16d32-dinov2.pt'
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)
    
    print(f"âœ… å·²æ›´æ–°VA-VAEé…ç½®: {{config_path}}")

if __name__ == "__main__":
    main()
'''
    
    with open("run_official_inference.py", 'w') as f:
        f.write(script_content)
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod("run_official_inference.py", 0o755)
    
    print("âœ… æ¨ç†è„šæœ¬å·²åˆ›å»º: run_official_inference.py")

if __name__ == "__main__":
    setup_official_models()

"""
æœ€å°åŒ–è®­ç»ƒè„šæœ¬ä¿®æ”¹
åŸºäºLightningDiTçš„è®­ç»ƒè„šæœ¬ï¼Œä»…æ·»åŠ ç”¨æˆ·æ¡ä»¶åŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, Callback
from pytorch_lightning.loggers import TensorBoardLogger
import os
import argparse
import time
from pathlib import Path

# å¯¼å…¥æˆ‘ä»¬çš„æœ€å°ä¿®æ”¹æ¨¡å—
from minimal_micro_doppler_dataset import create_micro_doppler_dataloader, MicroDopplerDataset
from minimal_vavae_modification import UserConditionedVAVAE

class TrainingSummaryCallback(Callback):
    """è‡ªå®šä¹‰å›è°ƒï¼šæä¾›æ¸…æ™°çš„è®­ç»ƒæ€»ç»“"""

    def __init__(self):
        super().__init__()
        self.epoch_start_time = None

    def on_train_epoch_start(self, trainer, pl_module):
        """è®°å½•epochå¼€å§‹æ—¶é—´"""
        # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡º
        if trainer.is_global_zero:
            self.epoch_start_time = time.time()
            print(f"\nğŸš€ Epoch {trainer.current_epoch + 1}/{trainer.max_epochs} å¼€å§‹è®­ç»ƒ...")

    def on_train_epoch_end(self, trainer, pl_module):
        """è®­ç»ƒepochç»“æŸæ€»ç»“"""
        # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡º
        if trainer.is_global_zero:
            epoch_time = time.time() - self.epoch_start_time if self.epoch_start_time else 0
            metrics = trainer.callback_metrics
            train_loss = metrics.get('train/loss', 0.0)

            print(f"â±ï¸  è®­ç»ƒå®Œæˆ - ç”¨æ—¶: {epoch_time:.1f}s, æŸå¤±: {train_loss:.6f}")

    def on_validation_epoch_end(self, trainer, pl_module):
        """éªŒè¯epochç»“æŸæ€»ç»“"""
        # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡º
        if trainer.is_global_zero:
            metrics = trainer.callback_metrics
            val_loss = metrics.get('val/loss', 0.0)
            train_loss = metrics.get('train/loss', 0.0)
            val_recon = metrics.get('val/recon', 0.0)
            val_kl = metrics.get('val/kl', 0.0)

            print(f"ğŸ“Š Epoch {trainer.current_epoch + 1} æ€»ç»“:")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.6f} | éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"   éªŒè¯é‡å»º: {val_recon:.6f} | éªŒè¯KL: {val_kl:.2f}")

            # æ˜¾ç¤ºæ”¹è¿›æƒ…å†µ
            if hasattr(self, 'best_val_loss'):
                if val_loss < self.best_val_loss:
                    print(f"   ğŸ‰ éªŒè¯æŸå¤±æ”¹è¿›! ({self.best_val_loss:.6f} â†’ {val_loss:.6f})")
                    self.best_val_loss = val_loss
                else:
                    print(f"   ğŸ“ˆ å½“å‰æœ€ä½³: {self.best_val_loss:.6f}")
            else:
                self.best_val_loss = val_loss
                print(f"   ğŸ¯ åˆå§‹éªŒè¯æŸå¤±: {val_loss:.6f}")

            print("=" * 70)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å¾®å¤šæ™®å‹’VA-VAEè®­ç»ƒ - æœ€å°ä¿®æ”¹ç‰ˆæœ¬')

    parser.add_argument('--data_dir', type=str, required=True, help='å¾®å¤šæ™®å‹’æ•°æ®ç›®å½•')
    parser.add_argument('--original_vavae', type=str, required=True, help='åŸå§‹VA-VAEæ¨¡å‹è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°ï¼ˆæ¯ä¸ªGPUï¼‰')
    parser.add_argument('--condition_dim', type=int, default=128, help='æ¡ä»¶å‘é‡ç»´åº¦')
    parser.add_argument('--kl_weight', type=float, default=1e-4, help='KLæ•£åº¦æƒé‡')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')

    # PyTorch Lightning è®­ç»ƒå‚æ•°ï¼ˆéµå¾ªåŸé¡¹ç›®æ–¹å¼ï¼‰
    parser.add_argument('--max_epochs', type=int, default=100, help='æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--devices', type=int, default=1, help='GPUæ•°é‡')
    parser.add_argument('--num_nodes', type=int, default=1, help='èŠ‚ç‚¹æ•°é‡')
    parser.add_argument('--strategy', type=str, default='auto', help='è®­ç»ƒç­–ç•¥ (auto, ddp, ddp_find_unused_parameters_true)')
    parser.add_argument('--accelerator', type=str, default='gpu', help='åŠ é€Ÿå™¨ç±»å‹')
    parser.add_argument('--precision', type=str, default='32', help='ç²¾åº¦ (16, 32, bf16)')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')

    return parser.parse_args()


class MicroDopplerVAVAEModule(pl.LightningModule):
    """
    PyTorch Lightningæ¨¡å—åŒ…è£…ç”¨æˆ·æ¡ä»¶åŒ–VA-VAE
    éµå¾ªåŸé¡¹ç›®çš„è®­ç»ƒæ–¹å¼
    """

    def __init__(self, original_vavae, num_users, condition_dim=128, lr=1e-4, kl_weight=1e-6):
        super().__init__()
        self.save_hyperparameters(ignore=['original_vavae'])

        # åˆ›å»ºç”¨æˆ·æ¡ä»¶åŒ–æ¨¡å‹
        self.model = UserConditionedVAVAE(
            original_vavae=original_vavae,
            num_users=num_users,
            condition_dim=condition_dim
        )

        self.lr = lr
        self.kl_weight = kl_weight
        self.kl_weight_max = 1e-3  # æœ€å¤§KLæƒé‡
        self.kl_weight_min = 1e-6  # æœ€å°KLæƒé‡

    def forward(self, x, user_ids=None):
        return self.model(x, user_ids)

    def training_step(self, batch, batch_idx):
        images = batch['image']
        user_ids = batch.get('user_id', None)

        # å‰å‘ä¼ æ’­
        reconstructed, posterior = self.model(images, user_ids)

        # è®¡ç®—æŸå¤±
        recon_loss = nn.functional.mse_loss(reconstructed, images, reduction='mean')
        kl_loss = posterior.kl().mean()

        # KLæ•£åº¦è£å‰ªï¼Œé˜²æ­¢çˆ†ç‚¸
        kl_loss = torch.clamp(kl_loss, max=1000.0)

        # åŠ¨æ€KLæƒé‡ï¼šæ ¹æ®KLå€¼å¤§å°è°ƒæ•´æƒé‡
        if kl_loss > 10000:
            current_kl_weight = self.kl_weight_min  # ä½¿ç”¨æœ€å°æƒé‡
        elif kl_loss > 1000:
            current_kl_weight = self.kl_weight_min * 10  # ç¨å¾®å¢åŠ 
        else:
            current_kl_weight = self.kl_weight  # ä½¿ç”¨åŸå§‹æƒé‡

        total_loss = recon_loss + current_kl_weight * kl_loss

        # è®°å½•æŸå¤±ï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
        self.log('train/loss', total_loss, prog_bar=True, logger=True)
        self.log('train/recon', recon_loss, prog_bar=False, logger=True)
        self.log('train/kl', kl_loss, prog_bar=False, logger=True)

        return total_loss

    def validation_step(self, batch, batch_idx):
        images = batch['image']
        user_ids = batch.get('user_id', None)

        # å‰å‘ä¼ æ’­
        reconstructed, posterior = self.model(images, user_ids)

        # è®¡ç®—æŸå¤±
        recon_loss = nn.functional.mse_loss(reconstructed, images, reduction='mean')
        kl_loss = posterior.kl().mean()

        # KLæ•£åº¦è£å‰ªï¼Œé˜²æ­¢çˆ†ç‚¸
        kl_loss = torch.clamp(kl_loss, max=1000.0)

        # åŠ¨æ€KLæƒé‡ï¼šæ ¹æ®KLå€¼å¤§å°è°ƒæ•´æƒé‡
        if kl_loss > 10000:
            current_kl_weight = self.kl_weight_min  # ä½¿ç”¨æœ€å°æƒé‡
        elif kl_loss > 1000:
            current_kl_weight = self.kl_weight_min * 10  # ç¨å¾®å¢åŠ 
        else:
            current_kl_weight = self.kl_weight  # ä½¿ç”¨åŸå§‹æƒé‡

        total_loss = recon_loss + current_kl_weight * kl_loss

        # è®°å½•æŸå¤±ï¼ˆæ·»åŠ åˆ†å¸ƒå¼åŒæ­¥ï¼‰
        self.log('val/loss', total_loss, prog_bar=True, logger=True, sync_dist=True)
        self.log('val/recon', recon_loss, prog_bar=False, logger=True, sync_dist=True)
        self.log('val/kl', kl_loss, prog_bar=False, logger=True, sync_dist=True)

        return total_loss

    def on_train_epoch_end(self):
        """è®­ç»ƒepochç»“æŸæ—¶çš„æ€»ç»“ - ç§»é™¤ï¼Œé¿å…é‡å¤"""
        pass

    def on_validation_epoch_end(self):
        """éªŒè¯epochç»“æŸæ—¶çš„æ€»ç»“ - ç§»é™¤ï¼Œé¿å…é‡å¤"""
        pass

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer


class MicroDopplerDataModule(pl.LightningDataModule):
    """
    PyTorch Lightningæ•°æ®æ¨¡å—
    éµå¾ªåŸé¡¹ç›®çš„æ•°æ®åŠ è½½æ–¹å¼
    """

    def __init__(self, data_dir, batch_size=16, num_workers=4):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.num_users = None

    def setup(self, stage=None):
        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        self.train_dataset = MicroDopplerDataset(self.data_dir, split='train')
        self.val_dataset = MicroDopplerDataset(self.data_dir, split='val')

        # è·å–ç”¨æˆ·æ•°é‡
        self.num_users = self.train_dataset.num_users

        print(f"è®­ç»ƒé›†å¤§å°: {len(self.train_dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(self.val_dataset)}")
        print(f"ç”¨æˆ·æ•°é‡: {self.num_users}")

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            drop_last=True
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )


def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    pl.seed_everything(seed)
    print(f"è®¾ç½®éšæœºç§å­: {seed}")


def create_dummy_vavae():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„DummyVAVAEæ¨¡å‹"""
    class DummyVAVAE(nn.Module):
        def __init__(self):
            super().__init__()
            # ç¼–ç å™¨: 256x256 -> 16x16 (16å€ä¸‹é‡‡æ ·)
            self.encoder = nn.Sequential(
                nn.Conv2d(3, 64, 4, 2, 1),    # 256->128
                nn.ReLU(),
                nn.Conv2d(64, 128, 4, 2, 1),  # 128->64
                nn.ReLU(),
                nn.Conv2d(128, 256, 4, 2, 1), # 64->32
                nn.ReLU(),
                nn.Conv2d(256, 512, 4, 2, 1), # 32->16
                nn.ReLU()
            )
            # è§£ç å™¨: 16x16 -> 256x256
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(512, 256, 4, 2, 1), # 16->32
                nn.ReLU(),
                nn.ConvTranspose2d(256, 128, 4, 2, 1), # 32->64
                nn.ReLU(),
                nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 64->128
                nn.ReLU(),
                nn.ConvTranspose2d(64, 3, 4, 2, 1),    # 128->256
                nn.Sigmoid()
            )
            # é‡åŒ–å±‚
            self.quant_conv = nn.Conv2d(512, 64, 1)  # è¾“å‡º32é€šé“çš„å‡å€¼å’Œæ–¹å·®
            self.post_quant_conv = nn.Conv2d(32, 512, 1)

        def encode(self, x):
            """ç¼–ç """
            h = self.encoder(x)
            moments = self.quant_conv(h)
            # åˆ†ç¦»å‡å€¼å’Œæ–¹å·®
            mean, logvar = torch.chunk(moments, 2, dim=1)
            return mean, logvar

        def decode(self, z):
            """è§£ç """
            z = self.post_quant_conv(z)
            return self.decoder(z)

        def forward(self, x):
            """å‰å‘ä¼ æ’­"""
            mean, logvar = self.encode(x)
            # é‡å‚æ•°åŒ–æŠ€å·§
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mean + eps * std
            recon = self.decode(z)
            return recon, mean, logvar

    return DummyVAVAE()


def main():
    """ä¸»è®­ç»ƒå‡½æ•° - ä½¿ç”¨PyTorch Lightning"""
    args = parse_args()

    # è®¾ç½®éšæœºç§å­
    setup_seed(args.seed)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("ğŸ¯ å¾®å¤šæ™®å‹’VA-VAEç”¨æˆ·æ¡ä»¶åŒ–è®­ç»ƒ")
    print("=" * 50)

    # 1. åˆ›å»ºæ•°æ®æ¨¡å—
    print("ğŸ“Š åˆ›å»ºæ•°æ®æ¨¡å—...")
    data_module = MicroDopplerDataModule(
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        num_workers=4
    )

    # è®¾ç½®æ•°æ®æ¨¡å—ä»¥è·å–ç”¨æˆ·æ•°é‡
    data_module.setup()
    num_users = data_module.num_users

    # 2. åˆ›å»ºæ¨¡å‹
    print("ğŸ¤– åˆ›å»ºç”¨æˆ·æ¡ä»¶åŒ–VA-VAEæ¨¡å‹...")

    # åŠ è½½é¢„è®­ç»ƒçš„VA-VAEæ¨¡å‹
    if os.path.exists(args.original_vavae):
        print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {Path(args.original_vavae).name}")
        try:
            # ä»LightningDiTå¯¼å…¥VA-VAEæ¨¡å‹
            import sys
            sys.path.append('LightningDiT')
            from tokenizer.autoencoder import AutoencoderKL

            # åˆ›å»ºVA-VAEæ¨¡å‹å®ä¾‹å¹¶ç›´æ¥åŠ è½½æƒé‡
            original_vavae = AutoencoderKL(
                embed_dim=32,  # f16d32é…ç½®
                ch_mult=(1, 1, 2, 2, 4),
                ckpt_path=args.original_vavae,  # ç›´æ¥ä½¿ç”¨ckpt_pathå‚æ•°
                model_type='vavae'
            )

            print("âœ… é¢„è®­ç»ƒVA-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            print(f"âŒ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
            print("ä½¿ç”¨DummyVAVAEè¿›è¡Œæµ‹è¯•...")
            original_vavae = create_dummy_vavae()
    else:
        print(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.original_vavae}")
        print("ä½¿ç”¨DummyVAVAEè¿›è¡Œæµ‹è¯•...")
        original_vavae = create_dummy_vavae()

    # åˆ›å»ºLightningæ¨¡å—
    model = MicroDopplerVAVAEModule(
        original_vavae=original_vavae,
        num_users=num_users,
        condition_dim=args.condition_dim,
        lr=args.lr,
        kl_weight=args.kl_weight
    )

    # 3. è®¾ç½®å›è°ƒå‡½æ•°
    print("3. è®¾ç½®è®­ç»ƒå›è°ƒ...")
    callbacks = [
        ModelCheckpoint(
            dirpath=output_dir / 'checkpoints',
            filename='best-{epoch:02d}-{val/loss:.4f}',
            monitor='val/loss',
            mode='min',
            save_top_k=1,
            save_last=True
        ),
        LearningRateMonitor(logging_interval='epoch'),
        TrainingSummaryCallback()  # æ·»åŠ è‡ªå®šä¹‰è®­ç»ƒæ€»ç»“å›è°ƒ
    ]

    # 4. è®¾ç½®æ—¥å¿—è®°å½•å™¨
    logger = TensorBoardLogger(
        save_dir=output_dir,
        name='lightning_logs'
    )

    # 5. åˆ›å»ºè®­ç»ƒå™¨
    print("4. åˆ›å»ºPyTorch Lightningè®­ç»ƒå™¨...")
    trainer = Trainer(
        max_epochs=args.max_epochs,
        devices=args.devices,
        num_nodes=args.num_nodes,
        strategy=args.strategy,
        accelerator=args.accelerator,
        precision=args.precision,
        callbacks=callbacks,
        logger=logger,
        log_every_n_steps=200,  # å‡å°‘æ—¥å¿—é¢‘ç‡
        val_check_interval=1.0,
        enable_progress_bar=True,
        enable_model_summary=False,  # ç®€åŒ–è¾“å‡º
        enable_checkpointing=True,
        num_sanity_val_steps=0  # è·³è¿‡sanity checkï¼Œå‡å°‘è¾“å‡º
    )

    print(f"è®­ç»ƒé…ç½®:")
    print(f"  - æœ€å¤§è½®æ•°: {args.max_epochs}")
    print(f"  - GPUæ•°é‡: {args.devices}")
    print(f"  - è®­ç»ƒç­–ç•¥: {args.strategy}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size} (æ¯ä¸ªGPU)")
    print(f"  - å­¦ä¹ ç‡: {args.lr}")

    # 6. å¼€å§‹è®­ç»ƒ
    print("5. å¼€å§‹è®­ç»ƒ...")
    trainer.fit(model, data_module)

    print("\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {output_dir / 'checkpoints'}")
    print(f"è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨: {output_dir / 'lightning_logs'}")


if __name__ == '__main__':
    main()




"""
åˆ†æçœŸå®å¾®å¤šæ™®å‹’æ•°æ®çš„ç­›é€‰æŒ‡æ ‡åˆ†å¸ƒ
ç”¨äºç¡®å®šåˆç†çš„ç­›é€‰é˜ˆå€¼ï¼Œé¿å…åˆæˆæ ·æœ¬è´¨é‡æ±¡æŸ“
"""

import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import argparse
from PIL import Image
import torchvision.transforms as transforms
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

class DomainAdaptiveClassifier(nn.Module):
    """ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„åˆ†ç±»å™¨ç»“æ„"""
    def __init__(self, num_classes=31, dropout_rate=0.3, feature_dim=512):
        super().__init__()
        import torchvision.models as models
        self.backbone = models.resnet18(pretrained=False)
        backbone_dim = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()
        
        self.feature_projector = nn.Sequential(
            nn.Linear(backbone_dim, feature_dim),
            nn.BatchNorm1d(feature_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
        
        # ç”¨äºå­˜å‚¨ç”¨æˆ·åŸå‹
        self.register_buffer('feature_bank', torch.zeros(num_classes, feature_dim))
        self.register_buffer('feature_count', torch.zeros(num_classes))

    def forward(self, x):
        backbone_features = self.backbone(x)
        features = self.feature_projector(backbone_features)
        logits = self.classifier(features)
        return logits
    
    def extract_features(self, x):
        """æå–ç‰¹å¾ç”¨äºå¤šæ ·æ€§åˆ†æ"""
        backbone_features = self.backbone(x)
        features = self.feature_projector(backbone_features)
        return features

def load_classifier(checkpoint_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„åˆ†ç±»å™¨"""
    model = DomainAdaptiveClassifier(num_classes=31)
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    return model

def load_real_data(data_dir, max_samples_per_user=50):
    """åŠ è½½çœŸå®å¾®å¤šæ™®å‹’æ•°æ®"""
    data_dir = Path(data_dir)
    samples = []
    labels = []
    
    print(f"ğŸ” æ‰«æçœŸå®æ•°æ®ç›®å½•: {data_dir}")
    
    for user_id in range(1, 32):  # ID_1 åˆ° ID_31
        user_dir = data_dir / f"ID_{user_id}"
        if not user_dir.exists():
            continue
            
        user_samples = []
        for img_path in user_dir.glob("*.jpg"):
            user_samples.append(str(img_path))
            if len(user_samples) >= max_samples_per_user:
                break
        
        samples.extend(user_samples)
        labels.extend([user_id - 1] * len(user_samples))  # ID_1 -> label 0, ID_2 -> label 1, etc.
        print(f"ID_{user_id}: {len(user_samples)} samples")
    
    print(f"ğŸ“Š æ€»è®¡åŠ è½½: {len(samples)} æ ·æœ¬")
    return samples, labels

def calculate_metrics_batch(classifier, images, labels, device):
    """æ‰¹é‡è®¡ç®—æ‰€æœ‰ç­›é€‰æŒ‡æ ‡"""
    classifier.eval()
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    batch_metrics = {
        'confidence': [],
        'margin': [],
        'user_specificity': [],
        'predicted_user': [],
        'true_user': [],
        'features': []
    }
    
    with torch.no_grad():
        for img_path, true_label in tqdm(zip(images, labels), total=len(images), desc="è®¡ç®—æŒ‡æ ‡"):
            try:
                # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
                img = Image.open(img_path).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device)
                
                # å‰å‘ä¼ æ’­
                logits = classifier(img_tensor)
                probs = torch.softmax(logits, dim=1)
                features = classifier.extract_features(img_tensor)
                
                # 1. ç½®ä¿¡åº¦ (æœ€å¤§æ¦‚ç‡)
                confidence = torch.max(probs).item()
                
                # 2. å†³ç­–è¾¹ç•Œ (æœ€å¤§æ¦‚ç‡ - æ¬¡å¤§æ¦‚ç‡)
                sorted_probs = torch.sort(probs, descending=True)[0]
                margin = (sorted_probs[0, 0] - sorted_probs[0, 1]).item()
                
                # 3. ç”¨æˆ·ç‰¹å¼‚æ€§ (é¢„æµ‹ç”¨æˆ·æ¦‚ç‡ - å…¶ä»–ç”¨æˆ·æœ€å¤§æ¦‚ç‡)
                predicted_user = torch.argmax(probs).item()
                user_prob = probs[0, predicted_user].item()
                other_max = torch.max(torch.cat([probs[0, :predicted_user], 
                                               probs[0, predicted_user+1:]])).item()
                user_specificity = user_prob - other_max
                
                # å­˜å‚¨ç»“æœ
                batch_metrics['confidence'].append(confidence)
                batch_metrics['margin'].append(margin)
                batch_metrics['user_specificity'].append(user_specificity)
                batch_metrics['predicted_user'].append(predicted_user)
                batch_metrics['true_user'].append(true_label)
                batch_metrics['features'].append(features.cpu().numpy().flatten())
                
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å›¾åƒ {img_path} æ—¶å‡ºé”™: {e}")
                continue
    
    return batch_metrics

def analyze_diversity(features):
    """åˆ†æç‰¹å¾å¤šæ ·æ€§"""
    if len(features) < 2:
        return []
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = cosine_similarity(features)
    
    # æå–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆé¿å…é‡å¤å’Œå¯¹è§’çº¿ï¼‰
    triu_indices = np.triu_indices_from(similarity_matrix, k=1)
    similarities = similarity_matrix[triu_indices]
    
    return similarities

def analyze_user_differences(metrics):
    """åˆ†æç”¨æˆ·é—´å·®å¼‚"""
    df = pd.DataFrame(metrics)
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„ç»Ÿè®¡
    user_stats = df.groupby('true_user').agg({
        'confidence': ['mean', 'std', 'min', 'max'],
        'margin': ['mean', 'std', 'min', 'max'],
        'user_specificity': ['mean', 'std', 'min', 'max']
    }).round(4)
    
    return user_stats

def plot_metric_distributions(metrics, output_dir):
    """ç»˜åˆ¶æŒ‡æ ‡åˆ†å¸ƒå›¾"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 1. ç½®ä¿¡åº¦åˆ†å¸ƒ
    axes[0, 0].hist(metrics['confidence'], bins=50, alpha=0.7, edgecolor='black')
    axes[0, 0].axvline(np.mean(metrics['confidence']), color='red', linestyle='--', 
                       label=f'å‡å€¼: {np.mean(metrics["confidence"]):.3f}')
    axes[0, 0].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
    axes[0, 0].set_xlabel('ç½®ä¿¡åº¦')
    axes[0, 0].legend()
    
    # 2. å†³ç­–è¾¹ç•Œåˆ†å¸ƒ
    axes[0, 1].hist(metrics['margin'], bins=50, alpha=0.7, edgecolor='black')
    axes[0, 1].axvline(np.mean(metrics['margin']), color='red', linestyle='--',
                       label=f'å‡å€¼: {np.mean(metrics["margin"]):.3f}')
    axes[0, 1].set_title('å†³ç­–è¾¹ç•Œåˆ†å¸ƒ')
    axes[0, 1].set_xlabel('å†³ç­–è¾¹ç•Œ')
    axes[0, 1].legend()
    
    # 3. ç”¨æˆ·ç‰¹å¼‚æ€§åˆ†å¸ƒ
    axes[0, 2].hist(metrics['user_specificity'], bins=50, alpha=0.7, edgecolor='black')
    axes[0, 2].axvline(np.mean(metrics['user_specificity']), color='red', linestyle='--',
                       label=f'å‡å€¼: {np.mean(metrics["user_specificity"]):.3f}')
    axes[0, 2].set_title('ç”¨æˆ·ç‰¹å¼‚æ€§åˆ†å¸ƒ')
    axes[0, 2].set_xlabel('ç”¨æˆ·ç‰¹å¼‚æ€§')
    axes[0, 2].legend()
    
    # 4. å‡†ç¡®ç‡åˆ†æ
    accuracy = np.mean(np.array(metrics['predicted_user']) == np.array(metrics['true_user']))
    axes[1, 0].bar(['æ­£ç¡®', 'é”™è¯¯'], [accuracy, 1-accuracy])
    axes[1, 0].set_title(f'åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.3f}')
    axes[1, 0].set_ylabel('æ¯”ä¾‹')
    
    # 5. ç‰¹å¾å¤šæ ·æ€§
    if len(metrics['features']) > 1:
        similarities = analyze_diversity(metrics['features'])
        axes[1, 1].hist(similarities, bins=50, alpha=0.7, edgecolor='black')
        axes[1, 1].axvline(np.mean(similarities), color='red', linestyle='--',
                           label=f'å‡å€¼: {np.mean(similarities):.3f}')
        axes[1, 1].set_title('ç‰¹å¾ç›¸ä¼¼æ€§åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('ä½™å¼¦ç›¸ä¼¼åº¦')
        axes[1, 1].legend()
    
    # 6. ç½®ä¿¡åº¦vså‡†ç¡®æ€§
    correct_mask = np.array(metrics['predicted_user']) == np.array(metrics['true_user'])
    correct_conf = np.array(metrics['confidence'])[correct_mask]
    wrong_conf = np.array(metrics['confidence'])[~correct_mask]
    
    axes[1, 2].hist([correct_conf, wrong_conf], bins=30, alpha=0.7, 
                    label=['æ­£ç¡®é¢„æµ‹', 'é”™è¯¯é¢„æµ‹'], edgecolor='black')
    axes[1, 2].set_title('ç½®ä¿¡åº¦vsé¢„æµ‹å‡†ç¡®æ€§')
    axes[1, 2].set_xlabel('ç½®ä¿¡åº¦')
    axes[1, 2].legend()
    
    plt.tight_layout()
    plt.savefig(output_dir / 'real_data_metrics_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()

def recommend_thresholds(metrics):
    """åŸºäºçœŸå®æ•°æ®åˆ†å¸ƒæ¨èé˜ˆå€¼"""
    print("\n" + "="*60)
    print("ğŸ“Š åŸºäºçœŸå®æ•°æ®çš„é˜ˆå€¼æ¨è")
    print("="*60)
    
    # è®¡ç®—åˆ†ä½æ•°
    conf_percentiles = np.percentile(metrics['confidence'], [25, 50, 75, 90, 95])
    margin_percentiles = np.percentile(metrics['margin'], [25, 50, 75, 90, 95])
    spec_percentiles = np.percentile(metrics['user_specificity'], [25, 50, 75, 90, 95])
    
    if len(metrics['features']) > 1:
        similarities = analyze_diversity(metrics['features'])
        sim_percentiles = np.percentile(similarities, [5, 10, 25, 50, 75])
    
    print(f"ğŸ“ˆ ç½®ä¿¡åº¦åˆ†ä½æ•°: P25={conf_percentiles[0]:.3f}, P50={conf_percentiles[1]:.3f}, P75={conf_percentiles[2]:.3f}")
    print(f"ğŸ“ˆ å†³ç­–è¾¹ç•Œåˆ†ä½æ•°: P25={margin_percentiles[0]:.3f}, P50={margin_percentiles[1]:.3f}, P75={margin_percentiles[2]:.3f}")
    print(f"ğŸ“ˆ ç”¨æˆ·ç‰¹å¼‚æ€§åˆ†ä½æ•°: P25={spec_percentiles[0]:.3f}, P50={spec_percentiles[1]:.3f}, P75={spec_percentiles[2]:.3f}")
    
    if len(metrics['features']) > 1:
        print(f"ğŸ“ˆ ç›¸ä¼¼åº¦åˆ†ä½æ•°: P5={sim_percentiles[0]:.3f}, P25={sim_percentiles[2]:.3f}, P50={sim_percentiles[3]:.3f}")
    
    print("\nğŸ¯ æ¨èçš„ç­›é€‰é˜ˆå€¼ï¼ˆåŸºäºçœŸå®æ•°æ®ï¼‰:")
    
    # ä¿å®ˆç­–ç•¥ï¼šä¿ç•™75%çš„çœŸå®æ ·æœ¬è´¨é‡
    print("ã€ä¿å®ˆç­–ç•¥ - ä¿ç•™75%çœŸå®æ ·æœ¬è´¨é‡ã€‘")
    print(f"  - ç½®ä¿¡åº¦: {conf_percentiles[0]:.3f} (P25)")
    print(f"  - å†³ç­–è¾¹ç•Œ: {margin_percentiles[0]:.3f} (P25)")  
    print(f"  - ç”¨æˆ·ç‰¹å¼‚æ€§: {spec_percentiles[0]:.3f} (P25)")
    if len(metrics['features']) > 1:
        print(f"  - å¤šæ ·æ€§é˜ˆå€¼: {1-sim_percentiles[2]:.3f} (1-P25ç›¸ä¼¼åº¦)")
    
    # ä¸­ç­‰ç­–ç•¥ï¼šä¿ç•™50%çš„çœŸå®æ ·æœ¬è´¨é‡  
    print("\nã€ä¸­ç­‰ç­–ç•¥ - ä¿ç•™50%çœŸå®æ ·æœ¬è´¨é‡ã€‘")
    print(f"  - ç½®ä¿¡åº¦: {conf_percentiles[1]:.3f} (P50)")
    print(f"  - å†³ç­–è¾¹ç•Œ: {margin_percentiles[1]:.3f} (P50)")
    print(f"  - ç”¨æˆ·ç‰¹å¼‚æ€§: {spec_percentiles[1]:.3f} (P50)")
    if len(metrics['features']) > 1:
        print(f"  - å¤šæ ·æ€§é˜ˆå€¼: {1-sim_percentiles[3]:.3f} (1-P50ç›¸ä¼¼åº¦)")
    
    # ä¸¥æ ¼ç­–ç•¥ï¼šåªä¿ç•™25%æœ€å¥½çš„çœŸå®æ ·æœ¬è´¨é‡
    print("\nã€ä¸¥æ ¼ç­–ç•¥ - åªä¿ç•™25%æœ€é«˜è´¨é‡ã€‘")
    print(f"  - ç½®ä¿¡åº¦: {conf_percentiles[2]:.3f} (P75)")
    print(f"  - å†³ç­–è¾¹ç•Œ: {margin_percentiles[2]:.3f} (P75)")
    print(f"  - ç”¨æˆ·ç‰¹å¼‚æ€§: {spec_percentiles[2]:.3f} (P75)")
    if len(metrics['features']) > 1:
        print(f"  - å¤šæ ·æ€§é˜ˆå€¼: {1-sim_percentiles[1]:.3f} (1-P10ç›¸ä¼¼åº¦)")

def main():
    parser = argparse.ArgumentParser(description='åˆ†æçœŸå®å¾®å¤šæ™®å‹’æ•°æ®çš„ç­›é€‰æŒ‡æ ‡åˆ†å¸ƒ')
    parser.add_argument('--real_data_dir', type=str, required=True,
                       help='çœŸå®å¾®å¤šæ™®å‹’æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--classifier_checkpoint', type=str, required=True,
                       help='è®­ç»ƒå¥½çš„åˆ†ç±»å™¨checkpointè·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./real_data_analysis',
                       help='åˆ†æç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--max_samples_per_user', type=int, default=50,
                       help='æ¯ä¸ªç”¨æˆ·æœ€å¤§æ ·æœ¬æ•°ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰')
    
    args = parser.parse_args()
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # åŠ è½½åˆ†ç±»å™¨
    print("ğŸ”„ åŠ è½½åˆ†ç±»å™¨...")
    classifier = load_classifier(args.classifier_checkpoint, device)
    
    # åŠ è½½çœŸå®æ•°æ®
    print("ğŸ”„ åŠ è½½çœŸå®æ•°æ®...")
    images, labels = load_real_data(args.real_data_dir, args.max_samples_per_user)
    
    if len(images) == 0:
        print("âŒ æœªæ‰¾åˆ°çœŸå®æ•°æ®ï¼è¯·æ£€æŸ¥æ•°æ®ç›®å½•è·¯å¾„")
        return
    
    # è®¡ç®—æŒ‡æ ‡
    print("ğŸ”„ è®¡ç®—ç­›é€‰æŒ‡æ ‡...")
    metrics = calculate_metrics_batch(classifier, images, labels, device)
    
    # åˆ†æç”¨æˆ·é—´å·®å¼‚
    print("ğŸ”„ åˆ†æç”¨æˆ·é—´å·®å¼‚...")
    user_stats = analyze_user_differences(metrics)
    print("\nç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯:")
    print(user_stats)
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    print("ğŸ”„ ç”Ÿæˆåˆ†æå›¾è¡¨...")
    plot_metric_distributions(metrics, output_dir)
    
    # æ¨èé˜ˆå€¼
    recommend_thresholds(metrics)
    
    # ä¿å­˜åˆ†æç»“æœ
    results = {
        'total_samples': len(images),
        'accuracy': np.mean(np.array(metrics['predicted_user']) == np.array(metrics['true_user'])),
        'mean_confidence': np.mean(metrics['confidence']),
        'mean_margin': np.mean(metrics['margin']),
        'mean_user_specificity': np.mean(metrics['user_specificity']),
        'user_statistics': user_stats.to_dict()
    }
    
    if len(metrics['features']) > 1:
        similarities = analyze_diversity(metrics['features'])
        results['mean_feature_similarity'] = np.mean(similarities)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    import json
    with open(output_dir / 'analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åˆ°: {output_dir}")
    print(f"ğŸ“Š åˆ†æäº† {len(images)} ä¸ªçœŸå®æ ·æœ¬ï¼Œåˆ†ç±»å‡†ç¡®ç‡: {results['accuracy']:.3f}")

if __name__ == '__main__':
    main()

"""
ç»Ÿä¸€çš„æ ‡å‡†å·®çˆ†ç‚¸é—®é¢˜ä¿®å¤éªŒè¯è„šæœ¬
éªŒè¯æ–°çš„æ ‡å‡†åŒ–/åæ ‡å‡†åŒ–æ–¹æ³•æ˜¯å¦æ­£ç¡®è§£å†³äº†latent stdçˆ†ç‚¸é—®é¢˜

ä¿®å¤å‰ï¼šç”Ÿæˆçš„latent stdçˆ†ç‚¸åˆ°100-200
ä¿®å¤åï¼šç”Ÿæˆçš„latent stdåº”è¯¥åœ¨æ­£ç¡®èŒƒå›´å†…ï¼ˆ~8.5ï¼‰
"""
import torch
import math


def verify_theoretical_fix():
    """ç†è®ºéªŒè¯ï¼šå¯¹æ¯”æ–°æ—§æ–¹æ³•çš„æ•°å­¦å·®å¼‚"""
    print("="*60)
    print("ğŸ“Š ç†è®ºéªŒè¯ï¼šæ ‡å‡†å·®çˆ†ç‚¸é—®é¢˜ä¿®å¤")
    print("="*60)
    
    # å‚æ•°è®¾ç½®
    latent_mean = 0.059
    latent_std = 1.54
    
    # æ—§æ–¹æ³•ï¼ˆæœ‰é—®é¢˜ï¼‰
    old_scale_factor = 1.0 / latent_std  # â‰ˆ 0.649
    
    # æ–°æ–¹æ³•ï¼ˆä¿®å¤åï¼‰  
    new_scale_factor = 0.18215  # Stable Diffusionæ ‡å‡†
    
    print(f"\nğŸ” å‚æ•°å¯¹æ¯”:")
    print(f"   latent_mean: {latent_mean}")
    print(f"   latent_std: {latent_std}")
    print(f"   æ—§scale_factor: {old_scale_factor:.4f}")
    print(f"   æ–°scale_factor: {new_scale_factor:.4f}")
    
    # æ¨¡æ‹Ÿç”Ÿæˆåˆå§‹åŒ–ï¼šä»N(0,1)å¼€å§‹
    normalized_latents = torch.randn(2, 32, 16, 16)
    print(f"\nğŸ¯ åˆå§‹åŒ–latents (æ ‡å‡†åŒ–ç©ºé—´N(0,1)):")
    print(f"   mean: {normalized_latents.mean():.4f}")
    print(f"   std: {normalized_latents.std():.4f}")
    
    # æ—§æ–¹æ³•è§£ç ï¼ˆé—®é¢˜æ–¹æ³•ï¼‰
    print(f"\nâŒ æ—§æ–¹æ³•è§£ç ç»“æœ:")
    old_decoded = normalized_latents / old_scale_factor
    print(f"   å…¬å¼: latents / {old_scale_factor:.4f}")
    print(f"   ç»“æœ: mean={old_decoded.mean():.4f}, std={old_decoded.std():.2f}")
    print(f"   é—®é¢˜: stdæ”¾å¤§äº† {old_decoded.std():.1f}xï¼Œå¯¼è‡´çˆ†ç‚¸ï¼")
    
    # æ–°æ–¹æ³•è§£ç ï¼ˆä¿®å¤åï¼‰
    print(f"\nâœ… æ–°æ–¹æ³•è§£ç ç»“æœ:")
    new_decoded = (normalized_latents / new_scale_factor) * latent_std + latent_mean
    print(f"   å…¬å¼: (latents / {new_scale_factor}) * {latent_std} + {latent_mean}")
    print(f"   ç»“æœ: mean={new_decoded.mean():.4f}, std={new_decoded.std():.2f}")
    expected_std = latent_std  # åº”è¯¥æ¢å¤åˆ°åŸå§‹std
    print(f"   é¢„æœŸ: std â‰ˆ {expected_std:.2f} (æ¥è¿‘åŸå§‹VAEåˆ†å¸ƒ)")
    

def verify_roundtrip_consistency():
    """éªŒè¯ç¼–ç -è§£ç å¾€è¿”ä¸€è‡´æ€§"""
    print("\n" + "="*60)
    print("ğŸ”„ å¾€è¿”ä¸€è‡´æ€§éªŒè¯")
    print("="*60)
    
    # å‚æ•°
    latent_mean = 0.059
    latent_std = 1.54
    new_scale_factor = 0.18215
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®VAE latentåˆ†å¸ƒï¼‰
    original_latents = torch.randn(2, 32, 16, 16) * latent_std + latent_mean
    print(f"\nğŸ¯ åŸå§‹latents:")
    print(f"   mean: {original_latents.mean():.4f}")
    print(f"   std: {original_latents.std():.4f}")
    
    # ç¼–ç åˆ°æ ‡å‡†åŒ–ç©ºé—´
    encoded = ((original_latents - latent_mean) / latent_std) * new_scale_factor
    print(f"\nğŸ“ ç¼–ç åˆ°æ ‡å‡†åŒ–ç©ºé—´:")
    print(f"   å…¬å¼: ((latents - {latent_mean}) / {latent_std}) * {new_scale_factor}")
    print(f"   ç»“æœ: mean={encoded.mean():.4f}, std={encoded.std():.4f}")
    print(f"   é¢„æœŸ: æ¥è¿‘N(0, {new_scale_factor:.4f})")
    
    # è§£ç å›åŸå§‹ç©ºé—´
    decoded = (encoded / new_scale_factor) * latent_std + latent_mean
    print(f"\nğŸ”„ è§£ç å›åŸå§‹ç©ºé—´:")
    print(f"   å…¬å¼: (latents / {new_scale_factor}) * {latent_std} + {latent_mean}")
    print(f"   ç»“æœ: mean={decoded.mean():.4f}, std={decoded.std():.4f}")
    
    # è®¡ç®—å¾€è¿”è¯¯å·®
    roundtrip_error = (decoded - original_latents).abs().max().item()
    print(f"\nğŸ“Š å¾€è¿”è¯¯å·®: {roundtrip_error:.8f}")
    
    if roundtrip_error < 1e-6:
        print("   âœ… å¾€è¿”ä¸€è‡´æ€§éªŒè¯é€šè¿‡!")
    else:
        print("   âŒ å¾€è¿”ä¸€è‡´æ€§éªŒè¯å¤±è´¥!")


def analyze_generation_process():
    """åˆ†æç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ ‡å‡†å·®å˜åŒ–"""
    print("\n" + "="*60)
    print("ğŸ¨ ç”Ÿæˆè¿‡ç¨‹æ ‡å‡†å·®åˆ†æ")
    print("="*60)
    
    latent_mean = 0.059
    latent_std = 1.54
    new_scale_factor = 0.18215
    
    print(f"\nğŸš€ æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹:")
    print(f"   1. ä»æ ‡å‡†åŒ–ç©ºé—´N(0,1)åˆå§‹åŒ–")
    print(f"   2. ç»è¿‡å»å™ªè¿‡ç¨‹ï¼ˆscheduler.stepï¼‰")  
    print(f"   3. è§£ç åˆ°åŸå§‹VAEç©ºé—´")
    
    # æ­¥éª¤1ï¼šæ ‡å‡†åŒ–ç©ºé—´åˆå§‹åŒ–ï¼ˆä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„åˆå§‹åˆ†å¸ƒï¼‰
    initial_latents = torch.randn(2, 32, 16, 16) * new_scale_factor
    print(f"\nğŸ“ æ­¥éª¤1 - æ ‡å‡†åŒ–ç©ºé—´åˆå§‹åŒ– (N(0, {new_scale_factor:.4f})):")
    print(f"   mean: {initial_latents.mean():.4f}")
    print(f"   std: {initial_latents.std():.4f}")
    
    # æ­¥éª¤2ï¼šæ¨¡æ‹Ÿå»å™ªåï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå‡è®¾ä»ç„¶æ˜¯æ ‡å‡†åˆ†å¸ƒï¼‰
    denoised_latents = initial_latents * 0.8  # æ¨¡æ‹Ÿå»å™ªåçš„ç¼©æ”¾
    print(f"\nğŸ”„ æ­¥éª¤2 - å»å™ªå (æ¨¡æ‹Ÿ):")
    print(f"   mean: {denoised_latents.mean():.4f}")
    print(f"   std: {denoised_latents.std():.4f}")
    
    # æ­¥éª¤3ï¼šè§£ç åˆ°åŸå§‹ç©ºé—´ï¼ˆå…³é”®æ­¥éª¤ï¼‰
    final_latents = (denoised_latents / new_scale_factor) * latent_std + latent_mean
    print(f"\nğŸ¯ æ­¥éª¤3 - è§£ç åˆ°åŸå§‹VAEç©ºé—´:")
    print(f"   mean: {final_latents.mean():.4f}")
    print(f"   std: {final_latents.std():.4f}")
    
    # é‡æ–°åˆ†æï¼šå¦‚æœæ‰©æ•£è¾“å‡ºstdâ‰ˆlatent_multiplierï¼Œé‚£ä¹ˆæœ€ç»ˆåº”è¯¥å¾—åˆ°stdâ‰ˆlatent_std
    expected_final_std = latent_std * 0.8  # è€ƒè™‘å»å™ªç¼©æ”¾ï¼Œåº”è¯¥æ¥è¿‘1.54
    print(f"   é¢„æœŸstd: ~{expected_final_std:.2f} (åº”è¯¥æ¥è¿‘åŸå§‹VAEåˆ†å¸ƒçš„std={latent_std})")
    
    if 1.0 < final_latents.std() < 3.0:  # åˆç†èŒƒå›´ï¼šæ¥è¿‘1.54
        print(f"   âœ… ä¿®å¤æˆåŠŸï¼std={final_latents.std():.2f} æ¥è¿‘é¢„æœŸ {latent_std}")
    else:
        print(f"   âŒ ä»æœ‰é—®é¢˜ï¼std={final_latents.std():.2f} åº”è¯¥æ¥è¿‘ {latent_std}")


def print_fix_summary():
    """æ‰“å°ä¿®å¤æ€»ç»“"""
    print("\n" + "="*80)
    print("ğŸ“‹ æ ‡å‡†å·®çˆ†ç‚¸é—®é¢˜ä¿®å¤æ€»ç»“")
    print("="*80)
    
    print("\nğŸ” é—®é¢˜æ ¹æº:")
    print("   æ—§æ–¹æ³•ä½¿ç”¨ scale_factor = 1/std â‰ˆ 0.649")
    print("   ä»N(0,1)è§£ç æ—¶ï¼šstd â‰ˆ 1/0.649 â‰ˆ 1.54")
    print("   ä½†å®é™…ç”Ÿæˆæ—¶ä¼šè¿›ä¸€æ­¥æ”¾å¤§ï¼Œå¯¼è‡´stdçˆ†ç‚¸åˆ°100-200")
    
    print("\nğŸ’¡ ä¿®å¤æ–¹æ¡ˆ:")
    print("   1. ä½¿ç”¨æ ‡å‡†çš„ latent_multiplier = 0.18215 (Stable Diffusion)")
    print("   2. ä¿®æ”¹ç¼–ç å…¬å¼ï¼š(latents - mean) * scale_factor / std")
    print("   3. ä¿®æ”¹è§£ç å…¬å¼ï¼š(latents * std) / scale_factor + mean")
    
    print("\nğŸ“ ä»£ç ä¿®æ”¹:")
    print("   âœ… enhanced_conditional_diffusion.py: æ›´æ–°æ„é€ å‡½æ•°å’Œç¼–è§£ç æ–¹æ³•")
    print("   âœ… train_enhanced_conditional.py: ä¼ å…¥latent_multiplierå‚æ•°")
    
    print("\nğŸ¯ é¢„æœŸæ•ˆæœ:")
    print("   ä¿®å¤å‰ï¼šç”Ÿæˆlatent std ~ 100-200 (çˆ†ç‚¸)")
    print("   ä¿®å¤åï¼šç”Ÿæˆlatent std ~ 1.54 (æ­£ç¡®ï¼Œæ¥è¿‘åŸå§‹VAEåˆ†å¸ƒ)")
    
    print("\nğŸ”— å‚è€ƒä¾æ®:")
    print("   LightningDiTå®ç°ï¼šsamples = (samples * std) / multiplier + mean")
    print("   Stable Diffusionæ ‡å‡†ï¼šlatent_multiplier = 0.18215")


if __name__ == "__main__":
    # æ‰§è¡Œæ‰€æœ‰éªŒè¯
    verify_theoretical_fix()
    verify_roundtrip_consistency() 
    analyze_generation_process()
    print_fix_summary()
    
    print("\n" + "="*80)
    print("\nğŸ‰ éªŒè¯å®Œæˆï¼ä¿®å¤åº”è¯¥è§£å†³æ ‡å‡†å·®çˆ†ç‚¸é—®é¢˜ã€‚")
    print("   å…³é”®ï¼šæœ€ç»ˆç”Ÿæˆçš„latent stdåº”è¯¥æ¥è¿‘1.54ï¼Œä¸æ˜¯8.5ï¼")
    print("   ç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒè„šæœ¬éªŒè¯å®é™…æ•ˆæœã€‚")
    print("="*80)

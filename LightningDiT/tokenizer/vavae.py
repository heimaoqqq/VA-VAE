"""
Vision Foundation Model Aligned VAE.
It has exactly the same architecture as the LDM VAE (or VQGAN-KL).
Here we first provide its inference implementation with diffusers. 
The training code will be provided later. 

"LightningDiT + VA_VAE" achieves state-of-the-art Latent Diffusion System
with 0.27 rFID and 1.35 FID on ImageNet 256x256.

by Maple (Jingfeng Yao) from HUST-VL
"""

import torch
import numpy as np
from PIL import Image
from omegaconf import OmegaConf
from torchvision import transforms
from tokenizer.autoencoder import AutoencoderKL

class VA_VAE:
    """Vision Foundation Model Aligned VAE Implementation"""
    
    def __init__(self, config, img_size=256, horizon_flip=0.5, fp16=True):
        """Initialize VA_VAE
        Args:
            config: Configuration dict containing img_size, horizon_flip and fp16 parameters
        """
        self.config = OmegaConf.load(config)
        self.embed_dim = self.config.model.params.embed_dim
        self.ckpt_path = self.config.ckpt_path
        self.img_size = img_size
        self.horizon_flip = horizon_flip
        self.load()

    def load(self):
        """Load and initialize VAE model"""
        self.model = AutoencoderKL(
            embed_dim=self.embed_dim,
            ch_mult=(1, 1, 2, 2, 4),
            ckpt_path=self.ckpt_path
        ).cuda().eval()
        return self
    
    def img_transform(self, p_hflip=0, img_size=None):
        """Image preprocessing transforms
        Args:
            p_hflip: Probability of horizontal flip
            img_size: Target image size, use default if None
        Returns:
            transforms.Compose: Image transform pipeline
        """
        img_size = img_size if img_size is not None else self.img_size
        img_transforms = [
            transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, img_size)),
            transforms.RandomHorizontalFlip(p=p_hflip),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)
        ]
        return transforms.Compose(img_transforms)

    def encode_images(self, images):
        """Encode images to latent representations
        Args:
            images: Input image tensor
        Returns:
            torch.Tensor: Encoded latent representation
        """
        with torch.no_grad():
            posterior = self.model.encode(images.cuda())
            return posterior.sample()

    def decode_to_images(self, z):
        """Decode latent representations to images with proper micro-Doppler colormap
        Args:
            z: Latent representation tensor
        Returns:
            np.ndarray: Decoded image array with proper colormap
        """
        with torch.no_grad():
            # è§£ç åˆ°[-1, 1]èŒƒå›´
            images = self.model.decode(z.cuda())

            # è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” VA-VAEè§£ç è°ƒè¯•:")
            print(f"  è§£ç å‰æ½œåœ¨ç‰¹å¾èŒƒå›´: [{z.min():.3f}, {z.max():.3f}]")
            print(f"  è§£ç åå›¾åƒèŒƒå›´: [{images.min():.3f}, {images.max():.3f}]")

            # å¼ºåˆ¶å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´ (å¤„ç†è¶…å‡ºèŒƒå›´çš„æƒ…å†µ)
            images = 2.0 * (images - images.min()) / (images.max() - images.min()) - 1.0
            images = torch.clamp(images, -1.0, 1.0)
            print(f"  å½’ä¸€åŒ–åå›¾åƒèŒƒå›´: [{images.min():.3f}, {images.max():.3f}]")

            # å¯¹äºå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ï¼Œåº”ç”¨æ­£ç¡®çš„é¢œè‰²æ˜ å°„
            processed_images = []
            for i in range(images.shape[0]):
                img = images[i]  # (C, H, W)

                # å¦‚æœæ˜¯RGBå›¾åƒï¼Œè½¬æ¢ä¸ºç°åº¦ä½œä¸ºå¼ºåº¦
                if img.shape[0] == 3:
                    # ä½¿ç”¨æ ‡å‡†RGBåˆ°ç°åº¦çš„æƒé‡
                    intensity = 0.299 * img[0] + 0.587 * img[1] + 0.114 * img[2]
                else:
                    intensity = img[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé€šé“

                # å½’ä¸€åŒ–åˆ°[0, 1]
                intensity = (intensity + 1.0) / 2.0
                intensity = torch.clamp(intensity, 0, 1)
                print(f"  å¼ºåº¦å›¾èŒƒå›´: [{intensity.min():.3f}, {intensity.max():.3f}]")

                # åº”ç”¨å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾çš„é¢œè‰²æ˜ å°„ (ç±»ä¼¼matplotlibçš„jet colormap)
                colored_img = self._apply_micro_doppler_colormap(intensity)
                print(f"  é¢œè‰²æ˜ å°„åèŒƒå›´: [{colored_img.min():.3f}, {colored_img.max():.3f}]")
                processed_images.append(colored_img)

            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            result = torch.stack(processed_images).permute(0, 2, 3, 1).to("cpu", dtype=torch.uint8).numpy()

            print(f"  æœ€ç»ˆå›¾åƒèŒƒå›´: [{result.min()}, {result.max()}]")
            print(f"  æœ€ç»ˆå›¾åƒå½¢çŠ¶: {result.shape}")

        return result

    def _apply_micro_doppler_colormap(self, intensity):
        """Apply micro-Doppler specific colormap (blue to red)
        Args:
            intensity: Normalized intensity tensor (H, W) in [0, 1]
        Returns:
            torch.Tensor: RGB image (3, H, W) in [0, 255]
        """
        # åˆ›å»ºç±»ä¼¼jet colormapçš„å¾®å¤šæ™®å‹’é¢œè‰²æ˜ å°„
        # ä½å¼ºåº¦ -> æ·±è“è‰²ï¼Œé«˜å¼ºåº¦ -> çº¢è‰²

        # å®šä¹‰é¢œè‰²æ˜ å°„çš„å…³é”®ç‚¹ (å¼ºåº¦ -> RGB)
        # 0.0: æ·±è“ (0, 0, 128)
        # 0.25: è“è‰² (0, 0, 255)
        # 0.5: é’è‰² (0, 255, 255)
        # 0.75: é»„è‰² (255, 255, 0)
        # 1.0: çº¢è‰² (255, 0, 0)

        h, w = intensity.shape
        rgb_img = torch.zeros(3, h, w, device=intensity.device)

        # Red channel
        rgb_img[0] = torch.where(intensity < 0.5,
                                0,
                                torch.where(intensity < 0.75,
                                           (intensity - 0.5) * 4 * 255,
                                           255))

        # Green channel
        rgb_img[1] = torch.where(intensity < 0.25,
                                0,
                                torch.where(intensity < 0.5,
                                           (intensity - 0.25) * 4 * 255,
                                           torch.where(intensity < 0.75,
                                                      255,
                                                      255 - (intensity - 0.75) * 4 * 255)))

        # Blue channel
        rgb_img[2] = torch.where(intensity < 0.25,
                                128 + intensity * 4 * 127,  # æ·±è“åˆ°è“
                                torch.where(intensity < 0.5,
                                           255 - (intensity - 0.25) * 4 * 255,
                                           0))

        return torch.clamp(rgb_img, 0, 255)

def center_crop_arr(pil_image, image_size):
    """
    Center cropping implementation from ADM.
    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126
    """
    while min(*pil_image.size) >= 2 * image_size:
        pil_image = pil_image.resize(
            tuple(x // 2 for x in pil_image.size), resample=Image.BOX
        )

    scale = image_size / min(*pil_image.size)
    pil_image = pil_image.resize(
        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC
    )

    arr = np.array(pil_image)
    crop_y = (arr.shape[0] - image_size) // 2
    crop_x = (arr.shape[1] - image_size) // 2
    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])


if __name__ == "__main__":
    vae = VA_VAE('tokenizer/configs/vavae_f16d32_vfdinov2.yaml')
    vae.load()
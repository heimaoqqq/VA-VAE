#!/usr/bin/env python3
"""
å‡†å¤‡31ç”¨æˆ·å¾®å¤šæ™®å‹’æ•°æ®é›†åˆ’åˆ†
ä»/kaggle/input/datasetè¯»å–31ä¸ªç”¨æˆ·æ–‡ä»¶å¤¹ï¼ŒæŒ‰8:2åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
"""

import json
import random
from pathlib import Path
from collections import defaultdict
import argparse

def create_dataset_split(dataset_root, output_file, train_ratio=0.8, seed=42):
    """
    åˆ›å»ºæ•°æ®é›†åˆ’åˆ†
    
    Args:
        dataset_root: æ•°æ®é›†æ ¹ç›®å½• (/kaggle/input/dataset)
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        seed: éšæœºç§å­
    """
    
    dataset_root = Path(dataset_root)
    if not dataset_root.exists():
        raise FileNotFoundError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_root}")
    
    print(f"ğŸ” æ‰«ææ•°æ®é›†: {dataset_root}")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    
    # æ‰«æç”¨æˆ·æ–‡ä»¶å¤¹
    user_folders = [d for d in dataset_root.iterdir() if d.is_dir()]
    user_folders.sort()  # ç¡®ä¿é¡ºåºä¸€è‡´
    
    print(f"ğŸ“Š å‘ç° {len(user_folders)} ä¸ªç”¨æˆ·æ–‡ä»¶å¤¹")
    
    if len(user_folders) != 31:
        print(f"âš ï¸ é¢„æœŸ31ä¸ªç”¨æˆ·ï¼Œå®é™…å‘ç°{len(user_folders)}ä¸ª")
    
    # ç»Ÿè®¡å’Œåˆ’åˆ†æ•°æ®
    train_data = {}
    val_data = {}
    total_images = 0
    
    for user_folder in user_folders:
        user_id = user_folder.name
        print(f"ğŸ”„ å¤„ç†ç”¨æˆ·: {user_id}")
        
        # è·å–æ‰€æœ‰jpgå›¾åƒ
        image_files = list(user_folder.glob("*.jpg")) + list(user_folder.glob("*.JPG"))
        image_paths = [str(img_file) for img_file in image_files]
        
        if not image_paths:
            print(f"âš ï¸ ç”¨æˆ· {user_id} æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            continue
        
        print(f"   æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒ")
        total_images += len(image_paths)
        
        # éšæœºæ‰“ä¹±
        random.shuffle(image_paths)
        
        # æŒ‰æ¯”ä¾‹åˆ’åˆ†
        n_train = int(len(image_paths) * train_ratio)
        
        train_data[user_id] = image_paths[:n_train]
        val_data[user_id] = image_paths[n_train:]
        
        print(f"   è®­ç»ƒé›†: {len(train_data[user_id])} å¼ ")
        print(f"   éªŒè¯é›†: {len(val_data[user_id])} å¼ ")
    
    # åˆ›å»ºå®Œæ•´çš„æ•°æ®åˆ’åˆ†
    data_split = {
        'train': train_data,
        'val': val_data,
        'metadata': {
            'total_users': len(user_folders),
            'total_images': total_images,
            'train_ratio': train_ratio,
            'seed': seed,
            'dataset_root': str(dataset_root)
        }
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = Path(output_file)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w') as f:
        json.dump(data_split, f, indent=2)
    
    # ç»Ÿè®¡ä¿¡æ¯
    train_total = sum(len(images) for images in train_data.values())
    val_total = sum(len(images) for images in val_data.values())
    
    print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
    print(f"   ç”¨æˆ·æ•°: {len(user_folders)}")
    print(f"   æ€»å›¾åƒ: {total_images}")
    print(f"   è®­ç»ƒé›†: {train_total} å¼  ({train_total/total_images*100:.1f}%)")
    print(f"   éªŒè¯é›†: {val_total} å¼  ({val_total/total_images*100:.1f}%)")
    print(f"   å¹³å‡æ¯ç”¨æˆ·è®­ç»ƒæ ·æœ¬: {train_total/len(user_folders):.1f}")
    print(f"   å¹³å‡æ¯ç”¨æˆ·éªŒè¯æ ·æœ¬: {val_total/len(user_folders):.1f}")
    print(f"   ä¿å­˜è·¯å¾„: {output_file}")
    
    return output_file

def validate_split(split_file):
    """éªŒè¯æ•°æ®åˆ’åˆ†æ–‡ä»¶"""
    print(f"\nğŸ” éªŒè¯æ•°æ®åˆ’åˆ†: {split_file}")
    
    with open(split_file, 'r') as f:
        data_split = json.load(f)
    
    train_data = data_split['train']
    val_data = data_split['val']
    metadata = data_split['metadata']
    
    print(f"ğŸ“Š å…ƒæ•°æ®:")
    for key, value in metadata.items():
        print(f"   {key}: {value}")
    
    # æ£€æŸ¥æ¯ä¸ªç”¨æˆ·
    print(f"\nğŸ‘¥ ç”¨æˆ·è¯¦æƒ…:")
    for user_id in sorted(train_data.keys()):
        train_count = len(train_data[user_id])
        val_count = len(val_data.get(user_id, []))
        total_count = train_count + val_count
        
        print(f"   {user_id}: {total_count} æ€»è®¡ ({train_count} è®­ç»ƒ, {val_count} éªŒè¯)")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        missing_files = []
        for img_path in train_data[user_id][:3]:  # åªæ£€æŸ¥å‰3ä¸ªæ–‡ä»¶
            if not Path(img_path).exists():
                missing_files.append(img_path)
        
        if missing_files:
            print(f"     âš ï¸ å‘ç°ç¼ºå¤±æ–‡ä»¶: {missing_files}")
    
    print(f"\nâœ… éªŒè¯å®Œæˆ")

def main():
    parser = argparse.ArgumentParser(description='åˆ›å»º31ç”¨æˆ·å¾®å¤šæ™®å‹’æ•°æ®é›†åˆ’åˆ†')
    parser.add_argument('--dataset_root', type=str, default='/kaggle/input/dataset',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--output_file', type=str, default='/kaggle/working/dataset_split.json',
                       help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--validate', action='store_true',
                       help='éªŒè¯ç”Ÿæˆçš„åˆ’åˆ†æ–‡ä»¶')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæ•°æ®åˆ’åˆ†
        output_file = create_dataset_split(
            dataset_root=args.dataset_root,
            output_file=args.output_file,
            train_ratio=args.train_ratio,
            seed=args.seed
        )
        
        # éªŒè¯åˆ’åˆ†
        if args.validate:
            validate_split(output_file)
        
        print(f"\nğŸ‰ æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
        print(f"ğŸ“ ä¸‹ä¸€æ­¥: ä½¿ç”¨ {output_file} å¼€å§‹è®­ç»ƒ")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())

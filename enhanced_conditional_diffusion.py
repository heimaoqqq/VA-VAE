#!/usr/bin/env python3
"""
å¢å¼ºæ¡ä»¶æ‰©æ•£ç”Ÿæˆï¼šä¸“é—¨å¤„ç†å°æ•°æ®é›†å¾®å·®å¼‚æ¡ä»¶ç”Ÿæˆ
ç»“åˆPrototypical Networksã€Contrastive Learningã€Few-Shot Meta-Learning
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers import UNet2DConditionModel, DDPMScheduler, DDIMScheduler
import numpy as np
from sklearn.cluster import KMeans
import random
from tqdm import tqdm

class PrototypicalEncoder(nn.Module):
    """åŸå‹ç¼–ç å™¨ï¼šä»å°‘é‡æ ·æœ¬å­¦ä¹ ç”¨æˆ·åŸå‹"""
    
    def __init__(self, input_dim=8192, prototype_dim=768):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 512),
            nn.ReLU(), 
            nn.Dropout(0.1),
            nn.Linear(512, prototype_dim)
        )
        
    def forward(self, latents):
        """
        Args:
            latents: [N, 32, 16, 16] - ç”¨æˆ·çš„latentæ ·æœ¬
        Returns:
            prototypes: [N, prototype_dim] - åŸå‹ç‰¹å¾
        """
        # å±•å¹³latent
        flat_latents = latents.view(latents.size(0), -1)
        prototypes = self.encoder(flat_latents)
        return F.normalize(prototypes, dim=1)  # L2å½’ä¸€åŒ–

class ContrastiveLearning(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼šå¢å¼ºç”¨æˆ·é—´åˆ¤åˆ«èƒ½åŠ›"""
    
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
        
    def forward(self, features, user_ids):
        """
        Args:
            features: [B, D] ç”¨æˆ·åŸå‹ç‰¹å¾
            user_ids: [B] ç”¨æˆ·ID
        Returns:
            contrastive_loss: å¯¹æ¯”æŸå¤±
        """
        # å½’ä¸€åŒ–ç‰¹å¾
        features = F.normalize(features, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(features, features.T) / self.temperature
        
        # åˆ›å»ºæ ‡ç­¾çŸ©é˜µ
        user_ids = user_ids.view(-1, 1)
        labels = (user_ids == user_ids.T).float()
        
        # åˆ›å»ºmaskï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
        batch_size = features.size(0)
        mask = torch.eye(batch_size, device=features.device).bool()
        labels = labels.masked_fill(mask, 0)
        
        # è®¡ç®—InfoNCEæŸå¤±
        exp_sim = torch.exp(similarity_matrix)
        pos_sim = (exp_sim * labels).sum(dim=1)
        all_sim = exp_sim.sum(dim=1) - torch.diag(exp_sim)  # æ’é™¤è‡ªå·±
        
        # é¿å…é™¤é›¶
        pos_sim = torch.clamp(pos_sim, min=1e-8)
        all_sim = torch.clamp(all_sim, min=1e-8)
        
        loss = -torch.log(pos_sim / all_sim).mean()
        
        return loss


class EnhancedConditionalDiffusion(nn.Module):
    """
    å¢å¼ºæ¡ä»¶æ‰©æ•£æ¨¡å‹ - æ­£ç¡®å®ç°ç‰ˆæœ¬
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ä½¿ç”¨ç¼©æ”¾å› å­æ ‡å‡†åŒ–latentåˆ°N(0,1)ç©ºé—´ï¼ˆç±»ä¼¼Stable Diffusionï¼‰
    2. è®­ç»ƒå’Œç”Ÿæˆéƒ½åœ¨æ ‡å‡†åŒ–ç©ºé—´è¿›è¡Œ
    3. æ— ç¡¬é™åˆ¶æˆ–clampï¼Œè®©æ¨¡å‹è‡ªç„¶å­¦ä¹ åˆ†å¸ƒ
    4. ç”Ÿæˆååæ ‡å‡†åŒ–å›åŸå§‹VAEç©ºé—´
    """
    
    def __init__(
        self,
        vae,
        num_timesteps=1000,
        noise_schedule="linear", 
        num_users=31,
        prototype_dim=768,
        latent_mean=0.059,
        latent_std=1.54,
    ):
        super().__init__()
        
        # VAEç»„ä»¶
        self.vae = vae
        
        # æ ¸å¿ƒï¼šç¼©æ”¾å› å­ï¼ˆå°†VAE latentæ ‡å‡†åŒ–åˆ°æ¥è¿‘N(0,1)ï¼‰
        # ç±»ä¼¼Stable Diffusionçš„0.18215ï¼Œæˆ‘ä»¬ä½¿ç”¨1/std
        self.scale_factor = 1.0 / latent_std  # â‰ˆ 0.649
        print(f"ä½¿ç”¨ç¼©æ”¾å› å­: {self.scale_factor:.4f}")
        
        # è®°å½•åŸå§‹åˆ†å¸ƒå‚æ•°
        self.register_buffer('latent_mean', torch.tensor(latent_mean))
        self.register_buffer('latent_std', torch.tensor(latent_std))
        
        # åŸå‹å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ ç»„ä»¶
        self.prototype_encoder = PrototypicalEncoder(
            input_dim=32*16*16,  # VA-VAE latentç»´åº¦
            prototype_dim=prototype_dim
        )
        self.contrastive_learning = ContrastiveLearning(temperature=0.07)
        
        # UNetæ‰©æ•£æ¨¡å‹ - åœ¨æ ‡å‡†åŒ–ç©ºé—´å·¥ä½œ
        self.unet = UNet2DConditionModel(
            in_channels=32,  # VA-VAEæœ‰32ä¸ªé€šé“
            out_channels=32,
            down_block_types=("DownBlock2D", "DownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D"),
            up_block_types=("CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "UpBlock2D", "UpBlock2D"),
            block_out_channels=(128, 256, 384, 512),
            layers_per_block=2,
            attention_head_dim=8,
            cross_attention_dim=prototype_dim,
            sample_size=16
        )
        
        self.num_timesteps = num_timesteps
        
        # å™ªå£°è°ƒåº¦å™¨ - æ ‡å‡†é…ç½®ï¼Œæ— å‰ªè£
        self.scheduler = DDPMScheduler(
            num_train_timesteps=num_timesteps,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule=noise_schedule,
            clip_sample=False,  # å…³é”®ï¼šä¸å‰ªè£
            prediction_type="epsilon",
        )
        
        self.ddim_scheduler = DDIMScheduler(
            num_train_timesteps=num_timesteps,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule=noise_schedule,
            clip_sample=False,
            prediction_type="epsilon",
        )
        
        # ç”¨æˆ·åŸå‹ç¼“å­˜
        self.user_prototypes = {}
    
    def encode_to_standard_space(self, latents):
        """ç¼–ç åˆ°æ ‡å‡†åŒ–ç©ºé—´ï¼ˆç±»ä¼¼Stable Diffusionï¼‰"""
        return latents * self.scale_factor
    
    def decode_to_original_space(self, latents):
        """è§£ç å›åŸå§‹VAEç©ºé—´"""
        return latents / self.scale_factor
    
    def update_user_prototypes(self, user_samples_dict):
        """æ›´æ–°ç”¨æˆ·åŸå‹ç¼“å­˜"""
        self.user_prototypes = {}
        
        for user_id, samples in user_samples_dict.items():
            # samples: [N, 32, 16, 16] - åœ¨åŸå§‹ç©ºé—´
            with torch.no_grad():
                prototypes = self.prototype_encoder(samples)
                user_prototype = torch.mean(prototypes, dim=0, keepdim=True)
                self.user_prototypes[user_id] = user_prototype
    
    def set_training_stats(self, mean, std):
        """æ›´æ–°è®­ç»ƒåˆ†å¸ƒç»Ÿè®¡é‡"""
        if mean is not None:
            self.latent_mean.copy_(torch.tensor(mean))
        if std is not None:
            self.latent_std.copy_(torch.tensor(std))
            self.scale_factor = 1.0 / std
        print(f"âœ… å·²æ›´æ–°åˆ†å¸ƒ: mean={self.latent_mean:.4f}, std={self.latent_std:.4f}")
        print(f"   ç¼©æ”¾å› å­: {self.scale_factor:.4f}")
    
    def get_user_condition(self, user_ids):
        """è·å–ç”¨æˆ·æ¡ä»¶ç¼–ç """
        batch_size = len(user_ids)
        device = next(self.parameters()).device
        
        # ç¡®ä¿è¿”å›æ­£ç¡®çš„3Då¼ é‡å½¢çŠ¶ [batch_size, sequence_length=1, hidden_dim=768]
        conditions = torch.zeros(batch_size, 1, 768, device=device)
        
        for i, user_id in enumerate(user_ids):
            user_id_int = user_id.item() if torch.is_tensor(user_id) else user_id
            if user_id_int in self.user_prototypes:
                # ç¡®ä¿åŸå‹æ˜¯æ­£ç¡®çš„å½¢çŠ¶ [1, 768]
                prototype = self.user_prototypes[user_id_int]
                if prototype.dim() == 1:
                    prototype = prototype.unsqueeze(0)  # [768] -> [1, 768]
                conditions[i] = prototype
            else:
                # éšæœºåˆå§‹åŒ–ä¸º [1, 768]
                conditions[i] = torch.randn(1, 768, device=device) * 0.1
        
        # ç¡®ä¿è¿”å›çš„å¼ é‡å½¢çŠ¶æ­£ç¡®
        if conditions.dim() != 3 or conditions.shape[-1] != 768:
            print(f"âš ï¸ è­¦å‘Šï¼šuser_conditionså½¢çŠ¶å¼‚å¸¸: {conditions.shape}")
            conditions = conditions.view(batch_size, 1, 768)
            
        return conditions
    
    def training_step(self, clean_latents, user_conditions):
        """
        è®­ç»ƒæ­¥éª¤ - æ­£ç¡®å®ç°ç‰ˆæœ¬
        åœ¨æ ‡å‡†åŒ–ç©ºé—´è¿›è¡Œè®­ç»ƒ
        """
        batch_size = clean_latents.shape[0]
        device = clean_latents.device
        
        # âœ… æ­¥éª¤1ï¼šç¼–ç åˆ°æ ‡å‡†åŒ–ç©ºé—´N(0,1)
        latents_standard = self.encode_to_standard_space(clean_latents)
        
        # éšæœºæ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.scheduler.num_train_timesteps,
            (batch_size,), device=device
        ).long()
        
        # âœ… æ­¥éª¤2ï¼šåœ¨æ ‡å‡†åŒ–ç©ºé—´æ·»åŠ å™ªå£°N(0,1)
        noise = torch.randn_like(latents_standard)
        noisy_latents = self.scheduler.add_noise(latents_standard, noise, timesteps)
        
        # âœ… æ­¥éª¤3ï¼šé¢„æµ‹å™ªå£°ï¼ˆåœ¨æ ‡å‡†åŒ–ç©ºé—´ï¼‰
        # ç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
        noisy_latents = noisy_latents.to(device)
        timesteps = timesteps.to(device)
        user_conditions = user_conditions.to(device)
        
        # è°ƒè¯•ï¼šæ‰“å°å¼ é‡å½¢çŠ¶
        print(f"ğŸ” Debug shapes - noisy_latents: {noisy_latents.shape}, user_conditions: {user_conditions.shape}")
        
        # ç¡®ä¿user_conditionsæ˜¯3Då¼ é‡ [batch_size, seq_len, hidden_dim]
        if user_conditions.dim() == 2:
            user_conditions = user_conditions.unsqueeze(1)  # [B, H] -> [B, 1, H]
        elif user_conditions.dim() == 1:
            user_conditions = user_conditions.unsqueeze(0).unsqueeze(0)  # [H] -> [1, 1, H]
        
        noise_pred = self.unet(
            noisy_latents,
            timesteps,
            encoder_hidden_states=user_conditions,
        ).sample
        
        # æ‰©æ•£æŸå¤±
        noise = noise.to(device)  # ç¡®ä¿å™ªå£°åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        diff_loss = F.mse_loss(noise_pred, noise)
        
        # å¯¹æ¯”æŸå¤±ï¼ˆéœ€è¦åœ¨åŸå§‹ç©ºé—´è®¡ç®—åŸå‹ï¼‰
        predicted_clean = self.scheduler.step(
            noise_pred, timesteps[0], noisy_latents
        ).pred_original_sample
        
        # åæ ‡å‡†åŒ–ä»¥è®¡ç®—åŸå‹
        predicted_clean_orig = self.decode_to_original_space(predicted_clean)
        pred_features = self.prototype_encoder(predicted_clean_orig)
        
        # ä¸ºå¯¹æ¯”å­¦ä¹ åˆ›å»ºç”¨æˆ·IDå¼ é‡
        batch_size = user_conditions.shape[0]
        user_ids_tensor = torch.arange(batch_size, device=device)  # ç®€åŒ–çš„ç”¨æˆ·ID
        contrastive_loss = self.contrastive_learning(pred_features, user_ids_tensor)
        
        # æ€»æŸå¤±
        total_loss = diff_loss + 0.1 * contrastive_loss
        
        return total_loss, diff_loss, contrastive_loss
    
    @torch.no_grad()
    def generate(
        self,
        user_ids,
        num_samples_per_user=4,
        num_inference_steps=50,
        guidance_scale=7.5,
        use_ddim=True,
    ):
        """
        ç”Ÿæˆæ ·æœ¬ - æ­£ç¡®å®ç°ç‰ˆæœ¬
        åœ¨æ ‡å‡†åŒ–ç©ºé—´ç”Ÿæˆï¼Œæœ€ååæ ‡å‡†åŒ–
        """
        self.eval()
        
        if isinstance(user_ids, int):
            user_ids = [user_ids]
        
        batch_size = len(user_ids) * num_samples_per_user
        device = next(self.parameters()).device
        
        # é€‰æ‹©è°ƒåº¦å™¨
        scheduler = self.ddim_scheduler if use_ddim else self.scheduler
        scheduler.set_timesteps(num_inference_steps, device=device)
        
        # âœ… æ­¥éª¤1ï¼šä»æ ‡å‡†é«˜æ–¯N(0,1)å¼€å§‹ï¼ˆåœ¨æ ‡å‡†åŒ–ç©ºé—´ï¼‰
        shape = (batch_size, 32, 16, 16)
        latents = torch.randn(shape, device=device)
        
        print(f"ğŸ“Š ç”Ÿæˆé…ç½®:")
        print(f"   æ ‡å‡†åŒ–ç©ºé—´N(0,1)åˆå§‹åŒ–")
        print(f"   åˆå§‹std: {latents.std():.4f}")
        
        # å‡†å¤‡æ¡ä»¶
        user_ids_expanded = []
        for uid in user_ids:
            user_ids_expanded.extend([uid] * num_samples_per_user)
        user_conditions = self.get_user_condition(user_ids_expanded)
        
        # CFGå‡†å¤‡
        if guidance_scale > 1.0:
            uncond_conditions = torch.zeros_like(user_conditions)
        
        # âœ… æ­¥éª¤2ï¼šå»å™ªå¾ªç¯ï¼ˆåœ¨æ ‡å‡†åŒ–ç©ºé—´ï¼Œæ— ä»»ä½•é™åˆ¶ï¼‰
        for t in tqdm(scheduler.timesteps, desc="ç”Ÿæˆä¸­"):
            # æ¡ä»¶é¢„æµ‹
            noise_pred_cond = self.unet(
                latents, t, encoder_hidden_states=user_conditions
            ).sample
            
            if guidance_scale > 1.0:
                # æ— æ¡ä»¶é¢„æµ‹
                noise_pred_uncond = self.unet(
                    latents, t, encoder_hidden_states=uncond_conditions
                ).sample
                
                # CFG
                noise_pred = noise_pred_uncond + guidance_scale * (
                    noise_pred_cond - noise_pred_uncond
                )
            else:
                noise_pred = noise_pred_cond
            
            # å»å™ªæ­¥éª¤ï¼ˆæ— clampæˆ–é™åˆ¶ï¼‰
            latents = scheduler.step(noise_pred, t, latents).prev_sample
        
        # âœ… æ­¥éª¤3ï¼šåæ ‡å‡†åŒ–åˆ°åŸå§‹VAEç©ºé—´
        latents_original = self.decode_to_original_space(latents)
        
        # éªŒè¯åˆ†å¸ƒ
        print(f"ğŸ“Š ç”Ÿæˆlatentåˆ†å¸ƒ:")
        print(f"   æ ‡å‡†åŒ–ç©ºé—´: mean={latents.mean():.4f}, std={latents.std():.4f}")
        print(f"   åŸå§‹ç©ºé—´: mean={latents_original.mean():.4f}, std={latents_original.std():.4f}")
        print(f"   âœ… æœŸæœ›: meanâ‰ˆ{self.latent_mean:.3f}, stdâ‰ˆ{self.latent_std:.3f}")
        
        return latents_original


# ä½¿ç”¨ç¤ºä¾‹
def create_enhanced_diffusion_system(vae_checkpoint, num_users=31):
    """åˆ›å»ºå¢å¼ºæ¡ä»¶æ‰©æ•£ç³»ç»Ÿ"""
    
    # åŠ è½½ç®€åŒ–VA-VAE
    from simplified_vavae import SimplifiedVAVAE
    vae = SimplifiedVAVAE(vae_checkpoint)
    
    # åˆ›å»ºå¢å¼ºæ‰©æ•£æ¨¡å‹
    diffusion_model = EnhancedConditionalDiffusion(
        vae=vae,
        num_users=num_users,
        prototype_dim=768
    )
    
    return diffusion_model, vae


if __name__ == "__main__":
    print("âœ… å¢å¼ºæ¡ä»¶æ‰©æ•£æ¨¡å‹ - æ­£ç¡®æ ‡å‡†åŒ–ç‰ˆæœ¬")
    print("ğŸ”‘ æ ¸å¿ƒæ”¹è¿›:")
    print("   - ä½¿ç”¨ç¼©æ”¾å› å­æ ‡å‡†åŒ–latentåˆ°N(0,1)ç©ºé—´")
    print("   - è®­ç»ƒå’Œç”Ÿæˆéƒ½åœ¨æ ‡å‡†åŒ–ç©ºé—´è¿›è¡Œ")
    print("   - æ— ç¡¬é™åˆ¶ï¼Œè®©æ¨¡å‹è‡ªç„¶å­¦ä¹ åˆ†å¸ƒ")
    print("   - ç”Ÿæˆååæ ‡å‡†åŒ–å›åŸå§‹VAEç©ºé—´")
    print("ğŸ“ ä½¿ç”¨ train_enhanced_conditional.py è¿›è¡Œè®­ç»ƒ")

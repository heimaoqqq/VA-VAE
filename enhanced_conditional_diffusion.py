#!/usr/bin/env python3
"""
å¢å¼ºæ¡ä»¶æ‰©æ•£ç”Ÿæˆï¼šä¸“é—¨å¤„ç†å°æ•°æ®é›†å¾®å·®å¼‚æ¡ä»¶ç”Ÿæˆ
ç»“åˆPrototypical Networksã€Contrastive Learningã€Few-Shot Meta-Learning
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers import UNet2DConditionModel, DDPMScheduler, DDIMScheduler
import numpy as np
from sklearn.cluster import KMeans
import random

class PrototypicalEncoder(nn.Module):
    """åŸå‹ç¼–ç å™¨ï¼šä»å°‘é‡æ ·æœ¬å­¦ä¹ ç”¨æˆ·åŸå‹"""
    
    def __init__(self, input_dim=8192, prototype_dim=256):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 512),
            nn.ReLU(), 
            nn.Dropout(0.1),
            nn.Linear(512, prototype_dim)
        )
        
    def forward(self, latents):
        """
        Args:
            latents: [N, 32, 16, 16] - ç”¨æˆ·çš„latentæ ·æœ¬
        Returns:
            prototypes: [N, prototype_dim] - åŸå‹ç‰¹å¾
        """
        # å±•å¹³latent
        flat_latents = latents.view(latents.size(0), -1)
        prototypes = self.encoder(flat_latents)
        return F.normalize(prototypes, dim=1)  # L2å½’ä¸€åŒ–

class ContrastiveLearning(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼šå¢å¼ºç”¨æˆ·é—´åˆ¤åˆ«èƒ½åŠ›"""
    
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
        
    def triplet_loss(self, anchor, positive, negative, margin=0.2):
        """ä¸‰å…ƒç»„æŸå¤±"""
        pos_dist = F.pairwise_distance(anchor, positive)
        neg_dist = F.pairwise_distance(anchor, negative)
        loss = F.relu(pos_dist - neg_dist + margin)
        return loss.mean()
    
    def infonce_loss(self, query, keys, positive_mask):
        """InfoNCEæŸå¤±"""
        # è®¡ç®—ç›¸ä¼¼åº¦
        logits = torch.matmul(query, keys.T) / self.temperature
        
        # åˆ›å»ºæ ‡ç­¾
        labels = positive_mask.float()
        
        # äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(logits, labels)
        return loss
    
    def forward(self, prototypes, user_ids):
        """
        è®¡ç®—å¯¹æ¯”æŸå¤±
        Args:
            prototypes: [B, D] - åŸå‹ç‰¹å¾
            user_ids: [B] - ç”¨æˆ·ID
        """
        batch_size = prototypes.size(0)
        loss = 0
        count = 0
        
        for i in range(batch_size):
            # æ‰¾åŒç”¨æˆ·æ ·æœ¬ï¼ˆpositiveï¼‰
            same_user = (user_ids == user_ids[i])
            same_user[i] = False  # æ’é™¤è‡ªå·±
            
            if same_user.any():
                positive_idx = torch.where(same_user)[0]
                positive = prototypes[positive_idx[0]]
                
                # æ‰¾ä¸åŒç”¨æˆ·æ ·æœ¬ï¼ˆnegativeï¼‰
                diff_user = (user_ids != user_ids[i])
                if diff_user.any():
                    negative_idx = torch.where(diff_user)[0]
                    negative = prototypes[negative_idx[0]]
                    
                    # è®¡ç®—ä¸‰å…ƒç»„æŸå¤±
                    anchor = prototypes[i]
                    loss += self.triplet_loss(anchor.unsqueeze(0), 
                                            positive.unsqueeze(0), 
                                            negative.unsqueeze(0))
                    count += 1
        
        return loss / max(count, 1)


class EnhancedConditionalDiffusion(nn.Module):
    """å¢å¼ºæ¡ä»¶æ‰©æ•£æ¨¡å‹"""
    
    def __init__(self, num_users=31, prototype_dim=256):
        super().__init__()
        
        # æ ¸å¿ƒç»„ä»¶
        self.prototype_encoder = PrototypicalEncoder(
            input_dim=8192, prototype_dim=prototype_dim
        )
        self.contrastive_learning = ContrastiveLearning(temperature=0.07)
        
        # U-Netæ‰©æ•£æ¨¡å‹
        self.unet = UNet2DConditionModel(
            sample_size=(16, 16),
            in_channels=32,
            out_channels=32,
            layers_per_block=2,
            block_out_channels=(128, 256, 512, 512),
            down_block_types=[
                "DownBlock2D",
                "DownBlock2D", 
                "AttnDownBlock2D",
                "DownBlock2D"
            ],
            up_block_types=[
                "UpBlock2D",
                "AttnUpBlock2D", 
                "UpBlock2D",
                "UpBlock2D"
            ],
            attention_head_dim=8,
            cross_attention_dim=prototype_dim,
            encoder_hid_dim=prototype_dim
        )
        
        # è°ƒåº¦å™¨
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule="linear"
        )
        
        # DDIMè°ƒåº¦å™¨ç”¨äºç”Ÿæˆ
        self.ddim_scheduler = DDIMScheduler(
            num_train_timesteps=1000,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule="linear",
            clip_sample=False,
            set_alpha_to_one=False,
            steps_offset=1
        )
        
        # ç”¨æˆ·åŸå‹ç¼“å­˜
        self.user_prototypes = {}
        
    def update_user_prototypes(self, user_samples_dict):
        """æ›´æ–°ç”¨æˆ·åŸå‹ç¼“å­˜"""
        self.user_prototypes = {}
        
        for user_id, samples in user_samples_dict.items():
            # samples: [N, 32, 16, 16]
            with torch.no_grad():
                prototypes = self.prototype_encoder(samples)
                # ä½¿ç”¨å‡å€¼ä½œä¸ºç”¨æˆ·åŸå‹
                user_prototype = torch.mean(prototypes, dim=0, keepdim=True)
                self.user_prototypes[user_id] = user_prototype
    
    def get_user_condition(self, user_ids):
        """è·å–ç”¨æˆ·æ¡ä»¶ç¼–ç """
        batch_size = len(user_ids)
        device = next(self.parameters()).device
        
        conditions = torch.zeros(batch_size, 1, 256, device=device)
        
        for i, user_id in enumerate(user_ids):
            if user_id in self.user_prototypes:
                conditions[i] = self.user_prototypes[user_id]
            else:
                # å¦‚æœæ²¡æœ‰ç¼“å­˜åŸå‹ï¼Œä½¿ç”¨é›¶å‘é‡
                conditions[i] = torch.zeros(1, 256, device=device)
        
        return conditions
    
    def training_step(self, clean_latents, user_ids, support_ratio=0.3):
        """
        è®­ç»ƒæ­¥éª¤ï¼šç»“åˆåŸå‹å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ 
        Args:
            clean_latents: [B, 32, 16, 16]
            user_ids: [B]
            support_ratio: ç”¨ä½œsupport setçš„æ¯”ä¾‹
        """
        batch_size = clean_latents.size(0)
        device = clean_latents.device
        
        # åˆ†å‰²supportå’Œquery set
        n_support = int(batch_size * support_ratio)
        indices = torch.randperm(batch_size)
        support_indices = indices[:n_support]
        query_indices = indices[n_support:]
        
        # åŸå‹å­¦ä¹ 
        if len(support_indices) > 0:
            support_latents = clean_latents[support_indices]
            support_user_ids = user_ids[support_indices]
            
            # ç¼–ç åŸå‹
            prototypes = self.prototype_encoder(support_latents)
            
            # å¯¹æ¯”å­¦ä¹ æŸå¤±
            contrastive_loss = self.contrastive_learning(prototypes, support_user_ids)
        else:
            contrastive_loss = torch.tensor(0.0, device=device)
        
        # æ‰©æ•£æŸå¤±ï¼ˆåœ¨query setä¸Šï¼‰
        if len(query_indices) > 0:
            query_latents = clean_latents[query_indices]
            query_user_ids = user_ids[query_indices]
        else:
            query_latents = clean_latents
            query_user_ids = user_ids
        
        # æ‰©æ•£è®­ç»ƒ
        timesteps = torch.randint(
            0, self.noise_scheduler.num_train_timesteps,
            (query_latents.size(0),), device=device
        ).long()
        
        noise = torch.randn_like(query_latents)
        noisy_latents = self.noise_scheduler.add_noise(query_latents, noise, timesteps)
        
        # è·å–ç”¨æˆ·æ¡ä»¶
        user_conditions = self.get_user_condition(query_user_ids.tolist())
        
        # é¢„æµ‹å™ªå£°
        noise_pred = self.unet(
            noisy_latents,
            timesteps,
            encoder_hidden_states=user_conditions
        ).sample
        
        # æ‰©æ•£æŸå¤±
        diffusion_loss = F.mse_loss(noise_pred, noise)
        
        # æ€»æŸå¤±
        total_loss = diffusion_loss + 0.1 * contrastive_loss
        
        return {
            'total_loss': total_loss,
            'diffusion_loss': diffusion_loss,
            'contrastive_loss': contrastive_loss
        }
    
    def generate(self, user_ids, num_samples_per_user=4, num_inference_steps=100, 
                 guidance_scale=2.0, use_ddim=True):
        """ç”Ÿæˆæ ·æœ¬ï¼ˆæ”¯æŒDDIMå’ŒDDPMï¼‰"""
        self.eval()
        
        if isinstance(user_ids, int):
            user_ids = [user_ids]
        
        total_samples = len(user_ids) * num_samples_per_user
        device = next(self.parameters()).device
        
        # æ‰©å±•ç”¨æˆ·ID
        expanded_user_ids = []
        for user_id in user_ids:
            expanded_user_ids.extend([user_id] * num_samples_per_user)
        
        # åˆå§‹å™ªå£°
        latents = torch.randn(total_samples, 32, 16, 16, device=device)
        
        # é€‰æ‹©è°ƒåº¦å™¨
        scheduler = self.ddim_scheduler if use_ddim else self.noise_scheduler
        scheduler.set_timesteps(num_inference_steps, device=device)
        
        # è·å–ç”¨æˆ·æ¡ä»¶
        user_conditions = self.get_user_condition(expanded_user_ids)
        
        # å»å™ªè¿‡ç¨‹
        for t in scheduler.timesteps:
            with torch.no_grad():
                # æ¡ä»¶é¢„æµ‹
                noise_pred_cond = self.unet(
                    latents, t, encoder_hidden_states=user_conditions
                ).sample
                
                # æ— æ¡ä»¶é¢„æµ‹ï¼ˆç”¨äºCFGï¼‰
                if guidance_scale > 1.0:
                    uncond_conditions = torch.zeros_like(user_conditions)
                    noise_pred_uncond = self.unet(
                        latents, t, encoder_hidden_states=uncond_conditions
                    ).sample
                    
                    # Classifier-free guidance
                    noise_pred = noise_pred_uncond + guidance_scale * (
                        noise_pred_cond - noise_pred_uncond
                    )
                else:
                    noise_pred = noise_pred_cond
                
                # å»å™ªæ­¥éª¤
                latents = scheduler.step(noise_pred, t, latents).prev_sample
        
        return latents

# ä½¿ç”¨ç¤ºä¾‹
def create_enhanced_diffusion_system(vae_checkpoint, num_users=31):
    """åˆ›å»ºå¢å¼ºæ¡ä»¶æ‰©æ•£ç³»ç»Ÿ"""
    
    # åŠ è½½ç®€åŒ–VA-VAE
    from simplified_vavae import SimplifiedVAVAE
    vae = SimplifiedVAVAE(vae_checkpoint)
    
    # åˆ›å»ºå¢å¼ºæ‰©æ•£æ¨¡å‹
    enhanced_diffusion = EnhancedConditionalDiffusion(num_users=num_users)
    
    return vae, enhanced_diffusion

if __name__ == "__main__":
    # ä½¿ç”¨ train_enhanced_conditional.py è¿›è¡Œå®Œæ•´è®­ç»ƒ
    print("âœ… å¢å¼ºæ¡ä»¶æ‰©æ•£æ¨¡å‹å®šä¹‰å®Œæˆ")
    print("ğŸ¯ ä¸“é—¨å¤„ç†å°æ•°æ®é›†å¾®å·®å¼‚æ¡ä»¶ç”ŸæˆæŒ‘æˆ˜")
    print("ğŸ“Š é›†æˆ: Prototypical Networks + Contrastive Learning")
    print("ğŸ“ ä½¿ç”¨ train_enhanced_conditional.py è¿›è¡Œè®­ç»ƒ")
